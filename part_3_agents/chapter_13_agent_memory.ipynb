{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Agent Memory Systems\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- Types of memory (short-term vs. long-term)\n",
    "- Conversation memory implementation\n",
    "- Summary memory for long conversations\n",
    "- Entity memory and knowledge graphs\n",
    "- Vector databases for semantic memory\n",
    "- Implementing memory in LangChain\n",
    "- Memory management strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.1: Types of memory (short-term vs. long-term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13.1 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1.1: Memory Classification\n",
    "\n",
    "For each scenario below, identify whether the information would best be stored in short-term memory, long-term memory, or both. Explain your reasoning.\n",
    "\n",
    "1. The user says: \"Let's talk about my upcoming trip to Paris.\"\n",
    "2. The user mentions: \"I'm allergic to peanuts.\"\n",
    "3. The user asks: \"What did I just say about the hotel?\"\n",
    "4. The agent calculates an intermediate result while solving a math problem.\n",
    "5. The user says: \"Remember, I prefer bullet points over long paragraphs.\"\n",
    "6. The user shares: \"Today's weather is really nice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1.2: Design a Memory Schema\n",
    "\n",
    "You're building a personal finance assistant agent. Design a memory schema that specifies:\n",
    "\n",
    "1. **What to store in short-term memory** during a conversation about budgeting\n",
    "2. **What to store in long-term memory** across sessions\n",
    "3. **How information might move** from short-term to long-term storage\n",
    "4. **What should probably NOT be stored** (and why)\n",
    "\n",
    "Write out your schema as a structured outline or diagram. Think about data types, categories, and how you'd organize the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1.3: Memory Retrieval Strategy\n",
    "\n",
    "Consider this scenario: You have a personal assistant agent with long-term memory containing hundreds of stored facts about a user. The user asks: \"What restaurant should I try this weekend?\"\n",
    "\n",
    "Design a retrieval strategy that answers:\n",
    "\n",
    "1. What memory categories might be relevant to this question?\n",
    "2. How would you decide which specific memories to retrieve? (You can't load them all into context)\n",
    "3. How would you handle conflicting or outdated information? (e.g., \"User said they love sushi\" from 2 years ago vs. \"User mentioned trying to eat less fish\" from last month)\n",
    "4. How would you format the retrieved memories for the LLM to use effectively?\n",
    "\n",
    "Write out your strategy as a step-by-step algorithm or flowchart, with explanations for each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.2: Conversation memory implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: no_memory_demo.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.2\n",
    "# File: no_memory_demo.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def chat(user_message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Conversation attempt\n",
    "print(chat(\"Hi! My name is Alex.\"))\n",
    "# \"Hello Alex! Nice to meet you!\"\n",
    "\n",
    "print(chat(\"What's my name?\"))\n",
    "# \"I don't know your name. Could you tell me?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.2\n",
    "# File: simple_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "conversation_history = []\n",
    "\n",
    "def chat(user_message):\n",
    "    # Add the user's message to history\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    })\n",
    "    \n",
    "    # Send entire history to the API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    \n",
    "    # Extract and store the assistant's reply\n",
    "    assistant_message = response.choices[0].message.content\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_message\n",
    "    })\n",
    "    \n",
    "    return assistant_message\n",
    "\n",
    "# Now let's try again\n",
    "print(chat(\"Hi! My name is Alex.\"))\n",
    "# \"Hello Alex! Nice to meet you! How can I help you today?\"\n",
    "\n",
    "print(chat(\"What's my name?\"))\n",
    "# \"Your name is Alex!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: conversation_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.2\n",
    "# File: conversation_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationMemory:\n",
    "    def __init__(self, system_prompt=None, max_messages=50):\n",
    "        self.client = OpenAI()\n",
    "        self.max_messages = max_messages\n",
    "        self.messages = []\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            })\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        self._trim_if_needed()\n",
    "    \n",
    "    def add_assistant_message(self, content):\n",
    "        self.messages.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def _trim_if_needed(self):\n",
    "        \"\"\"Remove oldest messages if we exceed max_messages.\"\"\"\n",
    "        while len(self.messages) > self.max_messages:\n",
    "            # Find first non-system message and remove it\n",
    "            for i, msg in enumerate(self.messages):\n",
    "                if msg[\"role\"] != \"system\":\n",
    "                    self.messages.pop(i)\n",
    "                    break\n",
    "    \n",
    "    def get_messages_for_api(self):\n",
    "        \"\"\"Return messages formatted for API call (without timestamps).\"\"\"\n",
    "        return [\n",
    "            {\"role\": m[\"role\"], \"content\": m[\"content\"]} \n",
    "            for m in self.messages\n",
    "        ]\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        self.add_user_message(user_input)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.get_messages_for_api()\n",
    "        )\n",
    "        \n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        self.add_assistant_message(assistant_reply)\n",
    "        \n",
    "        return assistant_reply\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Return conversation history for inspection.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear conversation but keep system prompt.\"\"\"\n",
    "        system_msgs = [m for m in self.messages if m[\"role\"] == \"system\"]\n",
    "        self.messages = system_msgs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a memory-enabled conversation\n",
    "    memory = ConversationMemory(\n",
    "        system_prompt=\"You are a friendly travel advisor.\",\n",
    "        max_messages=30\n",
    "    )\n",
    "\n",
    "    # Chat naturally\n",
    "    print(memory.chat(\"I'm planning a trip to Japan.\"))\n",
    "    print(memory.chat(\"What's the best time to visit?\"))\n",
    "    print(memory.chat(\"And what about the trip I mentioned?\"))  # It remembers!\n",
    "\n",
    "    # Check history if needed\n",
    "    for msg in memory.get_history():\n",
    "        print(f\"{msg['role']}: {msg['content'][:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: conversation_manager.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.2\n",
    "# File: conversation_manager.py\n",
    "\n",
    "from conversation_memory import ConversationMemory\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self, system_prompt=None):\n",
    "        self.conversations = {}  # user_id -> ConversationMemory\n",
    "        self.system_prompt = system_prompt\n",
    "    \n",
    "    def get_or_create(self, user_id):\n",
    "        \"\"\"Get existing conversation or create new one.\"\"\"\n",
    "        if user_id not in self.conversations:\n",
    "            self.conversations[user_id] = ConversationMemory(\n",
    "                system_prompt=self.system_prompt\n",
    "            )\n",
    "        return self.conversations[user_id]\n",
    "    \n",
    "    def chat(self, user_id, message):\n",
    "        \"\"\"Chat with a specific user's conversation.\"\"\"\n",
    "        memory = self.get_or_create(user_id)\n",
    "        return memory.chat(message)\n",
    "    \n",
    "    def clear_conversation(self, user_id):\n",
    "        \"\"\"Clear a specific user's history.\"\"\"\n",
    "        if user_id in self.conversations:\n",
    "            self.conversations[user_id].clear()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    manager = ConversationManager(system_prompt=\"You are a helpful assistant.\")\n",
    "\n",
    "    # Different users, different conversations\n",
    "    manager.chat(\"user_123\", \"My name is Alice\")\n",
    "    manager.chat(\"user_456\", \"My name is Bob\")\n",
    "\n",
    "    print(manager.chat(\"user_123\", \"What's my name?\"))  # \"Alice\"\n",
    "    print(manager.chat(\"user_456\", \"What's my name?\"))  # \"Bob\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: persistent_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.2\n",
    "# File: persistent_memory.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from conversation_memory import ConversationMemory\n",
    "\n",
    "class PersistentConversationMemory(ConversationMemory):\n",
    "    def __init__(self, user_id, storage_dir=\"conversations\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.user_id = user_id\n",
    "        self.storage_path = Path(storage_dir) / f\"{user_id}.json\"\n",
    "        self.storage_path.parent.mkdir(exist_ok=True)\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        \"\"\"Load conversation from disk if it exists.\"\"\"\n",
    "        if self.storage_path.exists():\n",
    "            with open(self.storage_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.messages = data.get(\"messages\", [])\n",
    "    \n",
    "    def _save(self):\n",
    "        \"\"\"Save conversation to disk.\"\"\"\n",
    "        with open(self.storage_path, 'w') as f:\n",
    "            json.dump({\"messages\": self.messages}, f, indent=2)\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        super().add_user_message(content)\n",
    "        self._save()\n",
    "    \n",
    "    def add_assistant_message(self, content):\n",
    "        super().add_assistant_message(content)\n",
    "        self._save()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # First run - creates new conversation\n",
    "    memory = PersistentConversationMemory(\n",
    "        user_id=\"alice\",\n",
    "        system_prompt=\"You are a helpful assistant.\"\n",
    "    )\n",
    "    \n",
    "    print(memory.chat(\"Hi! I'm planning a vacation.\"))\n",
    "    print(f\"Saved to: {memory.storage_path}\")\n",
    "    \n",
    "    # Second run - would load existing conversation\n",
    "    # memory2 = PersistentConversationMemory(user_id=\"alice\")\n",
    "    # print(memory2.chat(\"What was I planning?\"))  # It remembers!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2.1: Basic Memory Implementation\n",
    "\n",
    "Create a simple conversation memory system that:\n",
    "1. Stores messages in a list\n",
    "2. Includes a system prompt: \"You are a helpful math tutor.\"\n",
    "3. Has a `chat()` function that maintains history\n",
    "4. Prints the total message count after each exchange\n",
    "\n",
    "Test it with this sequence:\n",
    "- \"What is 5 + 3?\"\n",
    "- \"Now multiply that by 2\"\n",
    "- \"What were we calculating?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2.2: Smart Trimming\n",
    "\n",
    "Extend the basic memory system to implement token-aware trimming:\n",
    "1. Count tokens using a simple approximation (words \u00d7 1.3)\n",
    "2. Set a maximum token limit of 500 tokens\n",
    "3. When trimming, keep the system prompt and most recent messages\n",
    "4. Print a warning when trimming occurs\n",
    "\n",
    "Test with a conversation that would exceed 500 tokens to verify trimming works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2.3: Conversation Analytics\n",
    "\n",
    "Build a ConversationMemory class that includes analytics:\n",
    "1. Track message counts by role (user vs assistant)\n",
    "2. Track average message length\n",
    "3. Store timestamps and calculate conversation duration\n",
    "4. Identify the longest message in the conversation\n",
    "5. Provide a `get_stats()` method returning all analytics\n",
    "\n",
    "Create a sample conversation and display the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.3: Summary memory for long conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: summary_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.3\n",
    "# File: summary_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class SummaryMemory:\n",
    "    def __init__(self, system_prompt=None, max_messages=30, keep_recent=10):\n",
    "        self.client = OpenAI()\n",
    "        self.max_messages = max_messages\n",
    "        self.keep_recent = keep_recent\n",
    "        self.messages = []\n",
    "        self.summaries = []  # Track all summaries for debugging\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            })\n",
    "    \n",
    "    def _count_non_system(self):\n",
    "        return len([m for m in self.messages if m[\"role\"] != \"system\"])\n",
    "    \n",
    "    def _should_summarize(self):\n",
    "        return self._count_non_system() > self.max_messages\n",
    "    \n",
    "    def _generate_summary(self, messages_to_summarize):\n",
    "        conversation_text = \"\\n\".join(\n",
    "            f\"{m['role'].upper()}: {m['content']}\" \n",
    "            for m in messages_to_summarize\n",
    "        )\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Summarize this conversation concisely, keeping key facts, \n",
    "decisions, and context needed to continue the discussion:\n",
    "\n",
    "{conversation_text}\n",
    "\n",
    "Provide a clear, structured summary:\"\"\"\n",
    "            }],\n",
    "            max_tokens=400\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _perform_summarization(self):\n",
    "        # Separate system messages from conversation\n",
    "        system_msgs = [m for m in self.messages if m[\"role\"] == \"system\" \n",
    "                       and \"Summary of earlier\" not in m[\"content\"]]\n",
    "        conversation = [m for m in self.messages if m[\"role\"] != \"system\"\n",
    "                        or \"Summary of earlier\" in m[\"content\"]]\n",
    "        \n",
    "        # Select what to summarize vs keep\n",
    "        to_summarize = conversation[:-self.keep_recent]\n",
    "        to_keep = conversation[-self.keep_recent:]\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = self._generate_summary(to_summarize)\n",
    "        self.summaries.append(summary)\n",
    "        \n",
    "        # Rebuild messages\n",
    "        summary_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Summary of earlier conversation:\\n{summary}\"\n",
    "        }\n",
    "        self.messages = system_msgs + [summary_msg] + to_keep\n",
    "        \n",
    "        print(f\"\ud83d\udcdd Summarized {len(to_summarize)} messages into {len(summary.split())} words\")\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        # Add user message\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Check if summarization needed\n",
    "        if self._should_summarize():\n",
    "            self._perform_summarization()\n",
    "        \n",
    "        # Make API call\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "        \n",
    "        return assistant_reply\n",
    "    \n",
    "    def get_message_count(self):\n",
    "        return len(self.messages)\n",
    "    \n",
    "    def get_summaries(self):\n",
    "        return self.summaries\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    assistant = SummaryMemory(\n",
    "        system_prompt=\"\"\"You are a project planning assistant. Help users plan \n",
    "and track their projects. Remember details about their projects, team members, \n",
    "deadlines, and constraints.\"\"\",\n",
    "        max_messages=20,\n",
    "        keep_recent=8\n",
    "    )\n",
    "    \n",
    "    # Test conversation\n",
    "    test_messages = [\n",
    "        \"I'm starting a new mobile app project.\",\n",
    "        \"The deadline is March 15th.\",\n",
    "        \"My team has 2 developers and 1 designer.\",\n",
    "        \"What should we focus on first?\",\n",
    "    ]\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        print(f\"User: {msg}\")\n",
    "        response = assistant.chat(msg)\n",
    "        print(f\"Assistant: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: hybrid_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.3\n",
    "# File: hybrid_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class HybridMemory:\n",
    "    \"\"\"\n",
    "    Combines multiple memory strategies:\n",
    "    - Full history for persistence/auditing\n",
    "    - Active context for LLM (with summarization)\n",
    "    - Extracted key facts for quick reference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=None, max_active_messages=30):\n",
    "        self.client = OpenAI()\n",
    "        self.full_history = []          # Complete record (for persistence)\n",
    "        self.active_context = []        # What we send to LLM\n",
    "        self.summaries = []             # Generated summaries\n",
    "        self.key_facts = {}             # Extracted important facts\n",
    "        self.max_active = max_active_messages\n",
    "        \n",
    "        if system_prompt:\n",
    "            msg = {\"role\": \"system\", \"content\": system_prompt}\n",
    "            self.full_history.append(msg)\n",
    "            self.active_context.append(msg)\n",
    "    \n",
    "    def _extract_facts(self, content):\n",
    "        \"\"\"Extract key facts from content (placeholder for entity extraction).\"\"\"\n",
    "        # In a full implementation, this would use NLP or LLM to extract\n",
    "        # entities, preferences, and key information\n",
    "        # See section 13.4 for full entity extraction\n",
    "        pass\n",
    "    \n",
    "    def _should_summarize(self):\n",
    "        non_system = [m for m in self.active_context if m[\"role\"] != \"system\"]\n",
    "        return len(non_system) > self.max_active\n",
    "    \n",
    "    def _summarize(self):\n",
    "        \"\"\"Summarize older messages in active context.\"\"\"\n",
    "        system_msgs = [m for m in self.active_context \n",
    "                       if m[\"role\"] == \"system\" and \"Summary\" not in m[\"content\"]]\n",
    "        conversation = [m for m in self.active_context if m[\"role\"] != \"system\"]\n",
    "        \n",
    "        # Keep recent messages\n",
    "        keep_recent = 10\n",
    "        to_summarize = conversation[:-keep_recent]\n",
    "        to_keep = conversation[-keep_recent:]\n",
    "        \n",
    "        if not to_summarize:\n",
    "            return\n",
    "        \n",
    "        # Generate summary\n",
    "        text = \"\\n\".join(f\"{m['role']}: {m['content']}\" for m in to_summarize)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize concisely:\\n\\n{text}\"\n",
    "            }],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "        self.summaries.append(summary)\n",
    "        \n",
    "        # Rebuild active context\n",
    "        summary_msg = {\"role\": \"system\", \"content\": f\"Earlier conversation summary:\\n{summary}\"}\n",
    "        self.active_context = system_msgs + [summary_msg] + to_keep\n",
    "    \n",
    "    def _trim_if_needed(self):\n",
    "        \"\"\"Final safety trim if still too long after summarization.\"\"\"\n",
    "        max_total = self.max_active + 15  # Buffer for summaries\n",
    "        while len(self.active_context) > max_total:\n",
    "            # Remove oldest non-system message\n",
    "            for i, m in enumerate(self.active_context):\n",
    "                if m[\"role\"] != \"system\":\n",
    "                    self.active_context.pop(i)\n",
    "                    break\n",
    "    \n",
    "    def process_message(self, role, content):\n",
    "        \"\"\"Process and store a message.\"\"\"\n",
    "        msg = {\"role\": role, \"content\": content}\n",
    "        \n",
    "        # Always store full history\n",
    "        self.full_history.append(msg)\n",
    "        self.active_context.append(msg)\n",
    "        \n",
    "        # Extract key facts\n",
    "        self._extract_facts(content)\n",
    "        \n",
    "        # Summarize if needed\n",
    "        if self._should_summarize():\n",
    "            self._summarize()\n",
    "        \n",
    "        # Token-trim if still too long\n",
    "        self._trim_if_needed()\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Chat with the assistant.\"\"\"\n",
    "        self.process_message(\"user\", user_input)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.active_context\n",
    "        )\n",
    "        \n",
    "        reply = response.choices[0].message.content\n",
    "        self.process_message(\"assistant\", reply)\n",
    "        \n",
    "        return reply\n",
    "    \n",
    "    def get_full_history(self):\n",
    "        \"\"\"Return complete conversation history.\"\"\"\n",
    "        return self.full_history\n",
    "    \n",
    "    def get_active_context_size(self):\n",
    "        \"\"\"Return current active context size.\"\"\"\n",
    "        return len(self.active_context)\n",
    "    \n",
    "    def get_key_facts(self):\n",
    "        \"\"\"Return extracted key facts.\"\"\"\n",
    "        return self.key_facts\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = HybridMemory(\n",
    "        system_prompt=\"You are a helpful assistant.\",\n",
    "        max_active_messages=15\n",
    "    )\n",
    "    \n",
    "    print(memory.chat(\"My name is Alice and I love hiking.\"))\n",
    "    print(f\"Active context: {memory.get_active_context_size()} messages\")\n",
    "    print(f\"Full history: {len(memory.get_full_history())} messages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.3.1: Basic Summarization\n",
    "\n",
    "Write a function `summarize_messages(messages)` that:\n",
    "1. Takes a list of message dictionaries (role/content)\n",
    "2. Formats them into readable text\n",
    "3. Uses an LLM to generate a summary\n",
    "4. Returns the summary string\n",
    "\n",
    "Test it with a sample 5-message conversation about planning a vacation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.3.2: Triggered Summarization\n",
    "\n",
    "Build a `SmartMemory` class that:\n",
    "1. Tracks messages normally\n",
    "2. Automatically triggers summarization when message count exceeds 15\n",
    "3. Keeps the 5 most recent messages intact\n",
    "4. Stores the summary as a system message\n",
    "5. Prints a notification when summarization occurs\n",
    "\n",
    "Test with a conversation that grows past the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.3.3: Domain-Specific Summaries\n",
    "\n",
    "Create a summarization system for a **medical consultation assistant** that:\n",
    "1. Uses a specialized summary prompt that extracts:\n",
    "   - Symptoms mentioned\n",
    "   - Duration of symptoms\n",
    "   - Medications discussed\n",
    "   - Recommendations given\n",
    "   - Follow-up items\n",
    "2. Structures the summary in a specific format (not free-form prose)\n",
    "3. Validates that key medical information isn't lost\n",
    "4. Includes a `get_medical_summary()` method returning structured data\n",
    "\n",
    "Test with a mock medical consultation conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.4: Entity memory and knowledge graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: entity_extraction.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.4\n",
    "# File: entity_extraction.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_entities(message, client):\n",
    "    \"\"\"Extract entities (people, orgs, projects, locations) from a message.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Extract entities from this message. Return JSON:\n",
    "{{\n",
    "    \"entities\": [\n",
    "        {{\"name\": \"...\", \"type\": \"person|org|project|location|concept\", \"info\": \"...\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Message: {message}\n",
    "\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "        }],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content)[\"entities\"]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    client = OpenAI()\n",
    "    \n",
    "    test_message = \"Tell Sarah Chen to schedule a meeting with the Globex team about the Q4 launch.\"\n",
    "    entities = extract_entities(test_message, client)\n",
    "    \n",
    "    print(\"Extracted entities:\")\n",
    "    for e in entities:\n",
    "        print(f\"  - {e['name']} ({e['type']}): {e.get('info', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: entity_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.4\n",
    "# File: entity_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class EntityMemory:\n",
    "    \"\"\"Store and retrieve entity information from conversations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entities = {}  # name -> entity data\n",
    "        self.client = OpenAI()\n",
    "    \n",
    "    def update_entity(self, name, entity_type, new_info):\n",
    "        \"\"\"Add or update an entity with new information.\"\"\"\n",
    "        name_key = name.lower().strip()\n",
    "        \n",
    "        if name_key not in self.entities:\n",
    "            self.entities[name_key] = {\n",
    "                \"name\": name,\n",
    "                \"type\": entity_type,\n",
    "                \"facts\": [],\n",
    "                \"first_seen\": datetime.now().isoformat(),\n",
    "                \"mention_count\": 0\n",
    "            }\n",
    "        \n",
    "        entity = self.entities[name_key]\n",
    "        entity[\"mention_count\"] += 1\n",
    "        entity[\"last_seen\"] = datetime.now().isoformat()\n",
    "        \n",
    "        # Add new fact if not duplicate\n",
    "        if new_info and new_info not in entity[\"facts\"]:\n",
    "            entity[\"facts\"].append(new_info)\n",
    "    \n",
    "    def get_entity(self, name):\n",
    "        \"\"\"Retrieve entity by name.\"\"\"\n",
    "        return self.entities.get(name.lower().strip())\n",
    "    \n",
    "    def get_relevant_entities(self, message):\n",
    "        \"\"\"Find entities mentioned in a message.\"\"\"\n",
    "        relevant = []\n",
    "        message_lower = message.lower()\n",
    "        \n",
    "        for key, entity in self.entities.items():\n",
    "            if key in message_lower or entity[\"name\"].lower() in message_lower:\n",
    "                relevant.append(entity)\n",
    "        \n",
    "        return relevant\n",
    "    \n",
    "    def format_for_context(self, entities):\n",
    "        \"\"\"Format entities for injection into LLM context.\"\"\"\n",
    "        if not entities:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = [\"Relevant information about mentioned entities:\"]\n",
    "        for e in entities:\n",
    "            facts = \"; \".join(e[\"facts\"][-5:])  # Last 5 facts\n",
    "            lines.append(f\"- {e['name']} ({e['type']}): {facts}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def get_all_entities(self):\n",
    "        \"\"\"Return all stored entities.\"\"\"\n",
    "        return list(self.entities.values())\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = EntityMemory()\n",
    "    \n",
    "    # Add some entities\n",
    "    memory.update_entity(\"Sarah Chen\", \"person\", \"Account Manager at Acme\")\n",
    "    memory.update_entity(\"Sarah Chen\", \"person\", \"prefers Slack over email\")\n",
    "    memory.update_entity(\"Globex\", \"org\", \"major client\")\n",
    "    memory.update_entity(\"Q4 Launch\", \"project\", \"deadline is December 15\")\n",
    "    \n",
    "    # Retrieve\n",
    "    print(\"Sarah's info:\", memory.get_entity(\"sarah chen\"))\n",
    "    \n",
    "    # Find relevant entities in a message\n",
    "    relevant = memory.get_relevant_entities(\"What's the status of Q4 Launch?\")\n",
    "    print(\"\\nRelevant entities:\")\n",
    "    print(memory.format_for_context(relevant))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: entity_aware_agent.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.4\n",
    "# File: entity_aware_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from entity_memory import EntityMemory\n",
    "from entity_extraction import extract_entities\n",
    "\n",
    "\n",
    "class EntityAwareAgent:\n",
    "    \"\"\"An agent that automatically learns and recalls entity information.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt):\n",
    "        self.client = OpenAI()\n",
    "        self.entity_memory = EntityMemory()\n",
    "        self.conversation = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def process_message(self, user_message):\n",
    "        # 1. RETRIEVE: Find relevant entities\n",
    "        relevant = self.entity_memory.get_relevant_entities(user_message)\n",
    "        entity_context = self.entity_memory.format_for_context(relevant)\n",
    "        \n",
    "        # 2. BUILD CONTEXT: Inject entity knowledge\n",
    "        messages = self.conversation.copy()\n",
    "        if entity_context:\n",
    "            messages.insert(1, {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": entity_context\n",
    "            })\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # 3. REASON & ACT: Get agent response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        \n",
    "        # 4. STORE: Extract and save new entities\n",
    "        self._extract_and_store(user_message)\n",
    "        self._extract_and_store(reply)\n",
    "        \n",
    "        # Update conversation\n",
    "        self.conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        \n",
    "        return reply\n",
    "    \n",
    "    def _extract_and_store(self, text):\n",
    "        \"\"\"Extract entities from text and update memory.\"\"\"\n",
    "        entities = extract_entities(text, self.client)\n",
    "        for e in entities:\n",
    "            self.entity_memory.update_entity(\n",
    "                e[\"name\"], e[\"type\"], e.get(\"info\", \"\")\n",
    "            )\n",
    "    \n",
    "    def get_known_entities(self):\n",
    "        \"\"\"Return all entities the agent knows about.\"\"\"\n",
    "        return self.entity_memory.get_all_entities()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = EntityAwareAgent(\n",
    "        system_prompt=\"You are a helpful project management assistant.\"\n",
    "    )\n",
    "    \n",
    "    # Have a conversation\n",
    "    print(\"User: I'm working with Sarah Chen on the Q4 Launch project.\")\n",
    "    response = agent.process_message(\"I'm working with Sarah Chen on the Q4 Launch project.\")\n",
    "    print(f\"Agent: {response}\\n\")\n",
    "    \n",
    "    print(\"User: Sarah said the deadline is December 15th.\")\n",
    "    response = agent.process_message(\"Sarah said the deadline is December 15th.\")\n",
    "    print(f\"Agent: {response}\\n\")\n",
    "    \n",
    "    # Show what the agent learned\n",
    "    print(\"Known entities:\")\n",
    "    for entity in agent.get_known_entities():\n",
    "        print(f\"  - {entity['name']} ({entity['type']}): {entity['facts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: knowledge_graph.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.4\n",
    "# File: knowledge_graph.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    \"\"\"Store entities and their relationships.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entities = {}      # name -> attributes\n",
    "        self.relationships = [] # (entity1, relation, entity2)\n",
    "    \n",
    "    def add_entity(self, name, entity_type, attributes=None):\n",
    "        \"\"\"Add or update an entity.\"\"\"\n",
    "        key = name.lower()\n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"name\": name,\n",
    "                \"type\": entity_type,\n",
    "                \"attributes\": attributes or {}\n",
    "            }\n",
    "        elif attributes:\n",
    "            self.entities[key][\"attributes\"].update(attributes)\n",
    "    \n",
    "    def add_relationship(self, entity1, relation, entity2):\n",
    "        \"\"\"Add a relationship between entities.\"\"\"\n",
    "        self.relationships.append({\n",
    "            \"from\": entity1.lower(),\n",
    "            \"relation\": relation,\n",
    "            \"to\": entity2.lower(),\n",
    "            \"added\": datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_connections(self, entity_name):\n",
    "        \"\"\"Get all relationships involving an entity.\"\"\"\n",
    "        name = entity_name.lower()\n",
    "        connections = []\n",
    "        \n",
    "        for rel in self.relationships:\n",
    "            if rel[\"from\"] == name:\n",
    "                connections.append(f\"{rel['relation']} {rel['to']}\")\n",
    "            elif rel[\"to\"] == name:\n",
    "                connections.append(f\"{rel['from']} {rel['relation']} this\")\n",
    "        \n",
    "        return connections\n",
    "    \n",
    "    def extract_relationships(self, message, client):\n",
    "        \"\"\"Use LLM to extract relationships from text.\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Extract relationships from this message.\n",
    "Return JSON: {{\"relationships\": [{{\"from\": \"...\", \"relation\": \"...\", \"to\": \"...\"}}]}}\n",
    "\n",
    "Examples of relations: works_on, manages, reports_to, is_part_of, depends_on, located_in\n",
    "\n",
    "Message: {message}\n",
    "JSON:\"\"\"\n",
    "            }],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(response.choices[0].message.content)\n",
    "            for rel in data.get(\"relationships\", []):\n",
    "                self.add_relationship(rel[\"from\"], rel[\"relation\"], rel[\"to\"])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def format_graph(self):\n",
    "        \"\"\"Return a formatted string representation of the graph.\"\"\"\n",
    "        lines = [\"Entities:\"]\n",
    "        for key, entity in self.entities.items():\n",
    "            lines.append(f\"  - {entity['name']} ({entity['type']})\")\n",
    "        \n",
    "        lines.append(\"\\nRelationships:\")\n",
    "        for rel in self.relationships:\n",
    "            lines.append(f\"  - {rel['from']} --{rel['relation']}--> {rel['to']}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    client = OpenAI()\n",
    "    graph = KnowledgeGraph()\n",
    "    \n",
    "    # Add entities\n",
    "    graph.add_entity(\"Sarah Chen\", \"person\", {\"role\": \"Engineer\"})\n",
    "    graph.add_entity(\"Q4 Launch\", \"project\")\n",
    "    graph.add_entity(\"Acme Corp\", \"org\")\n",
    "    \n",
    "    # Add relationships manually\n",
    "    graph.add_relationship(\"Sarah Chen\", \"works_on\", \"Q4 Launch\")\n",
    "    graph.add_relationship(\"Q4 Launch\", \"is_for\", \"Acme Corp\")\n",
    "    \n",
    "    # Extract relationships from text\n",
    "    graph.extract_relationships(\n",
    "        \"The marketing team reports to Jennifer, who manages the NYC office.\",\n",
    "        client\n",
    "    )\n",
    "    \n",
    "    print(graph.format_graph())\n",
    "    print(\"\\nConnections for Sarah Chen:\", graph.get_connections(\"Sarah Chen\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: context_aware_agent.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.4\n",
    "# File: context_aware_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from entity_memory import EntityMemory\n",
    "from knowledge_graph import KnowledgeGraph\n",
    "from entity_extraction import extract_entities\n",
    "\n",
    "\n",
    "class ContextAwareAgent:\n",
    "    \"\"\"\n",
    "    Agent pattern combining conversation history, entity memory, and knowledge graph.\n",
    "    This forms the backbone of production AI assistants.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, system_prompt):\n",
    "        self.name = name\n",
    "        self.client = OpenAI()\n",
    "        self.system_prompt = system_prompt\n",
    "        \n",
    "        # Memory systems\n",
    "        self.conversation = []\n",
    "        self.entities = EntityMemory()\n",
    "        self.graph = KnowledgeGraph()\n",
    "    \n",
    "    def build_context(self, user_message):\n",
    "        \"\"\"Assemble full context for the agent.\"\"\"\n",
    "        # Start with system prompt\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        # Add entity context\n",
    "        relevant = self.entities.get_relevant_entities(user_message)\n",
    "        if relevant:\n",
    "            entity_info = self.entities.format_for_context(relevant)\n",
    "            messages.append({\"role\": \"system\", \"content\": entity_info})\n",
    "        \n",
    "        # Add conversation history (last N messages)\n",
    "        messages.extend(self.conversation[-20:])\n",
    "        \n",
    "        # Add current message\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def run(self, user_message):\n",
    "        \"\"\"Main agent loop.\"\"\"\n",
    "        # Build context with entity knowledge\n",
    "        messages = self.build_context(user_message)\n",
    "        \n",
    "        # Get response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        \n",
    "        # Learn from this exchange\n",
    "        self._learn(user_message)\n",
    "        self._learn(reply)\n",
    "        \n",
    "        # Update conversation\n",
    "        self.conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        \n",
    "        return reply\n",
    "    \n",
    "    def _learn(self, text):\n",
    "        \"\"\"Extract entities and relationships from text.\"\"\"\n",
    "        # Extract entities\n",
    "        entities = extract_entities(text, self.client)\n",
    "        for e in entities:\n",
    "            self.entities.update_entity(e[\"name\"], e[\"type\"], e.get(\"info\"))\n",
    "        \n",
    "        # Extract relationships\n",
    "        self.graph.extract_relationships(text, self.client)\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        \"\"\"Return statistics about what the agent has learned.\"\"\"\n",
    "        return {\n",
    "            \"conversation_messages\": len(self.conversation),\n",
    "            \"known_entities\": len(self.entities.entities),\n",
    "            \"relationships\": len(self.graph.relationships)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ContextAwareAgent(\n",
    "        name=\"ProjectBot\",\n",
    "        system_prompt=\"You are a helpful project management assistant. Track people, projects, and deadlines.\"\n",
    "    )\n",
    "    \n",
    "    # Simulate a conversation\n",
    "    messages = [\n",
    "        \"I'm starting a new project called Phoenix with my team lead Maria.\",\n",
    "        \"Maria said the deadline is end of Q1. She's also working with Tom on design.\",\n",
    "        \"What do you know about the Phoenix project so far?\",\n",
    "        \"Who is working on Phoenix?\"\n",
    "    ]\n",
    "    \n",
    "    for msg in messages:\n",
    "        print(f\"User: {msg}\")\n",
    "        response = agent.run(msg)\n",
    "        print(f\"Agent: {response}\\n\")\n",
    "    \n",
    "    # Show what was learned\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Memory Stats:\", agent.get_memory_stats())\n",
    "    print(\"\\nKnown Entities:\")\n",
    "    for entity in agent.entities.get_all_entities():\n",
    "        print(f\"  - {entity['name']} ({entity['type']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.4.1: Basic Entity Extraction\n",
    "\n",
    "Write a function that extracts entities from a message and prints them. Test with: \"John from marketing wants to discuss the Phoenix project with the Tokyo team next Tuesday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.4.2: Entity Memory Class\n",
    "\n",
    "Create an `EntityMemory` class that:\n",
    "1. Stores entities with names, types, and facts\n",
    "2. Updates existing entities with new information\n",
    "3. Retrieves entities by name\n",
    "4. Formats relevant entities for LLM context\n",
    "\n",
    "Test by processing 3-4 messages that mention overlapping entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.4.3: Entity-Aware Agent\n",
    "\n",
    "Build a simple agent that:\n",
    "1. Maintains conversation history\n",
    "2. Extracts and stores entities from each exchange\n",
    "3. Retrieves relevant entity context before responding\n",
    "4. Shows what entities it knows when asked \"What do you know about [name]?\"\n",
    "\n",
    "Keep the implementation focused\u2014under 80 lines total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.5: Vector databases for semantic memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: get_embeddings.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.5\n",
    "# File: get_embeddings.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Convert text to a vector embedding.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get embeddings for test sentences\n",
    "    vec1 = get_embedding(\"I love programming\")\n",
    "    vec2 = get_embedding(\"Coding is my passion\")\n",
    "    vec3 = get_embedding(\"The weather is nice today\")\n",
    "    \n",
    "    print(f\"Vector length: {len(vec1)}\")  # 1536 dimensions\n",
    "    \n",
    "    # Calculate similarities\n",
    "    print(f\"'programming' vs 'coding': {cosine_similarity(vec1, vec2):.3f}\")  # ~0.85\n",
    "    print(f\"'programming' vs 'weather': {cosine_similarity(vec1, vec3):.3f}\")  # ~0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_chromadb.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.5\n",
    "# File: test_chromadb.py\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# Create a client (in-memory by default)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a collection (like a table)\n",
    "collection = client.create_collection(\"test\")\n",
    "\n",
    "# Add some data\n",
    "collection.add(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    documents=[\"Hello world\", \"Goodbye world\"]\n",
    ")\n",
    "\n",
    "print(f\"Collection has {collection.count()} items\")\n",
    "# Output: Collection has 2 items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: semantic_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.5\n",
    "# File: semantic_memory.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class SemanticMemory:\n",
    "    \"\"\"Vector-based memory for semantic search.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name=\"memories\"):\n",
    "        self.openai = OpenAI()\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.collection = self.chroma.create_collection(name=collection_name)\n",
    "        self.memory_count = 0\n",
    "    \n",
    "    def _get_embedding(self, text):\n",
    "        \"\"\"Get embedding vector for text.\"\"\"\n",
    "        response = self.openai.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def add(self, content, metadata=None):\n",
    "        \"\"\"Store a memory with its embedding.\"\"\"\n",
    "        self.memory_count += 1\n",
    "        memory_id = f\"mem_{self.memory_count}\"\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=[memory_id],\n",
    "            embeddings=[self._get_embedding(content)],\n",
    "            documents=[content],\n",
    "            metadatas=[metadata or {}]\n",
    "        )\n",
    "        return memory_id\n",
    "    \n",
    "    def search(self, query, n_results=5):\n",
    "        \"\"\"Find memories similar to the query.\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[self._get_embedding(query)],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        memories = []\n",
    "        for i, doc in enumerate(results[\"documents\"][0]):\n",
    "            memories.append({\n",
    "                \"content\": doc,\n",
    "                \"metadata\": results[\"metadatas\"][0][i],\n",
    "                \"id\": results[\"ids\"][0][i]\n",
    "            })\n",
    "        return memories\n",
    "    \n",
    "    def count(self):\n",
    "        \"\"\"Return total number of memories.\"\"\"\n",
    "        return self.collection.count()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = SemanticMemory()\n",
    "    \n",
    "    # Add various memories\n",
    "    memory.add(\"Sarah is leading the backend team\", {\"type\": \"person\"})\n",
    "    memory.add(\"The project deadline is March 15th\", {\"type\": \"project\"})\n",
    "    memory.add(\"Team velocity has improved by 20%\", {\"type\": \"metrics\"})\n",
    "    \n",
    "    print(f\"Total memories: {memory.count()}\")\n",
    "    \n",
    "    # Search by meaning\n",
    "    results = memory.search(\"How is the team performing?\")\n",
    "    print(\"\\nSearch results for 'How is the team performing?':\")\n",
    "    for r in results:\n",
    "        print(f\"  - {r['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: semantic_agent.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.5\n",
    "# File: semantic_agent.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SemanticMemoryAgent:\n",
    "    \"\"\"Agent that uses semantic memory for context retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt):\n",
    "        self.openai = OpenAI()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.conversation = []\n",
    "        \n",
    "        # Set up semantic memory\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.memories = self.chroma.create_collection(\"agent_memories\")\n",
    "        self.memory_id = 0\n",
    "    \n",
    "    def _embed(self, text):\n",
    "        \"\"\"Get embedding for text.\"\"\"\n",
    "        response = self.openai.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def _store_memory(self, content, memory_type=\"conversation\"):\n",
    "        \"\"\"Store a piece of information in semantic memory.\"\"\"\n",
    "        self.memory_id += 1\n",
    "        self.memories.add(\n",
    "            ids=[f\"m{self.memory_id}\"],\n",
    "            embeddings=[self._embed(content)],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"type\": memory_type, \"timestamp\": datetime.now().isoformat()}]\n",
    "        )\n",
    "    \n",
    "    def _retrieve_relevant(self, query, n=3):\n",
    "        \"\"\"Retrieve memories relevant to the query.\"\"\"\n",
    "        if self.memories.count() == 0:\n",
    "            return []\n",
    "        \n",
    "        results = self.memories.query(\n",
    "            query_embeddings=[self._embed(query)],\n",
    "            n_results=min(n, self.memories.count())\n",
    "        )\n",
    "        return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        \"\"\"Process user input and return response.\"\"\"\n",
    "        # 1. Retrieve relevant memories\n",
    "        relevant = self._retrieve_relevant(user_input)\n",
    "        \n",
    "        # 2. Build context\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "        if relevant:\n",
    "            memory_context = \"Relevant information from memory:\\n\" + \"\\n\".join(f\"- {m}\" for m in relevant)\n",
    "            messages.append({\"role\": \"system\", \"content\": memory_context})\n",
    "        \n",
    "        messages.extend(self.conversation[-10:])  # Recent conversation\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # 3. Generate response\n",
    "        response = self.openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        \n",
    "        # 4. Update conversation history\n",
    "        self.conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        \n",
    "        # 5. Store this exchange in semantic memory\n",
    "        self._store_memory(f\"User said: {user_input}\")\n",
    "        self._store_memory(f\"Assistant replied about: {user_input[:50]}\")\n",
    "        \n",
    "        return reply\n",
    "    \n",
    "    def remember(self, fact):\n",
    "        \"\"\"Explicitly store a fact in memory.\"\"\"\n",
    "        self._store_memory(fact, memory_type=\"explicit\")\n",
    "        return f\"I'll remember: {fact}\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = SemanticMemoryAgent(\n",
    "        system_prompt=\"You are a helpful project management assistant.\"\n",
    "    )\n",
    "    \n",
    "    # Have a conversation\n",
    "    print(\"Agent: \", agent.chat(\"Our project is called Phoenix and it's due March 15th\"))\n",
    "    print(\"Agent: \", agent.chat(\"Sarah is the tech lead and Tom handles backend\"))\n",
    "    print(\"Agent: \", agent.chat(\"We're worried about the authentication module\"))\n",
    "    \n",
    "    # Later, ask something related\n",
    "    print(\"\\nAgent: \", agent.chat(\"What are the main concerns with our project?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: document_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.5\n",
    "# File: document_memory.py\n",
    "\n",
    "from semantic_memory import SemanticMemory\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "class DocumentMemory(SemanticMemory):\n",
    "    \"\"\"Semantic memory that handles long documents by chunking.\"\"\"\n",
    "    \n",
    "    def add_document(self, content, source=None):\n",
    "        \"\"\"Add a long document by chunking it.\"\"\"\n",
    "        chunks = chunk_text(content)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.add(\n",
    "                content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": source or \"unknown\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return len(chunks)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    doc_memory = DocumentMemory()\n",
    "    \n",
    "    meeting_notes = \"\"\"\n",
    "    Project Phoenix kickoff meeting - January 10th.\n",
    "    Attendees: Sarah (tech lead), Tom (backend), Alex (design), Mike (PM).\n",
    "    \n",
    "    We discussed the project timeline. The deadline is March 15th, giving us \n",
    "    roughly 10 weeks. Sarah raised concerns about the authentication module\n",
    "    complexity. Tom suggested using Auth0 to save time.\n",
    "    \n",
    "    Budget was confirmed at $50,000. This needs to cover all development \n",
    "    and third-party services. Alex will present design mockups next week.\n",
    "    \n",
    "    Action items:\n",
    "    - Sarah: Research Auth0 integration (due Jan 15)\n",
    "    - Tom: Set up development environment (due Jan 12)\n",
    "    - Alex: Complete wireframes (due Jan 17)\n",
    "    - Mike: Create detailed project timeline (due Jan 14)\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks_added = doc_memory.add_document(meeting_notes, source=\"kickoff_meeting\")\n",
    "    print(f\"Added {chunks_added} chunks\")\n",
    "    \n",
    "    # Now search across the document\n",
    "    results = doc_memory.search(\"What are the concerns about the project?\")\n",
    "    print(\"\\nSearch results:\")\n",
    "    for r in results[:3]:\n",
    "        print(f\"  - {r['content'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.5.1: Basic Semantic Search\n",
    "\n",
    "Create a `SemanticMemory` class that:\n",
    "1. Stores text with embeddings using ChromaDB\n",
    "2. Has `add(text)` and `search(query)` methods\n",
    "3. Returns the top 3 most similar results\n",
    "\n",
    "Test by adding 5 facts about different topics and searching for related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.5.2: Memory with Categories\n",
    "\n",
    "Extend your semantic memory to:\n",
    "1. Store memories with a \"category\" metadata field\n",
    "2. Add a `search_category(query, category)` method that filters by category\n",
    "3. Track how many memories exist per category\n",
    "\n",
    "Test with memories in categories like \"work\", \"personal\", \"ideas\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.5.3: Conversational Agent with Semantic Recall\n",
    "\n",
    "Build an agent that:\n",
    "1. Maintains conversation history\n",
    "2. Stores each exchange in semantic memory\n",
    "3. Retrieves relevant past conversations when responding\n",
    "4. Has a `remember(fact)` method for explicit memory storage\n",
    "5. Shows retrieved memories in debug output\n",
    "\n",
    "Test with a multi-turn conversation where later questions relate to earlier topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.6: Implementing memory in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_langchain_setup.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: test_langchain_setup.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.invoke([HumanMessage(content=\"Hello!\")])\n",
    "print(response.content)\n",
    "print(\"LangChain is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: basic_memory_chain.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: basic_memory_chain.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Store for session histories (in production, use a database)\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Retrieve or create history for a session.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Create the prompt with a placeholder for history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrap with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Use it with a session ID\n",
    "config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi! My name is Alice.\"},\n",
    "    config=config\n",
    ")\n",
    "print(response1.content)\n",
    "\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config=config\n",
    ")\n",
    "print(response2.content)  # It remembers Alice!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: trimmed_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: trimmed_memory.py\n",
    "#\n",
    "# Demonstrates using trim_messages to limit conversation history size.\n",
    "# This prevents context window overflow in long conversations.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Create a message trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=200,  # Keep roughly this many tokens\n",
    "    strategy=\"last\",  # Keep the most recent messages\n",
    "    token_counter=ChatOpenAI(model=\"gpt-3.5-turbo\"),  # Use LLM to count\n",
    "    include_system=True,  # Always keep system message\n",
    "    start_on=\"human\",  # Start sequence on human message\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Remember details users share with you.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# Use RunnablePassthrough.assign to trim history before passing to prompt\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=lambda x: trimmer.invoke(x.get(\"history\", []))\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "\n",
    "def chat(message: str, session_id: str = \"user_456\") -> str:\n",
    "    \"\"\"Send a message and get a response.\"\"\"\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    response = chain_with_history.invoke({\"input\": message}, config=config)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Long conversation - older messages will be trimmed\n",
    "if __name__ == \"__main__\":\n",
    "    messages = [\n",
    "        \"My name is Bob.\",\n",
    "        \"I live in Seattle.\",\n",
    "        \"I work as a data scientist.\",\n",
    "        \"My favorite language is Python.\",\n",
    "        \"I have a dog named Max.\",\n",
    "        \"I enjoy hiking on weekends.\",\n",
    "        \"What do you remember about me?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Trimmed Memory Demo (200 token limit)\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    for msg in messages:\n",
    "        print(f\"Human: {msg}\")\n",
    "        response = chat(msg)\n",
    "        print(f\"AI: {response}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"With only ~200 tokens, early messages get trimmed.\")\n",
    "    print(\"The bot may not remember name/city from the start.\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: summary_memory_modern.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: summary_memory_modern.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "class SummaryMemory:\n",
    "    \"\"\"Custom summary memory implementation for modern LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_messages=10):\n",
    "        self.llm = llm\n",
    "        self.max_messages = max_messages\n",
    "        self.messages = []\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def add_exchange(self, human_msg: str, ai_msg: str):\n",
    "        \"\"\"Add a human-AI exchange to memory.\"\"\"\n",
    "        self.messages.append(HumanMessage(content=human_msg))\n",
    "        self.messages.append(AIMessage(content=ai_msg))\n",
    "        \n",
    "        if len(self.messages) > self.max_messages:\n",
    "            self._summarize()\n",
    "    \n",
    "    def _summarize(self):\n",
    "        \"\"\"Summarize older messages to compress memory.\"\"\"\n",
    "        to_summarize = self.messages[:-4]\n",
    "        to_keep = self.messages[-4:]\n",
    "        \n",
    "        summary_prompt = f\"\"\"Summarize concisely, preserving key facts:\n",
    "Previous: {self.summary}\n",
    "New: {self._format(to_summarize)}\n",
    "Updated summary:\"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "        self.summary = response.content\n",
    "        self.messages = to_keep\n",
    "    \n",
    "    def _format(self, messages):\n",
    "        \"\"\"Format messages for summary prompt.\"\"\"\n",
    "        return \"\\n\".join(\n",
    "            f\"{'H' if isinstance(m, HumanMessage) else 'A'}: {m.content}\" \n",
    "            for m in messages\n",
    "        )\n",
    "    \n",
    "    def get_context(self):\n",
    "        \"\"\"Get messages to include in context.\"\"\"\n",
    "        context = []\n",
    "        if self.summary:\n",
    "            context.append(SystemMessage(content=f\"Earlier: {self.summary}\"))\n",
    "        context.extend(self.messages)\n",
    "        return context\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    memory = SummaryMemory(llm, max_messages=6)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm\n",
    "    \n",
    "    test_messages = [\n",
    "        \"I'm planning a Japan trip.\",\n",
    "        \"Budget is $5000.\",\n",
    "        \"I love temples.\",\n",
    "        \"What should I prioritize?\"\n",
    "    ]\n",
    "    \n",
    "    for msg in test_messages:\n",
    "        messages = memory.get_context() + [HumanMessage(content=msg)]\n",
    "        response = chain.invoke({\"history\": messages, \"input\": msg})\n",
    "        print(f\"H: {msg}\\nA: {response.content[:80]}...\\n\")\n",
    "        memory.add_exchange(msg, response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: persistent_sqlite.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: persistent_sqlite.py\n",
    "#\n",
    "# IMPORTANT: This uses LangChain's built-in SQLChatMessageHistory, which\n",
    "# creates a schema WITHOUT timestamps. For retention policies and cleanup\n",
    "# that require timestamps, see retention_policy.py and production_memory_manager.py\n",
    "# which use a custom schema.\n",
    "#\n",
    "# The SQLChatMessageHistory schema:\n",
    "#   - id (INTEGER PRIMARY KEY)\n",
    "#   - session_id (TEXT)\n",
    "#   - message (TEXT - JSON blob)\n",
    "#\n",
    "# For timestamp-based retention, use the custom schema in Section 13.7 files.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "# Use a unique database name to avoid conflicts with other examples\n",
    "DB_PATH = \"sqlite:///langchain_chat.db\"\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Get SQLite-backed message history.\n",
    "    \n",
    "    Note: LangChain's SQLChatMessageHistory does not store timestamps.\n",
    "    For production use with retention policies, consider using a custom\n",
    "    schema (see retention_policy.py).\n",
    "    \"\"\"\n",
    "    return SQLChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        connection=DB_PATH\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Remember what the user tells you.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "\n",
    "def chat(session_id: str, message: str) -> str:\n",
    "    \"\"\"Send a message and get a response.\"\"\"\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    response = chain_with_history.invoke({\"input\": message}, config=config)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Persistent SQLite Memory Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Database: {DB_PATH}\")\n",
    "    print()\n",
    "    \n",
    "    # First interaction\n",
    "    print(\"Human: My favorite color is blue.\")\n",
    "    response = chat(\"alice_session\", \"My favorite color is blue.\")\n",
    "    print(f\"AI: {response}\")\n",
    "    print()\n",
    "    \n",
    "    # Second interaction - should remember\n",
    "    print(\"Human: What's my favorite color?\")\n",
    "    response = chat(\"alice_session\", \"What's my favorite color?\")\n",
    "    print(f\"AI: {response}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Try stopping and restarting this script.\")\n",
    "    print(\"The memory will persist in langchain_chat.db!\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multiuser_chat.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: multiuser_chat.py\n",
    "#\n",
    "# Demonstrates multi-user support with isolated memory per user.\n",
    "# Each user_id (session_id) gets completely separate conversation history.\n",
    "#\n",
    "# Note: Uses LangChain's SQLChatMessageHistory which does not include\n",
    "# timestamps. For production with retention policies, see Section 13.7.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "# Unique database for multi-user demo\n",
    "DB_PATH = \"sqlite:///multiuser.db\"\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Get isolated SQLite history per user.\n",
    "    \n",
    "    Each session_id gets completely separate conversation history.\n",
    "    Users cannot see each other's messages.\n",
    "    \"\"\"\n",
    "    return SQLChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        connection=DB_PATH\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Remember what each user tells you.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chat = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "\n",
    "def send_message(user_id: str, message: str) -> str:\n",
    "    \"\"\"Send a message as a specific user.\"\"\"\n",
    "    config = {\"configurable\": {\"session_id\": user_id}}\n",
    "    response = chat.invoke({\"input\": message}, config=config)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Different users, different memories\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Multi-User Chat Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Database: {DB_PATH}\")\n",
    "    print()\n",
    "    \n",
    "    # Alice and Bob have separate memories\n",
    "    print(\"--- Setting up preferences ---\")\n",
    "    print(\"Alice: I love Python.\")\n",
    "    print(f\"  AI: {send_message('alice', 'I love Python.')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Bob: I prefer JavaScript.\")\n",
    "    print(f\"  AI: {send_message('bob', 'I prefer JavaScript.')}\")\n",
    "    print()\n",
    "    \n",
    "    # Each user's memory is isolated\n",
    "    print(\"--- Testing memory isolation ---\")\n",
    "    print(\"Alice: What language do I love?\")\n",
    "    print(f\"  AI: {send_message('alice', 'What language do I love?')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Bob: What language do I prefer?\")\n",
    "    print(f\"  AI: {send_message('bob', 'What language do I prefer?')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Notice: Alice and Bob have separate memories!\")\n",
    "    print(\"Alice knows about Python, Bob knows about JavaScript.\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: agent_with_memory.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.6\n",
    "# File: agent_with_memory.py\n",
    "# Note: This uses modern LangChain patterns. Check langchain docs for latest API.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a math expression like '2+2' or '10*5'.\"\"\"\n",
    "    try:\n",
    "        return f\"Result: {eval(expression)}\"\n",
    "    except:\n",
    "        return \"Error evaluating expression\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a city.\"\"\"\n",
    "    weather_data = {\"Seattle\": \"Rainy, 55\u00b0F\", \"NYC\": \"Sunny, 68\u00b0F\"}\n",
    "    return weather_data.get(city, f\"Weather in {city}: Partly cloudy, 70\u00b0F\")\n",
    "\n",
    "\n",
    "tools = [calculator, get_weather]\n",
    "\n",
    "# Initialize model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ReAct prompt template - must include {tools}, {tool_names}, and {agent_scratchpad}\n",
    "REACT_PROMPT = \"\"\"You are a helpful assistant with access to tools.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(REACT_PROMPT)\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Simple conversation with manual history management\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "def format_chat_history(history):\n",
    "    \"\"\"Format chat history as a string for the prompt.\"\"\"\n",
    "    if not history:\n",
    "        return \"No previous conversation.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for msg in history:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            formatted.append(f\"Human: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            formatted.append(f\"Assistant: {msg.content}\")\n",
    "        elif isinstance(msg, str):\n",
    "            formatted.append(f\"Assistant: {msg}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def chat(user_input):\n",
    "    \"\"\"Chat with the agent, maintaining history.\"\"\"\n",
    "    response = agent_executor.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": format_chat_history(chat_history)\n",
    "    })\n",
    "    \n",
    "    # Update history\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    chat_history.append(AIMessage(content=response[\"output\"]))\n",
    "    \n",
    "    return response[\"output\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First interaction\n",
    "    print(\"Agent:\", chat(\"What's the weather in Seattle?\"))\n",
    "    \n",
    "    # Follow-up - agent should remember\n",
    "    print(\"Agent:\", chat(\"What city did I just ask about?\"))\n",
    "    \n",
    "    # Use a tool\n",
    "    print(\"Agent:\", chat(\"Calculate 25 * 4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 13.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.6.1: Basic Chat with Memory\n",
    "\n",
    "Create a chat using `RunnableWithMessageHistory` that:\n",
    "1. Uses in-memory `ChatMessageHistory`\n",
    "2. Remembers the user's name across 3 messages\n",
    "3. Prints the full history at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.6.2: Windowed Memory\n",
    "\n",
    "Build a chatbot that:\n",
    "1. Uses `trim_messages` to keep only ~100 tokens\n",
    "2. Has a 6-message conversation\n",
    "3. Demonstrates early messages are forgotten\n",
    "4. Shows what the bot remembers at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.6.3: Agent with Memory Management\n",
    "\n",
    "Create an agent that:\n",
    "1. Has 2 tools (calculator, weather)\n",
    "2. Manages memory with automatic summarization\n",
    "3. Keeps only recent messages after summarizing\n",
    "4. Demonstrates memory works across tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 13.7: Memory management strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: retention_policy.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: retention_policy.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "\n",
    "class RetentionPolicy:\n",
    "    \"\"\"Manage conversation data retention and GDPR compliance.\n",
    "    \n",
    "    This class creates its own table schema with timestamps for proper\n",
    "    retention management. The default LangChain SQLChatMessageHistory\n",
    "    doesn't include timestamps, so we manage our own schema here.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"retention_demo.db\", retention_days=30):\n",
    "        self.db_path = db_path\n",
    "        self.retention_days = retention_days\n",
    "        self._init_db()\n",
    "    \n",
    "    def _init_db(self):\n",
    "        \"\"\"Initialize database with timestamp-enabled schema.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table with timestamp column\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS message_store (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                session_id TEXT NOT NULL,\n",
    "                message_type TEXT NOT NULL,\n",
    "                content TEXT NOT NULL,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create index for efficient cleanup queries\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_timestamp \n",
    "            ON message_store(timestamp)\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_session_id \n",
    "            ON message_store(session_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def add_message(self, session_id: str, message_type: str, content: str):\n",
    "        \"\"\"Add a message with automatic timestamp.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO message_store (session_id, message_type, content, timestamp)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (session_id, message_type, content, datetime.now()))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def cleanup_old_conversations(self):\n",
    "        \"\"\"Delete conversations older than retention period.\"\"\"\n",
    "        cutoff_date = datetime.now() - timedelta(days=self.retention_days)\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Delete old messages\n",
    "        cursor.execute(\"\"\"\n",
    "            DELETE FROM message_store \n",
    "            WHERE timestamp < ?\n",
    "        \"\"\", (cutoff_date,))\n",
    "        \n",
    "        deleted_count = cursor.rowcount\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return deleted_count\n",
    "    \n",
    "    def get_user_data(self, session_id: str):\n",
    "        \"\"\"Retrieve all data for a specific user (GDPR data export).\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT id, session_id, message_type, content, timestamp \n",
    "            FROM message_store \n",
    "            WHERE session_id = ?\n",
    "            ORDER BY timestamp\n",
    "        \"\"\", (session_id,))\n",
    "        \n",
    "        columns = ['id', 'session_id', 'message_type', 'content', 'timestamp']\n",
    "        data = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "        conn.close()\n",
    "        return data\n",
    "    \n",
    "    def export_user_data_json(self, session_id: str) -> str:\n",
    "        \"\"\"Export user data as JSON (GDPR compliance).\"\"\"\n",
    "        data = self.get_user_data(session_id)\n",
    "        # Convert datetime objects to strings\n",
    "        for record in data:\n",
    "            if record['timestamp']:\n",
    "                record['timestamp'] = str(record['timestamp'])\n",
    "        return json.dumps(data, indent=2)\n",
    "    \n",
    "    def delete_user_data(self, session_id: str):\n",
    "        \"\"\"Delete all data for a specific user (GDPR right to erasure).\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            DELETE FROM message_store \n",
    "            WHERE session_id = ?\n",
    "        \"\"\", (session_id,))\n",
    "        \n",
    "        deleted_count = cursor.rowcount\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return deleted_count\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM message_store\")\n",
    "        total_messages = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(DISTINCT session_id) FROM message_store\")\n",
    "        total_sessions = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT MIN(timestamp), MAX(timestamp) FROM message_store\")\n",
    "        date_range = cursor.fetchone()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"total_messages\": total_messages,\n",
    "            \"total_sessions\": total_sessions,\n",
    "            \"oldest_message\": date_range[0],\n",
    "            \"newest_message\": date_range[1]\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    policy = RetentionPolicy(db_path=\"retention_demo.db\", retention_days=30)\n",
    "    \n",
    "    # Add some sample messages\n",
    "    print(\"Adding sample messages...\")\n",
    "    policy.add_message(\"user_123\", \"human\", \"Hello, I need help with my account\")\n",
    "    policy.add_message(\"user_123\", \"ai\", \"I'd be happy to help! What do you need?\")\n",
    "    policy.add_message(\"user_456\", \"human\", \"What's the weather like?\")\n",
    "    policy.add_message(\"user_456\", \"ai\", \"I don't have access to weather data.\")\n",
    "    \n",
    "    # Show stats\n",
    "    stats = policy.get_stats()\n",
    "    print(f\"\\nDatabase stats: {stats}\")\n",
    "    \n",
    "    # Export user data (GDPR)\n",
    "    print(f\"\\nUser data for user_123:\")\n",
    "    print(policy.export_user_data_json(\"user_123\"))\n",
    "    \n",
    "    # Run cleanup (in production, use a scheduled job)\n",
    "    deleted = policy.cleanup_old_conversations()\n",
    "    print(f\"\\nCleaned up {deleted} old messages (older than 30 days)\")\n",
    "    \n",
    "    # Handle user deletion request (GDPR right to erasure)\n",
    "    deleted_user = policy.delete_user_data(\"user_456\")\n",
    "    print(f\"Deleted {deleted_user} messages for user_456\")\n",
    "    \n",
    "    # Show final stats\n",
    "    stats = policy.get_stats()\n",
    "    print(f\"\\nFinal stats: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: pii_filter.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: pii_filter.py\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "class PIIFilter:\n",
    "    \"\"\"Filter personally identifiable information from messages.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Patterns for common PII\n",
    "        self.patterns = {\n",
    "            'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'phone': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "            'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
    "            'credit_card': r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b',\n",
    "        }\n",
    "    \n",
    "    def filter_message(self, text: str) -> str:\n",
    "        \"\"\"Replace PII with placeholders.\"\"\"\n",
    "        filtered = text\n",
    "        \n",
    "        # Replace email addresses\n",
    "        filtered = re.sub(self.patterns['email'], '[EMAIL]', filtered)\n",
    "        \n",
    "        # Replace phone numbers\n",
    "        filtered = re.sub(self.patterns['phone'], '[PHONE]', filtered)\n",
    "        \n",
    "        # Replace SSNs\n",
    "        filtered = re.sub(self.patterns['ssn'], '[SSN]', filtered)\n",
    "        \n",
    "        # Replace credit cards\n",
    "        filtered = re.sub(self.patterns['credit_card'], '[CARD]', filtered)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def contains_pii(self, text: str) -> bool:\n",
    "        \"\"\"Check if text contains any PII.\"\"\"\n",
    "        for pattern in self.patterns.values():\n",
    "            if re.search(pattern, text):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def safe_add_message(memory, user_input, pii_filter):\n",
    "    \"\"\"Add message to memory after filtering PII.\"\"\"\n",
    "    if pii_filter.contains_pii(user_input):\n",
    "        print(\"Warning: PII detected and filtered\")\n",
    "        user_input = pii_filter.filter_message(user_input)\n",
    "    \n",
    "    # Now safe to store\n",
    "    memory.add_message(user_input)\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    filter = PIIFilter()\n",
    "    \n",
    "    test_message = \"My email is john@example.com and phone is 555-123-4567\"\n",
    "    filtered = filter.filter_message(test_message)\n",
    "    print(f\"Original: {test_message}\")\n",
    "    print(f\"Filtered: {filtered}\")\n",
    "    # Output: \"My email is [EMAIL] and phone is [PHONE]\"\n",
    "    \n",
    "    # Test detection\n",
    "    print(f\"Contains PII: {filter.contains_pii(test_message)}\")\n",
    "    print(f\"Contains PII after filter: {filter.contains_pii(filtered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: time_based_cleanup.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: time_based_cleanup.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class TimeBasedCleanup:\n",
    "    \"\"\"Delete conversations older than N days.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_age_days=30):\n",
    "        self.max_age_days = max_age_days\n",
    "        self.conversations = {}  # session_id -> (timestamp, messages)\n",
    "    \n",
    "    def add_conversation(self, session_id, messages):\n",
    "        \"\"\"Add a conversation with current timestamp.\"\"\"\n",
    "        self.conversations[session_id] = (datetime.now(), messages)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove conversations older than max_age_days.\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=self.max_age_days)\n",
    "        \n",
    "        old_sessions = [\n",
    "            sid for sid, (timestamp, _) in self.conversations.items()\n",
    "            if timestamp < cutoff\n",
    "        ]\n",
    "        \n",
    "        for sid in old_sessions:\n",
    "            del self.conversations[sid]\n",
    "        \n",
    "        return len(old_sessions)\n",
    "    \n",
    "    def get_conversation(self, session_id):\n",
    "        \"\"\"Get conversation if it exists.\"\"\"\n",
    "        if session_id in self.conversations:\n",
    "            return self.conversations[session_id][1]\n",
    "        return None\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    cleanup = TimeBasedCleanup(max_age_days=7)\n",
    "    \n",
    "    # Add some test conversations\n",
    "    cleanup.add_conversation(\"user_1\", [\"Hello\", \"Hi there!\"])\n",
    "    cleanup.add_conversation(\"user_2\", [\"What's the weather?\", \"It's sunny!\"])\n",
    "    \n",
    "    # Simulate cleanup\n",
    "    deleted = cleanup.cleanup()\n",
    "    print(f\"Removed {deleted} old conversations\")\n",
    "    print(f\"Remaining: {len(cleanup.conversations)} conversations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: size_based_cleanup.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: size_based_cleanup.py\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SizeBasedMemory:\n",
    "    \"\"\"Keep only the N most recent conversations per user.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_conversations_per_user=10):\n",
    "        self.max_conversations = max_conversations_per_user\n",
    "        self.user_conversations = {}  # user_id -> deque of conversations\n",
    "    \n",
    "    def add_conversation(self, user_id, conversation):\n",
    "        \"\"\"Add conversation, automatically dropping oldest when full.\"\"\"\n",
    "        if user_id not in self.user_conversations:\n",
    "            self.user_conversations[user_id] = deque(maxlen=self.max_conversations)\n",
    "        \n",
    "        # Automatically drops oldest when full\n",
    "        self.user_conversations[user_id].append(conversation)\n",
    "    \n",
    "    def get_recent_conversations(self, user_id, n=5):\n",
    "        \"\"\"Return n most recent conversations for user.\"\"\"\n",
    "        if user_id not in self.user_conversations:\n",
    "            return []\n",
    "        \n",
    "        # Return n most recent\n",
    "        return list(self.user_conversations[user_id])[-n:]\n",
    "    \n",
    "    def get_all_conversations(self, user_id):\n",
    "        \"\"\"Get all stored conversations for user.\"\"\"\n",
    "        if user_id not in self.user_conversations:\n",
    "            return []\n",
    "        return list(self.user_conversations[user_id])\n",
    "    \n",
    "    def get_conversation_count(self, user_id):\n",
    "        \"\"\"Get number of stored conversations for user.\"\"\"\n",
    "        if user_id not in self.user_conversations:\n",
    "            return 0\n",
    "        return len(self.user_conversations[user_id])\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = SizeBasedMemory(max_conversations_per_user=3)\n",
    "    \n",
    "    # Add conversations for user\n",
    "    for i in range(5):\n",
    "        memory.add_conversation(\"user_123\", {\"id\": i, \"messages\": [f\"Conversation {i}\"]})\n",
    "    \n",
    "    # Only 3 most recent are kept\n",
    "    conversations = memory.get_all_conversations(\"user_123\")\n",
    "    print(f\"Stored {len(conversations)} conversations (max 3)\")\n",
    "    for conv in conversations:\n",
    "        print(f\"  - {conv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: importance_based_cleanup.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: importance_based_cleanup.py\n",
    "\n",
    "\n",
    "class ImportanceBasedMemory:\n",
    "    \"\"\"Keep important conversations, discard routine ones.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conversations = {}\n",
    "    \n",
    "    def add_conversation(self, conv_id, conversation):\n",
    "        \"\"\"Add a conversation.\"\"\"\n",
    "        self.conversations[conv_id] = conversation\n",
    "    \n",
    "    def calculate_importance(self, conversation):\n",
    "        \"\"\"Score conversation importance based on multiple factors.\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Long conversations are more important\n",
    "        score += min(len(conversation.get(\"messages\", [])), 10)\n",
    "        \n",
    "        # User-marked favorites\n",
    "        if conversation.get(\"is_favorite\"):\n",
    "            score += 20\n",
    "        \n",
    "        # Contains entities (people, projects mentioned)\n",
    "        score += len(conversation.get(\"entities\", [])) * 2\n",
    "        \n",
    "        # Tool usage indicates complex task\n",
    "        score += len(conversation.get(\"tool_calls\", [])) * 3\n",
    "        \n",
    "        # Has summary (indicates substantial conversation)\n",
    "        if conversation.get(\"summary\"):\n",
    "            score += 5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def cleanup_low_importance(self, threshold=5):\n",
    "        \"\"\"Remove conversations below importance threshold.\"\"\"\n",
    "        to_delete = []\n",
    "        \n",
    "        for conv_id, conv in self.conversations.items():\n",
    "            if self.calculate_importance(conv) < threshold:\n",
    "                to_delete.append(conv_id)\n",
    "        \n",
    "        for conv_id in to_delete:\n",
    "            del self.conversations[conv_id]\n",
    "        \n",
    "        return len(to_delete)\n",
    "    \n",
    "    def get_important_conversations(self, min_importance=10):\n",
    "        \"\"\"Get conversations above importance threshold.\"\"\"\n",
    "        return {\n",
    "            conv_id: conv \n",
    "            for conv_id, conv in self.conversations.items()\n",
    "            if self.calculate_importance(conv) >= min_importance\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = ImportanceBasedMemory()\n",
    "    \n",
    "    # Add conversations with different importance levels\n",
    "    memory.add_conversation(\"conv_1\", {\n",
    "        \"messages\": [\"hi\", \"bye\"],\n",
    "        \"entities\": [],\n",
    "        \"tool_calls\": []\n",
    "    })\n",
    "    \n",
    "    memory.add_conversation(\"conv_2\", {\n",
    "        \"messages\": [\"Tell me about Project Alpha\", \"What's the status?\", \"Update the timeline\"],\n",
    "        \"entities\": [\"Project Alpha\", \"Sarah\"],\n",
    "        \"tool_calls\": [\"search\", \"calendar\"],\n",
    "        \"is_favorite\": True\n",
    "    })\n",
    "    \n",
    "    # Show importance scores\n",
    "    for conv_id, conv in memory.conversations.items():\n",
    "        score = memory.calculate_importance(conv)\n",
    "        print(f\"{conv_id}: importance = {score}\")\n",
    "    \n",
    "    # Cleanup low importance\n",
    "    deleted = memory.cleanup_low_importance(threshold=5)\n",
    "    print(f\"\\nDeleted {deleted} low-importance conversations\")\n",
    "    print(f\"Remaining: {list(memory.conversations.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: partitioned_storage.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: partitioned_storage.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "class PartitionedStorage:\n",
    "    \"\"\"Partition conversation data by date for large-scale applications.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=\"chat_history\"):\n",
    "        self.base_path = base_path\n",
    "    \n",
    "    def get_db_path(self, date=None):\n",
    "        \"\"\"Get database path for specific date partition.\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now()\n",
    "        \n",
    "        year_month = date.strftime(\"%Y_%m\")\n",
    "        return f\"{self.base_path}_{year_month}.db\"\n",
    "    \n",
    "    def get_session_history(self, session_id, date=None):\n",
    "        \"\"\"Get history from appropriate partition.\"\"\"\n",
    "        db_path = self.get_db_path(date)\n",
    "        return SQLChatMessageHistory(\n",
    "            session_id=session_id,\n",
    "            connection=f\"sqlite:///{db_path}\"\n",
    "        )\n",
    "    \n",
    "    def list_partitions(self):\n",
    "        \"\"\"List all existing partitions.\"\"\"\n",
    "        import glob\n",
    "        return glob.glob(f\"{self.base_path}_*.db\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    storage = PartitionedStorage()\n",
    "    \n",
    "    # Current month\n",
    "    current_history = storage.get_session_history(\"user_123\")\n",
    "    print(f\"Current DB: {storage.get_db_path()}\")\n",
    "    \n",
    "    # Specific month (for historical queries)\n",
    "    jan_2024 = datetime(2024, 1, 1)\n",
    "    old_db = storage.get_db_path(date=jan_2024)\n",
    "    print(f\"January 2024 DB: {old_db}\")\n",
    "    \n",
    "    # List all partitions\n",
    "    print(f\"Existing partitions: {storage.list_partitions()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: memory_cache.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: memory_cache.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class CachedMemoryStore:\n",
    "    \"\"\"Cache frequently accessed conversations for performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_duration_minutes=15):\n",
    "        self.cache_duration = timedelta(minutes=cache_duration_minutes)\n",
    "        self.cache = {}  # session_id -> (timestamp, messages)\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get_messages(self, session_id):\n",
    "        \"\"\"Get messages with caching.\"\"\"\n",
    "        # Check cache first\n",
    "        if session_id in self.cache:\n",
    "            timestamp, messages = self.cache[session_id]\n",
    "            if datetime.now() - timestamp < self.cache_duration:\n",
    "                self.hits += 1\n",
    "                return messages\n",
    "        \n",
    "        # Cache miss - load from database\n",
    "        self.misses += 1\n",
    "        messages = self._load_from_db(session_id)\n",
    "        self.cache[session_id] = (datetime.now(), messages)\n",
    "        return messages\n",
    "    \n",
    "    def _load_from_db(self, session_id):\n",
    "        \"\"\"Load from actual storage (implement per your backend).\"\"\"\n",
    "        # Placeholder - implement actual database loading\n",
    "        return []\n",
    "    \n",
    "    def invalidate_cache(self, session_id):\n",
    "        \"\"\"Clear cache for a session.\"\"\"\n",
    "        if session_id in self.cache:\n",
    "            del self.cache[session_id]\n",
    "    \n",
    "    def clear_expired(self):\n",
    "        \"\"\"Remove expired cache entries.\"\"\"\n",
    "        now = datetime.now()\n",
    "        expired = [\n",
    "            sid for sid, (timestamp, _) in self.cache.items()\n",
    "            if now - timestamp >= self.cache_duration\n",
    "        ]\n",
    "        for sid in expired:\n",
    "            del self.cache[sid]\n",
    "        return len(expired)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": f\"{hit_rate:.2%}\",\n",
    "            \"cached_sessions\": len(self.cache)\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    cache = CachedMemoryStore(cache_duration_minutes=15)\n",
    "    \n",
    "    # Simulate access pattern\n",
    "    for _ in range(5):\n",
    "        cache.get_messages(\"user_123\")  # First is miss, rest are hits\n",
    "    \n",
    "    cache.get_messages(\"user_456\")  # New user, cache miss\n",
    "    \n",
    "    print(\"Cache stats:\", cache.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: lazy_loading.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: lazy_loading.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "class LazyLoadMemory:\n",
    "    \"\"\"Don't load entire conversation history upfront.\"\"\"\n",
    "    \n",
    "    def __init__(self, session_id, db_path):\n",
    "        self.session_id = session_id\n",
    "        self.db_path = db_path\n",
    "        self._messages = None  # Not loaded yet\n",
    "        self._loaded = False\n",
    "    \n",
    "    @property\n",
    "    def messages(self):\n",
    "        \"\"\"Load messages only when accessed.\"\"\"\n",
    "        if self._messages is None:\n",
    "            self._messages = self._load_messages()\n",
    "            self._loaded = True\n",
    "        return self._messages\n",
    "    \n",
    "    def _load_messages(self):\n",
    "        \"\"\"Load from database.\"\"\"\n",
    "        history = SQLChatMessageHistory(\n",
    "            session_id=self.session_id,\n",
    "            connection=self.db_path\n",
    "        )\n",
    "        return history.messages\n",
    "    \n",
    "    def get_recent(self, n=10):\n",
    "        \"\"\"Get only recent messages without loading all.\"\"\"\n",
    "        # For efficiency, could implement a direct DB query\n",
    "        # This is a simplified version\n",
    "        return self.messages[-n:] if self.messages else []\n",
    "    \n",
    "    def is_loaded(self):\n",
    "        \"\"\"Check if messages have been loaded.\"\"\"\n",
    "        return self._loaded\n",
    "    \n",
    "    def reload(self):\n",
    "        \"\"\"Force reload from database.\"\"\"\n",
    "        self._messages = None\n",
    "        self._loaded = False\n",
    "        return self.messages\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    memory = LazyLoadMemory(\"user_123\", \"sqlite:///chat.db\")\n",
    "    \n",
    "    print(f\"Loaded: {memory.is_loaded()}\")  # False\n",
    "    \n",
    "    # Messages loaded on first access\n",
    "    recent = memory.get_recent(5)\n",
    "    print(f\"Loaded: {memory.is_loaded()}\")  # True\n",
    "    print(f\"Recent messages: {len(recent)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: batch_operations.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: batch_operations.py\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class BatchMemoryProcessor:\n",
    "    \"\"\"Process multiple conversations efficiently with batch operations.\n",
    "    \n",
    "    Note: This assumes a message_store table with columns:\n",
    "    - session_id, message_type, content, timestamp\n",
    "    \n",
    "    See retention_policy.py or production_memory_manager.py for\n",
    "    the table creation schema.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self._init_db()\n",
    "    \n",
    "    def _init_db(self):\n",
    "        \"\"\"Initialize database with required schema.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS message_store (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                session_id TEXT NOT NULL,\n",
    "                message_type TEXT NOT NULL,\n",
    "                content TEXT NOT NULL,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_session_id \n",
    "            ON message_store(session_id)\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_timestamp \n",
    "            ON message_store(timestamp)\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def add_message(self, session_id: str, message_type: str, content: str, \n",
    "                    timestamp: datetime = None):\n",
    "        \"\"\"Add a message (for testing purposes).\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO message_store (session_id, message_type, content, timestamp)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (session_id, message_type, content, timestamp or datetime.now()))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def batch_cleanup(self, session_ids, cutoff_date):\n",
    "        \"\"\"Delete old messages for multiple users in one query.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Use IN clause for batch operation\n",
    "        placeholders = ','.join('?' * len(session_ids))\n",
    "        cursor.execute(f\"\"\"\n",
    "            DELETE FROM message_store \n",
    "            WHERE session_id IN ({placeholders})\n",
    "            AND timestamp < ?\n",
    "        \"\"\", (*session_ids, cutoff_date))\n",
    "        \n",
    "        deleted = cursor.rowcount\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return deleted\n",
    "    \n",
    "    def batch_export(self, session_ids):\n",
    "        \"\"\"Export data for multiple users efficiently.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        placeholders = ','.join('?' * len(session_ids))\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT session_id, message_type, content, timestamp \n",
    "            FROM message_store \n",
    "            WHERE session_id IN ({placeholders})\n",
    "            ORDER BY session_id, timestamp\n",
    "        \"\"\", session_ids)\n",
    "        \n",
    "        results = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        # Group by session_id\n",
    "        exports = {}\n",
    "        for session_id, msg_type, content, timestamp in results:\n",
    "            if session_id not in exports:\n",
    "                exports[session_id] = []\n",
    "            exports[session_id].append({\n",
    "                \"type\": msg_type,\n",
    "                \"content\": content,\n",
    "                \"timestamp\": str(timestamp)\n",
    "            })\n",
    "        \n",
    "        return exports\n",
    "    \n",
    "    def batch_delete_users(self, session_ids):\n",
    "        \"\"\"Delete all data for multiple users (GDPR bulk request).\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        placeholders = ','.join('?' * len(session_ids))\n",
    "        cursor.execute(f\"\"\"\n",
    "            DELETE FROM message_store \n",
    "            WHERE session_id IN ({placeholders})\n",
    "        \"\"\", session_ids)\n",
    "        \n",
    "        deleted = cursor.rowcount\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return deleted\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM message_store\")\n",
    "        total = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(DISTINCT session_id) FROM message_store\")\n",
    "        sessions = cursor.fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        return {\"total_messages\": total, \"total_sessions\": sessions}\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Use a fresh test database\n",
    "    test_db = \"batch_test.db\"\n",
    "    if os.path.exists(test_db):\n",
    "        os.remove(test_db)\n",
    "    \n",
    "    processor = BatchMemoryProcessor(test_db)\n",
    "    \n",
    "    # Add sample data - some old, some new\n",
    "    old_date = datetime.now() - timedelta(days=60)\n",
    "    recent_date = datetime.now() - timedelta(days=5)\n",
    "    \n",
    "    print(\"Adding sample messages...\")\n",
    "    \n",
    "    # Old messages (will be cleaned up)\n",
    "    processor.add_message(\"user_1\", \"human\", \"Old message 1\", old_date)\n",
    "    processor.add_message(\"user_1\", \"ai\", \"Old response 1\", old_date)\n",
    "    processor.add_message(\"user_2\", \"human\", \"Old message 2\", old_date)\n",
    "    \n",
    "    # Recent messages (will be kept)\n",
    "    processor.add_message(\"user_1\", \"human\", \"Recent message\", recent_date)\n",
    "    processor.add_message(\"user_2\", \"human\", \"Recent message\", recent_date)\n",
    "    processor.add_message(\"user_3\", \"human\", \"User 3 message\", recent_date)\n",
    "    \n",
    "    print(f\"Stats before cleanup: {processor.get_stats()}\")\n",
    "    \n",
    "    # Batch cleanup for multiple users (messages older than 30 days)\n",
    "    users_to_clean = [\"user_1\", \"user_2\", \"user_3\"]\n",
    "    cutoff = datetime.now() - timedelta(days=30)\n",
    "    \n",
    "    deleted = processor.batch_cleanup(users_to_clean, cutoff)\n",
    "    print(f\"\\nBatch cleanup: deleted {deleted} old messages\")\n",
    "    print(f\"Stats after cleanup: {processor.get_stats()}\")\n",
    "    \n",
    "    # Batch export\n",
    "    print(\"\\nBatch export:\")\n",
    "    exports = processor.batch_export([\"user_1\", \"user_2\"])\n",
    "    for user_id, messages in exports.items():\n",
    "        print(f\"  {user_id}: {len(messages)} messages\")\n",
    "    \n",
    "    # Batch delete (GDPR)\n",
    "    deleted = processor.batch_delete_users([\"user_3\"])\n",
    "    print(f\"\\nBatch delete user_3: {deleted} messages deleted\")\n",
    "    print(f\"Final stats: {processor.get_stats()}\")\n",
    "    \n",
    "    # Cleanup test file\n",
    "    os.remove(test_db)\n",
    "    print(\"\\nTest complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: cost_optimization.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: cost_optimization.py\n",
    "\n",
    "\n",
    "class CostOptimizedVectorMemory:\n",
    "    \"\"\"Only create embeddings for important messages to save costs.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection, important_threshold=0.7):\n",
    "        self.collection = collection\n",
    "        self.important_threshold = important_threshold\n",
    "        self.vectorized_count = 0\n",
    "        self.skipped_count = 0\n",
    "    \n",
    "    def should_store_vector(self, message):\n",
    "        \"\"\"Decide if message is important enough to vectorize.\"\"\"\n",
    "        # Skip very short messages\n",
    "        if len(message) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Skip routine responses\n",
    "        routine_phrases = [\"ok\", \"thanks\", \"bye\", \"hello\", \"hi\", \"yes\", \"no\"]\n",
    "        if message.lower().strip() in routine_phrases:\n",
    "            return False\n",
    "        \n",
    "        # Skip if mostly punctuation\n",
    "        alpha_ratio = sum(c.isalpha() for c in message) / max(len(message), 1)\n",
    "        if alpha_ratio < 0.5:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def add_message(self, message, metadata=None):\n",
    "        \"\"\"Only create embeddings for important messages.\"\"\"\n",
    "        if self.should_store_vector(message):\n",
    "            # Create embedding and store (costs money)\n",
    "            self.collection.add(\n",
    "                documents=[message],\n",
    "                metadatas=[metadata or {}],\n",
    "                ids=[f\"msg_{self.vectorized_count}\"]\n",
    "            )\n",
    "            self.vectorized_count += 1\n",
    "            return True\n",
    "        else:\n",
    "            # Skip - store in cheaper text-only storage if needed\n",
    "            self.skipped_count += 1\n",
    "            return False\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cost optimization statistics.\"\"\"\n",
    "        total = self.vectorized_count + self.skipped_count\n",
    "        savings = self.skipped_count / total if total > 0 else 0\n",
    "        return {\n",
    "            \"vectorized\": self.vectorized_count,\n",
    "            \"skipped\": self.skipped_count,\n",
    "            \"savings_rate\": f\"{savings:.2%}\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Mock collection for demo\n",
    "    class MockCollection:\n",
    "        def add(self, documents, metadatas, ids):\n",
    "            print(f\"  Stored: {documents[0][:50]}...\")\n",
    "    \n",
    "    memory = CostOptimizedVectorMemory(MockCollection())\n",
    "    \n",
    "    test_messages = [\n",
    "        \"hi\",  # Skip\n",
    "        \"thanks\",  # Skip\n",
    "        \"Can you explain how machine learning algorithms work?\",  # Store\n",
    "        \"ok\",  # Skip\n",
    "        \"I need help planning my project timeline for Q2\",  # Store\n",
    "        \"bye\",  # Skip\n",
    "    ]\n",
    "    \n",
    "    print(\"Processing messages:\")\n",
    "    for msg in test_messages:\n",
    "        stored = memory.add_message(msg)\n",
    "        status = \"STORED\" if stored else \"SKIPPED\"\n",
    "        print(f\"  [{status}] {msg[:40]}...\")\n",
    "    \n",
    "    print(f\"\\nStats: {memory.get_stats()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tiered_storage.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: tiered_storage.py\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TieredStorage:\n",
    "    \"\"\"Move old conversations to cheaper storage tiers.\n",
    "    \n",
    "    Tiers:\n",
    "    - Hot (< 7 days): Fast access, e.g., Redis\n",
    "    - Warm (7-30 days): Moderate speed, e.g., SQLite\n",
    "    - Cold (> 30 days): Slow but cheap, e.g., S3\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hot_storage = {}   # Recent - in-memory or Redis\n",
    "        self.warm_storage = {}  # 7-30 days - SQLite\n",
    "        self.cold_storage = {}  # 30+ days - S3 or similar\n",
    "    \n",
    "    def get_tier(self, message_date):\n",
    "        \"\"\"Determine storage tier based on message age.\"\"\"\n",
    "        age_days = (datetime.now() - message_date).days\n",
    "        \n",
    "        if age_days < 7:\n",
    "            return \"hot\"\n",
    "        elif age_days < 30:\n",
    "            return \"warm\"\n",
    "        else:\n",
    "            return \"cold\"\n",
    "    \n",
    "    def store_message(self, session_id, message, timestamp=None):\n",
    "        \"\"\"Store message in appropriate tier.\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now()\n",
    "        \n",
    "        tier = self.get_tier(timestamp)\n",
    "        \n",
    "        if tier == \"hot\":\n",
    "            self._store_hot(session_id, message, timestamp)\n",
    "        elif tier == \"warm\":\n",
    "            self._store_warm(session_id, message, timestamp)\n",
    "        else:\n",
    "            self._store_cold(session_id, message, timestamp)\n",
    "    \n",
    "    def get_messages(self, session_id, message_date=None):\n",
    "        \"\"\"Route to appropriate storage tier.\"\"\"\n",
    "        if message_date is None:\n",
    "            # Get from all tiers\n",
    "            messages = []\n",
    "            messages.extend(self._get_from_hot(session_id))\n",
    "            messages.extend(self._get_from_warm(session_id))\n",
    "            messages.extend(self._get_from_cold(session_id))\n",
    "            return messages\n",
    "        \n",
    "        tier = self.get_tier(message_date)\n",
    "        \n",
    "        if tier == \"hot\":\n",
    "            return self._get_from_hot(session_id)\n",
    "        elif tier == \"warm\":\n",
    "            return self._get_from_warm(session_id)\n",
    "        else:\n",
    "            return self._get_from_cold(session_id)\n",
    "    \n",
    "    def migrate_to_colder_tier(self):\n",
    "        \"\"\"Move aged data to appropriate tier (run periodically).\"\"\"\n",
    "        moved = {\"hot_to_warm\": 0, \"warm_to_cold\": 0}\n",
    "        \n",
    "        # Move from hot to warm\n",
    "        for session_id, messages in list(self.hot_storage.items()):\n",
    "            for msg in messages[:]:\n",
    "                if self.get_tier(msg[\"timestamp\"]) == \"warm\":\n",
    "                    self._store_warm(session_id, msg[\"content\"], msg[\"timestamp\"])\n",
    "                    messages.remove(msg)\n",
    "                    moved[\"hot_to_warm\"] += 1\n",
    "        \n",
    "        # Move from warm to cold\n",
    "        for session_id, messages in list(self.warm_storage.items()):\n",
    "            for msg in messages[:]:\n",
    "                if self.get_tier(msg[\"timestamp\"]) == \"cold\":\n",
    "                    self._store_cold(session_id, msg[\"content\"], msg[\"timestamp\"])\n",
    "                    messages.remove(msg)\n",
    "                    moved[\"warm_to_cold\"] += 1\n",
    "        \n",
    "        return moved\n",
    "    \n",
    "    # Tier-specific implementations\n",
    "    def _store_hot(self, session_id, message, timestamp):\n",
    "        if session_id not in self.hot_storage:\n",
    "            self.hot_storage[session_id] = []\n",
    "        self.hot_storage[session_id].append({\"content\": message, \"timestamp\": timestamp})\n",
    "    \n",
    "    def _store_warm(self, session_id, message, timestamp):\n",
    "        if session_id not in self.warm_storage:\n",
    "            self.warm_storage[session_id] = []\n",
    "        self.warm_storage[session_id].append({\"content\": message, \"timestamp\": timestamp})\n",
    "    \n",
    "    def _store_cold(self, session_id, message, timestamp):\n",
    "        if session_id not in self.cold_storage:\n",
    "            self.cold_storage[session_id] = []\n",
    "        self.cold_storage[session_id].append({\"content\": message, \"timestamp\": timestamp})\n",
    "    \n",
    "    def _get_from_hot(self, session_id):\n",
    "        return self.hot_storage.get(session_id, [])\n",
    "    \n",
    "    def _get_from_warm(self, session_id):\n",
    "        return self.warm_storage.get(session_id, [])\n",
    "    \n",
    "    def _get_from_cold(self, session_id):\n",
    "        return self.cold_storage.get(session_id, [])\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    storage = TieredStorage()\n",
    "    \n",
    "    # Store recent message (goes to hot)\n",
    "    storage.store_message(\"user_123\", \"Hello!\")\n",
    "    \n",
    "    # Store older message (simulated)\n",
    "    from datetime import timedelta\n",
    "    old_date = datetime.now() - timedelta(days=15)\n",
    "    storage.store_message(\"user_123\", \"Old message\", old_date)\n",
    "    \n",
    "    very_old_date = datetime.now() - timedelta(days=45)\n",
    "    storage.store_message(\"user_123\", \"Very old message\", very_old_date)\n",
    "    \n",
    "    print(f\"Hot storage: {len(storage.hot_storage.get('user_123', []))} messages\")\n",
    "    print(f\"Warm storage: {len(storage.warm_storage.get('user_123', []))} messages\")\n",
    "    print(f\"Cold storage: {len(storage.cold_storage.get('user_123', []))} messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: memory_monitoring.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: memory_monitoring.py\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class MemoryMonitor:\n",
    "    \"\"\"Track memory system health and performance metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"total_conversations\": 0,\n",
    "            \"total_messages\": 0,\n",
    "            \"avg_conversation_length\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"storage_operations\": 0,\n",
    "            \"errors\": 0,\n",
    "        }\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def log_conversation(self, message_count):\n",
    "        \"\"\"Log a conversation.\"\"\"\n",
    "        self.metrics[\"total_conversations\"] += 1\n",
    "        self.metrics[\"total_messages\"] += message_count\n",
    "        self._update_average()\n",
    "    \n",
    "    def _update_average(self):\n",
    "        \"\"\"Update average conversation length.\"\"\"\n",
    "        if self.metrics[\"total_conversations\"] > 0:\n",
    "            self.metrics[\"avg_conversation_length\"] = (\n",
    "                self.metrics[\"total_messages\"] / self.metrics[\"total_conversations\"]\n",
    "            )\n",
    "    \n",
    "    def log_access(self, session_id, is_cache_hit):\n",
    "        \"\"\"Log memory access pattern.\"\"\"\n",
    "        if is_cache_hit:\n",
    "            self.metrics[\"cache_hits\"] += 1\n",
    "        else:\n",
    "            self.metrics[\"cache_misses\"] += 1\n",
    "    \n",
    "    def log_storage_operation(self):\n",
    "        \"\"\"Log a storage operation.\"\"\"\n",
    "        self.metrics[\"storage_operations\"] += 1\n",
    "    \n",
    "    def log_error(self):\n",
    "        \"\"\"Log an error.\"\"\"\n",
    "        self.metrics[\"errors\"] += 1\n",
    "    \n",
    "    def get_cache_hit_rate(self):\n",
    "        \"\"\"Calculate cache effectiveness.\"\"\"\n",
    "        total = self.metrics[\"cache_hits\"] + self.metrics[\"cache_misses\"]\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return self.metrics[\"cache_hits\"] / total\n",
    "    \n",
    "    def get_uptime(self):\n",
    "        \"\"\"Get time since monitoring started.\"\"\"\n",
    "        return datetime.now() - self.start_time\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate memory system report.\"\"\"\n",
    "        return {\n",
    "            **self.metrics,\n",
    "            \"cache_hit_rate\": f\"{self.get_cache_hit_rate():.2%}\",\n",
    "            \"uptime\": str(self.get_uptime()),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all metrics.\"\"\"\n",
    "        self.__init__()\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    monitor = MemoryMonitor()\n",
    "    \n",
    "    # Simulate activity\n",
    "    monitor.log_conversation(message_count=10)\n",
    "    monitor.log_conversation(message_count=5)\n",
    "    monitor.log_access(\"user_123\", is_cache_hit=True)\n",
    "    monitor.log_access(\"user_456\", is_cache_hit=False)\n",
    "    monitor.log_access(\"user_123\", is_cache_hit=True)\n",
    "    monitor.log_storage_operation()\n",
    "    \n",
    "    print(\"Memory System Report:\")\n",
    "    report = monitor.get_report()\n",
    "    for key, value in report.items():\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: production_memory_manager.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13, Section 13.7\n",
    "# File: production_memory_manager.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Import from other files in this section\n",
    "from pii_filter import PIIFilter\n",
    "from memory_monitoring import MemoryMonitor\n",
    "\n",
    "\n",
    "class ProductionMemoryManager:\n",
    "    \"\"\"Complete production-ready memory manager.\n",
    "    \n",
    "    Combines:\n",
    "    - PII filtering\n",
    "    - Retention policies with timestamps\n",
    "    - Monitoring\n",
    "    - GDPR compliance (export/delete)\n",
    "    \n",
    "    Note: This uses a custom schema with timestamps rather than\n",
    "    the default SQLChatMessageHistory which lacks timestamp support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"prod_memory.db\", retention_days=30):\n",
    "        self.db_path = db_path\n",
    "        self.retention_days = retention_days\n",
    "        self.pii_filter = PIIFilter()\n",
    "        self.monitor = MemoryMonitor()\n",
    "        self._init_db()\n",
    "    \n",
    "    def _init_db(self):\n",
    "        \"\"\"Initialize database with timestamp-enabled schema.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table with timestamp column\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS message_store (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                session_id TEXT NOT NULL,\n",
    "                message_type TEXT NOT NULL,\n",
    "                content TEXT NOT NULL,\n",
    "                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes for efficient queries\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_session_id \n",
    "            ON message_store(session_id)\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_timestamp \n",
    "            ON message_store(timestamp)\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_session_history(self, session_id: str):\n",
    "        \"\"\"Get message history for a session.\"\"\"\n",
    "        self.monitor.log_access(session_id, is_cache_hit=False)\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT message_type, content FROM message_store\n",
    "            WHERE session_id = ?\n",
    "            ORDER BY timestamp\n",
    "        \"\"\", (session_id,))\n",
    "        \n",
    "        messages = []\n",
    "        for msg_type, content in cursor.fetchall():\n",
    "            if msg_type == \"human\":\n",
    "                messages.append(HumanMessage(content=content))\n",
    "            else:\n",
    "                messages.append(AIMessage(content=content))\n",
    "        \n",
    "        conn.close()\n",
    "        return messages\n",
    "    \n",
    "    def add_user_message(self, session_id: str, message: str):\n",
    "        \"\"\"Add message with PII filtering.\"\"\"\n",
    "        # Filter PII before storing\n",
    "        filtered_message = self.pii_filter.filter_message(message)\n",
    "        \n",
    "        if filtered_message != message:\n",
    "            print(f\"[INFO] PII detected and filtered for session {session_id}\")\n",
    "        \n",
    "        self._store_message(session_id, \"human\", filtered_message)\n",
    "        self.monitor.log_storage_operation()\n",
    "    \n",
    "    def add_ai_message(self, session_id: str, message: str):\n",
    "        \"\"\"Add AI response to history.\"\"\"\n",
    "        self._store_message(session_id, \"ai\", message)\n",
    "        self.monitor.log_storage_operation()\n",
    "    \n",
    "    def _store_message(self, session_id: str, message_type: str, content: str):\n",
    "        \"\"\"Store a message with timestamp.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO message_store (session_id, message_type, content, timestamp)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (session_id, message_type, content, datetime.now()))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def run_maintenance(self):\n",
    "        \"\"\"Run periodic maintenance tasks.\"\"\"\n",
    "        # Clean old data\n",
    "        cutoff = datetime.now() - timedelta(days=self.retention_days)\n",
    "        deleted = self._cleanup_old_data(cutoff)\n",
    "        \n",
    "        # Get metrics\n",
    "        report = self.monitor.get_report()\n",
    "        \n",
    "        return {\n",
    "            \"deleted_messages\": deleted,\n",
    "            \"metrics\": report,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def _cleanup_old_data(self, cutoff_date):\n",
    "        \"\"\"Delete messages older than cutoff date.\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                \"DELETE FROM message_store WHERE timestamp < ?\", \n",
    "                (cutoff_date,)\n",
    "            )\n",
    "            deleted = cursor.rowcount\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return deleted\n",
    "        except Exception as e:\n",
    "            self.monitor.log_error()\n",
    "            print(f\"[ERROR] Cleanup failed: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def export_user_data(self, session_id: str):\n",
    "        \"\"\"Export all user data (GDPR compliance).\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT message_type, content, timestamp FROM message_store\n",
    "            WHERE session_id = ?\n",
    "            ORDER BY timestamp\n",
    "        \"\"\", (session_id,))\n",
    "        \n",
    "        messages = [\n",
    "            {\"type\": row[0], \"content\": row[1], \"timestamp\": str(row[2])}\n",
    "            for row in cursor.fetchall()\n",
    "        ]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"session_id\": session_id,\n",
    "            \"messages\": messages,\n",
    "            \"export_date\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def delete_user_data(self, session_id: str):\n",
    "        \"\"\"Delete all user data (GDPR right to erasure).\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                \"DELETE FROM message_store WHERE session_id = ?\", \n",
    "                (session_id,)\n",
    "            )\n",
    "            deleted = cursor.rowcount\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return deleted\n",
    "        except Exception as e:\n",
    "            self.monitor.log_error()\n",
    "            print(f\"[ERROR] Delete failed: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get current system metrics.\"\"\"\n",
    "        return self.monitor.get_report()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM message_store\")\n",
    "        total_messages = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(DISTINCT session_id) FROM message_store\")\n",
    "        total_sessions = cursor.fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"total_messages\": total_messages,\n",
    "            \"total_sessions\": total_sessions\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    manager = ProductionMemoryManager(retention_days=30)\n",
    "    \n",
    "    # Normal operation - PII is filtered\n",
    "    print(\"Adding messages with PII filtering...\")\n",
    "    manager.add_user_message(\n",
    "        \"user_123\", \n",
    "        \"My email is sensitive@example.com and I need help\"\n",
    "    )\n",
    "    manager.add_ai_message(\n",
    "        \"user_123\",\n",
    "        \"I'd be happy to help! What do you need?\"\n",
    "    )\n",
    "    \n",
    "    # Add another user\n",
    "    manager.add_user_message(\"user_456\", \"Hello there!\")\n",
    "    manager.add_ai_message(\"user_456\", \"Hi! How can I assist you?\")\n",
    "    \n",
    "    # Check stats\n",
    "    print(\"\\nDatabase stats:\")\n",
    "    stats = manager.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Check metrics\n",
    "    print(\"\\nSystem metrics:\")\n",
    "    metrics = manager.get_metrics()\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Periodic maintenance (run daily via cron)\n",
    "    report = manager.run_maintenance()\n",
    "    print(f\"\\nMaintenance: deleted {report['deleted_messages']} old messages\")\n",
    "    \n",
    "    # Handle GDPR requests\n",
    "    print(\"\\n--- GDPR Export ---\")\n",
    "    user_data = manager.export_user_data(\"user_123\")\n",
    "    print(f\"Exported {len(user_data['messages'])} messages for user_123\")\n",
    "    print(json.dumps(user_data, indent=2))\n",
    "    \n",
    "    print(\"\\n--- GDPR Delete ---\")\n",
    "    deleted = manager.delete_user_data(\"user_456\")\n",
    "    print(f\"Deleted {deleted} messages for user_456\")\n",
    "    \n",
    "    # Final stats\n",
    "    print(\"\\nFinal stats:\")\n",
    "    stats = manager.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: personal_assistant_challenge.py\n",
    "\n",
    "# From: AI Agents Book - Chapter 13\n",
    "# File: personal_assistant_challenge.py\n",
    "# Chapter 13 Challenge: Personal AI Assistant\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "Chapter 13 Challenge: Build a Complete Personal AI Assistant\n",
    "\n",
    "Requirements:\n",
    "1. Remember conversations across sessions (persistence)\n",
    "2. Track entities mentioned in conversations (people, projects, preferences)\n",
    "3. Use tools (at least 2: calculator and weather/time)\n",
    "4. Implement semantic search to recall relevant past conversations\n",
    "5. Auto-summarize when conversations get long (>500 tokens)\n",
    "6. Handle privacy with PII filtering\n",
    "7. Implement cleanup with 30-day retention policy\n",
    "8. Support multiple users with isolated memories\n",
    "\n",
    "Bonus Challenges:\n",
    "- Add a \"remember\" command for explicit fact storage\n",
    "- Implement importance-based cleanup (keep important convos longer)\n",
    "- Add conversation export feature (GDPR compliance)\n",
    "- Build a simple CLI or web interface\n",
    "- Track and report memory system metrics\n",
    "\n",
    "Time Estimate: 3-5 hours\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "# import chromadb  # Uncomment when implementing semantic search\n",
    "\n",
    "\n",
    "class PersonalAssistant:\n",
    "    \"\"\"Your personal AI assistant with production-ready memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.setup_memory()\n",
    "        self.setup_agent()\n",
    "    \n",
    "    def setup_memory(self):\n",
    "        \"\"\"Set up conversation memory, entity memory, vector memory.\"\"\"\n",
    "        # TODO: Initialize conversation memory (SQLite-backed)\n",
    "        # TODO: Initialize entity memory\n",
    "        # TODO: Initialize vector memory (ChromaDB)\n",
    "        # TODO: Initialize PII filter\n",
    "        # TODO: Set up retention policy\n",
    "        pass\n",
    "    \n",
    "    def setup_agent(self):\n",
    "        \"\"\"Create agent with tools and memory.\"\"\"\n",
    "        # TODO: Define tools (calculator, weather, etc.)\n",
    "        # TODO: Create prompt with memory context\n",
    "        # TODO: Set up agent with summarization\n",
    "        pass\n",
    "    \n",
    "    def chat(self, message):\n",
    "        \"\"\"Process message, use tools, store in memory.\"\"\"\n",
    "        # TODO: Filter PII from input\n",
    "        # TODO: Retrieve relevant memories\n",
    "        # TODO: Get entity context\n",
    "        # TODO: Run agent\n",
    "        # TODO: Store conversation in memory\n",
    "        # TODO: Extract and store entities\n",
    "        # TODO: Check if summarization needed\n",
    "        pass\n",
    "    \n",
    "    def remember(self, fact):\n",
    "        \"\"\"Store explicit fact in semantic memory.\"\"\"\n",
    "        # TODO: Add fact to vector database\n",
    "        pass\n",
    "    \n",
    "    def recall(self, query):\n",
    "        \"\"\"Search relevant past conversations.\"\"\"\n",
    "        # TODO: Semantic search in vector database\n",
    "        # TODO: Return relevant memories\n",
    "        pass\n",
    "    \n",
    "    def export_data(self):\n",
    "        \"\"\"Export all user data (GDPR compliance).\"\"\"\n",
    "        # TODO: Gather all user data\n",
    "        # TODO: Format for export\n",
    "        pass\n",
    "    \n",
    "    def delete_data(self):\n",
    "        \"\"\"Delete all user data (GDPR right to erasure).\"\"\"\n",
    "        # TODO: Delete from all memory systems\n",
    "        pass\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Run retention policy.\"\"\"\n",
    "        # TODO: Delete old conversations\n",
    "        # TODO: Report metrics\n",
    "        pass\n",
    "\n",
    "\n",
    "# Define your tools here\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a math expression like '2+2' or '10*5'.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a city.\"\"\"\n",
    "    # Mock implementation - replace with real API\n",
    "    weather_data = {\n",
    "        \"Seattle\": \"Rainy, 55\u00b0F\",\n",
    "        \"NYC\": \"Sunny, 68\u00b0F\",\n",
    "        \"Austin\": \"Hot, 95\u00b0F\",\n",
    "    }\n",
    "    return weather_data.get(city, f\"Weather in {city}: Partly cloudy, 70\u00b0F\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get current time in specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # Simple implementation - enhance with pytz for real timezones\n",
    "    return f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "\n",
    "# Test your assistant\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Chapter 13 Challenge: Personal AI Assistant\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThis is a starter template. Implement the TODO items!\")\n",
    "    print(\"\\nExpected behavior when complete:\")\n",
    "    print(\"  1. assistant.chat() processes messages with memory\")\n",
    "    print(\"  2. assistant.remember() stores facts\")\n",
    "    print(\"  3. assistant.recall() searches past conversations\")\n",
    "    print(\"  4. PII is filtered automatically\")\n",
    "    print(\"  5. Old conversations are cleaned up\")\n",
    "    print(\"  6. Multiple users have isolated memories\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Uncomment when implementing:\n",
    "    # assistant = PersonalAssistant(\"user_123\")\n",
    "    # assistant.chat(\"My name is Alice and I'm a Python developer.\")\n",
    "    # assistant.chat(\"What's the weather in Seattle?\")\n",
    "    # assistant.chat(\"What did I tell you about myself?\")\n",
    "    # assistant.remember(\"Alice prefers dark mode\")\n",
    "    # print(assistant.recall(\"preferences\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_13_agent_memory_solutions.ipynb**\n",
    "- Proceed to **Chapter 14**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}