{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Tools and Function Calling\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- Understanding tool use in agents\n",
    "- Creating custom tools\n",
    "- Built-in LangChain tools\n",
    "- Function calling with LLMs\n",
    "- Tool selection strategies\n",
    "- Error handling in tool execution\n",
    "- Building a multi-tool agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.1: Understanding tool use in agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: no_tools_comparison.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.1\n",
    "# File: no_tools_comparison.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load your OpenAI API key\n",
    "load_dotenv()\n",
    "\n",
    "# Create our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Without tools - the LLM can only guess\n",
    "print(\"WITHOUT TOOLS - The LLM tries its best:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"What is 15,847 * 3,921?\")\n",
    "])\n",
    "\n",
    "print(\"User: What is 15,847 * 3,921?\")\n",
    "print(f\"AI: {response.content}\")\n",
    "print(\"\\n(The AI might attempt the math, but could be wrong!)\")\n",
    "\n",
    "# Try another question that needs real-world data\n",
    "response2 = llm.invoke([\n",
    "    HumanMessage(content=\"What's the current temperature in Tokyo?\")\n",
    "])\n",
    "\n",
    "print(\"\\nUser: What's the current temperature in Tokyo?\")\n",
    "print(f\"AI: {response2.content}\")\n",
    "print(\"\\n(The AI can only guess or use outdated training data!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: first_tool_agent.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.1\n",
    "# File: first_tool_agent.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Use a pre-built tool (we'll learn to make our own in 12.2!)\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Create an agent with the search tool\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, [search], prompt)\n",
    "executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=[search], \n",
    "    verbose=True,  # Show the thinking process!\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "# Ask something that needs current information\n",
    "print(\"AGENT WITH SEARCH CAPABILITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = executor.invoke({\n",
    "    \"input\": \"What is Python programming language known for?\"\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal Answer: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1.1: Tool or No Tool?\n",
    "\n",
    "For each scenario below, decide whether you need a tool or if pure LLM is sufficient. Explain why!\n",
    "\n",
    "1. Write a haiku about summer\n",
    "2. Get today's date\n",
    "3. Explain quantum physics\n",
    "4. Check if a file exists\n",
    "5. Translate 'hello' to Spanish\n",
    "6. Find the latest news about AI\n",
    "7. Generate a business name\n",
    "8. Calculate compound interest\n",
    "\n",
    "For each one, answer:\n",
    "- **Tool or No Tool?**\n",
    "- **Why?** What makes you choose one over the other?\n",
    "- **If tool needed:** What kind of tool would you use?\n",
    "- **If no tool:** What would happen if you tried to use a tool anyway?\n",
    "\n",
    "Remember the golden rule: Current data and precise computation need tools; creativity and knowledge don't!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1.2: Trace the Tool Flow\n",
    "\n",
    "Given this conversation, trace what happens at each step behind the scenes:\n",
    "```\n",
    "User: \"Search for information about LangChain and calculate 15% of 2000\"\n",
    "\n",
    "AI: \"I'll help you with both tasks.\n",
    "\n",
    "LangChain is a framework for developing applications powered by \n",
    "language models. It was created by Harrison Chase and provides \n",
    "tools for building context-aware reasoning applications.\n",
    "\n",
    "15% of 2000 is 300.\"\n",
    "```\n",
    "\n",
    "Write out each step that happened:\n",
    "- Step 1: User sends message with two requests\n",
    "- Step 2: AI recognizes... [continue]\n",
    "- Step 3: ...\n",
    "- Step 4: ...\n",
    "\n",
    "Think about:\n",
    "- How did the AI know to use two different tools?\n",
    "- Which tool was called first? Why?\n",
    "- How did the AI combine the results into one response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1.3: Design a Tool Set\n",
    "\n",
    "You're building an AI assistant for a small business (a local bakery). Design 5 tools it would need:\n",
    "\n",
    "For each tool, describe:\n",
    "- **Tool name:** What you'd call it\n",
    "- **Purpose:** What it does\n",
    "- **When used:** Example user requests that would trigger it\n",
    "- **Inputs:** What information it needs\n",
    "- **Outputs:** What it returns\n",
    "- **Safety considerations:** What could go wrong and how to prevent it\n",
    "\n",
    "Example:\n",
    "- **Tool name:** `check_inventory`\n",
    "- **Purpose:** Check current stock levels of ingredients\n",
    "- **When used:** \"Do we have enough flour for tomorrow's orders?\"\n",
    "- **Inputs:** Ingredient name, optional date\n",
    "- **Outputs:** Current quantity and unit (e.g., \"150 pounds of flour\")\n",
    "- **Safety:** Read-only database access, validate ingredient names against known list\n",
    "\n",
    "Now design 5 tools that would make this bakery assistant truly helpful! Think about daily operations, customer service, and business management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.2: Creating custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: first_custom_tool.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: first_custom_tool.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Step 1: Create a regular Python function\n",
    "def greet(name: str) -> str:\n",
    "    \"\"\"Generate a friendly greeting.\"\"\"\n",
    "    return f\"Hello {name}! Welcome to the world of custom tools!\"\n",
    "\n",
    "# Step 2: Wrap it as a LangChain tool\n",
    "greeting_tool = Tool(\n",
    "    name=\"Greeter\",  # What the LLM will call this\n",
    "    func=greet,      # The function to run\n",
    "    description=\"Use this to greet someone by name\"  # When to use it\n",
    ")\n",
    "\n",
    "# Test it directly\n",
    "result = greeting_tool.func(\"Alice\")\n",
    "print(result)  # \"Hello Alice! Welcome to the world of custom tools!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: calculator_tool.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: calculator_tool.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import re\n",
    "\n",
    "def safe_calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    A calculator that safely evaluates mathematical expressions.\n",
    "    Returns the result or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the expression - remove spaces and commas\n",
    "        expression = expression.replace(\" \", \"\").replace(\",\", \"\")\n",
    "        \n",
    "        # Only allow numbers and basic math operators\n",
    "        if not re.match(r'^[0-9+\\-*/().\\s]+$', expression):\n",
    "            return \"Error: Only numbers and +, -, *, /, () allowed\"\n",
    "        \n",
    "        # Calculate the result\n",
    "        result = eval(expression)\n",
    "        \n",
    "        # Format nicely for large numbers\n",
    "        if isinstance(result, (int, float)):\n",
    "            if result > 1000:\n",
    "                return f\"{result:,.2f}\"\n",
    "            return str(result)\n",
    "            \n",
    "    except ZeroDivisionError:\n",
    "        return \"Error: Cannot divide by zero\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Invalid expression\"\n",
    "\n",
    "# Create the tool\n",
    "calculator = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=safe_calculator,\n",
    "    description=\"Performs mathematical calculations. Input should be a math expression like '2+2' or '100*3.14'\"\n",
    ")\n",
    "\n",
    "# Test it\n",
    "print(calculator.func(\"2 + 2\"))           # \"4\"\n",
    "print(calculator.func(\"1000 * 1.5\"))      # \"1,500.00\"\n",
    "print(calculator.func(\"10 / 0\"))          # \"Error: Cannot divide by zero\"\n",
    "print(calculator.func(\"hack()\"))          # \"Error: Only numbers and +, -, *, /, () allowed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_io_examples.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: tool_io_examples.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Tool with multiple inputs (passed as formatted string)\n",
    "def create_reminder(reminder_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a reminder with date and message.\n",
    "    Format: 'DATE|MESSAGE' like '2024-12-25|Christmas Day'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = reminder_text.split('|')\n",
    "        if len(parts) != 2:\n",
    "            return \"Error: Use format 'DATE|MESSAGE'\"\n",
    "        \n",
    "        date_str, message = parts\n",
    "        # Validate date\n",
    "        reminder_date = datetime.strptime(date_str.strip(), '%Y-%m-%d')\n",
    "        \n",
    "        return f\"\u2713 Reminder set for {date_str}: {message}\"\n",
    "    except ValueError:\n",
    "        return \"Error: Invalid date format. Use YYYY-MM-DD\"\n",
    "\n",
    "# Tool that returns structured data (as formatted string)\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city (simulated).\"\"\"\n",
    "    # In real life, this would call an API\n",
    "    weather_data = {\n",
    "        \"New York\": {\"temp\": 72, \"condition\": \"Sunny\", \"humidity\": 65},\n",
    "        \"London\": {\"temp\": 59, \"condition\": \"Cloudy\", \"humidity\": 80},\n",
    "        \"Tokyo\": {\"temp\": 68, \"condition\": \"Clear\", \"humidity\": 55}\n",
    "    }\n",
    "    \n",
    "    if city in weather_data:\n",
    "        data = weather_data[city]\n",
    "        return f\"Weather in {city}: {data['temp']}\u00b0F, {data['condition']}, {data['humidity']}% humidity\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {city}\"\n",
    "\n",
    "# Create the tools\n",
    "reminder_tool = Tool(\n",
    "    name=\"ReminderCreator\",\n",
    "    func=create_reminder,\n",
    "    description=\"Create a reminder. Input format: 'YYYY-MM-DD|message text'\"\n",
    ")\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"WeatherChecker\", \n",
    "    func=get_weather,\n",
    "    description=\"Get current weather for a city. Input: city name\"\n",
    ")\n",
    "\n",
    "# Test them\n",
    "print(reminder_tool.func(\"2024-12-25|Christmas Day\"))\n",
    "print(weather_tool.func(\"New York\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: description_importance.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: description_importance.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "def same_function(text: str) -> str:\n",
    "    \"\"\"This function does text analysis.\"\"\"\n",
    "    words = text.split()\n",
    "    return f\"Word count: {len(words)}\"\n",
    "\n",
    "# Same function, different descriptions\n",
    "vague_tool = Tool(\n",
    "    name=\"Analyzer\",\n",
    "    func=same_function,\n",
    "    description=\"Analyzes text\"  # Too vague!\n",
    ")\n",
    "\n",
    "clear_tool = Tool(\n",
    "    name=\"WordCounter\",\n",
    "    func=same_function,\n",
    "    description=\"Counts the number of words in text. Use when someone asks for word count, length, or size of text.\"\n",
    ")\n",
    "\n",
    "# The LLM will reliably use clear_tool for \"How many words are in this?\"\n",
    "# But might not use vague_tool for the same question!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: bulletproof_tool.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: bulletproof_tool.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import re\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "def bulletproof_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    A production-ready search tool with all safety features.\n",
    "    \"\"\"\n",
    "    # 1. Input validation\n",
    "    if not query or not isinstance(query, str):\n",
    "        return \"Error: Please provide a search query\"\n",
    "    \n",
    "    # 2. Length limits\n",
    "    if len(query) > 200:\n",
    "        return \"Error: Query too long (max 200 characters)\"\n",
    "    \n",
    "    # 3. Clean dangerous characters\n",
    "    query = re.sub(r'[^\\w\\s\\-.]', '', query)\n",
    "    query = query.strip()\n",
    "    \n",
    "    if not query:\n",
    "        return \"Error: Invalid search query\"\n",
    "    \n",
    "    # 4. Rate limiting (in production, use proper rate limiting)\n",
    "    time.sleep(0.1)  # Prevent hammering\n",
    "    \n",
    "    try:\n",
    "        # 5. Timeout for external calls\n",
    "        # In real tool: requests.get(url, timeout=5)\n",
    "        \n",
    "        # 6. Simulated search results\n",
    "        results = f\"Search results for '{query}': Found 3 relevant articles...\"\n",
    "        \n",
    "        # 7. Output sanitization\n",
    "        return results[:500]  # Limit response size\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 8. Generic error handling\n",
    "        return \"Error: Search temporarily unavailable\"\n",
    "\n",
    "# Create production-ready tool\n",
    "search_tool = Tool(\n",
    "    name=\"WebSearch\",\n",
    "    func=bulletproof_search,\n",
    "    description=\"Search the web for information. Input: search query (max 200 chars)\"\n",
    ")\n",
    "\n",
    "# Test edge cases\n",
    "print(search_tool.func(\"\"))                    # Error handling\n",
    "print(search_tool.func(\"Normal search\"))       # Success\n",
    "print(search_tool.func(\"x\" * 300))            # Length limit\n",
    "print(search_tool.func(\"'; DROP TABLE;\"))      # SQL injection attempt - cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: api_tool_pattern.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: api_tool_pattern.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def create_api_tool(endpoint_name: str):\n",
    "    \"\"\"\n",
    "    Factory function to create API tools.\n",
    "    This is a pattern - adapt for your specific APIs!\n",
    "    \"\"\"\n",
    "    def api_caller(parameters: str) -> str:\n",
    "        # Check for API key\n",
    "        api_key = os.getenv(f\"{endpoint_name.upper()}_API_KEY\")\n",
    "        if not api_key:\n",
    "            return f\"Error: {endpoint_name} API key not configured\"\n",
    "        \n",
    "        try:\n",
    "            # In real implementation:\n",
    "            # import requests\n",
    "            # response = requests.get(\n",
    "            #     f\"https://api.{endpoint_name}.com/v1/query\",\n",
    "            #     params={\"q\": parameters},\n",
    "            #     headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
    "            #     timeout=10\n",
    "            # )\n",
    "            # return response.json()\n",
    "            \n",
    "            # Simulated response\n",
    "            return f\"API response for {parameters} from {endpoint_name}\"\n",
    "            \n",
    "        except TimeoutError:\n",
    "            return f\"Error: {endpoint_name} API timeout\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {endpoint_name} API failed\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=f\"{endpoint_name.title()}API\",\n",
    "        func=api_caller,\n",
    "        description=f\"Query the {endpoint_name} API. Input: query parameters\"\n",
    "    )\n",
    "\n",
    "# Create tools for different APIs\n",
    "weather_api = create_api_tool(\"weather\")\n",
    "news_api = create_api_tool(\"news\")\n",
    "stock_api = create_api_tool(\"stocks\")\n",
    "\n",
    "# Each tool follows the same pattern but connects to different services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_composition.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.2\n",
    "# File: tool_composition.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import json\n",
    "\n",
    "# Tool 1: Fetches data\n",
    "def fetch_data(source: str) -> str:\n",
    "    \"\"\"Fetch data from a source.\"\"\"\n",
    "    # Simulated data fetching\n",
    "    data = {\n",
    "        \"sales\": [100, 150, 120, 180, 200],\n",
    "        \"costs\": [80, 90, 85, 95, 100]\n",
    "    }\n",
    "    return json.dumps(data)\n",
    "\n",
    "# Tool 2: Analyzes data\n",
    "def analyze_data(json_data: str) -> str:\n",
    "    \"\"\"Analyze data and return insights.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_data)\n",
    "        sales = data.get(\"sales\", [])\n",
    "        costs = data.get(\"costs\", [])\n",
    "        \n",
    "        total_sales = sum(sales)\n",
    "        total_costs = sum(costs)\n",
    "        profit = total_sales - total_costs\n",
    "        margin = (profit / total_sales * 100) if total_sales > 0 else 0\n",
    "        \n",
    "        return f\"Analysis: Total Sales: ${total_sales}, Total Costs: ${total_costs}, Profit: ${profit}, Margin: {margin:.1f}%\"\n",
    "    except:\n",
    "        return \"Error: Invalid data format for analysis\"\n",
    "\n",
    "# Tool 3: Formats reports\n",
    "def format_report(analysis: str) -> str:\n",
    "    \"\"\"Format analysis into a nice report.\"\"\"\n",
    "    if \"Error\" in analysis:\n",
    "        return analysis\n",
    "    \n",
    "    report = f\"\"\"\n",
    "    \ud83d\udcca BUSINESS REPORT\n",
    "    ==================\n",
    "    {analysis}\n",
    "    \n",
    "    Status: \u2705 Profitable\n",
    "    Recommendation: Continue current strategy\n",
    "    \"\"\"\n",
    "    return report.strip()\n",
    "\n",
    "# Create the tool chain\n",
    "fetch_tool = Tool(name=\"DataFetcher\", func=fetch_data, \n",
    "                  description=\"Fetch business data from a source\")\n",
    "analyze_tool = Tool(name=\"DataAnalyzer\", func=analyze_data,\n",
    "                    description=\"Analyze JSON data and return insights\")\n",
    "format_tool = Tool(name=\"ReportFormatter\", func=format_report,\n",
    "                   description=\"Format analysis into a professional report\")\n",
    "\n",
    "# These tools naturally work together:\n",
    "# 1. Fetch data \u2192 2. Analyze it \u2192 3. Format report\n",
    "# The LLM orchestrates this flow automatically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2.1: Unit Converter Tool (Easy)\n",
    "\n",
    "Create a tool that converts between units (meters to feet, kg to pounds, etc.):\n",
    "- Handle at least 3 types of conversions\n",
    "- Parse input like \"5 meters to feet\"\n",
    "- Return clear, formatted results\n",
    "- Handle invalid conversions gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2.2: Text Processor Tool Suite (Medium)\n",
    "\n",
    "Build three related tools that work together:\n",
    "1. `text_stats`: Returns word count, sentence count, average word length\n",
    "2. `extract_keywords`: Finds the 5 most common meaningful words\n",
    "3. `summarize`: Creates a one-sentence summary\n",
    "\n",
    "Make sure the outputs of one can be inputs to another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2.3: Smart File Manager (Hard)\n",
    "\n",
    "Create a tool that manages a simple file system:\n",
    "- Commands: \"create file.txt with [content]\", \"read file.txt\", \"list files\", \"delete file.txt\"\n",
    "- Store files in a dictionary (simulate a file system)\n",
    "- Add safety: prevent overwriting without confirmation\n",
    "- Return user-friendly messages\n",
    "- Bonus: Add a search function to find files containing specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.3: Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: exploring_duckduckgo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.3\n",
    "# File: exploring_duckduckgo.py\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Initialize the search tool - that's it!\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Test it directly - just like calling a function!\n",
    "print(\"Testing DuckDuckGo Search Tool\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Search for something current\n",
    "result = search_tool.run(\"Python programming language latest version 2024\")\n",
    "print(\"Search for Python version:\")\n",
    "print(result[:300] + \"...\")  # First 300 chars\n",
    "print()\n",
    "\n",
    "# Search for news\n",
    "news_result = search_tool.run(\"artificial intelligence breakthrough today\")\n",
    "print(\"Search for AI news:\")\n",
    "print(news_result[:300] + \"...\")\n",
    "print()\n",
    "\n",
    "# Search for factual information\n",
    "fact_result = search_tool.run(\"height of Mount Everest in meters\")\n",
    "print(\"Search for facts:\")\n",
    "print(fact_result[:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: exploring_wikipedia.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.3\n",
    "# File: exploring_wikipedia.py\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# Configure Wikipedia tool\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=1,  # Number of results to return\n",
    "        doc_content_chars_max=1000  # Max characters per result\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Testing Wikipedia Tool\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Look up a programming language\n",
    "result = wikipedia_tool.run(\"Python (programming language)\")\n",
    "print(\"Python programming language:\")\n",
    "print(result[:400] + \"...\")\n",
    "print()\n",
    "\n",
    "# Look up a concept\n",
    "ml_result = wikipedia_tool.run(\"Machine Learning\")\n",
    "print(\"Machine Learning:\")\n",
    "# Get just the first two sentences\n",
    "sentences = ml_result.split('. ')[:2]\n",
    "print('. '.join(sentences) + '.')\n",
    "print()\n",
    "\n",
    "# Look up a person\n",
    "turing_result = wikipedia_tool.run(\"Alan Turing\")\n",
    "print(\"Alan Turing:\")\n",
    "sentences = turing_result.split('. ')[:2]\n",
    "print('. '.join(sentences) + '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: exploring_file_tools.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.3\n",
    "# File: exploring_file_tools.py\n",
    "\n",
    "from langchain_community.tools import (\n",
    "    WriteFileTool,\n",
    "    ReadFileTool,\n",
    "    ListDirectoryTool\n",
    ")\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory to work in safely\n",
    "working_dir = tempfile.mkdtemp(prefix=\"tools_exploration_\")\n",
    "print(f\"Working directory: {working_dir}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize file tools with the working directory\n",
    "write_tool = WriteFileTool(root_dir=working_dir)\n",
    "read_tool = ReadFileTool(root_dir=working_dir)\n",
    "list_tool = ListDirectoryTool(root_dir=working_dir)\n",
    "\n",
    "# Test 1: Write a file\n",
    "print(\"\\n1. Testing WriteFileTool:\")\n",
    "write_result = write_tool.run({\n",
    "    \"file_path\": \"test_note.txt\",\n",
    "    \"text\": \"Hello from LangChain tools!\\nThis is a test file.\"\n",
    "})\n",
    "print(f\"   Result: {write_result}\")\n",
    "\n",
    "# Test 2: List directory contents\n",
    "print(\"\\n2. Testing ListDirectoryTool:\")\n",
    "list_result = list_tool.run({\"dir_path\": \".\"})\n",
    "print(f\"   Contents: {list_result}\")\n",
    "\n",
    "# Test 3: Read the file back\n",
    "print(\"\\n3. Testing ReadFileTool:\")\n",
    "read_result = read_tool.run({\"file_path\": \"test_note.txt\"})\n",
    "print(f\"   File contents: {read_result}\")\n",
    "\n",
    "# Test 4: Create a more complex file\n",
    "print(\"\\n4. Creating a Python script:\")\n",
    "python_code = '''def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(greet(\"World\"))\n",
    "'''\n",
    "write_tool.run({\n",
    "    \"file_path\": \"hello.py\",\n",
    "    \"text\": python_code\n",
    "})\n",
    "print(\"   Created hello.py\")\n",
    "\n",
    "# List files again\n",
    "final_list = list_tool.run({\"dir_path\": \".\"})\n",
    "print(f\"\\n5. Final directory contents: {final_list}\")\n",
    "\n",
    "# Clean up\n",
    "print(f\"\\nFiles created in: {working_dir}\")\n",
    "print(\"(This is a temporary directory)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: exploring_python_repl.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.3\n",
    "# File: exploring_python_repl.py\n",
    "\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# Create the Python REPL tool\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "print(\"Testing Python REPL Tool\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\u26a0\ufe0f  This tool can execute ANY Python code - use with caution!\")\n",
    "print()\n",
    "\n",
    "# Test 1: Simple calculation\n",
    "print(\"1. Simple math:\")\n",
    "result = python_tool.run(\"print(2 ** 10)\")\n",
    "print(f\"   2^10 = {result}\")\n",
    "print()\n",
    "\n",
    "# Test 2: Generate Fibonacci sequence\n",
    "print(\"2. Fibonacci sequence:\")\n",
    "fibonacci_code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    elif n == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fib = [0, 1]\n",
    "        for i in range(2, n):\n",
    "            fib.append(fib[-1] + fib[-2])\n",
    "        return fib\n",
    "\n",
    "result = fibonacci(10)\n",
    "print(f\"First 10 Fibonacci numbers: {result}\")\n",
    "print(f\"Sum: {sum(result)}\")\n",
    "\"\"\"\n",
    "result = python_tool.run(fibonacci_code)\n",
    "print(f\"   {result}\")\n",
    "print()\n",
    "\n",
    "# Test 3: Data analysis\n",
    "print(\"3. Data analysis:\")\n",
    "analysis_code = \"\"\"\n",
    "data = [23, 45, 67, 89, 12, 34, 56, 78, 90, 11]\n",
    "mean = sum(data) / len(data)\n",
    "maximum = max(data)\n",
    "minimum = min(data)\n",
    "print(f\"Data: {data}\")\n",
    "print(f\"Mean: {mean:.2f}, Max: {maximum}, Min: {minimum}\")\n",
    "\"\"\"\n",
    "result = python_tool.run(analysis_code)\n",
    "print(f\"   {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tools_working_together.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.3\n",
    "# File: tools_working_together.py\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WriteFileTool\n",
    "import tempfile\n",
    "\n",
    "# Initialize our tools\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
    ")\n",
    "working_dir = tempfile.mkdtemp(prefix=\"research_\")\n",
    "write_tool = WriteFileTool(root_dir=working_dir)\n",
    "\n",
    "print(\"Demonstrating How Tools Complement Each Other\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Research task: Learn about a topic from multiple angles\n",
    "topic = \"Large Language Models\"\n",
    "\n",
    "# Step 1: Get established facts from Wikipedia\n",
    "print(f\"\\n1. Wikipedia (Established Facts) about {topic}:\")\n",
    "wiki_result = wikipedia_tool.run(topic)\n",
    "wiki_summary = wiki_result[:200] + \"...\"\n",
    "print(wiki_summary)\n",
    "\n",
    "# Step 2: Get current developments from search\n",
    "print(f\"\\n2. Web Search (Recent News) about {topic}:\")\n",
    "search_query = f\"{topic} breakthrough 2024 news\"\n",
    "search_result = search_tool.run(search_query)\n",
    "search_summary = search_result[:200] + \"...\"\n",
    "print(search_summary)\n",
    "\n",
    "# Step 3: Save the combined research\n",
    "print(f\"\\n3. Saving Research to File:\")\n",
    "research_report = f\"\"\"Research Report: {topic}\n",
    "Generated on: {tempfile.gettempdir()}\n",
    "\n",
    "ESTABLISHED FACTS (Wikipedia):\n",
    "{wiki_result[:400]}\n",
    "\n",
    "RECENT DEVELOPMENTS (Web Search):\n",
    "{search_result[:400]}\n",
    "\n",
    "Summary: This report combines foundational knowledge with current developments.\n",
    "\"\"\"\n",
    "\n",
    "write_tool.run({\n",
    "    \"file_path\": \"research_report.txt\",\n",
    "    \"text\": research_report\n",
    "})\n",
    "print(f\"   Report saved to: {working_dir}/research_report.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Notice how each tool serves a different purpose:\")\n",
    "print(\"- Wikipedia: Authoritative, stable information\")\n",
    "print(\"- Web Search: Current events and recent developments\")\n",
    "print(\"- File Tools: Persistence and report generation\")\n",
    "print(\"\\nWhen agents use these together, they become research assistants!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3.1: Tool Explorer (Easy)\n",
    "\n",
    "Test each tool type and document what it returns:\n",
    "- Use DuckDuckGo to search for 3 different types of queries (news, facts, tutorials)\n",
    "- Use Wikipedia to look up 3 topics (person, place, concept)\n",
    "- Compare the type and quality of information each provides\n",
    "- Which tool would be better for which type of question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3.2: Research Workflow (Medium)\n",
    "\n",
    "Create a research workflow using tools directly (not with an agent):\n",
    "- Pick a technology topic (like \"quantum computing\" or \"blockchain\")\n",
    "- Use Wikipedia to get foundational information\n",
    "- Use DuckDuckGo to find recent developments\n",
    "- Use file tools to create a structured report\n",
    "- Save the report with clear sections for \"Background\" and \"Recent News\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.3.3: Data Processing Pipeline (Hard)\n",
    "\n",
    "Build a data processing workflow:\n",
    "- Use the Python REPL tool to generate sample data (like 100 random numbers)\n",
    "- Use the Python REPL to calculate statistics (mean, median, mode, std deviation)\n",
    "- Create a formatted report of the analysis\n",
    "- Use file tools to save both the raw data and the analysis report\n",
    "- Test edge cases: What happens with empty data? Invalid calculations?\n",
    "\n",
    "\n",
    "Remember: We're exploring these tools directly for now. In the next section, you'll learn to connect them to LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.4: Function calling with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: function_calling_intro.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: function_calling_intro.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the input schema\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city or location to get weather for\")\n",
    "    unit: str = Field(default=\"fahrenheit\", description=\"Temperature unit: 'fahrenheit' or 'celsius'\")\n",
    "\n",
    "# Define a simple function\n",
    "def get_weather(location: str, unit: str = \"fahrenheit\") -> str:\n",
    "    \"\"\"Get the weather for a location.\"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"New York\": {\"f\": 72, \"c\": 22},\n",
    "        \"London\": {\"f\": 59, \"c\": 15},\n",
    "        \"Tokyo\": {\"f\": 68, \"c\": 20}\n",
    "    }\n",
    "    \n",
    "    if location in weather_data:\n",
    "        temp = weather_data[location][\"c\" if unit == \"celsius\" else \"f\"]\n",
    "        unit_symbol = \"\u00b0C\" if unit == \"celsius\" else \"\u00b0F\"\n",
    "        return f\"The weather in {location} is {temp}{unit_symbol}\"\n",
    "    return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Convert to a tool with schema\n",
    "weather_tool = Tool(\n",
    "    name=\"get_weather\",\n",
    "    func=get_weather,\n",
    "    description=\"Get weather for a location\",\n",
    "    args_schema=WeatherInput  # THIS IS CRITICAL!\n",
    ")\n",
    "\n",
    "# Modern way: Bind tools directly to the model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([weather_tool])\n",
    "\n",
    "# Ask a question that needs the tool\n",
    "response = llm_with_tools.invoke([\n",
    "    HumanMessage(content=\"What's the weather in New York?\")\n",
    "])\n",
    "\n",
    "print(\"LLM Response:\", response)\n",
    "\n",
    "# If the LLM wants to use a tool, it tells us EXACTLY how\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    print(f\"\\nTool to call: {tool_call['name']}\")\n",
    "    print(f\"Arguments: {tool_call['args']}\")\n",
    "    \n",
    "    # Execute the function with the exact arguments\n",
    "    result = get_weather(**tool_call['args'])\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: structured_inputs.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: structured_inputs.py\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the structure of your inputs using Pydantic\n",
    "class FlightSearchInput(BaseModel):\n",
    "    origin: str = Field(description=\"Departure city\")\n",
    "    destination: str = Field(description=\"Arrival city\")  \n",
    "    date: str = Field(description=\"Date in YYYY-MM-DD format\")\n",
    "    passengers: int = Field(default=1, description=\"Number of passengers\")\n",
    "    class_type: Optional[str] = Field(default=\"economy\", description=\"economy, business, or first\")\n",
    "\n",
    "def search_flights(\n",
    "    origin: str,\n",
    "    destination: str,\n",
    "    date: str,\n",
    "    passengers: int = 1,\n",
    "    class_type: str = \"economy\"\n",
    ") -> str:\n",
    "    \"\"\"Search for flights based on criteria.\"\"\"\n",
    "    # Simulated search\n",
    "    price = 200 * passengers\n",
    "    if class_type == \"business\":\n",
    "        price *= 3\n",
    "    elif class_type == \"first\":\n",
    "        price *= 5\n",
    "    \n",
    "    return f\"Found flights from {origin} to {destination} on {date}: ${price} for {passengers} passenger(s) in {class_type}\"\n",
    "\n",
    "# Create a structured tool\n",
    "flight_tool = StructuredTool.from_function(\n",
    "    func=search_flights,\n",
    "    name=\"search_flights\",\n",
    "    description=\"Search for available flights\",\n",
    "    args_schema=FlightSearchInput\n",
    ")\n",
    "\n",
    "# Use with function calling\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([flight_tool])\n",
    "\n",
    "# Complex request\n",
    "response = llm_with_tools.invoke([\n",
    "    HumanMessage(content=\"Find me business class flights from New York to London on March 15th, 2024 for 2 people\")\n",
    "])\n",
    "\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    print(\"Structured input parsed perfectly:\")\n",
    "    for key, value in tool_call['args'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Execute with confidence - parameters are guaranteed correct!\n",
    "    result = search_flights(**tool_call['args'])\n",
    "    print(f\"\\nResult: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: reliable_selection.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: reliable_selection.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define multiple tools with clear purposes\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Perform mathematical calculations.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Calculation error\"\n",
    "\n",
    "def translate(text: str, target_language: str) -> str:\n",
    "    \"\"\"Translate text to another language.\"\"\"\n",
    "    # Simulated translation\n",
    "    translations = {\n",
    "        \"spanish\": {\"hello\": \"hola\", \"goodbye\": \"adi\u00f3s\"},\n",
    "        \"french\": {\"hello\": \"bonjour\", \"goodbye\": \"au revoir\"}\n",
    "    }\n",
    "    # Simple word lookup (real implementation would use an API)\n",
    "    return f\"Translation to {target_language}: {text} \u2192 ...\"\n",
    "\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for current information.\"\"\"\n",
    "    return f\"Search results for '{query}': Latest news and updates...\"\n",
    "\n",
    "# Create tools\n",
    "tools = [\n",
    "    Tool(name=\"calculator\", func=calculate, description=\"For math calculations\"),\n",
    "    Tool(name=\"translator\", func=translate, description=\"For language translation\"),\n",
    "    Tool(name=\"search\", func=search, description=\"For current information\")\n",
    "]\n",
    "\n",
    "# Bind all tools\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Test different queries\n",
    "test_queries = [\n",
    "    \"What is 25 * 4?\",\n",
    "    \"How do you say hello in Spanish?\",\n",
    "    \"What's the latest news about AI?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    response = llm_with_tools.invoke([HumanMessage(content=query)])\n",
    "    \n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        print(f\"Selected tool: {tool_call['name']}\")\n",
    "        print(f\"Confidence: High (structured selection)\")\n",
    "    else:\n",
    "        print(\"No tool needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_response_flow.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: tool_response_flow.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define input schema for stock price tool\n",
    "class StockInput(BaseModel):\n",
    "    symbol: str = Field(description=\"Stock ticker symbol (e.g., AAPL, GOOGL, MSFT)\")\n",
    "\n",
    "def get_stock_price(symbol: str) -> str:\n",
    "    \"\"\"Get current stock price.\"\"\"\n",
    "    # Simulated prices\n",
    "    prices = {\n",
    "        \"AAPL\": 178.50,\n",
    "        \"GOOGL\": 142.30,\n",
    "        \"MSFT\": 405.20\n",
    "    }\n",
    "    return json.dumps({\n",
    "        \"symbol\": symbol,\n",
    "        \"price\": prices.get(symbol.upper(), 0),\n",
    "        \"currency\": \"USD\"\n",
    "    })\n",
    "\n",
    "# Create tool with proper args_schema\n",
    "stock_tool = Tool(\n",
    "    name=\"get_stock_price\",\n",
    "    func=get_stock_price,\n",
    "    description=\"Get current stock price for a symbol\",\n",
    "    args_schema=StockInput  # Required for bind_tools!\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([stock_tool])\n",
    "\n",
    "# Start conversation\n",
    "messages = [\n",
    "    HumanMessage(content=\"What's the current price of Apple stock?\")\n",
    "]\n",
    "\n",
    "# Step 1: LLM decides to use a tool\n",
    "ai_response = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_response)\n",
    "\n",
    "print(\"Step 1 - LLM wants to use a tool:\")\n",
    "if ai_response.tool_calls:\n",
    "    print(f\"  Tool: {ai_response.tool_calls[0]['name']}\")\n",
    "    print(f\"  Args: {ai_response.tool_calls[0]['args']}\")\n",
    "    \n",
    "    # Step 2: Execute the tool\n",
    "    tool_call = ai_response.tool_calls[0]\n",
    "    tool_result = get_stock_price(**tool_call['args'])\n",
    "    \n",
    "    # Step 3: Send tool result back to LLM\n",
    "    tool_message = ToolMessage(\n",
    "        content=tool_result,\n",
    "        tool_call_id=tool_call['id']\n",
    "    )\n",
    "    messages.append(tool_message)\n",
    "    \n",
    "    print(\"\\nStep 2 - Tool execution result:\")\n",
    "    print(f\"  {tool_result}\")\n",
    "    \n",
    "    # Step 4: LLM incorporates result into final response\n",
    "    final_response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    print(\"\\nStep 3 - Final response to user:\")\n",
    "    print(f\"  {final_response.content}\")\n",
    "else:\n",
    "    print(\"No tool calls were made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: parallel_tools.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: parallel_tools.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define input schema for city-based tools\n",
    "class CityInput(BaseModel):\n",
    "    city: str = Field(description=\"Name of the city\")\n",
    "\n",
    "# Define multiple tools\n",
    "def get_temperature(city: str) -> str:\n",
    "    temps = {\"New York\": 72, \"London\": 59, \"Tokyo\": 68}\n",
    "    return f\"{temps.get(city, 'Unknown')}\u00b0F\"\n",
    "\n",
    "def get_time(city: str) -> str:\n",
    "    times = {\"New York\": \"10:30 AM\", \"London\": \"3:30 PM\", \"Tokyo\": \"11:30 PM\"}\n",
    "    return times.get(city, \"Unknown\")\n",
    "\n",
    "def get_population(city: str) -> str:\n",
    "    pops = {\"New York\": \"8.3M\", \"London\": \"9.5M\", \"Tokyo\": \"14M\"}\n",
    "    return pops.get(city, \"Unknown\")\n",
    "\n",
    "# Create tools with proper args_schema\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"get_temperature\",\n",
    "        func=get_temperature,\n",
    "        description=\"Get temperature for a city\",\n",
    "        args_schema=CityInput  # Required for bind_tools!\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_time\",\n",
    "        func=get_time,\n",
    "        description=\"Get current time for a city\",\n",
    "        args_schema=CityInput  # Required for bind_tools!\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"get_population\",\n",
    "        func=get_population,\n",
    "        description=\"Get population for a city\",\n",
    "        args_schema=CityInput  # Required for bind_tools!\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Ask for multiple pieces of information\n",
    "response = llm_with_tools.invoke([\n",
    "    HumanMessage(content=\"Tell me the temperature, time, and population of New York\")\n",
    "])\n",
    "\n",
    "print(\"Parallel tool calls:\")\n",
    "if response.tool_calls:\n",
    "    for i, tool_call in enumerate(response.tool_calls, 1):\n",
    "        print(f\"\\nTool Call {i}:\")\n",
    "        print(f\"  Function: {tool_call['name']}\")\n",
    "        print(f\"  Arguments: {tool_call['args']}\")\n",
    "        \n",
    "        # Execute the function properly\n",
    "        if tool_call['name'] == 'get_temperature':\n",
    "            result = get_temperature(**tool_call['args'])\n",
    "        elif tool_call['name'] == 'get_time':\n",
    "            result = get_time(**tool_call['args'])\n",
    "        elif tool_call['name'] == 'get_population':\n",
    "            result = get_population(**tool_call['args'])\n",
    "        else:\n",
    "            result = \"Unknown tool\"\n",
    "        \n",
    "        print(f\"  Result: {result}\")\n",
    "\n",
    "print(\"\\nAll three tools called in parallel - much faster than sequential!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: function_error_handling.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.4\n",
    "# File: function_error_handling.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define input schema for the function\n",
    "class DivideInput(BaseModel):\n",
    "    a: float = Field(description=\"The dividend (number to be divided)\")\n",
    "    b: float = Field(description=\"The divisor (number to divide by)\")\n",
    "\n",
    "def divide_numbers(a: float, b: float) -> str:\n",
    "    \"\"\"Safely divide two numbers with error handling.\"\"\"\n",
    "    if b == 0:\n",
    "        # Return structured error that LLM can understand\n",
    "        return json.dumps({\n",
    "            \"success\": False,\n",
    "            \"error\": \"Division by zero\",\n",
    "            \"suggestion\": \"Please provide a non-zero divisor\"\n",
    "        })\n",
    "    \n",
    "    result = a / b\n",
    "    return json.dumps({\n",
    "        \"success\": True,\n",
    "        \"result\": result,\n",
    "        \"calculation\": f\"{a} \u00f7 {b} = {result}\"\n",
    "    })\n",
    "\n",
    "# Create tool with proper args_schema\n",
    "calc_tool = Tool(\n",
    "    name=\"divide_numbers\",\n",
    "    func=divide_numbers,\n",
    "    description=\"Divide two numbers safely with error handling\",\n",
    "    args_schema=DivideInput  # THIS IS REQUIRED FOR bind_tools!\n",
    ")\n",
    "\n",
    "# Bind tool to LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tool = llm.bind_tools([calc_tool])\n",
    "\n",
    "# Test cases including error scenarios\n",
    "test_cases = [\n",
    "    \"What is 100 divided by 5?\",\n",
    "    \"Divide 50 by 0\",  # This will trigger error handling\n",
    "    \"Calculate 15.5 divided by 2.5\"\n",
    "]\n",
    "\n",
    "for query in test_cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('-'*60)\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm_with_tool.invoke([HumanMessage(content=query)])\n",
    "    \n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        print(f\"Tool called: {tool_call['name']}\")\n",
    "        print(f\"Arguments: {tool_call['args']}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        result = divide_numbers(**tool_call['args'])\n",
    "        print(f\"Tool result: {result}\")\n",
    "        \n",
    "        # Parse result to check for errors\n",
    "        result_data = json.loads(result)\n",
    "        \n",
    "        # Send result back to LLM for final response\n",
    "        tool_message = ToolMessage(\n",
    "            content=result,\n",
    "            tool_call_id=tool_call['id']\n",
    "        )\n",
    "        \n",
    "        final_response = llm_with_tool.invoke([\n",
    "            HumanMessage(content=query),\n",
    "            response,\n",
    "            tool_message\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nFinal answer: {final_response.content}\")\n",
    "        \n",
    "        # Show how the error was handled\n",
    "        if not result_data[\"success\"]:\n",
    "            print(f\"\u26a0\ufe0f Error handled gracefully: {result_data['error']}\")\n",
    "            print(f\"\ud83d\udca1 Suggestion: {result_data['suggestion']}\")\n",
    "    else:\n",
    "        print(\"No tool was called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.4.1: Weather Assistant with Units (Easy)\n",
    "\n",
    "Create a weather tool that:\n",
    "- Accepts city and temperature unit (Celsius/Fahrenheit)\n",
    "- Uses function calling for structured inputs\n",
    "- Returns formatted weather data\n",
    "- Test with: \"What's the weather in London in Celsius?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.4.2: Multi-Step Calculator (Medium)\n",
    "\n",
    "Build a calculator that can:\n",
    "- Handle multiple operations in one request\n",
    "- Use parallel function calls for independent calculations\n",
    "- Return all results together\n",
    "- Test with: \"Calculate 15\\*4, 100/5, and 78+22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.4.3: Smart Task Manager (Hard)\n",
    "\n",
    "Create a task management system with function calling:\n",
    "- Add tasks with title, priority, and due date\n",
    "- List tasks (all, by priority, by date)\n",
    "- Mark tasks complete\n",
    "- Use structured inputs for all operations\n",
    "- Handle errors gracefully (invalid dates, missing tasks)\n",
    "- Test with complex requests like \"Add a high priority task to call mom tomorrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.5: Tool selection strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_selection_basics.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.5\n",
    "# File: tool_selection_basics.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create tools with varying specificity\n",
    "def general_search(query: str) -> str:\n",
    "    return f\"General web results for: {query}\"\n",
    "\n",
    "def news_search(query: str) -> str:\n",
    "    return f\"Latest news about: {query}\"\n",
    "\n",
    "def academic_search(query: str) -> str:\n",
    "    return f\"Academic papers about: {query}\"\n",
    "\n",
    "def weather_check(location: str) -> str:\n",
    "    return f\"Weather in {location}: Sunny, 72\u00b0F\"\n",
    "\n",
    "# Create tools with clear, specific descriptions\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"general_search\",\n",
    "        func=general_search,\n",
    "        description=\"Search the web for any kind of information\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"news_search\",\n",
    "        func=news_search,\n",
    "        description=\"Search specifically for recent news and current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"academic_search\",\n",
    "        func=academic_search,\n",
    "        description=\"Search for academic papers, research, and scholarly articles\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"weather\",\n",
    "        func=weather_check,\n",
    "        description=\"Get current weather for a specific location\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create LLM with tools\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Test different queries to see tool selection\n",
    "test_queries = [\n",
    "    \"What's the weather in Paris?\",\n",
    "    \"Find recent news about AI\",\n",
    "    \"Search for information about Python\",\n",
    "    \"Find academic research on machine learning\"\n",
    "]\n",
    "\n",
    "print(\"TOOL SELECTION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    response = llm_with_tools.invoke([HumanMessage(content=query)])\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    if response.tool_calls:\n",
    "        selected_tool = response.tool_calls[0]['name']\n",
    "        print(f\"Selected: {selected_tool}\")\n",
    "        print(f\"Reasoning: Matched keywords and intent to tool description\")\n",
    "    else:\n",
    "        print(\"No tool selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: description_improvement.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.5\n",
    "# File: description_improvement.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Bad descriptions - vague and ambiguous\n",
    "bad_calculator = Tool(\n",
    "    name=\"calc\",\n",
    "    func=lambda x: eval(x),\n",
    "    description=\"calculator\"  # Too vague!\n",
    ")\n",
    "\n",
    "bad_search = Tool(\n",
    "    name=\"srch\",\n",
    "    func=lambda x: f\"Results for {x}\",\n",
    "    description=\"searches stuff\"  # What stuff? When?\n",
    ")\n",
    "\n",
    "# Good descriptions - clear and specific\n",
    "good_calculator = Tool(\n",
    "    name=\"calculator\",\n",
    "    func=lambda x: eval(x),\n",
    "    description=(\n",
    "        \"Use this for mathematical calculations and arithmetic. \"\n",
    "        \"Input: mathematical expression like '2+2' or '15*3.14'. \"\n",
    "        \"Output: numerical result as a string.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "good_search = Tool(\n",
    "    name=\"web_search\",\n",
    "    func=lambda x: f\"Results for {x}\",\n",
    "    description=(\n",
    "        \"Use this to search the web for current information, news, or facts. \"\n",
    "        \"Best for: recent events, current data, or topics after 2021. \"\n",
    "        \"Input: search query string. \"\n",
    "        \"Output: relevant web snippets and summaries.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"DESCRIPTION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n\u274c BAD DESCRIPTIONS:\")\n",
    "print(f\"Calculator: '{bad_calculator.description}'\")\n",
    "print(f\"Search: '{bad_search.description}'\")\n",
    "\n",
    "print(\"\\n\u2705 GOOD DESCRIPTIONS:\")\n",
    "print(f\"Calculator: '{good_calculator.description}'\")\n",
    "print(f\"Search: '{good_search.description}'\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 The difference:\")\n",
    "print(\"- Good descriptions explain WHEN to use the tool\")\n",
    "print(\"- They specify input/output formats\")\n",
    "print(\"- They include examples or use cases\")\n",
    "print(\"- They differentiate from similar tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_routing_patterns.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.5\n",
    "# File: tool_routing_patterns.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pattern 1: Hierarchical Tools (General \u2192 Specific)\n",
    "def quick_answer(question: str) -> str:\n",
    "    return f\"Quick answer: {question[:50]}...\"\n",
    "\n",
    "def detailed_answer(question: str) -> str:\n",
    "    return f\"Detailed analysis: {question} [500 words]...\"\n",
    "\n",
    "def expert_answer(question: str) -> str:\n",
    "    return f\"Expert consultation: {question} [with citations]...\"\n",
    "\n",
    "hierarchy_tools = [\n",
    "    Tool(\n",
    "        name=\"quick_answer\",\n",
    "        func=quick_answer,\n",
    "        description=\"For simple questions needing fast, brief answers (1-2 sentences)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"detailed_answer\",\n",
    "        func=detailed_answer,\n",
    "        description=\"For complex questions needing thorough explanation (paragraph+)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"expert_answer\",\n",
    "        func=expert_answer,\n",
    "        description=\"For technical questions needing citations and expertise\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Pattern 2: Domain-Specific Tools\n",
    "def search_products(query: str) -> str:\n",
    "    return f\"Product results: {query}\"\n",
    "\n",
    "def search_documentation(query: str) -> str:\n",
    "    return f\"Documentation: {query}\"\n",
    "\n",
    "def search_forums(query: str) -> str:\n",
    "    return f\"Forum discussions: {query}\"\n",
    "\n",
    "domain_tools = [\n",
    "    Tool(\n",
    "        name=\"product_search\",\n",
    "        func=search_products,\n",
    "        description=\"Search for products, prices, and shopping information\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"docs_search\",\n",
    "        func=search_documentation,\n",
    "        description=\"Search technical documentation, APIs, and guides\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"forum_search\",\n",
    "        func=search_forums,\n",
    "        description=\"Search community forums, discussions, and Q&A\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Test routing\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "print(\"ROUTING PATTERN EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test hierarchical routing\n",
    "print(\"\\n1. HIERARCHICAL ROUTING:\")\n",
    "test_hierarchical = [\n",
    "    \"What's 2+2?\",  # Should use quick_answer\n",
    "    \"Explain how neural networks work\",  # Should use detailed_answer\n",
    "    \"Provide peer-reviewed analysis of CRISPR technology\"  # Should use expert_answer\n",
    "]\n",
    "\n",
    "llm_hierarchical = llm.bind_tools(hierarchy_tools)\n",
    "for query in test_hierarchical:\n",
    "    response = llm_hierarchical.invoke([HumanMessage(content=query)])\n",
    "    if response.tool_calls:\n",
    "        print(f\"'{query[:30]}...' \u2192 {response.tool_calls[0]['name']}\")\n",
    "\n",
    "# Test domain routing\n",
    "print(\"\\n2. DOMAIN-SPECIFIC ROUTING:\")\n",
    "test_domains = [\n",
    "    \"Find the best laptop under $1000\",  # Should use product_search\n",
    "    \"How to use pandas DataFrame\",  # Should use docs_search\n",
    "    \"Why is my Python code slow?\"  # Should use forum_search\n",
    "]\n",
    "\n",
    "llm_domain = llm.bind_tools(domain_tools)\n",
    "for query in test_domains:\n",
    "    response = llm_domain.invoke([HumanMessage(content=query)])\n",
    "    if response.tool_calls:\n",
    "        print(f\"'{query[:30]}...' \u2192 {response.tool_calls[0]['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: preventing_tool_loops.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.5\n",
    "# File: preventing_tool_loops.py\n",
    "\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a tool that might cause loops\n",
    "call_count = 0\n",
    "\n",
    "def problematic_search(query: str) -> str:\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    # This tool returns questions instead of answers - loop risk!\n",
    "    return f\"Did you mean to search for '{query}'? Try being more specific.\"\n",
    "\n",
    "def good_search(query: str) -> str:\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    # This tool returns definitive answers - no loop risk\n",
    "    return f\"Found 5 results for '{query}': Result 1, Result 2, Result 3...\"\n",
    "\n",
    "# Create tools\n",
    "problematic_tool = Tool(\n",
    "    name=\"problematic_search\",\n",
    "    func=problematic_search,\n",
    "    description=\"Search for information (might ask for clarification)\"\n",
    ")\n",
    "\n",
    "good_tool = Tool(\n",
    "    name=\"good_search\",\n",
    "    func=good_search,\n",
    "    description=\"Search for information (returns definitive results)\"\n",
    ")\n",
    "\n",
    "# Strategy 1: Set max_iterations\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "\n",
    "print(\"PREVENTING TOOL LOOPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with problematic tool (with safety limit)\n",
    "print(\"\\n1. WITH LOOP PREVENTION (max_iterations=3):\")\n",
    "call_count = 0\n",
    "agent_safe = create_react_agent(llm, [problematic_tool], prompt)\n",
    "executor_safe = AgentExecutor(\n",
    "    agent=agent_safe,\n",
    "    tools=[problematic_tool],\n",
    "    max_iterations=3,  # Safety limit!\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = executor_safe.invoke({\"input\": \"Find information about Python\"})\n",
    "    print(f\"Call count: {call_count}\")\n",
    "    print(f\"Result: {result['output']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Stopped after {call_count} iterations\")\n",
    "\n",
    "# Test with good tool (no loop risk)\n",
    "print(\"\\n2. WITH GOOD TOOL DESIGN (returns answers):\")\n",
    "call_count = 0\n",
    "agent_good = create_react_agent(llm, [good_tool], prompt)\n",
    "executor_good = AgentExecutor(\n",
    "    agent=agent_good,\n",
    "    tools=[good_tool],\n",
    "    max_iterations=3,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "result = executor_good.invoke({\"input\": \"Find information about Python\"})\n",
    "print(f\"Call count: {call_count}\")\n",
    "print(f\"Result: {result['output'][:100]}...\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Loop Prevention Strategies:\")\n",
    "print(\"1. Always set max_iterations (3-5 is usually enough)\")\n",
    "print(\"2. Tools should return answers, not questions\")\n",
    "print(\"3. Tools should indicate when they're done\")\n",
    "print(\"4. Use early_stopping_method='generate' for natural stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_tool_orchestration.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.5\n",
    "# File: multi_tool_orchestration.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a suite of complementary tools\n",
    "def get_date(dummy_input: str = \"\") -> str:\n",
    "    \"\"\"Get today's date. The dummy_input is ignored.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def search_events(input_str: str) -> str:\n",
    "    \"\"\"Search for events. Input format: 'date|location'\"\"\"\n",
    "    try:\n",
    "        date, location = input_str.split(\"|\")\n",
    "        return f\"Events on {date} in {location}: Concert at 7pm, Festival at noon\"\n",
    "    except:\n",
    "        return \"Error: Please provide input as 'date|location'\"\n",
    "\n",
    "def check_weather(input_str: str) -> str:\n",
    "    \"\"\"Check weather. Input format: 'date|location'\"\"\"\n",
    "    try:\n",
    "        date, location = input_str.split(\"|\")\n",
    "        return f\"Weather for {location} on {date}: Sunny, 75\u00b0F\"\n",
    "    except:\n",
    "        return \"Error: Please provide input as 'date|location'\"\n",
    "\n",
    "def make_recommendation(input_str: str) -> str:\n",
    "    \"\"\"Make recommendation. Input format: 'events|weather'\"\"\"\n",
    "    try:\n",
    "        events, weather = input_str.split(\"|\")\n",
    "        if \"Sunny\" in weather and \"Festival\" in events:\n",
    "            return \"Perfect day for the outdoor festival!\"\n",
    "        elif \"Concert\" in events:\n",
    "            return \"Evening concert would be great!\"\n",
    "        return \"Consider indoor activities.\"\n",
    "    except:\n",
    "        return \"Error: Please provide input as 'events|weather'\"\n",
    "\n",
    "# Design tools to work together\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"get_current_date\",\n",
    "        func=get_date,\n",
    "        description=\"Get today's date in YYYY-MM-DD format. Just call without arguments.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"search_events\",\n",
    "        func=search_events,\n",
    "        description=\"Find events. Input: 'date|location' like '2024-03-15|New York'\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"check_weather\",\n",
    "        func=check_weather,\n",
    "        description=\"Check weather. Input: 'date|location' like '2024-03-15|New York'\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"make_recommendation\",\n",
    "        func=make_recommendation,\n",
    "        description=\"Make recommendation. Input: 'events info|weather info'\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize LLM and get prompt\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Get the ReAct prompt from LangSmith\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "# Test orchestration\n",
    "test_queries = [\n",
    "    \"What should I do in New York today?\",\n",
    "    \"Find events and weather for Boston tomorrow\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = agent_executor.invoke({\"input\": query})\n",
    "    print(f\"\\nFinal Answer: {result['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.5.1: Tool Naming Challenge (Easy)\n",
    "\n",
    "You have 5 search-related functions. Create clear, distinctive names and descriptions for each:\n",
    "- General web search\n",
    "- News search\n",
    "- Academic paper search\n",
    "- Local business search\n",
    "- Social media search\n",
    "\n",
    "Make sure the agent would never confuse them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.5.2: Loop Prevention (Medium)\n",
    "\n",
    "Create a scenario with 3 tools where one might cause loops:\n",
    "- A question-answering tool\n",
    "- A clarification tool (potential loop risk!)\n",
    "- A definition tool\n",
    "\n",
    "Implement strategies to prevent the agent from getting stuck asking for clarification repeatedly. Test with various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.5.3: Complex Orchestration (Hard)\n",
    "\n",
    "Build a travel planning system with 6+ tools:\n",
    "- Date/time tools\n",
    "- Weather checking\n",
    "- Flight searching\n",
    "- Hotel searching\n",
    "- Activity recommendations\n",
    "- Itinerary creation\n",
    "\n",
    "Create an agent that can handle: \"Plan a 3-day trip to Tokyo next month\"\n",
    "The agent should orchestrate all tools to create a complete plan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.6: Error handling in tool execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: basic_error_handling.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: basic_error_handling.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# \u274c BAD: Tool that crashes\n",
    "def bad_calculator(expression: str) -> str:\n",
    "    result = eval(expression)  # This will crash on bad input!\n",
    "    return str(result)\n",
    "\n",
    "# \u2705 GOOD: Tool that handles errors\n",
    "def good_calculator(expression: str) -> str:\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except ZeroDivisionError:\n",
    "        return \"Error: Cannot divide by zero\"\n",
    "    except SyntaxError:\n",
    "        return \"Error: Invalid mathematical expression\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Calculation failed - {str(e)}\"\n",
    "\n",
    "# Test both versions\n",
    "test_cases = [\n",
    "    \"10 + 5\",      # Works fine\n",
    "    \"10 / 0\",      # Division by zero\n",
    "    \"10 +\",        # Syntax error\n",
    "    \"hello\",       # Name error\n",
    "]\n",
    "\n",
    "print(\"COMPARING ERROR HANDLING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create tools\n",
    "bad_tool = Tool(name=\"BadCalc\", func=bad_calculator, description=\"Unsafe calculator\")\n",
    "good_tool = Tool(name=\"GoodCalc\", func=good_calculator, description=\"Safe calculator\")\n",
    "\n",
    "for expression in test_cases:\n",
    "    print(f\"\\nTesting: {expression}\")\n",
    "    \n",
    "    # Try bad tool (wrapped to catch crashes)\n",
    "    try:\n",
    "        bad_result = bad_tool.func(expression)\n",
    "        print(f\"  Bad tool: {bad_result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Bad tool: \ud83d\udca5 CRASHED - {e}\")\n",
    "    \n",
    "    # Good tool always returns something\n",
    "    good_result = good_tool.func(expression)\n",
    "    print(f\"  Good tool: {good_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: input_validation.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: input_validation.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def search_with_validation(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search tool with comprehensive input validation.\n",
    "    \"\"\"\n",
    "    # 1. Check if input exists\n",
    "    if not query:\n",
    "        return \"Error: Search query cannot be empty\"\n",
    "    \n",
    "    # 2. Check type\n",
    "    if not isinstance(query, str):\n",
    "        return \"Error: Search query must be text\"\n",
    "    \n",
    "    # 3. Check length\n",
    "    if len(query) < 2:\n",
    "        return \"Error: Search query too short (minimum 2 characters)\"\n",
    "    if len(query) > 200:\n",
    "        return \"Error: Search query too long (maximum 200 characters)\"\n",
    "    \n",
    "    # 4. Clean dangerous characters\n",
    "    # Remove special characters that might break the search\n",
    "    cleaned = re.sub(r'[^\\w\\s\\-.]', '', query)\n",
    "    \n",
    "    # 5. Check if anything remains\n",
    "    if not cleaned.strip():\n",
    "        return \"Error: Search query contains only special characters\"\n",
    "    \n",
    "    # If we get here, input is valid!\n",
    "    return f\"Searching for: '{cleaned}'\"\n",
    "\n",
    "def date_parser_with_validation(date_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse dates with validation and multiple format support.\n",
    "    \"\"\"\n",
    "    if not date_string:\n",
    "        return \"Error: Date cannot be empty\"\n",
    "    \n",
    "    # Try multiple date formats\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",      # 2024-03-15\n",
    "        \"%m/%d/%Y\",      # 03/15/2024\n",
    "        \"%d/%m/%Y\",      # 15/03/2024\n",
    "        \"%B %d, %Y\",     # March 15, 2024\n",
    "        \"%b %d, %Y\",     # Mar 15, 2024\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_string.strip(), fmt)\n",
    "            # Return in standard format\n",
    "            return f\"Date: {parsed_date.strftime('%Y-%m-%d')}\"\n",
    "        except ValueError:\n",
    "            continue  # Try next format\n",
    "    \n",
    "    # If no format worked\n",
    "    return f\"Error: Could not parse date '{date_string}'. Try formats like: 2024-03-15, 03/15/2024, or March 15, 2024\"\n",
    "\n",
    "# Create validated tools\n",
    "search_tool = Tool(\n",
    "    name=\"ValidatedSearch\",\n",
    "    func=search_with_validation,\n",
    "    description=\"Search with input validation\"\n",
    ")\n",
    "\n",
    "date_tool = Tool(\n",
    "    name=\"DateParser\",\n",
    "    func=date_parser_with_validation,\n",
    "    description=\"Parse dates in various formats\"\n",
    ")\n",
    "\n",
    "# Test validation\n",
    "print(\"INPUT VALIDATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test search validation\n",
    "search_tests = [\n",
    "    \"Python programming\",  # Valid\n",
    "    \"\",                    # Empty\n",
    "    \"a\",                   # Too short\n",
    "    \"x\" * 250,            # Too long\n",
    "    \"'; DROP TABLE;\",     # SQL injection attempt\n",
    "    \"!!!***!!!\",          # Only special chars\n",
    "]\n",
    "\n",
    "print(\"\\nSearch Validation:\")\n",
    "for query in search_tests:\n",
    "    display = query[:30] + \"...\" if len(query) > 30 else query\n",
    "    result = search_tool.func(query)\n",
    "    status = \"\u2705\" if not result.startswith(\"Error\") else \"\u274c\"\n",
    "    print(f\"{status} '{display}' \u2192 {result}\")\n",
    "\n",
    "# Test date parsing\n",
    "print(\"\\nDate Parsing:\")\n",
    "date_tests = [\n",
    "    \"2024-03-15\",\n",
    "    \"03/15/2024\",\n",
    "    \"March 15, 2024\",\n",
    "    \"invalid\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "for date in date_tests:\n",
    "    result = date_tool.func(date)\n",
    "    status = \"\u2705\" if not result.startswith(\"Error\") else \"\u274c\"\n",
    "    print(f\"{status} '{date}' \u2192 {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: external_service_handling.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: external_service_handling.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Simulate an unreliable external service\n",
    "def unreliable_weather_api(city: str) -> dict:\n",
    "    \"\"\"Simulates an API that sometimes fails.\"\"\"\n",
    "    # 30% chance of failure\n",
    "    if random.random() < 0.3:\n",
    "        raise Exception(\"Service unavailable\")\n",
    "    # 20% chance of timeout\n",
    "    if random.random() < 0.2:\n",
    "        time.sleep(5)  # Simulate timeout\n",
    "        raise TimeoutError(\"Request timed out\")\n",
    "    # Otherwise return data\n",
    "    return {\"city\": city, \"temp\": 72, \"conditions\": \"Sunny\"}\n",
    "\n",
    "def robust_weather_tool(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Weather tool with retry logic and fallbacks.\n",
    "    \"\"\"\n",
    "    if not city:\n",
    "        return \"Error: City name required\"\n",
    "    \n",
    "    # Try up to 3 times\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            # Set a timeout\n",
    "            import signal\n",
    "            \n",
    "            def timeout_handler(signum, frame):\n",
    "                raise TimeoutError(\"API call timed out\")\n",
    "            \n",
    "            # Note: signal doesn't work on Windows, this is for illustration\n",
    "            # In production, use requests library with timeout parameter\n",
    "            \n",
    "            # Make the API call\n",
    "            result = unreliable_weather_api(city)\n",
    "            \n",
    "            # Success! Format and return\n",
    "            return f\"Weather in {result['city']}: {result['temp']}\u00b0F, {result['conditions']}\"\n",
    "            \n",
    "        except TimeoutError:\n",
    "            if attempt < 2:  # Don't sleep on last attempt\n",
    "                print(f\"  Attempt {attempt + 1} timed out, retrying...\")\n",
    "                time.sleep(1)  # Brief pause before retry\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < 2:\n",
    "                print(f\"  Attempt {attempt + 1} failed: {e}, retrying...\")\n",
    "                time.sleep(1)\n",
    "            continue\n",
    "    \n",
    "    # All attempts failed - return graceful error\n",
    "    return f\"Error: Unable to get weather for {city}. Please try again later.\"\n",
    "\n",
    "# Alternative: Fallback to a different service\n",
    "def weather_with_fallback(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Try primary service, fallback to secondary if needed.\n",
    "    \"\"\"\n",
    "    # Try primary service\n",
    "    try:\n",
    "        # Primary API call (simulated)\n",
    "        if random.random() < 0.5:  # 50% failure rate\n",
    "            raise Exception(\"Primary API failed\")\n",
    "        return f\"Weather from PRIMARY: {city} is 72\u00b0F\"\n",
    "    except:\n",
    "        # Fallback to secondary service\n",
    "        try:\n",
    "            # Secondary API call (simulated)\n",
    "            if random.random() < 0.3:  # 30% failure rate\n",
    "                raise Exception(\"Secondary API failed\")\n",
    "            return f\"Weather from BACKUP: {city} is 70\u00b0F\"\n",
    "        except:\n",
    "            # Both failed\n",
    "            return f\"Error: Weather services unavailable for {city}\"\n",
    "\n",
    "# Test the robust tool\n",
    "print(\"TESTING EXTERNAL SERVICE HANDLING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"RobustWeather\",\n",
    "    func=robust_weather_tool,\n",
    "    description=\"Get weather with retry logic\"\n",
    ")\n",
    "\n",
    "print(\"\\nTesting with retries (watch for retry messages):\")\n",
    "for i in range(5):\n",
    "    result = tool.func(\"New York\")\n",
    "    print(f\"Attempt {i+1}: {result}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: graceful_degradation.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: graceful_degradation.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "def smart_search_tool(query: str, options: str = \"{}\") -> str:\n",
    "    \"\"\"\n",
    "    Search tool that degrades gracefully based on available services.\n",
    "    \"\"\"\n",
    "    # Parse options\n",
    "    try:\n",
    "        opts = json.loads(options)\n",
    "    except:\n",
    "        opts = {}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Try premium search (most detailed)\n",
    "    try:\n",
    "        if \"premium\" in opts and opts[\"premium\"]:\n",
    "            # Simulate premium API\n",
    "            premium_result = f\"PREMIUM: Detailed analysis of '{query}' with citations\"\n",
    "            results.append(premium_result)\n",
    "    except:\n",
    "        pass  # Premium failed, continue with others\n",
    "    \n",
    "    # Try standard search\n",
    "    try:\n",
    "        # Simulate standard search\n",
    "        if random.random() > 0.3:  # 70% success rate\n",
    "            standard_result = f\"STANDARD: Basic information about '{query}'\"\n",
    "            results.append(standard_result)\n",
    "    except:\n",
    "        pass  # Standard failed, continue\n",
    "    \n",
    "    # Fallback: Use cached results\n",
    "    try:\n",
    "        # Simulate cache lookup\n",
    "        cache = {\n",
    "            \"python\": \"Python is a programming language\",\n",
    "            \"weather\": \"Weather varies by location\",\n",
    "            \"news\": \"Latest updates from various sources\"\n",
    "        }\n",
    "        \n",
    "        for key in cache:\n",
    "            if key.lower() in query.lower():\n",
    "                results.append(f\"CACHED: {cache[key]}\")\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Ultimate fallback\n",
    "    if not results:\n",
    "        # Provide generic but helpful response\n",
    "        return f\"Unable to search for '{query}' at this time. Try: 1) Simplifying your query, 2) Checking your connection, 3) Trying again in a moment\"\n",
    "    \n",
    "    # Return best available results\n",
    "    return \" | \".join(results)\n",
    "\n",
    "def calculation_with_fallback(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Calculator that falls back to simpler operations if complex ones fail.\n",
    "    \"\"\"\n",
    "    # Try advanced calculation\n",
    "    try:\n",
    "        # Attempt with math module for complex operations\n",
    "        import math\n",
    "        # Create safe namespace with math functions\n",
    "        safe_dict = {\n",
    "            'sin': math.sin, 'cos': math.cos, 'tan': math.tan,\n",
    "            'sqrt': math.sqrt, 'log': math.log, 'pi': math.pi,\n",
    "            'e': math.e\n",
    "        }\n",
    "        result = eval(expression, {\"__builtins__\": {}}, safe_dict)\n",
    "        return f\"Result: {result}\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback to basic arithmetic\n",
    "    try:\n",
    "        # Only allow basic operations\n",
    "        if all(c in '0123456789+-*/() .' for c in expression):\n",
    "            result = eval(expression)\n",
    "            return f\"Result (basic): {result}\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Ultimate fallback: Explain what went wrong\n",
    "    return f\"Error: Cannot calculate '{expression}'. Try using only numbers and basic operations (+, -, *, /).\"\n",
    "\n",
    "print(\"GRACEFUL DEGRADATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test search degradation\n",
    "search = Tool(name=\"SmartSearch\", func=smart_search_tool, description=\"Search with fallbacks\")\n",
    "\n",
    "queries = [\"python\", \"quantum computing\", \"xyz123abc\"]\n",
    "for q in queries:\n",
    "    print(f\"\\nSearching: {q}\")\n",
    "    print(f\"Result: {search.func(q)}\")\n",
    "\n",
    "# Test calculation degradation  \n",
    "calc = Tool(name=\"SmartCalc\", func=calculation_with_fallback, description=\"Calculate with fallbacks\")\n",
    "\n",
    "expressions = [\"2+2\", \"sin(3.14/2)\", \"invalid expression\"]\n",
    "for expr in expressions:\n",
    "    print(f\"\\nCalculating: {expr}\")\n",
    "    print(f\"Result: {calc.func(expr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: helpful_error_messages.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: helpful_error_messages.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "def file_tool_with_helpful_errors(command: str) -> str:\n",
    "    \"\"\"\n",
    "    File tool that provides actionable error messages.\n",
    "    \"\"\"\n",
    "    parts = command.split()\n",
    "    if not parts:\n",
    "        return \"\"\"Error: No command provided.\n",
    "        \n",
    "How to use this tool:\n",
    "- 'read <filename>' - Read a file\n",
    "- 'write <filename> <content>' - Write to a file\n",
    "- 'list' - List files in current directory\n",
    "        \n",
    "Example: 'read document.txt'\"\"\"\n",
    "    \n",
    "    action = parts[0].lower()\n",
    "    \n",
    "    if action == \"read\":\n",
    "        if len(parts) < 2:\n",
    "            return \"\"\"Error: Filename required for read command.\n",
    "            \n",
    "Correct format: 'read <filename>'\n",
    "Example: 'read report.pdf'\n",
    "            \n",
    "Available files: document.txt, data.csv, notes.md\"\"\"\n",
    "        \n",
    "        filename = parts[1]\n",
    "        \n",
    "        # Simulate file not found\n",
    "        if filename not in [\"document.txt\", \"data.csv\", \"notes.md\"]:\n",
    "            return f\"\"\"Error: File '{filename}' not found.\n",
    "            \n",
    "Did you mean one of these?\n",
    "- document.txt\n",
    "- data.csv  \n",
    "- notes.md\n",
    "            \n",
    "To see all files, use: 'list'\"\"\"\n",
    "        \n",
    "        return f\"Contents of {filename}: [file contents here]\"\n",
    "    \n",
    "    elif action == \"write\":\n",
    "        if len(parts) < 3:\n",
    "            return \"\"\"Error: Write command requires filename and content.\n",
    "            \n",
    "Correct format: 'write <filename> <content>'\n",
    "Example: 'write notes.txt Meeting at 3pm'\n",
    "            \n",
    "Note: Content will be written as a single line.\"\"\"\n",
    "        \n",
    "        return f\"Successfully wrote to {parts[1]}\"\n",
    "    \n",
    "    elif action == \"list\":\n",
    "        return \"\"\"Files in current directory:\n",
    "- document.txt (1.2 KB) - Last modified: 2024-03-15\n",
    "- data.csv (5.6 KB) - Last modified: 2024-03-14  \n",
    "- notes.md (850 B) - Last modified: 2024-03-15\"\"\"\n",
    "    \n",
    "    else:\n",
    "        return f\"\"\"Error: Unknown command '{action}'.\n",
    "        \n",
    "Available commands:\n",
    "- read: Read a file\n",
    "- write: Write to a file\n",
    "- list: Show all files\n",
    "        \n",
    "Type the command followed by any required parameters.\n",
    "Example: 'read document.txt'\"\"\"\n",
    "\n",
    "# Test helpful errors\n",
    "print(\"HELPFUL ERROR MESSAGES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"FileTool\",\n",
    "    func=file_tool_with_helpful_errors,\n",
    "    description=\"File operations with helpful errors\"\n",
    ")\n",
    "\n",
    "test_commands = [\n",
    "    \"\",                    # No command\n",
    "    \"read\",               # Missing filename\n",
    "    \"read missing.txt\",   # File not found\n",
    "    \"write notes.txt\",    # Missing content\n",
    "    \"invalid\",            # Unknown command\n",
    "    \"read document.txt\",  # Success case\n",
    "]\n",
    "\n",
    "for cmd in test_commands:\n",
    "    print(f\"\\nCommand: '{cmd}'\")\n",
    "    print(\"-\" * 40)\n",
    "    result = tool.func(cmd)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_logging.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.6\n",
    "# File: tool_logging.py\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class ToolMonitor:\n",
    "    \"\"\"Simple tool monitoring system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            \"total_calls\": 0,\n",
    "            \"successful_calls\": 0,\n",
    "            \"failed_calls\": 0,\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def log_call(self, tool_name: str, input_data: str, result: str):\n",
    "        \"\"\"Log a tool call.\"\"\"\n",
    "        self.stats[\"total_calls\"] += 1\n",
    "        \n",
    "        if result.startswith(\"Error\"):\n",
    "            self.stats[\"failed_calls\"] += 1\n",
    "            self.stats[\"errors\"].append({\n",
    "                \"tool\": tool_name,\n",
    "                \"input\": input_data,\n",
    "                \"error\": result,\n",
    "                \"time\": datetime.now().isoformat()\n",
    "            })\n",
    "            logging.error(f\"Tool {tool_name} failed: {result}\")\n",
    "        else:\n",
    "            self.stats[\"successful_calls\"] += 1\n",
    "            logging.info(f\"Tool {tool_name} succeeded\")\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Get monitoring report.\"\"\"\n",
    "        success_rate = (\n",
    "            self.stats[\"successful_calls\"] / self.stats[\"total_calls\"] * 100\n",
    "            if self.stats[\"total_calls\"] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return f\"\"\"\n",
    "Tool Monitoring Report\n",
    "=====================\n",
    "Total Calls: {self.stats[\"total_calls\"]}\n",
    "Successful: {self.stats[\"successful_calls\"]}\n",
    "Failed: {self.stats[\"failed_calls\"]}\n",
    "Success Rate: {success_rate:.1f}%\n",
    "\n",
    "Recent Errors: {len(self.stats[\"errors\"])}\n",
    "\"\"\"\n",
    "\n",
    "# Create a monitored tool\n",
    "monitor = ToolMonitor()\n",
    "\n",
    "def monitored_calculator(expression: str) -> str:\n",
    "    \"\"\"Calculator with monitoring.\"\"\"\n",
    "    tool_name = \"Calculator\"\n",
    "    \n",
    "    try:\n",
    "        result = str(eval(expression))\n",
    "        monitor.log_call(tool_name, expression, result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        monitor.log_call(tool_name, expression, error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test monitoring\n",
    "print(\"TOOL MONITORING EXAMPLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"MonitoredCalc\",\n",
    "    func=monitored_calculator,\n",
    "    description=\"Calculator with monitoring\"\n",
    ")\n",
    "\n",
    "# Run various calculations\n",
    "test_cases = [\n",
    "    \"10 + 5\",     # Success\n",
    "    \"20 * 3\",     # Success  \n",
    "    \"100 / 0\",    # Error\n",
    "    \"50 - 30\",    # Success\n",
    "    \"invalid\",    # Error\n",
    "]\n",
    "\n",
    "for expr in test_cases:\n",
    "    result = tool.func(expr)\n",
    "    print(f\"{expr} = {result}\")\n",
    "\n",
    "# Show monitoring report\n",
    "print(\"\\n\" + monitor.get_report())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 12.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.6.1: Robust URL Fetcher (Easy)\n",
    "\n",
    "Create a tool that fetches webpage content with:\n",
    "- URL validation (must start with http:// or https://)\n",
    "- Timeout handling (max 5 seconds)\n",
    "- Retry logic (up to 2 retries)\n",
    "- Helpful error messages for common issues\n",
    "\n",
    "Test with valid URLs, invalid URLs, and slow/failing endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.6.2: Smart Data Processor (Medium)\n",
    "\n",
    "Build a tool that processes CSV data with multiple fallback strategies:\n",
    "- Try to parse as CSV\n",
    "- If that fails, try TSV (tab-separated)\n",
    "- If that fails, try space-separated\n",
    "- Return helpful error with sample of expected format\n",
    "\n",
    "Include validation for minimum/maximum rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.6.3: Resilient API Client (Hard)\n",
    "\n",
    "Create a tool that calls an external API with:\n",
    "- Rate limiting (max 10 calls per minute)\n",
    "- Exponential backoff on failures\n",
    "- Circuit breaker pattern (stop trying after 5 consecutive failures)\n",
    "- Cache successful responses for 5 minutes\n",
    "- Detailed logging of all attempts\n",
    "\n",
    "Simulate various failure scenarios and verify your tool handles them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12.7: Building a multi-tool agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: first_agent.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.7\n",
    "# File: first_agent.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Create a tool\n",
    "def multiply(numbers: str) -> str:\n",
    "    \"\"\"Multiply two numbers separated by comma.\"\"\"\n",
    "    try:\n",
    "        a, b = map(float, numbers.split(','))\n",
    "        return str(a * b)\n",
    "    except:\n",
    "        return \"Error: Please provide two numbers separated by comma\"\n",
    "\n",
    "multiply_tool = Tool(\n",
    "    name=\"Multiplier\",\n",
    "    func=multiply,\n",
    "    description=\"Multiply two numbers. Input format: 'number1,number2'\"\n",
    ")\n",
    "\n",
    "# Step 2: Create the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Step 3: Get the ReAct prompt from LangSmith\n",
    "# You need LANGSMITH_API_KEY from https://smith.langchain.com/\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "\n",
    "# Step 4: Create the agent\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=[multiply_tool],\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Step 5: Create the executor (this runs the agent)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[multiply_tool],\n",
    "    verbose=True,  # See the thinking process!\n",
    "    max_iterations=3  # Safety limit\n",
    ")\n",
    "\n",
    "# Step 6: Use your agent!\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"What is 15 times 24?\"\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal Answer: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: agent_without_hub.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.7\n",
    "# File: agent_without_hub.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import AgentExecutor\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.agents.format_scratchpad import format_log_to_str\n",
    "from langchain_classic.agents.output_parsers import ReActSingleInputOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a tool\n",
    "def multiply(numbers: str) -> str:\n",
    "    \"\"\"Multiply two numbers separated by comma.\"\"\"\n",
    "    try:\n",
    "        a, b = map(float, numbers.split(','))\n",
    "        return str(a * b)\n",
    "    except:\n",
    "        return \"Error: Please provide two numbers separated by comma\"\n",
    "\n",
    "multiply_tool = Tool(\n",
    "    name=\"Multiplier\",\n",
    "    func=multiply,\n",
    "    description=\"Multiply two numbers. Input format: 'number1,number2'\"\n",
    ")\n",
    "\n",
    "# Create the ReAct prompt manually (this is what hub.pull does)\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_stop = llm.bind(stop=[\"\\nObservation\"])\n",
    "\n",
    "# Create the agent chain\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"tools\": lambda x: \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in [multiply_tool]]),\n",
    "        \"tool_names\": lambda x: \", \".join([tool.name for tool in [multiply_tool]]),\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_stop\n",
    "    | ReActSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "# Create executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[multiply_tool],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use it!\n",
    "result = agent_executor.invoke({\"input\": \"What is 15 times 24?\"})\n",
    "print(f\"\\nFinal Answer: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_tool_agent.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.7\n",
    "# File: multi_tool_agent.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create diverse tools\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Perform mathematical calculations.\"\"\"\n",
    "    try:\n",
    "        # Safety: only allow numbers and basic operations\n",
    "        allowed = set('0123456789+-*/().')\n",
    "        if all(c in allowed or c.isspace() for c in expression):\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        return \"Error: Only basic math operations allowed\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_time(location: str = \"local\") -> str:\n",
    "    \"\"\"Get current time.\"\"\"\n",
    "    current_time = datetime.now()\n",
    "    return f\"Current time: {current_time.strftime('%I:%M %p, %A, %B %d, %Y')}\"\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city (simulated).\"\"\"\n",
    "    # In production, this would call a real weather API\n",
    "    weather_data = {\n",
    "        \"New York\": {\"temp\": 72, \"condition\": \"Partly cloudy\"},\n",
    "        \"London\": {\"temp\": 59, \"condition\": \"Rainy\"},\n",
    "        \"Tokyo\": {\"temp\": 68, \"condition\": \"Clear\"},\n",
    "        \"Paris\": {\"temp\": 64, \"condition\": \"Cloudy\"},\n",
    "    }\n",
    "    \n",
    "    if city in weather_data:\n",
    "        data = weather_data[city]\n",
    "        return f\"Weather in {city}: {data['temp']}\u00b0F, {data['condition']}\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {city}\"\n",
    "\n",
    "def search_info(query: str) -> str:\n",
    "    \"\"\"Search for information (simulated).\"\"\"\n",
    "    # In production, this would use a real search API\n",
    "    responses = {\n",
    "        \"python\": \"Python is a high-level programming language known for its simplicity.\",\n",
    "        \"langchain\": \"LangChain is a framework for building applications with LLMs.\",\n",
    "        \"agent\": \"An AI agent is a system that can perceive, reason, and act autonomously.\",\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    for key, value in responses.items():\n",
    "        if key in query_lower:\n",
    "            return value\n",
    "    \n",
    "    return f\"Information about '{query}' is not in my current database.\"\n",
    "\n",
    "# Create tool objects\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=calculate,\n",
    "        description=\"Perform mathematical calculations. Input: math expression like '2+2' or '15*3.14'\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Clock\",\n",
    "        func=get_time,\n",
    "        description=\"Get the current date and time. Input: location (or 'local' for local time)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Weather\",\n",
    "        func=get_weather,\n",
    "        description=\"Get current weather for a city. Input: city name\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search_info,\n",
    "        description=\"Search for information about a topic. Input: search query\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create executor with safety limits\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=5,  # Prevent infinite loops\n",
    "    handle_parsing_errors=True  # Handle any parsing issues\n",
    ")\n",
    "\n",
    "print(\"\ud83e\udd16 MULTI-TOOL AI AGENT READY!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with various queries\n",
    "test_queries = [\n",
    "    \"What's 25 * 4?\",\n",
    "    \"What time is it?\",\n",
    "    \"What's the weather in London?\",\n",
    "    \"Tell me about Python\",\n",
    "    \"What's the weather in Paris and what's 100 divided by 4?\",  # Multiple tools!\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\ud83d\udcdd User: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = agent_executor.invoke({\"input\": query})\n",
    "    \n",
    "    print(f\"\\n\ud83e\udd16 Agent: {result['output']}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: research_assistant.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.7\n",
    "# File: research_assistant.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup built-in tools\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "wikipedia_tool = WikipediaQueryRun(\n",
    "    api_wrapper=WikipediaAPIWrapper(\n",
    "        top_k_results=1,\n",
    "        doc_content_chars_max=500\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create custom tools for the assistant\n",
    "def save_notes(content: str) -> str:\n",
    "    \"\"\"Save research notes to a file.\"\"\"\n",
    "    try:\n",
    "        # In production, implement proper file handling\n",
    "        with open(\"research_notes.txt\", \"a\") as f:\n",
    "            f.write(f\"\\n{content}\\n\")\n",
    "        return \"Notes saved successfully\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving notes: {e}\"\n",
    "\n",
    "def summarize(text: str) -> str:\n",
    "    \"\"\"Create a brief summary of text.\"\"\"\n",
    "    # Simple summary (in production, might use another LLM call)\n",
    "    words = text.split()\n",
    "    if len(words) > 50:\n",
    "        summary = ' '.join(words[:50]) + \"...\"\n",
    "    else:\n",
    "        summary = text\n",
    "    return f\"Summary: {summary}\"\n",
    "\n",
    "# Combine all tools\n",
    "tools = [\n",
    "    search_tool,\n",
    "    wikipedia_tool,\n",
    "    Tool(name=\"SaveNotes\", func=save_notes, \n",
    "         description=\"Save important information to notes. Input: text to save\"),\n",
    "    Tool(name=\"Summarize\", func=summarize,\n",
    "         description=\"Create a brief summary. Input: text to summarize\"),\n",
    "]\n",
    "\n",
    "# Create research assistant\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=6\n",
    ")\n",
    "\n",
    "print(\"\ud83d\udd2c RESEARCH ASSISTANT READY!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Complex research task\n",
    "research_query = \"\"\"\n",
    "Research LangChain framework:\n",
    "1. Find current information about it\n",
    "2. Check Wikipedia for background\n",
    "3. Save the key points to notes\n",
    "4. Give me a summary\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\ud83d\udcda Research Request: {research_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result = agent_executor.invoke({\"input\": research_query})\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Research Complete!\")\n",
    "print(f\"Final Report: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: personal_assistant_solution.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 12, Section 12.7\n",
    "# File: personal_assistant_solution.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Time Management Tools\n",
    "def get_time_zone(location: str) -> str:\n",
    "    \"\"\"Get time in different timezones.\"\"\"\n",
    "    timezone_offsets = {\n",
    "        \"NYC\": -5, \"London\": 0, \"Tokyo\": 9, \n",
    "        \"Paris\": 1, \"Sydney\": 11\n",
    "    }\n",
    "    \n",
    "    if location in timezone_offsets:\n",
    "        utc_now = datetime.utcnow()\n",
    "        local_time = utc_now + timedelta(hours=timezone_offsets[location])\n",
    "        return f\"Time in {location}: {local_time.strftime('%I:%M %p')}\"\n",
    "    return f\"Unknown timezone for {location}\"\n",
    "\n",
    "def set_reminder(reminder: str) -> str:\n",
    "    \"\"\"Save a reminder to file.\"\"\"\n",
    "    try:\n",
    "        with open(\"reminders.txt\", \"a\") as f:\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "            f.write(f\"[{timestamp}] {reminder}\\n\")\n",
    "        return f\"Reminder set: {reminder}\"\n",
    "    except:\n",
    "        return \"Error setting reminder\"\n",
    "\n",
    "# 2. Task Management Tools\n",
    "def manage_todo(action_and_task: str) -> str:\n",
    "    \"\"\"Manage todo list. Input: 'add:task' or 'complete:task' or 'list'\"\"\"\n",
    "    parts = action_and_task.split(':')\n",
    "    action = parts[0].lower()\n",
    "    \n",
    "    todos_file = \"todos.json\"\n",
    "    \n",
    "    # Load existing todos\n",
    "    if os.path.exists(todos_file):\n",
    "        with open(todos_file, 'r') as f:\n",
    "            todos = json.load(f)\n",
    "    else:\n",
    "        todos = []\n",
    "    \n",
    "    if action == \"add\" and len(parts) > 1:\n",
    "        task = ':'.join(parts[1:])\n",
    "        todos.append({\"task\": task, \"done\": False})\n",
    "        with open(todos_file, 'w') as f:\n",
    "            json.dump(todos, f)\n",
    "        return f\"Added task: {task}\"\n",
    "    \n",
    "    elif action == \"complete\" and len(parts) > 1:\n",
    "        task = ':'.join(parts[1:])\n",
    "        for todo in todos:\n",
    "            if todo[\"task\"] == task:\n",
    "                todo[\"done\"] = True\n",
    "        with open(todos_file, 'w') as f:\n",
    "            json.dump(todos, f)\n",
    "        return f\"Completed: {task}\"\n",
    "    \n",
    "    elif action == \"list\":\n",
    "        pending = [t[\"task\"] for t in todos if not t[\"done\"]]\n",
    "        return f\"Pending tasks: {', '.join(pending) if pending else 'None'}\"\n",
    "    \n",
    "    return \"Usage: 'add:task', 'complete:task', or 'list'\"\n",
    "\n",
    "# 3. Calculation Tools\n",
    "def unit_converter(conversion_request: str) -> str:\n",
    "    \"\"\"Convert units. Input: 'value unit to unit' like '10 meters to feet'\"\"\"\n",
    "    conversions = {\n",
    "        (\"meters\", \"feet\"): 3.28084,\n",
    "        (\"feet\", \"meters\"): 0.3048,\n",
    "        (\"kg\", \"pounds\"): 2.20462,\n",
    "        (\"celsius\", \"fahrenheit\"): lambda c: c * 9/5 + 32,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        parts = conversion_request.lower().split()\n",
    "        value = float(parts[0])\n",
    "        from_unit = parts[1]\n",
    "        to_unit = parts[3]\n",
    "        \n",
    "        key = (from_unit, to_unit)\n",
    "        if key in conversions:\n",
    "            if callable(conversions[key]):\n",
    "                result = conversions[key](value)\n",
    "            else:\n",
    "                result = value * conversions[key]\n",
    "            return f\"{value} {from_unit} = {result:.2f} {to_unit}\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \"Conversion not available. Try: '10 meters to feet'\"\n",
    "\n",
    "# 4. Weather Tool (enhanced)\n",
    "def weather_assistant(city: str) -> str:\n",
    "    \"\"\"Get weather and clothing suggestions.\"\"\"\n",
    "    weather_data = {\n",
    "        \"London\": {\"temp\": 59, \"condition\": \"Rainy\"},\n",
    "        \"Paris\": {\"temp\": 64, \"condition\": \"Cloudy\"},\n",
    "        \"NYC\": {\"temp\": 72, \"condition\": \"Sunny\"},\n",
    "    }\n",
    "    \n",
    "    if city in weather_data:\n",
    "        data = weather_data[city]\n",
    "        suggestion = \"\"\n",
    "        \n",
    "        if \"rain\" in data[\"condition\"].lower():\n",
    "            suggestion = \" Bring an umbrella!\"\n",
    "        elif data[\"temp\"] < 60:\n",
    "            suggestion = \" Wear a jacket!\"\n",
    "        elif data[\"temp\"] > 75:\n",
    "            suggestion = \" Light clothing recommended!\"\n",
    "        \n",
    "        return f\"Weather in {city}: {data['temp']}\u00b0F, {data['condition']}.{suggestion}\"\n",
    "    \n",
    "    return f\"No weather data for {city}\"\n",
    "\n",
    "# Create all tools\n",
    "tools = [\n",
    "    Tool(name=\"TimeZone\", func=get_time_zone, \n",
    "         description=\"Get time in a city. Input: city name like 'NYC' or 'London'\"),\n",
    "    Tool(name=\"Reminder\", func=set_reminder,\n",
    "         description=\"Set a reminder. Input: reminder text\"),\n",
    "    Tool(name=\"TodoList\", func=manage_todo,\n",
    "         description=\"Manage todos. Input: 'add:task', 'complete:task', or 'list'\"),\n",
    "    Tool(name=\"UnitConverter\", func=unit_converter,\n",
    "         description=\"Convert units. Input: 'value from_unit to to_unit'\"),\n",
    "    Tool(name=\"Weather\", func=weather_assistant,\n",
    "         description=\"Get weather and clothing suggestions. Input: city name\"),\n",
    "]\n",
    "\n",
    "# Create the personal assistant\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=langsmith_api_key)\n",
    "prompt = client.pull_prompt(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=6,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"\ud83e\udd16 PERSONAL ASSISTANT READY!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test complex query from the challenge\n",
    "test_query = \"\"\"What's the weather in Paris and NYC, which is warmer, \n",
    "and add 'pack umbrella' to my todo list if either is rainy\"\"\"\n",
    "\n",
    "print(f\"User: {test_query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "result = agent_executor.invoke({\"input\": test_query})\n",
    "print(f\"\\nAssistant: {result['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_12_tools_functions_solutions.ipynb**\n",
    "- Proceed to **Chapter 13**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}