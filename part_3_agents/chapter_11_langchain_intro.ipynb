{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Introduction to LangChain\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- What is LangChain and why use it?\n",
    "- Installing and setting up LangChain\n",
    "- Core concepts: chains, prompts, and models\n",
    "- Your first LangChain application\n",
    "- Using different LLM providers\n",
    "- Output parsers and structured output\n",
    "- Debugging LangChain applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.1: What is LangChain and why use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.1 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1.1: Framework Exploration\n",
    "\n",
    "Visit the LangChain documentation (python.langchain.com). Find and list:\n",
    "- Three different agent types available\n",
    "- Five different tool integrations\n",
    "- Three memory types\n",
    "- Two vector store options\n",
    "Write a brief note about which ones interest you most and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1.2: Use Case Planning\n",
    "\n",
    "Think about three real problems in your life or work that an AI agent could solve. For each one, write:\n",
    "- What the problem is\n",
    "- What tools the agent would need\n",
    "- What type of memory would be helpful\n",
    "- How LangChain could help build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1.3: Code Comparison\n",
    "\n",
    "Look at your chatbot code from Chapter 8. List five specific things that were challenging or repetitive (like managing message history, handling errors, formatting prompts). For each one, research how LangChain handles it. Would it simplify your code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.2: Installing and setting up LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_setup.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.2\n",
    "# File: test_setup.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key loaded\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"\u2705 API key loaded!\")\n",
    "    print(f\"   Key starts with: {api_key[:7]}...\")\n",
    "else:\n",
    "    print(\"\u274c No API key found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_langchain.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.2\n",
    "# File: test_langchain.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "\n",
    "# Create the simplest possible chain\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Test it\n",
    "response = llm.invoke(\"Say 'LangChain is working!'\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: check_versions.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.2\n",
    "# File: check_versions.py\n",
    "\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# Save your working setup\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"freeze\", \">\", \"requirements.txt\"], shell=True)\n",
    "print(\"Saved package versions to requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_env.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.2\n",
    "# File: test_env.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\".env exists: {os.path.exists('.env')}\")\n",
    "print(f\"Key loaded: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2.1: Environment Detective\n",
    "\n",
    "Create a script that reports on your setup:\n",
    "- Python version\n",
    "- LangChain version\n",
    "- Whether API key is present (not the key itself!)\n",
    "- Current working directory\n",
    "- List of installed packages\n",
    "\n",
    "Save it as `debug/environment_report.py` - you'll use this whenever something goes wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2.2: Setup Automation\n",
    "\n",
    "Create a simple bash script that:\n",
    "- Creates a new project folder\n",
    "- Sets up a virtual environment\n",
    "- Installs the basic packages\n",
    "- Creates template `.env` and `.gitignore` files\n",
    "\n",
    "This will save you time on future projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2.3: Connection Tester\n",
    "\n",
    "Write a script that:\n",
    "- Tries to connect to OpenAI\n",
    "- Handles errors gracefully\n",
    "- Reports success or explains what went wrong\n",
    "- Suggests fixes for common problems\n",
    "\n",
    "This becomes your go-to diagnostic tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.3: Core concepts: chains, prompts, and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_model.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: test_model.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a model instance\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Use it (same interface for ALL models!)\n",
    "response = model.invoke(\"Hello, AI!\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: temperature_test.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: temperature_test.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Focused and consistent (temperature = 0)\n",
    "focused_model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Creative and varied (temperature = 1)\n",
    "creative_model = ChatOpenAI(temperature=1)\n",
    "\n",
    "prompt = \"Write a tagline for a coffee shop\"\n",
    "\n",
    "print(\"Focused:\", focused_model.invoke(prompt).content)\n",
    "print(\"Creative:\", creative_model.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: prompt_problem.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: prompt_problem.py\n",
    "\n",
    "# The OLD way - messy and error-prone\n",
    "\n",
    "name = \"Alice\"\n",
    "topic = \"Python\"\n",
    "\n",
    "# String concatenation nightmare\n",
    "prompt = \"Hello \" + name + \", let me teach you about \" + topic\n",
    "\n",
    "# Or slightly better but still messy\n",
    "prompt = f\"Hello {name}, let me teach you about {topic}\"\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: prompt_solution.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: prompt_solution.py\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a reusable template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful teacher.\"),\n",
    "    (\"human\", \"Teach me about {topic} in simple terms.\")\n",
    "])\n",
    "\n",
    "# Use it with different inputs\n",
    "prompt1 = prompt_template.format_messages(topic=\"recursion\")\n",
    "prompt2 = prompt_template.format_messages(topic=\"databases\")\n",
    "\n",
    "print(\"First prompt:\", prompt1)\n",
    "print(\"\\nSecond prompt:\", prompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: few_shot_prompt.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: few_shot_prompt.py\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Teaching by example\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"\ud83d\ude0a\"},\n",
    "    {\"input\": \"sad\", \"output\": \"\ud83d\ude22\"},\n",
    "    {\"input\": \"love\", \"output\": \"\u2764\ufe0f\"}\n",
    "]\n",
    "\n",
    "# Build the teaching prompt\n",
    "messages = [\n",
    "    (\"system\", \"Convert words to emojis. Learn from these examples:\"),\n",
    "    (\"human\", \"happy\"),\n",
    "    (\"assistant\", \"\ud83d\ude0a\"),\n",
    "    (\"human\", \"sad\"), \n",
    "    (\"assistant\", \"\ud83d\ude22\"),\n",
    "    (\"human\", \"love\"),\n",
    "    (\"assistant\", \"\u2764\ufe0f\"),\n",
    "    (\"human\", \"{word}\")  # The actual input\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Test it\n",
    "test_prompt = prompt.format_messages(word=\"excited\")\n",
    "print(test_prompt[-1])  # Just show the last message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_chain.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: simple_chain.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create components\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Connect them with a chain (using the pipe operator!)\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_step_chain.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: multi_step_chain.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Generate a story\n",
    "story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 2-sentence story about {animal}\"\n",
    ")\n",
    "\n",
    "# Step 2: Extract the moral\n",
    "moral_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What's the moral of this story: {story}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()  # Extracts just the text\n",
    "\n",
    "# Build the chain\n",
    "story_chain = story_prompt | model | output_parser\n",
    "moral_chain = moral_prompt | model | output_parser\n",
    "\n",
    "# Run both steps\n",
    "story = story_chain.invoke({\"animal\": \"ant\"})\n",
    "print(\"Story:\", story)\n",
    "\n",
    "moral = moral_chain.invoke({\"story\": story})\n",
    "print(\"\\nMoral:\", moral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: lcel_comparison.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: lcel_comparison.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# The OLD way (still works but verbose)\n",
    "old_chain = LLMChain(llm=model, prompt=prompt)\n",
    "old_result = old_chain.run(text=\"Hello world\")\n",
    "print(\"Old way:\", old_result)\n",
    "\n",
    "# The NEW way with LCEL (clean and intuitive!)\n",
    "new_chain = prompt | model\n",
    "new_result = new_chain.invoke({\"text\": \"Hello world\"})\n",
    "print(\"New way:\", new_result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: writing_assistant.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: writing_assistant.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create specialized prompts for different improvements\n",
    "grammar_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Fix any grammar errors in this text. Return only the corrected text:\\n{text}\"\n",
    ")\n",
    "\n",
    "clarity_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Make this text clearer and more concise:\\n{text}\"\n",
    ")\n",
    "\n",
    "tone_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Adjust this text to be more {tone}:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Create the model and parser\n",
    "model = ChatOpenAI(temperature=0)  # Consistent for editing\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create chains for each task\n",
    "grammar_chain = grammar_prompt | model | parser\n",
    "clarity_chain = clarity_prompt | model | parser\n",
    "tone_chain = tone_prompt | model | parser\n",
    "\n",
    "# Test text\n",
    "text = \"The thing is that we should probably maybe consider thinking about it\"\n",
    "\n",
    "# Apply improvements\n",
    "grammar_fixed = grammar_chain.invoke({\"text\": text})\n",
    "print(\"Grammar fixed:\", grammar_fixed)\n",
    "\n",
    "clarity_improved = clarity_chain.invoke({\"text\": grammar_fixed})\n",
    "print(\"Clarity improved:\", clarity_improved)\n",
    "\n",
    "professional = tone_chain.invoke({\"text\": clarity_improved, \"tone\": \"professional\"})\n",
    "print(\"Professional tone:\", professional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: chain_composition.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.3\n",
    "# File: chain_composition.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Component 1: Idea generator\n",
    "idea_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a creative name for a {type} business\"\n",
    ")\n",
    "\n",
    "# Component 2: Slogan creator\n",
    "slogan_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Create a catchy slogan for a business called: {name}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# Create individual chains\n",
    "idea_chain = idea_prompt | model\n",
    "slogan_chain = slogan_prompt | model\n",
    "\n",
    "# Use them together (manually for now)\n",
    "business_type = \"coffee shop\"\n",
    "\n",
    "# Generate name\n",
    "name_response = idea_chain.invoke({\"type\": business_type})\n",
    "business_name = name_response.content\n",
    "\n",
    "print(f\"Business name: {business_name}\")\n",
    "\n",
    "# Generate slogan\n",
    "slogan_response = slogan_chain.invoke({\"name\": business_name})\n",
    "print(f\"Slogan: {slogan_response.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3.1: Prompt Variations\n",
    "\n",
    "Create three different prompt templates for the same task (summarizing text):\n",
    "- One for technical audiences\n",
    "- One for children\n",
    "- One for business executives\n",
    "\n",
    "Test all three with the same input text. Save as `basics/prompt_variations.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3.2: Chain Builder\n",
    "\n",
    "Build a chain that:\n",
    "1. Takes a topic as input\n",
    "2. Generates three questions about that topic\n",
    "3. Picks the most interesting question\n",
    "4. Answers it\n",
    "\n",
    "Use separate chains for each step. Save as `basics/question_chain.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3.3: Model Comparison\n",
    "\n",
    "Create a script that:\n",
    "- Sends the same prompt to two different temperature settings\n",
    "- Compares the outputs\n",
    "- Counts how many words differ\n",
    "\n",
    "This will help you understand temperature's impact. Save as `basics/model_comparison.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.4: Your first LangChain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: writing_helper.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: writing_helper.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a simple writing improvement chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Improve this text: {text}\\n\\nMake it clearer and more engaging.\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "chain = prompt | llm\n",
    "\n",
    "# Test it\n",
    "text = \"The thing is that we should probably consider maybe thinking about it\"\n",
    "result = chain.invoke({\"text\": text})\n",
    "print(\"Original:\", text)\n",
    "print(\"Improved:\", result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_chain.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: multi_chain.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain 1: Explain things simply\n",
    "explain_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms a beginner would understand.\"\n",
    ")\n",
    "explain_chain = explain_prompt | llm | parser\n",
    "\n",
    "# Chain 2: Generate ideas\n",
    "idea_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate 3 creative ideas for {topic}\"\n",
    ")\n",
    "idea_chain = idea_prompt | llm | parser\n",
    "\n",
    "# Use them\n",
    "explanation = explain_chain.invoke({\"topic\": \"recursion\"})\n",
    "print(\"Explanation:\", explanation[:200], \"...\\n\")\n",
    "\n",
    "ideas = idea_chain.invoke({\"topic\": \"a birthday party\"})\n",
    "print(\"Ideas:\", ideas[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_router.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: smart_router.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)  # Temperature 0 for consistent routing\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# First, classify the request\n",
    "classifier_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"What type of request is this:\n",
    "    - explain: if user wants explanation\n",
    "    - create: if user wants ideas or content\n",
    "    - analyze: if user wants analysis\n",
    "    \n",
    "    Request: {request}\n",
    "    \n",
    "    Reply with just one word: explain, create, or analyze\"\"\"\n",
    ")\n",
    "\n",
    "classifier = classifier_prompt | llm | parser\n",
    "\n",
    "# Test the classifier\n",
    "request = \"Help me understand how photosynthesis works\"\n",
    "request_type = classifier.invoke({\"request\": request})\n",
    "print(f\"Request: {request}\")\n",
    "print(f\"Type detected: {request_type.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: with_memory.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: with_memory.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Prompt that includes conversation history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt | llm\n",
    "\n",
    "# Have a conversation\n",
    "def chat(message):\n",
    "    # Get history\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    # Get response\n",
    "    response = chain.invoke({\n",
    "        \"history\": history,\n",
    "        \"input\": message\n",
    "    })\n",
    "    \n",
    "    # Save to memory\n",
    "    memory.save_context(\n",
    "        {\"input\": message},\n",
    "        {\"output\": response.content}\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Test memory\n",
    "print(chat(\"Hi! My name is Alice\"))\n",
    "print(chat(\"What's my name?\"))  # It should remember!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: interactive.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: interactive.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SimpleAssistant:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        \n",
    "        # Create the conversation chain\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful, friendly assistant named Alex.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "    def chat(self, message):\n",
    "        \"\"\"Process a message and return response\"\"\"\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "        \n",
    "        # Create chain\n",
    "        chain = self.prompt | self.llm\n",
    "        \n",
    "        # Get response\n",
    "        response = chain.invoke({\n",
    "            \"history\": history,\n",
    "            \"input\": message\n",
    "        })\n",
    "        \n",
    "        # Update memory\n",
    "        self.memory.save_context(\n",
    "            {\"input\": message},\n",
    "            {\"output\": response.content}\n",
    "        )\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Start a new conversation\"\"\"\n",
    "        self.memory.clear()\n",
    "        return \"Memory cleared! Starting fresh.\"\n",
    "\n",
    "# Interactive loop\n",
    "def run():\n",
    "    assistant = SimpleAssistant()\n",
    "    print(\"Assistant: Hi! I'm Alex. How can I help? (type 'quit' to exit)\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Assistant: Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'reset':\n",
    "            print(f\"Assistant: {assistant.reset()}\")\n",
    "        else:\n",
    "            response = assistant.chat(user_input)\n",
    "            print(f\"Assistant: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: mode_assistant.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.4\n",
    "# File: mode_assistant.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ModeAssistant:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(temperature=0.7)\n",
    "        self.mode = \"normal\"\n",
    "        \n",
    "        # Different prompts for different modes\n",
    "        self.prompts = {\n",
    "            \"normal\": \"Answer this: {input}\",\n",
    "            \"creative\": \"Be very creative and imaginative: {input}\",\n",
    "            \"teacher\": \"Explain this like a patient teacher: {input}\",\n",
    "            \"concise\": \"Answer in one sentence: {input}\"\n",
    "        }\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        \"\"\"Change conversation mode\"\"\"\n",
    "        if mode in self.prompts:\n",
    "            self.mode = mode\n",
    "            return f\"Mode changed to: {mode}\"\n",
    "        return \"Unknown mode\"\n",
    "    \n",
    "    def respond(self, message):\n",
    "        \"\"\"Respond based on current mode\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(self.prompts[self.mode])\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\"input\": message})\n",
    "        return response.content\n",
    "\n",
    "# Test different modes\n",
    "assistant = ModeAssistant()\n",
    "\n",
    "question = \"What is happiness?\"\n",
    "\n",
    "for mode in [\"normal\", \"creative\", \"teacher\", \"concise\"]:\n",
    "    assistant.set_mode(mode)\n",
    "    print(f\"\\n{mode.upper()} mode:\")\n",
    "    print(assistant.respond(question)[:150], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4.1: Specialized Assistant\n",
    "\n",
    "Create an assistant with three specialized modes:\n",
    "- Translator mode (translates to different styles: formal, casual, technical)\n",
    "- Summarizer mode (creates different length summaries)\n",
    "- Analyzer mode (provides different types of analysis)\n",
    "\n",
    "Each mode should have clear, distinct behavior. Save as `first_app/specialized_assistant.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4.2: Learning Tracker\n",
    "\n",
    "Build an assistant that:\n",
    "- Remembers topics you're learning\n",
    "- Can quiz you on previous topics\n",
    "- Tracks your progress\n",
    "- Provides encouragement\n",
    "\n",
    "Focus on using memory effectively. Save as `first_app/learning_tracker.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4.3: Writing Workshop\n",
    "\n",
    "Create an assistant that helps with writing by:\n",
    "- Offering different improvement styles (clarity, creativity, conciseness)\n",
    "- Remembering your writing goals\n",
    "- Providing consistent feedback\n",
    "- Tracking common issues\n",
    "\n",
    "Keep each function simple and focused. Save as `first_app/writing_workshop.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.5: Using different LLM providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: openai_comparison.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: openai_comparison.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create two different models\n",
    "fast_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "smart_model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Same prompt, different models\n",
    "prompt = \"Explain why the sky is blue\"\n",
    "\n",
    "fast_response = fast_model.invoke(prompt)\n",
    "print(\"GPT-3.5 says:\")\n",
    "print(fast_response.content[:200], \"...\\n\")\n",
    "\n",
    "smart_response = smart_model.invoke(prompt)\n",
    "print(\"GPT-4 says:\")\n",
    "print(smart_response.content[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: temperature_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: temperature_demo.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Temperature controls creativity\n",
    "focused_model = ChatOpenAI(temperature=0)    # Consistent, focused\n",
    "balanced_model = ChatOpenAI(temperature=0.5)  # Balanced\n",
    "creative_model = ChatOpenAI(temperature=1)    # Creative, varied\n",
    "\n",
    "prompt = \"Write a tagline for a coffee shop\"\n",
    "\n",
    "print(\"Focused (temp=0):\", focused_model.invoke(prompt).content)\n",
    "print(\"Balanced (temp=0.5):\", balanced_model.invoke(prompt).content)\n",
    "print(\"Creative (temp=1):\", creative_model.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: gemini_setup.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: gemini_setup.py\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Add GOOGLE_API_KEY=your-key-here to your .env file\n",
    "# Get a free API key from makersuite.google.com\n",
    "load_dotenv()\n",
    "\n",
    "# Create Gemini model\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# Use it exactly like OpenAI!\n",
    "response = gemini.invoke(\"Hello, Gemini!\")\n",
    "print(\"Gemini says:\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: local_model.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: local_model.py\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# No API key needed!\n",
    "local_model = Ollama(model=\"llama2\")\n",
    "\n",
    "# Works offline!\n",
    "response = local_model.invoke(\"Why is privacy important?\")\n",
    "print(\"Local model says:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: model_switcher.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: model_switcher.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ModelSwitcher:\n",
    "    def __init__(self):\n",
    "        # Initialize available models\n",
    "        self.models = {\n",
    "            \"fast\": ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "            \"smart\": ChatOpenAI(model=\"gpt-4\"),\n",
    "            \"local\": Ollama(model=\"llama2\")\n",
    "        }\n",
    "        self.current = \"fast\"\n",
    "    \n",
    "    def switch_to(self, model_name):\n",
    "        \"\"\"Switch to a different model\"\"\"\n",
    "        if model_name in self.models:\n",
    "            self.current = model_name\n",
    "            return f\"Switched to {model_name}\"\n",
    "        return \"Model not available\"\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"Ask current model\"\"\"\n",
    "        model = self.models[self.current]\n",
    "        response = model.invoke(question)\n",
    "        \n",
    "        # Handle different response types\n",
    "        if hasattr(response, 'content'):\n",
    "            return response.content\n",
    "        else:\n",
    "            return str(response)\n",
    "\n",
    "# Test it\n",
    "switcher = ModelSwitcher()\n",
    "\n",
    "question = \"What is happiness?\"\n",
    "\n",
    "for model_name in [\"fast\", \"smart\", \"local\"]:\n",
    "    switcher.switch_to(model_name)\n",
    "    print(f\"\\n{model_name.upper()} model:\")\n",
    "    print(switcher.ask(question)[:150], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: cost_aware.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: cost_aware.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class CostAwareAssistant:\n",
    "    def __init__(self):\n",
    "        # Approximate costs per 1000 tokens\n",
    "        self.costs = {\n",
    "            \"gpt-3.5-turbo\": 0.002,\n",
    "            \"gpt-4\": 0.06\n",
    "        }\n",
    "        self.budget_used = 0.0\n",
    "        \n",
    "    def choose_model(self, importance):\n",
    "        \"\"\"Choose model based on importance\"\"\"\n",
    "        if importance == \"high\":\n",
    "            model_name = \"gpt-4\"\n",
    "            print(f\"Using GPT-4 (important question)\")\n",
    "        else:\n",
    "            model_name = \"gpt-3.5-turbo\"\n",
    "            print(f\"Using GPT-3.5 (regular question)\")\n",
    "        \n",
    "        return ChatOpenAI(model=model_name), model_name\n",
    "    \n",
    "    def ask(self, question, importance=\"normal\"):\n",
    "        \"\"\"Ask with cost awareness\"\"\"\n",
    "        model, model_name = self.choose_model(importance)\n",
    "        \n",
    "        # Get response\n",
    "        response = model.invoke(question)\n",
    "        \n",
    "        # Estimate cost (rough calculation)\n",
    "        tokens = len(question.split()) + len(response.content.split())\n",
    "        cost = (tokens / 1000) * self.costs[model_name]\n",
    "        self.budget_used += cost\n",
    "        \n",
    "        print(f\"Cost: ${cost:.4f} | Total used: ${self.budget_used:.4f}\")\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "# Test it\n",
    "assistant = CostAwareAssistant()\n",
    "\n",
    "# Normal question - uses cheaper model\n",
    "print(assistant.ask(\"What's 2+2?\", importance=\"normal\"))\n",
    "\n",
    "# Important question - uses better model\n",
    "print(assistant.ask(\"Explain quantum computing\", importance=\"high\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: fallback_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: fallback_system.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ResilientAssistant:\n",
    "    def __init__(self):\n",
    "        # Primary model\n",
    "        self.primary = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Fallback model (local)\n",
    "        self.fallback = Ollama(model=\"llama2\")\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"Try primary, fall back if needed\"\"\"\n",
    "        try:\n",
    "            print(\"Trying primary model...\")\n",
    "            response = self.primary.invoke(question)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Primary failed: {e}\")\n",
    "            print(\"Using fallback model...\")\n",
    "            response = self.fallback.invoke(question)\n",
    "            return str(response)\n",
    "\n",
    "# Test it\n",
    "assistant = ResilientAssistant()\n",
    "answer = assistant.ask(\"What is resilience?\")\n",
    "print(\"Answer:\", answer[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_selection.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: smart_selection.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SmartModelSelector:\n",
    "    def __init__(self):\n",
    "        # Classifier to determine complexity\n",
    "        self.classifier = ChatOpenAI(temperature=0)\n",
    "        \n",
    "        self.classify_prompt = ChatPromptTemplate.from_template(\n",
    "            \"Is this question simple or complex? Reply with one word.\\n\"\n",
    "            \"Question: {question}\"\n",
    "        )\n",
    "        \n",
    "        # Different models for different complexities\n",
    "        self.simple_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        self.complex_model = ChatOpenAI(model=\"gpt-4\")\n",
    "    \n",
    "    def answer(self, question):\n",
    "        \"\"\"Pick model based on question complexity\"\"\"\n",
    "        # Classify question\n",
    "        classify_chain = self.classify_prompt | self.classifier\n",
    "        result = classify_chain.invoke({\"question\": question})\n",
    "        complexity = result.content.strip().lower()\n",
    "        \n",
    "        # Choose model\n",
    "        if complexity == \"complex\":\n",
    "            print(\"[Using GPT-4 for complex question]\")\n",
    "            model = self.complex_model\n",
    "        else:\n",
    "            print(\"[Using GPT-3.5 for simple question]\")\n",
    "            model = self.simple_model\n",
    "        \n",
    "        # Get answer\n",
    "        response = model.invoke(question)\n",
    "        return response.content\n",
    "\n",
    "# Test it\n",
    "selector = SmartModelSelector()\n",
    "\n",
    "print(\"Simple:\", selector.answer(\"What's the capital of France?\"))\n",
    "print(\"\\nComplex:\", selector.answer(\"Explain the philosophical implications of quantum mechanics\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: model_comparison.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: model_comparison.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def compare_models(question):\n",
    "    \"\"\"Compare different models on the same question\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        \"GPT-3.5\": ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        \"Local\": Ollama(model=\"llama2\")\n",
    "    }\n",
    "    \n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = model.invoke(question)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            # Get content\n",
    "            if hasattr(response, 'content'):\n",
    "                text = response.content\n",
    "            else:\n",
    "                text = str(response)\n",
    "            \n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Time: {elapsed:.2f}s\")\n",
    "            print(f\"  Response: {text[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name}: Failed - {e}\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "# Compare them\n",
    "compare_models(\"What makes a good friend?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_provider_assistant.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.5\n",
    "# File: multi_provider_assistant.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MultiProviderAssistant:\n",
    "    def __init__(self):\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        \n",
    "        # Available models\n",
    "        self.models = {\n",
    "            \"fast\": ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "            \"smart\": ChatOpenAI(model=\"gpt-4\"),\n",
    "            \"private\": Ollama(model=\"llama2\")\n",
    "        }\n",
    "        \n",
    "        self.current_model = \"fast\"\n",
    "        \n",
    "        # Conversation prompt\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "    \n",
    "    def switch_model(self, model_name):\n",
    "        \"\"\"Switch to different model\"\"\"\n",
    "        if model_name in self.models:\n",
    "            self.current_model = model_name\n",
    "            return f\"Switched to {model_name} model\"\n",
    "        return \"Model not available\"\n",
    "    \n",
    "    def chat(self, message):\n",
    "        \"\"\"Chat with current model\"\"\"\n",
    "        # Get history\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "        \n",
    "        # Build chain with current model\n",
    "        model = self.models[self.current_model]\n",
    "        chain = self.prompt | model\n",
    "        \n",
    "        # Get response\n",
    "        response = chain.invoke({\n",
    "            \"history\": history,\n",
    "            \"input\": message\n",
    "        })\n",
    "        \n",
    "        # Extract content\n",
    "        if hasattr(response, 'content'):\n",
    "            content = response.content\n",
    "        else:\n",
    "            content = str(response)\n",
    "        \n",
    "        # Save to memory\n",
    "        self.memory.save_context(\n",
    "            {\"input\": message},\n",
    "            {\"output\": content}\n",
    "        )\n",
    "        \n",
    "        return content\n",
    "\n",
    "# Interactive session\n",
    "def run():\n",
    "    assistant = MultiProviderAssistant()\n",
    "    \n",
    "    print(\"Multi-Provider Assistant Ready!\")\n",
    "    print(\"Commands: 'switch:fast/smart/private', 'quit'\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif user_input.startswith('switch:'):\n",
    "            model = user_input.split(':')[1]\n",
    "            print(assistant.switch_model(model))\n",
    "        else:\n",
    "            print(f\"Assistant ({assistant.current_model}):\", \n",
    "                  assistant.chat(user_input))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.5.1: Cost Tracker\n",
    "\n",
    "Build an assistant that:\n",
    "- Tracks spending across all models\n",
    "- Shows cost per conversation\n",
    "- Switches to cheaper models when budget is low\n",
    "- Provides daily spending reports\n",
    "\n",
    "Save as `providers/cost_tracker.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.5.2: Speed Optimizer\n",
    "\n",
    "Create a system that:\n",
    "- Measures response time for each model\n",
    "- Automatically picks the fastest available model\n",
    "- Falls back to slower models if fast ones fail\n",
    "- Shows performance statistics\n",
    "\n",
    "Save as `providers/speed_optimizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.5.3: Privacy Guardian\n",
    "\n",
    "Build an assistant that:\n",
    "- Detects if a question contains private information\n",
    "- Uses local models for private questions\n",
    "- Uses cloud models for general questions\n",
    "- Logs which model was used and why\n",
    "\n",
    "Save as `providers/privacy_guardian.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.6: Output parsers and structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: the_problem.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: the_problem.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Ask for structured data\n",
    "response = llm.invoke(\"\"\"\n",
    "List 3 books with title, author, and year.\n",
    "\"\"\")\n",
    "\n",
    "print(\"AI Response:\")\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Problem: How do we extract this data reliably?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: list_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: list_parser.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create a list parser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Create prompt with parser instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 5 {category}.\\n{format_instructions}\",\n",
    "    input_variables=[\"category\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Chain it together\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Get a real Python list!\n",
    "result = chain.invoke({\"category\": \"programming languages\"})\n",
    "print(\"Result type:\", type(result))\n",
    "print(\"Result:\", result)\n",
    "\n",
    "# Now you can use it like any list\n",
    "for i, lang in enumerate(result, 1):\n",
    "    print(f\"{i}. {lang}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: json_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: json_parser.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define what you want\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"title\", description=\"book title\"),\n",
    "    ResponseSchema(name=\"author\", description=\"book author\"),\n",
    "    ResponseSchema(name=\"year\", description=\"publication year\")\n",
    "]\n",
    "\n",
    "# Create parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Create prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Recommend one science fiction book.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Run it\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({})\n",
    "print(\"Structured data:\", result)\n",
    "print(f\"\\nTitle: {result['title']}\")\n",
    "print(f\"Author: {result['author']}\")\n",
    "print(f\"Year: {result['year']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: pydantic_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: pydantic_parser.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define your data model\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"person's full name\")\n",
    "    age: int = Field(description=\"age in years\")\n",
    "    occupation: str = Field(description=\"their job\")\n",
    "    hobby: str = Field(description=\"favorite hobby\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "# Create prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Create a fictional character profile.\\n{format_instructions}\",\n",
    "    input_variables=[],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Run it\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "person = chain.invoke({})\n",
    "\n",
    "# You get a real Python object!\n",
    "print(f\"Name: {person.name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Job: {person.occupation}\")\n",
    "print(f\"Hobby: {person.hobby}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: recipe_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: recipe_parser.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define recipe structure\n",
    "class Ingredient(BaseModel):\n",
    "    item: str = Field(description=\"ingredient name\")\n",
    "    amount: str = Field(description=\"quantity needed\")\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"recipe name\")\n",
    "    cook_time: int = Field(description=\"minutes to cook\")\n",
    "    difficulty: str = Field(description=\"easy, medium, or hard\")\n",
    "    ingredients: List[Ingredient] = Field(description=\"list of ingredients\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# Create prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Create a simple recipe for {dish}.\n",
    "\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"dish\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Build chain\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Get structured recipe\n",
    "recipe = chain.invoke({\"dish\": \"chocolate chip cookies\"})\n",
    "\n",
    "print(f\"Recipe: {recipe.name}\")\n",
    "print(f\"Time: {recipe.cook_time} minutes\")\n",
    "print(f\"Difficulty: {recipe.difficulty}\")\n",
    "print(\"\\nIngredients:\")\n",
    "for ing in recipe.ingredients:\n",
    "    print(f\"  - {ing.amount} {ing.item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: safe_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: safe_parser.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Product(BaseModel):\n",
    "    name: str = Field(description=\"product name\")\n",
    "    price: float = Field(description=\"price in dollars\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Product)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "def safe_parse(text: str) -> Product:\n",
    "    \"\"\"Parse with error handling\"\"\"\n",
    "    try:\n",
    "        # Try to parse\n",
    "        result = parser.parse(text)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Parse failed: {e}\")\n",
    "        print(\"Attempting to fix...\")\n",
    "        \n",
    "        # Ask LLM to fix it\n",
    "        fix_prompt = f\"\"\"Fix this JSON to match the required format:\n",
    "        {text}\n",
    "        \n",
    "        Required format: {parser.get_format_instructions()}\"\"\"\n",
    "        \n",
    "        fixed = llm.invoke(fix_prompt)\n",
    "        return parser.parse(fixed.content)\n",
    "\n",
    "# Test with bad JSON\n",
    "bad_json = '{\"name\": \"Laptop\", \"price\": \"one thousand\"}'  # price should be number!\n",
    "\n",
    "result = safe_parse(bad_json)\n",
    "print(f\"Fixed result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: custom_parser.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: custom_parser.py\n",
    "\n",
    "from langchain_core.output_parsers.base import BaseOutputParser\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class BulletPointParser(BaseOutputParser):\n",
    "    \"\"\"Parse bullet points into a list\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract bullet points from text\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        bullet_points = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Check for various bullet formats\n",
    "            if line.startswith(('\u2022', '-', '*', '\u2192')):\n",
    "                # Remove bullet and clean\n",
    "                clean_line = line[1:].strip()\n",
    "                bullet_points.append(clean_line)\n",
    "            elif line and line[0].isdigit() and '.' in line:\n",
    "                # Numbered list (1. Item)\n",
    "                parts = line.split('.', 1)\n",
    "                if len(parts) > 1:\n",
    "                    bullet_points.append(parts[1].strip())\n",
    "        \n",
    "        return bullet_points\n",
    "    \n",
    "    def get_format_instructions(self) -> str:\n",
    "        return \"Format your response as a bulleted list using \u2022 or - or *\"\n",
    "\n",
    "# Use it\n",
    "load_dotenv()\n",
    "\n",
    "parser = BulletPointParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 3 benefits of {topic}.\\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "benefits = chain.invoke({\"topic\": \"exercise\"})\n",
    "print(\"Parsed benefits:\")\n",
    "for i, benefit in enumerate(benefits, 1):\n",
    "    print(f\"{i}. {benefit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: form_extractor.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: form_extractor.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ContactForm(BaseModel):\n",
    "    name: str = Field(description=\"person's full name\")\n",
    "    email: str = Field(description=\"email address\")\n",
    "    phone: Optional[str] = Field(description=\"phone number if provided\")\n",
    "    company: Optional[str] = Field(description=\"company name if mentioned\")\n",
    "    request: str = Field(description=\"what they want\")\n",
    "    \n",
    "    @validator('email')\n",
    "    def email_must_be_valid(cls, v):\n",
    "        if '@' not in v:\n",
    "            raise ValueError('Invalid email')\n",
    "        return v\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ContactForm)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Extract contact information from this message:\n",
    "\n",
    "{message}\n",
    "\n",
    "{format_instructions}\"\"\",\n",
    "    input_variables=[\"message\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Test with a real email\n",
    "email = \"\"\"\n",
    "Hi there,\n",
    "\n",
    "My name is John Smith and I work at TechCorp. You can reach me at \n",
    "john.smith@techcorp.com or call me at 555-0123.\n",
    "\n",
    "I'm interested in getting a demo of your product for our team.\n",
    "\n",
    "Thanks,\n",
    "John\n",
    "\"\"\"\n",
    "\n",
    "contact = chain.invoke({\"message\": email})\n",
    "\n",
    "print(\"Extracted Contact:\")\n",
    "print(f\"Name: {contact.name}\")\n",
    "print(f\"Email: {contact.email}\")\n",
    "print(f\"Phone: {contact.phone}\")\n",
    "print(f\"Company: {contact.company}\")\n",
    "print(f\"Request: {contact.request}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: validation.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.6\n",
    "# File: validation.py\n",
    "\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Order(BaseModel):\n",
    "    quantity: int = Field(description=\"number of items\")\n",
    "    price_each: float = Field(description=\"price per item\")\n",
    "    discount_percent: int = Field(description=\"discount percentage (0-100)\")\n",
    "    \n",
    "    @validator('quantity')\n",
    "    def quantity_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError('Quantity must be positive')\n",
    "        return v\n",
    "    \n",
    "    @validator('discount_percent')\n",
    "    def discount_valid(cls, v):\n",
    "        if v < 0 or v > 100:\n",
    "            raise ValueError('Discount must be 0-100')\n",
    "        return v\n",
    "    \n",
    "    def calculate_total(self):\n",
    "        subtotal = self.quantity * self.price_each\n",
    "        discount = subtotal * (self.discount_percent / 100)\n",
    "        return subtotal - discount\n",
    "\n",
    "# Parser will enforce these rules!\n",
    "parser = PydanticOutputParser(pydantic_object=Order)\n",
    "\n",
    "# If LLM returns invalid data, you'll know immediately\n",
    "print(\"Parser ready with validation rules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6.1: Email Analyzer\n",
    "\n",
    "Build a parser that extracts from emails:\n",
    "- Sender details (name, email, company)\n",
    "- Email category (support, sales, complaint)\n",
    "- Sentiment (positive, negative, neutral)\n",
    "- Action required (yes/no)\n",
    "- Priority level (high, medium, low)\n",
    "\n",
    "Save as `parsers/email_analyzer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6.2: Meeting Notes Parser\n",
    "\n",
    "Create a system that extracts from meeting notes:\n",
    "- Attendees list\n",
    "- Key decisions made\n",
    "- Action items with owners\n",
    "- Follow-up dates\n",
    "- Main topics discussed\n",
    "\n",
    "Save as `parsers/meeting_parser.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6.3: Product Review Extractor\n",
    "\n",
    "Build a parser that extracts from reviews:\n",
    "- Overall rating (1-5)\n",
    "- Pros list\n",
    "- Cons list\n",
    "- Would recommend (yes/no)\n",
    "- Key product features mentioned\n",
    "\n",
    "Save as `parsers/review_parser.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11.7: Debugging LangChain applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: see_everything.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: see_everything.py\n",
    "\n",
    "from langchain_core.globals import set_debug, set_verbose\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Turn on debug mode\n",
    "set_debug(True)\n",
    "set_verbose(True)\n",
    "\n",
    "# Now run any LangChain code\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt | llm\n",
    "\n",
    "# This will show EVERYTHING\n",
    "result = chain.invoke({\"topic\": \"debugging\"})\n",
    "\n",
    "# Turn it off when done\n",
    "set_debug(False)\n",
    "set_verbose(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: common_problems.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: common_problems.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def debug_api_key():\n",
    "    \"\"\"Check if API key is loaded\"\"\"\n",
    "    key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not key:\n",
    "        print(\"\u274c No API key found!\")\n",
    "        print(\"Fix: Check your .env file\")\n",
    "    else:\n",
    "        print(f\"\u2705 API key loaded: {key[:7]}...\")\n",
    "\n",
    "def debug_chain_error():\n",
    "    \"\"\"Debug a broken chain\"\"\"\n",
    "    try:\n",
    "        # Intentionally broken chain\n",
    "        prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "        llm = ChatOpenAI()\n",
    "        chain = prompt | llm\n",
    "        \n",
    "        # Missing required variable!\n",
    "        result = chain.invoke({})  # No 'topic' provided\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "        print(\"Fix: Check all required variables are provided\")\n",
    "\n",
    "def debug_model_response():\n",
    "    \"\"\"Debug unexpected model responses\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    \n",
    "    # Add system message for consistency\n",
    "    from langchain_classic.schema import SystemMessage, HumanMessage\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant. Always respond with exactly 'OK' to test messages.\"),\n",
    "        HumanMessage(content=\"This is a test\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    if response.content.strip() == \"OK\":\n",
    "        print(\"\u2705 Model responding correctly\")\n",
    "    else:\n",
    "        print(f\"\u274c Unexpected response: {response.content}\")\n",
    "        print(\"Fix: Check temperature, prompts, and model settings\")\n",
    "\n",
    "# Run all checks\n",
    "print(\"Running diagnostics...\\n\")\n",
    "debug_api_key()\n",
    "debug_chain_error()\n",
    "debug_model_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: chain_debugger.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: chain_debugger.py (formerly agent_debugger.py)\n",
    "# Updated to focus on chain debugging instead of deprecated agent features\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.globals import set_debug, set_verbose\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Example 1: Debug a simple chain\n",
    "def debug_simple_chain():\n",
    "    \"\"\"Debug a basic prompt -> LLM -> parser chain\"\"\"\n",
    "    print(\"\\n=== Debugging Simple Chain ===\")\n",
    "    \n",
    "    # Turn on verbose mode to see what's happening\n",
    "    set_verbose(True)\n",
    "    \n",
    "    # Create chain components\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a helpful assistant. Answer this question: {question}\"\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    # Test with debugging enabled\n",
    "    print(\"Testing chain with debugging enabled...\")\n",
    "    try:\n",
    "        result = chain.invoke({\n",
    "            \"question\": \"What is the capital of France?\"\n",
    "        })\n",
    "        print(f\"\\nFinal answer: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Chain failed: {e}\")\n",
    "        print(\"\\nCommon fixes:\")\n",
    "        print(\"1. Check prompt variables match input\")\n",
    "        print(\"2. Verify API key is set\")\n",
    "        print(\"3. Check model name is valid\")\n",
    "    \n",
    "    # Turn off verbose mode\n",
    "    set_verbose(False)\n",
    "\n",
    "# Example 2: Debug a multi-step chain\n",
    "def debug_multi_step_chain():\n",
    "    \"\"\"Debug a chain with multiple steps\"\"\"\n",
    "    print(\"\\n=== Debugging Multi-Step Chain ===\")\n",
    "    \n",
    "    # Enable full debug mode for detailed output\n",
    "    set_debug(True)\n",
    "    \n",
    "    # Step 1: Generate a story\n",
    "    story_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Write a one-sentence story about {animal}\"\n",
    "    )\n",
    "    \n",
    "    # Step 2: Extract the moral\n",
    "    moral_prompt = ChatPromptTemplate.from_template(\n",
    "        \"What is the moral of this story: {story}\"\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0.7)\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # Build chains\n",
    "    story_chain = story_prompt | llm | parser\n",
    "    \n",
    "    try:\n",
    "        # Run first chain\n",
    "        print(\"Step 1: Generating story...\")\n",
    "        story = story_chain.invoke({\"animal\": \"a wise owl\"})\n",
    "        print(f\"Story: {story}\")\n",
    "        \n",
    "        # Run second chain with output from first\n",
    "        print(\"\\nStep 2: Extracting moral...\")\n",
    "        moral_chain = moral_prompt | llm | parser\n",
    "        moral = moral_chain.invoke({\"story\": story})\n",
    "        print(f\"Moral: {moral}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Multi-step chain failed: {e}\")\n",
    "        print(\"Debug tip: Check intermediate outputs between steps\")\n",
    "    \n",
    "    # Turn off debug mode\n",
    "    set_debug(False)\n",
    "\n",
    "# Example 3: Debug chain with error handling\n",
    "def debug_chain_with_errors():\n",
    "    \"\"\"Debug common chain errors\"\"\"\n",
    "    print(\"\\n=== Debugging Chain Errors ===\")\n",
    "    \n",
    "    # Test 1: Missing variable error\n",
    "    print(\"\\n1. Testing missing variable error:\")\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"Tell me about {topic} and {subtopic}\"\n",
    "        )\n",
    "        llm = ChatOpenAI()\n",
    "        chain = prompt | llm\n",
    "        \n",
    "        # This will fail - missing 'subtopic'\n",
    "        result = chain.invoke({\"topic\": \"Python\"})\n",
    "    except KeyError as e:\n",
    "        print(f\"\u2713 Caught expected error: Missing variable {e}\")\n",
    "        print(\"Fix: Ensure all template variables are provided\")\n",
    "    \n",
    "    # Test 2: Invalid model name\n",
    "    print(\"\\n2. Testing invalid model error:\")\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=\"invalid-model-xyz\")\n",
    "        result = llm.invoke(\"Test\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u2713 Caught expected error: {str(e)[:50]}...\")\n",
    "        print(\"Fix: Use valid model names like 'gpt-3.5-turbo' or 'gpt-4'\")\n",
    "    \n",
    "    # Test 3: Chain with recovery\n",
    "    print(\"\\n3. Testing chain with fallback:\")\n",
    "    try:\n",
    "        primary_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        fallback_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\"Answer: {question}\")\n",
    "        \n",
    "        # Try primary chain first\n",
    "        try:\n",
    "            chain = prompt | primary_llm\n",
    "            result = chain.invoke({\"question\": \"What is 2+2?\"})\n",
    "            print(f\"Primary chain succeeded: {result.content}\")\n",
    "        except:\n",
    "            # Fallback to simpler model\n",
    "            print(\"Primary failed, using fallback...\")\n",
    "            chain = prompt | fallback_llm\n",
    "            result = chain.invoke({\"question\": \"What is 2+2?\"})\n",
    "            print(f\"Fallback chain succeeded: {result.content}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Both chains failed: {e}\")\n",
    "\n",
    "# Example 4: Performance debugging\n",
    "def debug_chain_performance():\n",
    "    \"\"\"Debug chain performance\"\"\"\n",
    "    print(\"\\n=== Debugging Chain Performance ===\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "    llm = ChatOpenAI()\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # Time each component\n",
    "    print(\"Timing chain components:\")\n",
    "    \n",
    "    # Time prompt formatting\n",
    "    start = time.time()\n",
    "    formatted = prompt.format_messages(topic=\"testing\")\n",
    "    prompt_time = time.time() - start\n",
    "    print(f\"  Prompt formatting: {prompt_time:.4f}s\")\n",
    "    \n",
    "    # Time LLM call\n",
    "    start = time.time()\n",
    "    response = llm.invoke(\"Quick test\")\n",
    "    llm_time = time.time() - start\n",
    "    print(f\"  LLM call: {llm_time:.2f}s\")\n",
    "    \n",
    "    # Time full chain\n",
    "    start = time.time()\n",
    "    chain = prompt | llm | parser\n",
    "    result = chain.invoke({\"topic\": \"speed\"})\n",
    "    chain_time = time.time() - start\n",
    "    print(f\"  Full chain: {chain_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nBottleneck analysis:\")\n",
    "    if llm_time > chain_time * 0.9:\n",
    "        print(\"  \u2192 LLM call is the bottleneck (normal)\")\n",
    "    if prompt_time > 0.01:\n",
    "        print(\"  \u2192 Prompt formatting is slow (unusual)\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\ud83d\udd0d LangChain Chain Debugging Examples\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Run all debugging examples\n",
    "    debug_simple_chain()\n",
    "    debug_multi_step_chain()\n",
    "    debug_chain_with_errors()\n",
    "    debug_chain_performance()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\u2705 Debugging examples complete!\")\n",
    "    print(\"\\nKey debugging tools:\")\n",
    "    print(\"  - set_verbose(True): See chain execution\")\n",
    "    print(\"  - set_debug(True): See detailed internals\")\n",
    "    print(\"  - try/except: Handle and understand errors\")\n",
    "    print(\"  - time.time(): Measure performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: performance_check.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: performance_check.py\n",
    "\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def time_operation(name, func):\n",
    "    \"\"\"Time any operation\"\"\"\n",
    "    start = time.time()\n",
    "    result = func()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{name}: {elapsed:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "# Test different parts\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Time prompt formatting\n",
    "def format_prompt():\n",
    "    return prompt.format_messages(topic=\"testing\")\n",
    "\n",
    "# Time LLM call\n",
    "def call_llm():\n",
    "    return llm.invoke(\"Quick test\")\n",
    "\n",
    "# Time full chain\n",
    "def run_chain():\n",
    "    chain = prompt | llm\n",
    "    return chain.invoke({\"topic\": \"speed\"})\n",
    "\n",
    "print(\"Performance Analysis:\")\n",
    "time_operation(\"Prompt formatting\", format_prompt)\n",
    "time_operation(\"LLM call\", call_llm)\n",
    "time_operation(\"Full chain\", run_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: debug_wrapper.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: debug_wrapper.py\n",
    "\n",
    "import time\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class DebugChain:\n",
    "    \"\"\"Wrap any chain with debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, chain, name=\"Chain\"):\n",
    "        self.chain = chain\n",
    "        self.name = name\n",
    "        self.history = []\n",
    "    \n",
    "    def invoke(self, inputs):\n",
    "        \"\"\"Run with debugging info\"\"\"\n",
    "        \n",
    "        # Record start\n",
    "        start = time.time()\n",
    "        print(f\"\\n\ud83d\udd0d [{self.name}] Starting...\")\n",
    "        print(f\"\ud83d\udce5 Inputs: {json.dumps(inputs, indent=2)}\")\n",
    "        \n",
    "        try:\n",
    "            # Run the chain\n",
    "            result = self.chain.invoke(inputs)\n",
    "            \n",
    "            # Record success\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"\u2705 [{self.name}] Success ({elapsed:.2f}s)\")\n",
    "            \n",
    "            # Save to history\n",
    "            self.history.append({\n",
    "                \"inputs\": inputs,\n",
    "                \"output\": str(result)[:100],  # Truncate\n",
    "                \"time\": elapsed,\n",
    "                \"success\": True\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record failure\n",
    "            print(f\"\u274c [{self.name}] Failed: {e}\")\n",
    "            \n",
    "            self.history.append({\n",
    "                \"inputs\": inputs,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "            \n",
    "            raise\n",
    "    \n",
    "    def show_stats(self):\n",
    "        \"\"\"Show debugging statistics\"\"\"\n",
    "        total = len(self.history)\n",
    "        successes = sum(1 for h in self.history if h[\"success\"])\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca {self.name} Statistics:\")\n",
    "        print(f\"  Total runs: {total}\")\n",
    "        print(f\"  Successes: {successes}\")\n",
    "        print(f\"  Failures: {total - successes}\")\n",
    "        \n",
    "        if successes > 0:\n",
    "            avg_time = sum(h[\"time\"] for h in self.history if h[\"success\"]) / successes\n",
    "            print(f\"  Average time: {avg_time:.2f}s\")\n",
    "\n",
    "# Use it\n",
    "load_dotenv()\n",
    "\n",
    "# Create a normal chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "chain = prompt | llm\n",
    "\n",
    "# Wrap it for debugging\n",
    "debug_chain = DebugChain(chain, \"TopicExplainer\")\n",
    "\n",
    "# Use it normally\n",
    "debug_chain.invoke({\"topic\": \"Python\"})\n",
    "debug_chain.invoke({\"topic\": \"LangChain\"})\n",
    "\n",
    "# See statistics\n",
    "debug_chain.show_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: best_practices.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11, Section 11.7\n",
    "# File: best_practices.py\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 1. Always handle missing variables in prompts\n",
    "def safe_chain_invoke(chain, inputs):\n",
    "    \"\"\"Safely invoke a chain with error handling\"\"\"\n",
    "    try:\n",
    "        return chain.invoke(inputs)\n",
    "    except KeyError as e:\n",
    "        raise ValueError(\n",
    "            f\"Missing required input: {e}. \"\n",
    "            f\"Please provide all required variables.\"\n",
    "        )\n",
    "\n",
    "# 2. Use meaningful error messages\n",
    "def check_api_key(api_key):\n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"OpenAI API key not found. \"\n",
    "            \"Please set OPENAI_API_KEY in your .env file\"\n",
    "        )  # Clear problem AND solution\n",
    "\n",
    "# 3. Log important steps\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_chain_execution(input_data):\n",
    "    logger.info(f\"Starting chain with input: {input_data}\")\n",
    "    # result = chain.invoke(input_data)\n",
    "    # logger.info(f\"Chain completed successfully\")\n",
    "\n",
    "# 4. Test individual components\n",
    "def test_components_separately():\n",
    "    \"\"\"Test each part of your chain individually\"\"\"\n",
    "    # Test prompt alone\n",
    "    prompt = ChatPromptTemplate.from_template(\"Test: {input}\")\n",
    "    assert prompt.format(input=\"test\") == \"Test: test\"\n",
    "    \n",
    "    # Test model alone\n",
    "    llm = ChatOpenAI()\n",
    "    # response = llm.invoke(\"Test message\")\n",
    "    \n",
    "    # Test parser alone\n",
    "    parser = StrOutputParser()\n",
    "    # parsed = parser.parse(response)\n",
    "    \n",
    "    # THEN test the full chain\n",
    "    # chain = prompt | llm | parser\n",
    "\n",
    "# 5. Keep debug code separate\n",
    "DEBUG_MODE = os.getenv(\"DEBUG_MODE\", \"False\").lower() == \"true\"\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    from langchain_core.globals import set_debug\n",
    "    set_debug(True)\n",
    "    print(\"Debug mode enabled\")\n",
    "\n",
    "# 6. Build chains incrementally\n",
    "def build_robust_chain():\n",
    "    \"\"\"Build chains step by step with validation\"\"\"\n",
    "    # Step 1: Create and validate prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\"Answer: {question}\")\n",
    "    \n",
    "    # Step 2: Create model with fallback\n",
    "    primary_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "    fallback_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    \n",
    "    # Step 3: Add parser\n",
    "    parser = StrOutputParser()\n",
    "    \n",
    "    # Step 4: Compose with error handling\n",
    "    def safe_chain(question):\n",
    "        try:\n",
    "            chain = prompt | primary_llm | parser\n",
    "            return chain.invoke({\"question\": question})\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Primary chain failed: {e}, using fallback\")\n",
    "            chain = prompt | fallback_llm | parser\n",
    "            return chain.invoke({\"question\": question})\n",
    "    \n",
    "    return safe_chain\n",
    "\n",
    "# 7. Monitor chain performance\n",
    "import time\n",
    "\n",
    "def monitor_chain_performance(chain, inputs):\n",
    "    \"\"\"Monitor and log chain performance\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke(inputs)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"Chain executed in {execution_time:.2f} seconds\")\n",
    "        \n",
    "        if execution_time > 10:\n",
    "            logger.warning(\"Chain took longer than 10 seconds\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chain failed after {time.time() - start_time:.2f} seconds: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: challenge_project_starter.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 11 Challenge Project\n",
    "# File: challenge_project_starter.py\n",
    "# Smart Study Assistant - Starter Code\n",
    "\n",
    "\"\"\"\n",
    "Chapter 11 Challenge Project: Smart Study Assistant\n",
    "\n",
    "This starter code provides the structure for your study assistant.\n",
    "Your job is to implement all the methods and add the features!\n",
    "\n",
    "Requirements to implement:\n",
    "1. Multi-provider support (GPT-3.5, GPT-4, local models)\n",
    "2. Smart conversation modes (Teacher, Quiz, Summary, Discussion)\n",
    "3. Memory management (persistent between sessions)\n",
    "4. Structured output (parsed study materials)\n",
    "5. Production quality (error handling, monitoring, debugging)\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# DATA MODELS - Define structured outputs\n",
    "# ============================================================================\n",
    "\n",
    "class KeyPoint(BaseModel):\n",
    "    \"\"\"Model for a key learning point\"\"\"\n",
    "    concept: str = Field(description=\"The main concept or term\")\n",
    "    explanation: str = Field(description=\"Clear explanation of the concept\")\n",
    "    example: Optional[str] = Field(description=\"An example if relevant\")\n",
    "\n",
    "class StudyNotes(BaseModel):\n",
    "    \"\"\"Model for structured study notes\"\"\"\n",
    "    topic: str = Field(description=\"The main topic\")\n",
    "    key_points: List[KeyPoint] = Field(description=\"Key learning points\")\n",
    "    summary: str = Field(description=\"Brief summary of the topic\")\n",
    "\n",
    "class QuizQuestion(BaseModel):\n",
    "    \"\"\"Model for quiz questions\"\"\"\n",
    "    question: str = Field(description=\"The question\")\n",
    "    answer: str = Field(description=\"The correct answer\")\n",
    "    difficulty: str = Field(description=\"easy, medium, or hard\")\n",
    "\n",
    "# ============================================================================\n",
    "# SMART STUDY ASSISTANT CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class SmartStudyAssistant:\n",
    "    \"\"\"Your intelligent study assistant that helps you learn effectively\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the study assistant with all components\"\"\"\n",
    "        \n",
    "        # TODO: Initialize models (GPT-3.5, GPT-4, and fallback)\n",
    "        self.models = {\n",
    "            'simple': None,  # TODO: Initialize GPT-3.5\n",
    "            'complex': None,  # TODO: Initialize GPT-4\n",
    "            'private': None  # TODO: Initialize local model (optional)\n",
    "        }\n",
    "        \n",
    "        # TODO: Initialize memory system\n",
    "        self.memory = None  # TODO: Setup ConversationBufferMemory\n",
    "        \n",
    "        # TODO: Initialize conversation modes with different prompts\n",
    "        self.mode_prompts = {\n",
    "            'teacher': None,  # TODO: Create teacher mode prompt\n",
    "            'quiz': None,     # TODO: Create quiz mode prompt\n",
    "            'summary': None,  # TODO: Create summary mode prompt\n",
    "            'discussion': None # TODO: Create discussion mode prompt\n",
    "        }\n",
    "        \n",
    "        # TODO: Initialize output parsers\n",
    "        self.parsers = {\n",
    "            'notes': None,    # TODO: PydanticOutputParser for StudyNotes\n",
    "            'quiz': None      # TODO: PydanticOutputParser for QuizQuestion\n",
    "        }\n",
    "        \n",
    "        # Current state\n",
    "        self.current_mode = 'teacher'\n",
    "        self.current_topic = None\n",
    "        self.session_data = {\n",
    "            'topics_covered': [],\n",
    "            'total_messages': 0,\n",
    "            'session_start': datetime.now().isoformat(),\n",
    "            'cost_tracking': {'gpt-3.5': 0, 'gpt-4': 0}\n",
    "        }\n",
    "        \n",
    "        # Debug mode\n",
    "        self.debug_mode = False\n",
    "        \n",
    "        print(\"\ud83c\udf93 Smart Study Assistant Initialized!\")\n",
    "        print(\"Commands: /mode, /topic, /save, /load, /stats, /help, /quit\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CORE METHODS TO IMPLEMENT\n",
    "    # ========================================================================\n",
    "    \n",
    "    def classify_complexity(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify if the input requires simple or complex model\n",
    "        \n",
    "        TODO: Implement logic to determine complexity\n",
    "        - Simple questions \u2192 'simple'\n",
    "        - Complex topics \u2192 'complex'  \n",
    "        - Private data \u2192 'private'\n",
    "        \"\"\"\n",
    "        # TODO: Implement complexity classification\n",
    "        return 'simple'  # Default for now\n",
    "    \n",
    "    def select_model(self, complexity: str):\n",
    "        \"\"\"\n",
    "        Select the appropriate model based on complexity\n",
    "        \n",
    "        TODO: Return the right model from self.models\n",
    "        TODO: Add fallback logic if primary model fails\n",
    "        \"\"\"\n",
    "        # TODO: Implement model selection with fallback\n",
    "        pass\n",
    "    \n",
    "    def create_mode_prompts(self):\n",
    "        \"\"\"\n",
    "        Create specialized prompts for each learning mode\n",
    "        \n",
    "        TODO: Create ChatPromptTemplate for each mode:\n",
    "        - Teacher: Patient explanations with examples\n",
    "        - Quiz: Generate questions to test knowledge\n",
    "        - Summary: Create concise study notes\n",
    "        - Discussion: Socratic dialogue\n",
    "        \"\"\"\n",
    "        # TODO: Implement all mode prompts\n",
    "        pass\n",
    "    \n",
    "    def process_message(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Process user message and generate response\n",
    "        \n",
    "        TODO: Main processing logic:\n",
    "        1. Classify complexity\n",
    "        2. Select appropriate model\n",
    "        3. Use current mode's prompt\n",
    "        4. Include memory context\n",
    "        5. Parse output if structured\n",
    "        6. Handle errors gracefully\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # TODO: Implement complete message processing\n",
    "            \n",
    "            # Update session data\n",
    "            self.session_data['total_messages'] += 1\n",
    "            \n",
    "            # Placeholder response\n",
    "            return \"TODO: Implement message processing\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug: Error in process_message: {e}\")\n",
    "            return \"I encountered an error. Let me try a different approach...\"\n",
    "    \n",
    "    def save_session(self, filename: str = None):\n",
    "        \"\"\"\n",
    "        Save current session to file\n",
    "        \n",
    "        TODO: Save:\n",
    "        - Conversation memory\n",
    "        - Session data\n",
    "        - Current topic and mode\n",
    "        \"\"\"\n",
    "        if not filename:\n",
    "            filename = f\"study_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        # TODO: Implement session saving\n",
    "        print(f\"Session saved to {filename}\")\n",
    "    \n",
    "    def load_session(self, filename: str):\n",
    "        \"\"\"\n",
    "        Load a previous session\n",
    "        \n",
    "        TODO: Load:\n",
    "        - Conversation memory\n",
    "        - Session data\n",
    "        - Topic and mode\n",
    "        \"\"\"\n",
    "        # TODO: Implement session loading\n",
    "        print(f\"Session loaded from {filename}\")\n",
    "    \n",
    "    def generate_study_notes(self, topic: str) -> StudyNotes:\n",
    "        \"\"\"\n",
    "        Generate structured study notes for a topic\n",
    "        \n",
    "        TODO: Use the notes parser to create structured output\n",
    "        \"\"\"\n",
    "        # TODO: Implement study notes generation\n",
    "        pass\n",
    "    \n",
    "    def generate_quiz(self, topic: str, difficulty: str = 'medium') -> List[QuizQuestion]:\n",
    "        \"\"\"\n",
    "        Generate quiz questions on a topic\n",
    "        \n",
    "        TODO: Create quiz questions with the quiz parser\n",
    "        \"\"\"\n",
    "        # TODO: Implement quiz generation\n",
    "        pass\n",
    "    \n",
    "    def show_statistics(self):\n",
    "        \"\"\"Show session statistics\"\"\"\n",
    "        print(\"\\n\ud83d\udcca Session Statistics:\")\n",
    "        print(f\"Topics covered: {', '.join(self.session_data['topics_covered'])}\")\n",
    "        print(f\"Total messages: {self.session_data['total_messages']}\")\n",
    "        print(f\"Session duration: {datetime.now() - datetime.fromisoformat(self.session_data['session_start'])}\")\n",
    "        \n",
    "        # Cost estimation\n",
    "        total_cost = sum(self.session_data['cost_tracking'].values())\n",
    "        print(f\"Estimated cost: ${total_cost:.4f}\")\n",
    "        \n",
    "        for model, cost in self.session_data['cost_tracking'].items():\n",
    "            if cost > 0:\n",
    "                print(f\"  {model}: ${cost:.4f}\")\n",
    "    \n",
    "    def show_help(self):\n",
    "        \"\"\"Show available commands\"\"\"\n",
    "        help_text = \"\"\"\n",
    "        \ud83d\udcda Available Commands:\n",
    "        \n",
    "        /mode [teacher|quiz|summary|discussion] - Change learning mode\n",
    "        /topic [topic_name] - Set current study topic\n",
    "        /save [filename] - Save current session\n",
    "        /load [filename] - Load previous session\n",
    "        /notes - Generate study notes for current topic\n",
    "        /quiz [easy|medium|hard] - Generate quiz questions\n",
    "        /stats - Show session statistics\n",
    "        /debug - Toggle debug mode\n",
    "        /help - Show this help message\n",
    "        /quit - Exit the assistant\n",
    "        \n",
    "        Just type normally to have a conversation in the current mode!\n",
    "        \"\"\"\n",
    "        print(help_text)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAIN INTERACTION LOOP\n",
    "    # ========================================================================\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main interaction loop\"\"\"\n",
    "        print(\"\ud83c\udf93 Welcome to your Smart Study Assistant!\")\n",
    "        print(\"What would you like to learn about today?\")\n",
    "        print(\"(Type /help for commands)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                \n",
    "                # Handle commands\n",
    "                if user_input.startswith('/'):\n",
    "                    if user_input == '/quit':\n",
    "                        print(\"\ud83d\udc4b Thanks for studying! Goodbye!\")\n",
    "                        break\n",
    "                    \n",
    "                    elif user_input.startswith('/mode'):\n",
    "                        # TODO: Implement mode switching\n",
    "                        parts = user_input.split()\n",
    "                        if len(parts) > 1:\n",
    "                            new_mode = parts[1]\n",
    "                            if new_mode in self.mode_prompts:\n",
    "                                self.current_mode = new_mode\n",
    "                                print(f\"\u2705 Switched to {new_mode} mode\")\n",
    "                            else:\n",
    "                                print(\"\u274c Invalid mode. Choose: teacher, quiz, summary, discussion\")\n",
    "                    \n",
    "                    elif user_input.startswith('/topic'):\n",
    "                        # TODO: Implement topic setting\n",
    "                        parts = user_input.split(maxsplit=1)\n",
    "                        if len(parts) > 1:\n",
    "                            self.current_topic = parts[1]\n",
    "                            self.session_data['topics_covered'].append(self.current_topic)\n",
    "                            print(f\"\u2705 Studying: {self.current_topic}\")\n",
    "                    \n",
    "                    elif user_input == '/stats':\n",
    "                        self.show_statistics()\n",
    "                    \n",
    "                    elif user_input == '/help':\n",
    "                        self.show_help()\n",
    "                    \n",
    "                    elif user_input == '/debug':\n",
    "                        self.debug_mode = not self.debug_mode\n",
    "                        print(f\"Debug mode: {'ON' if self.debug_mode else 'OFF'}\")\n",
    "                    \n",
    "                    elif user_input.startswith('/save'):\n",
    "                        # TODO: Implement save command\n",
    "                        parts = user_input.split(maxsplit=1)\n",
    "                        filename = parts[1] if len(parts) > 1 else None\n",
    "                        self.save_session(filename)\n",
    "                    \n",
    "                    elif user_input.startswith('/load'):\n",
    "                        # TODO: Implement load command\n",
    "                        parts = user_input.split(maxsplit=1)\n",
    "                        if len(parts) > 1:\n",
    "                            self.load_session(parts[1])\n",
    "                    \n",
    "                    elif user_input == '/notes':\n",
    "                        # TODO: Generate study notes\n",
    "                        if self.current_topic:\n",
    "                            print(f\"\ud83d\udcdd Generating notes for: {self.current_topic}\")\n",
    "                            # notes = self.generate_study_notes(self.current_topic)\n",
    "                            print(\"TODO: Implement notes generation\")\n",
    "                        else:\n",
    "                            print(\"\u274c Please set a topic first with /topic\")\n",
    "                    \n",
    "                    elif user_input.startswith('/quiz'):\n",
    "                        # TODO: Generate quiz\n",
    "                        if self.current_topic:\n",
    "                            parts = user_input.split()\n",
    "                            difficulty = parts[1] if len(parts) > 1 else 'medium'\n",
    "                            print(f\"\ud83d\udcdd Generating {difficulty} quiz for: {self.current_topic}\")\n",
    "                            # questions = self.generate_quiz(self.current_topic, difficulty)\n",
    "                            print(\"TODO: Implement quiz generation\")\n",
    "                        else:\n",
    "                            print(\"\u274c Please set a topic first with /topic\")\n",
    "                    \n",
    "                    else:\n",
    "                        print(\"\u274c Unknown command. Type /help for available commands\")\n",
    "                \n",
    "                else:\n",
    "                    # Regular conversation\n",
    "                    response = self.process_message(user_input)\n",
    "                    print(f\"\\n\ud83e\udd16 Assistant ({self.current_mode} mode): {response}\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n\ud83d\udc4b Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\u274c An error occurred: {e}\")\n",
    "                if self.debug_mode:\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run the assistant\n",
    "    assistant = SmartStudyAssistant()\n",
    "    \n",
    "    # TODO: Initialize all components properly\n",
    "    # assistant.create_mode_prompts()\n",
    "    # assistant.setup_memory()\n",
    "    # assistant.setup_parsers()\n",
    "    \n",
    "    # Run the main loop\n",
    "    assistant.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 11.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.7.1: Debug Dashboard\n",
    "\n",
    "Create a dashboard that:\n",
    "- Shows chain component status\n",
    "- Displays performance metrics\n",
    "- Tracks error rates\n",
    "- Provides quick fixes\n",
    "- Generates health reports\n",
    "\n",
    "Save as `debug/debug_dashboard.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.7.2: Performance Monitor\n",
    "\n",
    "Build a system that:\n",
    "- Tracks execution time for each chain component\n",
    "- Identifies slow parts\n",
    "- Suggests optimizations\n",
    "- Creates performance graphs\n",
    "- Alerts on slowdowns\n",
    "\n",
    "Save as `debug/performance_monitor.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.7.3: Error Recovery System\n",
    "\n",
    "Create a wrapper that:\n",
    "- Catches common errors\n",
    "- Attempts automatic fixes\n",
    "- Retries with exponential backoff\n",
    "- Logs all attempts\n",
    "- Falls back to simpler methods\n",
    "\n",
    "Save as `debug/error_recovery.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_11_langchain_intro_solutions.ipynb**\n",
    "- Proceed to **Chapter 12**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}