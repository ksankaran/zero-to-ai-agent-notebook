{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 13: Agent Memory - Solutions\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "âš ï¸ **Try the exercises in the main notebook first before viewing solutions!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.1.1 Solution: Memory Classification\n",
    "\n",
    "### Scenario Analysis\n",
    "\n",
    "**1. \"Let's talk about my upcoming trip to Paris.\"**\n",
    "- **Classification: Both Short-Term and Long-Term**\n",
    "- Short-term: Current conversation topic needs to be in active context\n",
    "- Long-term: The fact that user is planning a Paris trip is worth storing for future sessions\n",
    "\n",
    "**2. \"I'm allergic to peanuts.\"**\n",
    "- **Classification: Long-Term (primarily)**\n",
    "- This is critical safety/health information that should persist indefinitely\n",
    "- Every future conversation involving food should have access to this fact\n",
    "\n",
    "**3. \"What did I just say about the hotel?\"**\n",
    "- **Classification: Short-Term**\n",
    "- The word \"just\" signals immediate context\n",
    "- This is a reference to current conversation, not something to store persistently\n",
    "\n",
    "**4. Intermediate calculation during math problem**\n",
    "- **Classification: Short-Term Only**\n",
    "- Working memory for current task only\n",
    "- No value after the problem is solved\n",
    "\n",
    "**5. \"Remember, I prefer bullet points over long paragraphs.\"**\n",
    "- **Classification: Long-Term (primarily)**\n",
    "- Persistent user preference that should affect ALL future interactions\n",
    "- The word \"remember\" explicitly signals storage request\n",
    "\n",
    "**6. \"Today's weather is really nice.\"**\n",
    "- **Classification: Short-Term Only**\n",
    "- Ephemeral information with no future value\n",
    "- Weather will be different tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.1.2 Solution: Design a Memory Schema\n",
    "\n",
    "### Personal Finance Assistant - Memory Schema\n",
    "\n",
    "**1. Short-Term Memory:**\n",
    "```\n",
    "short_term_memory:\n",
    "  conversation_history:\n",
    "    - messages[]\n",
    "    - current_topic\n",
    "  working_state:\n",
    "    - current_calculations\n",
    "    - draft_budget\n",
    "    - pending_actions[]\n",
    "```\n",
    "\n",
    "**2. Long-Term Memory:**\n",
    "```\n",
    "long_term_memory:\n",
    "  user_profile:\n",
    "    - name, preferred_currency, timezone\n",
    "  financial_profile:\n",
    "    - income_sources[]\n",
    "    - recurring_expenses[]\n",
    "    - accounts[]\n",
    "    - financial_goals[]\n",
    "  preferences:\n",
    "    - budget_style\n",
    "    - notification_preferences\n",
    "```\n",
    "\n",
    "**3. Transfer Rules:**\n",
    "- Automatic: income, recurring bills, goals, preferences\n",
    "- Prompted: budgets created, major financial changes\n",
    "\n",
    "**4. Never Store:**\n",
    "- Bank account numbers, passwords, SSN, full credit card numbers\n",
    "- Store ranges instead of exact salary amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.1.3 Solution: Memory Retrieval Strategy\n",
    "\n",
    "### Multi-Stage Retrieval Algorithm\n",
    "\n",
    "**Stage 1: Critical Safety (Always Include)**\n",
    "- Query: allergies, dietary restrictions\n",
    "- No limit, all matches included\n",
    "\n",
    "**Stage 2: Semantic Search**\n",
    "- Embed query, search food/dining memories\n",
    "- Top 10 by similarity > 0.7\n",
    "\n",
    "**Stage 3: Recent Context**\n",
    "- Filter: last 90 days, dining category\n",
    "- Sort by recency Ã— relevance\n",
    "\n",
    "**Stage 4: Location/Logistics**\n",
    "- 2-3 most recent location facts\n",
    "\n",
    "**Conflict Resolution:**\n",
    "- Recency wins for preferences\n",
    "- Safety info never expires\n",
    "- Explicit updates override old facts\n",
    "\n",
    "**Output Format:**\n",
    "```\n",
    "CRITICAL: Vegetarian, Shellfish allergy\n",
    "PREFERENCES (High): Outdoor seating, quiet\n",
    "PREFERENCES (Medium): Italian cuisine\n",
    "RECENT: Tried Olive Garden - \"just okay\"\n",
    "LOCATION: Downtown Seattle, no car\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.2.1 Solution: Basic Memory Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "conversation_history = []\n",
    "\n",
    "# Add system prompt\n",
    "conversation_history.append({\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful math tutor.\"\n",
    "})\n",
    "\n",
    "def chat(user_message):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    print(f\"[Total messages: {len(conversation_history)}]\")\n",
    "    return assistant_message\n",
    "\n",
    "# Test\n",
    "print(\"User: What is 5 + 3?\")\n",
    "print(f\"Assistant: {chat('What is 5 + 3?')}\\n\")\n",
    "\n",
    "print(\"User: Now multiply that by 2\")\n",
    "print(f\"Assistant: {chat('Now multiply that by 2')}\\n\")\n",
    "\n",
    "print(\"User: What were we calculating?\")\n",
    "print(f\"Assistant: {chat('What were we calculating?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.2.2 Solution: Smart Token-Aware Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class TokenAwareMemory:\n",
    "    def __init__(self, system_prompt, max_tokens=500):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def estimate_tokens(self, text):\n",
    "        return int(len(text.split()) * 1.3)\n",
    "    \n",
    "    def total_tokens(self):\n",
    "        return sum(self.estimate_tokens(m[\"content\"]) for m in self.messages)\n",
    "    \n",
    "    def trim_if_needed(self):\n",
    "        while self.total_tokens() > self.max_tokens and len(self.messages) > 2:\n",
    "            for i, msg in enumerate(self.messages):\n",
    "                if msg[\"role\"] != \"system\":\n",
    "                    removed = self.messages.pop(i)\n",
    "                    print(f\"âš ï¸ TRIMMED: {msg['role']} message ({self.estimate_tokens(removed['content'])} tokens)\")\n",
    "                    break\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.trim_if_needed()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        self.trim_if_needed()\n",
    "        \n",
    "        print(f\"[Tokens: {self.total_tokens()}/{self.max_tokens}]\")\n",
    "        return reply\n",
    "\n",
    "# Test\n",
    "memory = TokenAwareMemory(\"You are a storyteller.\", max_tokens=500)\n",
    "print(memory.chat(\"Tell me a short story about a knight.\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.2.3 Solution: Conversation Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class AnalyticsMemory:\n",
    "    def __init__(self, system_prompt=None):\n",
    "        self.messages = []\n",
    "        self.start_time = None\n",
    "        self.last_activity = None\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.messages.append({\n",
    "                \"role\": \"system\", \"content\": system_prompt,\n",
    "                \"timestamp\": datetime.now(), \"char_count\": len(system_prompt)\n",
    "            })\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        now = datetime.now()\n",
    "        if self.start_time is None:\n",
    "            self.start_time = now\n",
    "        self.last_activity = now\n",
    "        self.messages.append({\"role\": role, \"content\": content, \"timestamp\": now, \"char_count\": len(content)})\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        self.add_message(\"user\", user_message)\n",
    "        api_messages = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in self.messages]\n",
    "        response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=api_messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        self.add_message(\"assistant\", reply)\n",
    "        return reply\n",
    "    \n",
    "    def get_stats(self):\n",
    "        counts = {\"user\": 0, \"assistant\": 0, \"system\": 0}\n",
    "        lengths = {\"user\": [], \"assistant\": [], \"system\": []}\n",
    "        longest = {\"content\": \"\", \"role\": \"\", \"length\": 0}\n",
    "        \n",
    "        for msg in self.messages:\n",
    "            counts[msg[\"role\"]] += 1\n",
    "            lengths[msg[\"role\"]].append(msg[\"char_count\"])\n",
    "            if msg[\"char_count\"] > longest[\"length\"]:\n",
    "                longest = {\"content\": msg[\"content\"][:100], \"role\": msg[\"role\"], \"length\": msg[\"char_count\"]}\n",
    "        \n",
    "        avg = {r: sum(l)/len(l) if l else 0 for r, l in lengths.items()}\n",
    "        duration = (self.last_activity - self.start_time).total_seconds() if self.start_time else 0\n",
    "        \n",
    "        return {\"total\": len(self.messages), \"counts\": counts, \"avg_lengths\": avg, \"longest\": longest, \"duration\": duration}\n",
    "\n",
    "# Test\n",
    "memory = AnalyticsMemory(\"You are concise.\")\n",
    "memory.chat(\"Hello!\")\n",
    "memory.chat(\"Explain machine learning.\")\n",
    "print(memory.get_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.3.1 Solution: Basic Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def summarize_messages(messages):\n",
    "    \"\"\"Summarize a list of conversation messages.\"\"\"\n",
    "    formatted = \"\\n\\n\".join(f\"{m['role'].upper()}: {m['content']}\" for m in messages)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Summarize this conversation, preserving key facts and decisions:\n",
    "\n",
    "{formatted}\n",
    "\n",
    "Provide a concise summary:\"\"\"\n",
    "        }],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I want to plan a vacation to Italy in June.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Italy in June is wonderful! Cities or coastal areas?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rome and coastal. 10 days, $3000 budget. I'm vegetarian.\"}\n",
    "]\n",
    "\n",
    "print(\"SUMMARY:\")\n",
    "print(summarize_messages(test_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.3.2 Solution: Triggered Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class SmartMemory:\n",
    "    def __init__(self, system_prompt=None, max_messages=15, keep_recent=5):\n",
    "        self.max_messages = max_messages\n",
    "        self.keep_recent = keep_recent\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def _generate_summary(self, messages):\n",
    "        text = \"\\n\".join(f\"{m['role'].upper()}: {m['content']}\" for m in messages)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Summarize concisely:\\n\\n{text}\"}],\n",
    "            max_tokens=250\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _maybe_summarize(self):\n",
    "        conversation = [m for m in self.messages if m[\"role\"] in (\"user\", \"assistant\")]\n",
    "        if len(conversation) <= self.max_messages:\n",
    "            return\n",
    "        \n",
    "        to_summarize = conversation[:-self.keep_recent]\n",
    "        to_keep = conversation[-self.keep_recent:]\n",
    "        summary = self._generate_summary(to_summarize)\n",
    "        \n",
    "        system_prompt = [m for m in self.messages if m[\"role\"] == \"system\" and \"Summary:\" not in m[\"content\"]]\n",
    "        self.messages = system_prompt + [{\"role\": \"system\", \"content\": f\"Summary: {summary}\"}] + to_keep\n",
    "        \n",
    "        print(f\"\\nðŸ”„ SUMMARIZED {len(to_summarize)} messages\\n\")\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self._maybe_summarize()\n",
    "        \n",
    "        response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=self.messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "# Test\n",
    "memory = SmartMemory(\"You are a travel assistant.\", max_messages=10, keep_recent=4)\n",
    "for q in [\"I want to visit Japan.\", \"When should I go?\", \"How expensive?\", \"What cities?\", \"Tell me about Tokyo.\", \"What about Kyoto?\"]:\n",
    "    print(f\"User: {q}\")\n",
    "    print(f\"AI: {memory.chat(q)[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.3.3 Solution: Domain-Specific Medical Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class MedicalConsultationMemory:\n",
    "    def __init__(self):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": \"You are a medical consultation assistant.\"}]\n",
    "        self.medical_summary = {\"symptoms\": [], \"duration\": None, \"medications\": [], \"recommendations\": []}\n",
    "    \n",
    "    def _extract_medical_info(self, messages):\n",
    "        text = \"\\n\".join(f\"{m['role'].upper()}: {m['content']}\" for m in messages)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"\"\"Extract medical info as JSON:\n",
    "{{\"symptoms\": [], \"duration\": null, \"medications\": [], \"recommendations\": []}}\n",
    "\n",
    "{text}\n",
    "\n",
    "JSON:\"\"\"}],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        try:\n",
    "            return json.loads(response.choices[0].message.content.strip())\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _update_summary(self, data):\n",
    "        if not data:\n",
    "            return\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s not in self.medical_summary[\"symptoms\"]:\n",
    "                self.medical_summary[\"symptoms\"].append(s)\n",
    "        if data.get(\"duration\"):\n",
    "            self.medical_summary[\"duration\"] = data[\"duration\"]\n",
    "        for m in data.get(\"medications\", []):\n",
    "            if m not in self.medical_summary[\"medications\"]:\n",
    "                self.medical_summary[\"medications\"].append(m)\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=self.messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        \n",
    "        extracted = self._extract_medical_info(self.messages[-4:])\n",
    "        self._update_summary(extracted)\n",
    "        return reply\n",
    "    \n",
    "    def get_medical_summary(self):\n",
    "        return self.medical_summary\n",
    "\n",
    "# Test\n",
    "consult = MedicalConsultationMemory()\n",
    "consult.chat(\"I've had headaches for a week, taking ibuprofen.\")\n",
    "print(\"Medical Summary:\", consult.get_medical_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.4.1 Solution: Basic Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_entities(message):\n",
    "    \"\"\"Extract entities from a message using LLM.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Extract entities as JSON:\n",
    "{{\"entities\": [{{\"name\": \"...\", \"type\": \"person|org|project|location\", \"info\": \"...\"}}]}}\n",
    "\n",
    "Message: {message}\n",
    "JSON:\"\"\"\n",
    "        }],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content)[\"entities\"]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Test\n",
    "message = \"John from marketing wants to discuss the Phoenix project with the Tokyo team.\"\n",
    "entities = extract_entities(message)\n",
    "\n",
    "print(\"Extracted Entities:\")\n",
    "for e in entities:\n",
    "    print(f\"  - {e['name']} ({e['type']}): {e.get('info', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.4.2 Solution: Entity Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityMemory:\n",
    "    def __init__(self):\n",
    "        self.entities = {}\n",
    "    \n",
    "    def update(self, name, entity_type, fact=None):\n",
    "        key = name.lower()\n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\"name\": name, \"type\": entity_type, \"facts\": [], \"mentions\": 0}\n",
    "        self.entities[key][\"mentions\"] += 1\n",
    "        if fact and fact not in self.entities[key][\"facts\"]:\n",
    "            self.entities[key][\"facts\"].append(fact)\n",
    "    \n",
    "    def get(self, name):\n",
    "        return self.entities.get(name.lower())\n",
    "    \n",
    "    def format_context(self, names):\n",
    "        lines = []\n",
    "        for name in names:\n",
    "            e = self.get(name)\n",
    "            if e:\n",
    "                facts = \"; \".join(e[\"facts\"][-3:]) or \"no details\"\n",
    "                lines.append(f\"- {e['name']} ({e['type']}): {facts}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Test\n",
    "memory = EntityMemory()\n",
    "memory.update(\"Sarah Chen\", \"person\", \"works in engineering\")\n",
    "memory.update(\"Project Atlas\", \"project\", \"deadline March 1st\")\n",
    "memory.update(\"Sarah Chen\", \"person\", \"leading backend team\")\n",
    "\n",
    "print(\"Sarah:\", memory.get(\"sarah chen\"))\n",
    "print(\"\\nContext:\")\n",
    "print(memory.format_context([\"Sarah Chen\", \"Project Atlas\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.4.3 Solution: Entity-Aware Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class EntityAgent:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.entities = {}\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f'Extract as JSON: {{\"entities\": [{{\"name\":\"...\",\"type\":\"...\",\"info\":\"...\"}}]}} from: {text}'}],\n",
    "                max_tokens=150\n",
    "            )\n",
    "            return json.loads(resp.choices[0].message.content).get(\"entities\", [])\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def store_entities(self, text):\n",
    "        for e in self.extract_entities(text):\n",
    "            key = e[\"name\"].lower()\n",
    "            if key not in self.entities:\n",
    "                self.entities[key] = {\"name\": e[\"name\"], \"type\": e[\"type\"], \"facts\": []}\n",
    "            if e.get(\"info\"):\n",
    "                self.entities[key][\"facts\"].append(e[\"info\"])\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        if \"what do you know about\" in user_input.lower():\n",
    "            name = user_input.lower().split(\"what do you know about\")[-1].strip().rstrip(\"?\")\n",
    "            e = self.entities.get(name)\n",
    "            if e:\n",
    "                return f\"I know {e['name']} is a {e['type']}. Facts: {'; '.join(e['facts']) or 'none'}\"\n",
    "            return f\"I don't have information about '{name}'.\"\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are helpful.\"}] + self.history[-10:]\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        resp = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "        reply = resp.choices[0].message.content\n",
    "        \n",
    "        self.store_entities(user_input)\n",
    "        self.history.extend([{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": reply}])\n",
    "        return reply\n",
    "\n",
    "# Test\n",
    "agent = EntityAgent()\n",
    "print(agent.chat(\"I'm working with Maria on the Sunrise project. She handles design.\"))\n",
    "print(agent.chat(\"What do you know about Maria?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.5.1 Solution: Basic Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "class SemanticMemory:\n",
    "    def __init__(self):\n",
    "        self.openai = OpenAI()\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.collection = self.chroma.create_collection(\"memories\")\n",
    "        self.count = 0\n",
    "    \n",
    "    def _embed(self, text):\n",
    "        response = self.openai.embeddings.create(model=\"text-embedding-3-small\", input=text)\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def add(self, text):\n",
    "        self.count += 1\n",
    "        self.collection.add(ids=[f\"m{self.count}\"], embeddings=[self._embed(text)], documents=[text])\n",
    "    \n",
    "    def search(self, query, n=3):\n",
    "        results = self.collection.query(query_embeddings=[self._embed(query)], n_results=n)\n",
    "        return results[\"documents\"][0]\n",
    "\n",
    "# Test\n",
    "memory = SemanticMemory()\n",
    "memory.add(\"Python is a programming language\")\n",
    "memory.add(\"The Eiffel Tower is in Paris\")\n",
    "memory.add(\"Machine learning requires data\")\n",
    "\n",
    "print(\"Query: 'AI and data science'\")\n",
    "for r in memory.search(\"AI and data science\"):\n",
    "    print(f\"  - {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.5.2 Solution: Memory with Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "class CategorizedMemory:\n",
    "    def __init__(self):\n",
    "        self.openai = OpenAI()\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.collection = self.chroma.create_collection(\"categorized\")\n",
    "        self.count = 0\n",
    "        self.categories = {}\n",
    "    \n",
    "    def _embed(self, text):\n",
    "        return self.openai.embeddings.create(model=\"text-embedding-3-small\", input=text).data[0].embedding\n",
    "    \n",
    "    def add(self, text, category):\n",
    "        self.count += 1\n",
    "        self.collection.add(\n",
    "            ids=[f\"m{self.count}\"], embeddings=[self._embed(text)],\n",
    "            documents=[text], metadatas=[{\"category\": category}]\n",
    "        )\n",
    "        self.categories[category] = self.categories.get(category, 0) + 1\n",
    "    \n",
    "    def search(self, query, n=3):\n",
    "        results = self.collection.query(query_embeddings=[self._embed(query)], n_results=n)\n",
    "        return results[\"documents\"][0]\n",
    "    \n",
    "    def search_category(self, query, category, n=3):\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[self._embed(query)], n_results=n, where={\"category\": category}\n",
    "        )\n",
    "        return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    \n",
    "    def get_category_counts(self):\n",
    "        return self.categories.copy()\n",
    "\n",
    "# Test\n",
    "memory = CategorizedMemory()\n",
    "memory.add(\"Finish report by Friday\", \"work\")\n",
    "memory.add(\"Buy groceries\", \"personal\")\n",
    "memory.add(\"App idea: habit tracker\", \"ideas\")\n",
    "\n",
    "print(\"Categories:\", memory.get_category_counts())\n",
    "print(\"Work search:\", memory.search_category(\"deadlines\", \"work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.5.3 Solution: Conversational Agent with Semantic Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "class SemanticAgent:\n",
    "    def __init__(self, system_prompt):\n",
    "        self.openai = OpenAI()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.history = []\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.memories = self.chroma.create_collection(\"agent_mem\")\n",
    "        self.mem_id = 0\n",
    "    \n",
    "    def _embed(self, text):\n",
    "        return self.openai.embeddings.create(model=\"text-embedding-3-small\", input=text).data[0].embedding\n",
    "    \n",
    "    def _store(self, content):\n",
    "        self.mem_id += 1\n",
    "        self.memories.add(ids=[f\"m{self.mem_id}\"], embeddings=[self._embed(content)], documents=[content])\n",
    "    \n",
    "    def _retrieve(self, query, n=3):\n",
    "        if self.memories.count() == 0:\n",
    "            return []\n",
    "        results = self.memories.query(query_embeddings=[self._embed(query)], n_results=min(n, self.memories.count()))\n",
    "        return results[\"documents\"][0]\n",
    "    \n",
    "    def remember(self, fact):\n",
    "        self._store(f\"Fact: {fact}\")\n",
    "        return f\"Stored: {fact}\"\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        relevant = self._retrieve(user_input)\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        if relevant:\n",
    "            messages.append({\"role\": \"system\", \"content\": f\"Relevant: {relevant}\"})\n",
    "            print(f\"[DEBUG] Retrieved: {relevant}\")\n",
    "        messages.extend(self.history[-10:])\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        response = self.openai.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        \n",
    "        self.history.extend([{\"role\": \"user\", \"content\": user_input}, {\"role\": \"assistant\", \"content\": reply}])\n",
    "        self._store(f\"Discussed: {user_input[:80]}\")\n",
    "        return reply\n",
    "\n",
    "# Test\n",
    "agent = SemanticAgent(\"You are helpful with good memory.\")\n",
    "print(agent.remember(\"User's favorite color is blue\"))\n",
    "print(agent.chat(\"What are my preferences?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.6.1 Solution: Basic Chat with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are friendly.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chat = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")\n",
    "config = {\"configurable\": {\"session_id\": \"demo\"}}\n",
    "\n",
    "# 3-message conversation\n",
    "for msg in [\"Hi! My name is Sarah.\", \"I'm an engineer.\", \"What's my name and job?\"]:\n",
    "    print(f\"Human: {msg}\")\n",
    "    response = chat.invoke({\"input\": msg}, config=config)\n",
    "    print(f\"AI: {response.content}\\n\")\n",
    "\n",
    "print(\"--- Full History ---\")\n",
    "for msg in store[\"demo\"].messages:\n",
    "    print(f\"{msg.type}: {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.6.2 Solution: Windowed Memory with trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "trimmer = trim_messages(max_tokens=100, strategy=\"last\", token_counter=ChatOpenAI(model=\"gpt-3.5-turbo\"), include_system=True, start_on=\"human\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful. Remember details.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def trim_history(input_dict):\n",
    "    history = input_dict.get(\"history\", [])\n",
    "    return {**input_dict, \"history\": trimmer.invoke(history)}\n",
    "\n",
    "chain = RunnableLambda(trim_history) | prompt | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chat = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"input\", history_messages_key=\"history\")\n",
    "config = {\"configurable\": {\"session_id\": \"window\"}}\n",
    "\n",
    "# 6-message conversation\n",
    "messages = [\"My name is Alex.\", \"I live in Boston.\", \"I have a cat.\", \"I work at a startup.\", \"I love pizza.\", \"What do you remember about me?\"]\n",
    "\n",
    "for msg in messages:\n",
    "    print(f\"Human: {msg}\")\n",
    "    response = chat.invoke({\"input\": msg}, config=config)\n",
    "    print(f\"AI: {response.content}\\n\")\n",
    "\n",
    "print(\"Early messages should be forgotten, recent ones remembered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 13.6.3 Solution: Agent with Tools and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate math expressions like '2+2'.\"\"\"\n",
    "    try:\n",
    "        return f\"Result: {eval(expression)}\"\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city.\"\"\"\n",
    "    return {\"Seattle\": \"Rainy, 55F\", \"Austin\": \"Sunny, 85F\"}.get(city, \"Partly cloudy, 70F\")\n",
    "\n",
    "class AgentWithManagedMemory:\n",
    "    def __init__(self, max_messages=6, keep_recent=2):\n",
    "        self.model = ChatOpenAI(model=\"gpt-4o\").bind_tools([calculator, get_weather])\n",
    "        self.summary_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        self.max_messages = max_messages\n",
    "        self.keep_recent = keep_recent\n",
    "        self.messages = []\n",
    "        self.summary = \"\"\n",
    "    \n",
    "    def _maybe_summarize(self):\n",
    "        if len(self.messages) <= self.max_messages:\n",
    "            return\n",
    "        to_summarize = self.messages[:-self.keep_recent]\n",
    "        text = \"\\n\".join(f\"{'User' if isinstance(m, HumanMessage) else 'AI'}: {m.content[:100]}\" for m in to_summarize)\n",
    "        response = self.summary_model.invoke([HumanMessage(content=f\"Summarize briefly: {self.summary}\\n{text}\")])\n",
    "        self.summary = response.content\n",
    "        self.messages = self.messages[-self.keep_recent:]\n",
    "        print(f\"[Summarized {len(to_summarize)} messages]\")\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        context = f\"Previous: {self.summary}\" if self.summary else \"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": f\"You have tools. {context}\"}]\n",
    "        messages.extend([{\"role\": \"user\" if isinstance(m, HumanMessage) else \"assistant\", \"content\": m.content} for m in self.messages])\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        response = self.model.invoke(messages)\n",
    "        self.messages.append(HumanMessage(content=user_input))\n",
    "        self.messages.append(AIMessage(content=response.content))\n",
    "        self._maybe_summarize()\n",
    "        return response.content\n",
    "\n",
    "# Test\n",
    "agent = AgentWithManagedMemory(max_messages=6, keep_recent=2)\n",
    "print(\"1.\", agent.chat(\"What's the weather in Seattle?\"))\n",
    "print(\"2.\", agent.chat(\"Calculate 15 * 8\"))\n",
    "print(\"3.\", agent.chat(\"What city did I ask about?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed Part 3: Building AI Agents!\n",
    "\n",
    "Next: **Part 4: LangGraph** (Chapter 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
