{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Multi-Agent Systems\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- When and why to use multiple agents\n",
    "- Agent communication patterns\n",
    "- Supervisor-worker architectures\n",
    "- Collaborative agent systems\n",
    "- Building a research assistant team\n",
    "- Managing shared state between agents\n",
    "- Orchestration and coordination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.1: When and why to use multiple agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: single_agent_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.1\n",
    "# File: single_agent_demo.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates a single agent handling multiple responsibilities.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# One big prompt trying to do everything\n",
    "MEGA_PROMPT = \"\"\"You are a versatile assistant that can:\n",
    "1. Analyze text for sentiment, key themes, and statistics\n",
    "2. Summarize content in different styles\n",
    "3. Generate creative variations\n",
    "4. Translate between formats\n",
    "\n",
    "For any input, determine what the user needs and provide it.\n",
    "Be analytical when analyzing, creative when creating, \n",
    "concise when summarizing.\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "The new product launch exceeded expectations. Sales were up 150% \n",
    "compared to our previous launch. Customer feedback has been \n",
    "overwhelmingly positive, with 92% satisfaction ratings. However, \n",
    "we did face some supply chain challenges that delayed shipments \n",
    "to certain regions by 2-3 weeks.\n",
    "\"\"\"\n",
    "\n",
    "# Single agent tries to do analysis AND summary\n",
    "response = llm.invoke(f\"\"\"\n",
    "{MEGA_PROMPT}\n",
    "\n",
    "Please analyze this text AND provide a brief executive summary:\n",
    "\n",
    "{text}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Single Agent Response ===\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_agent_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.1\n",
    "# File: multi_agent_demo.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates specialized agents as graph nodes.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class ReportState(TypedDict):\n",
    "    raw_text: str\n",
    "    analysis: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "def analyst_agent(state: ReportState) -> dict:\n",
    "    \"\"\"Data Analyst Agent - extracts metrics and signals.\"\"\"\n",
    "    prompt = \"\"\"You are a data analyst. Your ONLY job is to:\n",
    "    - Extract key metrics and statistics\n",
    "    - Identify positive and negative signals\n",
    "    - Note any risks or concerns\n",
    "    Be precise. Use numbers. Format as bullet points.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(f\"{prompt}\\n\\nAnalyze:\\n{state['raw_text']}\")\n",
    "    return {\"analysis\": response.content}\n",
    "\n",
    "\n",
    "def summarizer_agent(state: ReportState) -> dict:\n",
    "    \"\"\"Executive Summarizer Agent - creates brief summaries.\"\"\"\n",
    "    prompt = \"\"\"You are an executive communication specialist.\n",
    "    Write a 2-3 sentence summary for busy executives.\n",
    "    Lead with the most important finding.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(f\"{prompt}\\n\\nBased on:\\n{state['analysis']}\")\n",
    "    return {\"summary\": response.content}\n",
    "\n",
    "\n",
    "# Build the multi-agent graph\n",
    "workflow = StateGraph(ReportState)\n",
    "\n",
    "# Add our specialist agents as nodes\n",
    "workflow.add_node(\"analyst\", analyst_agent)\n",
    "workflow.add_node(\"summarizer\", summarizer_agent)\n",
    "\n",
    "# Define the flow: analyst first, then summarizer\n",
    "workflow.add_edge(START, \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"summarizer\")\n",
    "workflow.add_edge(\"summarizer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the multi-agent system\n",
    "result = app.invoke({\n",
    "    \"raw_text\": \"\"\"The new product launch exceeded expectations. Sales were up \n",
    "    150% compared to our previous launch. Customer feedback has been \n",
    "    overwhelmingly positive, with 92% satisfaction ratings. However, \n",
    "    we did face some supply chain challenges that delayed shipments \n",
    "    to certain regions by 2-3 weeks.\"\"\",\n",
    "    \"analysis\": \"\",\n",
    "    \"summary\": \"\"\n",
    "})\n",
    "\n",
    "print(\"=== Analyst Agent Output ===\")\n",
    "print(result[\"analysis\"])\n",
    "print(\"\\n=== Summarizer Agent Output ===\")\n",
    "print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.1.1: Multi-Agent Decision Analysis\n",
    "\n",
    "You're designing an AI system for each scenario below. For each one, decide: single agent or multiple agents? If multiple, identify the agents you'd create.\n",
    "\n",
    "Scenarios:\n",
    "1. A chatbot that answers FAQs about a company's products\n",
    "2. An AI that reviews legal contracts, checks for compliance issues, and suggests revisions\n",
    "3. A customer service system that handles complaints, processes refunds, and escalates complex issues\n",
    "4. A translation tool that converts English documents to Spanish\n",
    "5. An AI research assistant that finds papers, summarizes them, identifies gaps, and suggests experiments\n",
    "\n",
    "Write a brief justification for each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.1.2: Agent Boundary Design\n",
    "\n",
    "You're building an AI-powered content creation pipeline for a marketing team. The workflow is:\n",
    "\n",
    "1. Research trending topics in the industry\n",
    "2. Generate content ideas based on research\n",
    "3. Write first draft of article\n",
    "4. Review for factual accuracy\n",
    "5. Edit for brand voice and style\n",
    "6. Generate social media snippets\n",
    "\n",
    "Design the agent architecture. For each agent you propose:\n",
    "- What is its name and single responsibility?\n",
    "- What tools would it need?\n",
    "- What context/instructions would it have?\n",
    "- What does it receive as input? What does it output?\n",
    "\n",
    "Draw a simple diagram showing how information flows between agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.1.3: Specialist vs. Generalist Comparison\n",
    "\n",
    "Create a practical comparison by implementing two approaches to this task:\n",
    "\n",
    "*\"Given a piece of code, identify bugs, suggest optimizations, and add documentation.\"*\n",
    "\n",
    "1. Build a single-agent version that does all three tasks\n",
    "2. Build a multi-agent version with three specialists (Bug Finder, Optimizer, Documenter)\n",
    "3. Run both on the same sample code\n",
    "4. Compare the quality and depth of outputs\n",
    "\n",
    "Sample code to analyze:\n",
    "```python\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for i in range(len(numbers)):\n",
    "        total = total + numbers[i]\n",
    "    average = total / len(numbers)\n",
    "    return average\n",
    "```\n",
    "\n",
    "Document your observations about the differences in output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.2: Agent communication patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: sequential_pipeline.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.2\n",
    "# File: sequential_pipeline.py\n",
    "\n",
    "\"\"\"\n",
    "Sequential pattern: Research \u2192 Draft \u2192 Edit pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class ArticleState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    draft: str\n",
    "    final: str\n",
    "\n",
    "\n",
    "def researcher_agent(state: ArticleState) -> dict:\n",
    "    \"\"\"Stage 1: Research the topic.\"\"\"\n",
    "    prompt = f\"\"\"Research this topic and provide 3 key facts:\n",
    "    Topic: {state['topic']}\n",
    "    \n",
    "    Format: Three bullet points with factual information.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcda Researcher complete\")\n",
    "    return {\"research\": response.content}\n",
    "\n",
    "\n",
    "def writer_agent(state: ArticleState) -> dict:\n",
    "    \"\"\"Stage 2: Write draft based on research.\"\"\"\n",
    "    prompt = f\"\"\"Write a short paragraph about {state['topic']}.\n",
    "    \n",
    "    Use these research points:\n",
    "    {state['research']}\n",
    "    \n",
    "    Keep it to 3-4 sentences.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\u270d\ufe0f Writer complete\")\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def editor_agent(state: ArticleState) -> dict:\n",
    "    \"\"\"Stage 3: Polish the draft.\"\"\"\n",
    "    prompt = f\"\"\"Edit this draft for clarity and impact:\n",
    "    \n",
    "    {state['draft']}\n",
    "    \n",
    "    Make it more engaging. Keep the same length.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcdd Editor complete\")\n",
    "    return {\"final\": response.content}\n",
    "\n",
    "\n",
    "# Build sequential pipeline\n",
    "workflow = StateGraph(ArticleState)\n",
    "\n",
    "workflow.add_node(\"researcher\", researcher_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "workflow.add_node(\"editor\", editor_agent)\n",
    "\n",
    "# Sequential flow: one after another\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"writer\")\n",
    "workflow.add_edge(\"writer\", \"editor\")\n",
    "workflow.add_edge(\"editor\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run it\n",
    "result = app.invoke({\n",
    "    \"topic\": \"The benefits of morning exercise\",\n",
    "    \"research\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"final\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL ARTICLE:\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"final\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: broadcast_pattern.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.2\n",
    "# File: broadcast_pattern.py\n",
    "\n",
    "\"\"\"\n",
    "Broadcast pattern: Multiple reviewers analyze the same code.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class ReviewState(TypedDict):\n",
    "    code: str\n",
    "    security_review: str\n",
    "    performance_review: str\n",
    "    style_review: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "def security_reviewer(state: ReviewState) -> dict:\n",
    "    \"\"\"Reviews code for security issues.\"\"\"\n",
    "    prompt = f\"\"\"As a security expert, review this code for vulnerabilities:\n",
    "    \n",
    "    {state['code']}\n",
    "    \n",
    "    List any security concerns (or say 'No issues found').\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd12 Security review complete\")\n",
    "    return {\"security_review\": response.content}\n",
    "\n",
    "\n",
    "def performance_reviewer(state: ReviewState) -> dict:\n",
    "    \"\"\"Reviews code for performance issues.\"\"\"\n",
    "    prompt = f\"\"\"As a performance expert, review this code for efficiency:\n",
    "    \n",
    "    {state['code']}\n",
    "    \n",
    "    List any performance concerns (or say 'No issues found').\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\u26a1 Performance review complete\")\n",
    "    return {\"performance_review\": response.content}\n",
    "\n",
    "\n",
    "def style_reviewer(state: ReviewState) -> dict:\n",
    "    \"\"\"Reviews code for style and readability.\"\"\"\n",
    "    prompt = f\"\"\"As a code quality expert, review this code for style:\n",
    "    \n",
    "    {state['code']}\n",
    "    \n",
    "    List any style/readability concerns (or say 'No issues found').\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83c\udfa8 Style review complete\")\n",
    "    return {\"style_review\": response.content}\n",
    "\n",
    "\n",
    "def aggregator(state: ReviewState) -> dict:\n",
    "    \"\"\"Combines all reviews into a summary.\"\"\"\n",
    "    prompt = f\"\"\"Summarize these code reviews into a brief action list:\n",
    "    \n",
    "    Security: {state['security_review']}\n",
    "    Performance: {state['performance_review']}  \n",
    "    Style: {state['style_review']}\n",
    "    \n",
    "    Prioritize the top 3 issues to fix.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"summary\": response.content}\n",
    "\n",
    "\n",
    "workflow = StateGraph(ReviewState)\n",
    "\n",
    "workflow.add_node(\"security\", security_reviewer)\n",
    "workflow.add_node(\"performance\", performance_reviewer)\n",
    "workflow.add_node(\"style\", style_reviewer)\n",
    "workflow.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Fan-out: all three reviewers start from START\n",
    "workflow.add_edge(START, \"security\")\n",
    "workflow.add_edge(START, \"performance\")\n",
    "workflow.add_edge(START, \"style\")\n",
    "\n",
    "# Fan-in: all feed into aggregator\n",
    "workflow.add_edge(\"security\", \"aggregator\")\n",
    "workflow.add_edge(\"performance\", \"aggregator\")\n",
    "workflow.add_edge(\"style\", \"aggregator\")\n",
    "\n",
    "workflow.add_edge(\"aggregator\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test it with some code\n",
    "sample_code = \"\"\"\n",
    "def get_user(user_id):\n",
    "    query = f\"SELECT * FROM users WHERE id = {user_id}\"\n",
    "    result = db.execute(query)\n",
    "    users = []\n",
    "    for row in result:\n",
    "        users.append(row)\n",
    "    return users[0] if users else None\n",
    "\"\"\"\n",
    "\n",
    "result = app.invoke({\n",
    "    \"code\": sample_code,\n",
    "    \"security_review\": \"\",\n",
    "    \"performance_review\": \"\",\n",
    "    \"style_review\": \"\",\n",
    "    \"summary\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMBINED REVIEW SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: supervisor_pattern.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.2\n",
    "# File: supervisor_pattern.py\n",
    "\n",
    "\"\"\"\n",
    "Supervisor pattern: Routes requests to appropriate specialists.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class CustomerState(TypedDict):\n",
    "    request: str\n",
    "    category: str\n",
    "    response: str\n",
    "\n",
    "\n",
    "def supervisor(state: CustomerState) -> dict:\n",
    "    \"\"\"Analyzes request and determines routing.\"\"\"\n",
    "    prompt = f\"\"\"Categorize this customer request into exactly one category:\n",
    "    - billing (payment, charges, invoices, refunds)\n",
    "    - technical (bugs, errors, how-to, features)\n",
    "    - general (other questions, feedback, complaints)\n",
    "    \n",
    "    Request: {state['request']}\n",
    "    \n",
    "    Reply with just the category name.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    category = response.content.strip().lower()\n",
    "    \n",
    "    # Normalize to valid categories\n",
    "    if \"billing\" in category:\n",
    "        category = \"billing\"\n",
    "    elif \"technical\" in category:\n",
    "        category = \"technical\"\n",
    "    else:\n",
    "        category = \"general\"\n",
    "    \n",
    "    print(f\"\ud83c\udfaf Supervisor routed to: {category}\")\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "def billing_agent(state: CustomerState) -> dict:\n",
    "    \"\"\"Handles billing-related requests.\"\"\"\n",
    "    prompt = f\"\"\"You are a billing specialist. Help with this request:\n",
    "    {state['request']}\n",
    "    \n",
    "    Be helpful and mention relevant policies.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcb3 Billing agent responded\")\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "\n",
    "def technical_agent(state: CustomerState) -> dict:\n",
    "    \"\"\"Handles technical support requests.\"\"\"\n",
    "    prompt = f\"\"\"You are a technical support specialist. Help with:\n",
    "    {state['request']}\n",
    "    \n",
    "    Provide clear steps and explanations.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd27 Technical agent responded\")\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "\n",
    "def general_agent(state: CustomerState) -> dict:\n",
    "    \"\"\"Handles general inquiries.\"\"\"\n",
    "    prompt = f\"\"\"You are a customer service representative. Help with:\n",
    "    {state['request']}\n",
    "    \n",
    "    Be friendly and helpful.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcac General agent responded\")\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "\n",
    "def route_to_worker(state: CustomerState) -> Literal[\"billing\", \"technical\", \"general\"]:\n",
    "    \"\"\"Routes to the appropriate worker based on category.\"\"\"\n",
    "    return state[\"category\"]\n",
    "\n",
    "\n",
    "workflow = StateGraph(CustomerState)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.add_node(\"billing\", billing_agent)\n",
    "workflow.add_node(\"technical\", technical_agent)\n",
    "workflow.add_node(\"general\", general_agent)\n",
    "\n",
    "# Supervisor first\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# Conditional routing based on supervisor's decision\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_to_worker,\n",
    "    {\n",
    "        \"billing\": \"billing\",\n",
    "        \"technical\": \"technical\",\n",
    "        \"general\": \"general\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All workers go to END\n",
    "workflow.add_edge(\"billing\", END)\n",
    "workflow.add_edge(\"technical\", END)\n",
    "workflow.add_edge(\"general\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test with different types of requests\n",
    "requests = [\n",
    "    \"I was charged twice for my subscription last month\",\n",
    "    \"The app keeps crashing when I try to upload photos\",\n",
    "    \"Do you offer student discounts?\"\n",
    "]\n",
    "\n",
    "for req in requests:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"REQUEST: {req}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"request\": req,\n",
    "        \"category\": \"\",\n",
    "        \"response\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nRESPONSE:\\n{result['response'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: shared_state_pattern.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.2\n",
    "# File: shared_state_pattern.py\n",
    "\n",
    "\"\"\"\n",
    "Shared State pattern: Agents collaborate via common knowledge base.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    problem: str\n",
    "    observations: list[str]   # Shared list all agents can append to\n",
    "    hypothesis: str\n",
    "    conclusion: str\n",
    "\n",
    "\n",
    "def data_collector(state: AnalysisState) -> dict:\n",
    "    \"\"\"Gathers initial observations about the problem.\"\"\"\n",
    "    prompt = f\"\"\"Analyze this problem and list 2-3 key observations:\n",
    "    \n",
    "    Problem: {state['problem']}\n",
    "    \n",
    "    Format: One observation per line, starting with a dash.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse observations and add to shared list\n",
    "    new_obs = [line.strip(\"- \").strip() \n",
    "               for line in response.content.split(\"\\n\") \n",
    "               if line.strip().startswith(\"-\")]\n",
    "    \n",
    "    print(f\"\ud83d\udcca Collector added {len(new_obs)} observations\")\n",
    "    return {\"observations\": state[\"observations\"] + new_obs}\n",
    "\n",
    "\n",
    "def pattern_finder(state: AnalysisState) -> dict:\n",
    "    \"\"\"Looks for patterns in collected observations.\"\"\"\n",
    "    current_obs = \"\\n\".join(f\"- {o}\" for o in state[\"observations\"])\n",
    "    \n",
    "    prompt = f\"\"\"Given these observations, identify 1-2 patterns or connections:\n",
    "    \n",
    "    {current_obs}\n",
    "    \n",
    "    Format: One pattern per line, starting with a dash.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    new_patterns = [line.strip(\"- \").strip() \n",
    "                   for line in response.content.split(\"\\n\") \n",
    "                   if line.strip().startswith(\"-\")]\n",
    "    \n",
    "    print(f\"\ud83d\udd0d Pattern finder added {len(new_patterns)} patterns\")\n",
    "    return {\"observations\": state[\"observations\"] + new_patterns}\n",
    "\n",
    "\n",
    "def hypothesis_maker(state: AnalysisState) -> dict:\n",
    "    \"\"\"Forms a hypothesis based on all observations.\"\"\"\n",
    "    all_obs = \"\\n\".join(f\"- {o}\" for o in state[\"observations\"])\n",
    "    \n",
    "    prompt = f\"\"\"Based on all these observations and patterns:\n",
    "    \n",
    "    {all_obs}\n",
    "    \n",
    "    Form a single hypothesis explaining the situation.\n",
    "    Keep it to one sentence.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udca1 Hypothesis formed\")\n",
    "    return {\"hypothesis\": response.content}\n",
    "\n",
    "\n",
    "def conclusion_maker(state: AnalysisState) -> dict:\n",
    "    \"\"\"Draws final conclusion from hypothesis and observations.\"\"\"\n",
    "    all_obs = \"\\n\".join(f\"- {o}\" for o in state[\"observations\"])\n",
    "    \n",
    "    prompt = f\"\"\"Given:\n",
    "    Observations: {all_obs}\n",
    "    Hypothesis: {state['hypothesis']}\n",
    "    \n",
    "    Write a brief conclusion with one recommended action.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\u2705 Conclusion reached\")\n",
    "    return {\"conclusion\": response.content}\n",
    "\n",
    "\n",
    "workflow = StateGraph(AnalysisState)\n",
    "\n",
    "workflow.add_node(\"collector\", data_collector)\n",
    "workflow.add_node(\"pattern_finder\", pattern_finder)\n",
    "workflow.add_node(\"hypothesis_maker\", hypothesis_maker)\n",
    "workflow.add_node(\"conclusion_maker\", conclusion_maker)\n",
    "\n",
    "workflow.add_edge(START, \"collector\")\n",
    "workflow.add_edge(\"collector\", \"pattern_finder\")\n",
    "workflow.add_edge(\"pattern_finder\", \"hypothesis_maker\")\n",
    "workflow.add_edge(\"hypothesis_maker\", \"conclusion_maker\")\n",
    "workflow.add_edge(\"conclusion_maker\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test collaborative analysis\n",
    "result = app.invoke({\n",
    "    \"problem\": \"Our e-commerce site's conversion rate dropped 30% last month\",\n",
    "    \"observations\": [],\n",
    "    \"hypothesis\": \"\",\n",
    "    \"conclusion\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SHARED OBSERVATIONS:\")\n",
    "for i, obs in enumerate(result[\"observations\"], 1):\n",
    "    print(f\"  {i}. {obs}\")\n",
    "\n",
    "print(\"\\nHYPOTHESIS:\")\n",
    "print(f\"  {result['hypothesis']}\")\n",
    "\n",
    "print(\"\\nCONCLUSION:\")\n",
    "print(f\"  {result['conclusion']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.2.1: Document Processing Pipeline\n",
    "\n",
    "Build a sequential pipeline for processing documents with these stages:\n",
    "1. **Extractor** - Pulls out key entities (people, places, dates)\n",
    "2. **Classifier** - Categorizes the document type (legal, medical, financial, other)\n",
    "3. **Summarizer** - Creates a summary appropriate for that document type\n",
    "\n",
    "Test with a sample document of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.2.2: Multi-Perspective Analysis\n",
    "\n",
    "Create a broadcast pattern for analyzing a business decision. Three agents should provide different perspectives:\n",
    "1. **Optimist** - Focuses on potential benefits and opportunities\n",
    "2. **Pessimist** - Focuses on risks and potential problems\n",
    "3. **Pragmatist** - Focuses on practical implementation concerns\n",
    "\n",
    "Add an aggregator that synthesizes a balanced recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.2.3: Smart Router with Fallback\n",
    "\n",
    "Build a hierarchical system that routes questions to specialists:\n",
    "- **Math Agent** - Handles calculations and math problems\n",
    "- **History Agent** - Handles historical questions\n",
    "- **Science Agent** - Handles science questions\n",
    "- **Fallback Agent** - Handles anything that doesn't fit\n",
    "\n",
    "The supervisor should route based on question content. Test with at least 5 different questions across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.3: Supervisor-worker architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: basic_supervisor.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.3\n",
    "# File: basic_supervisor.py\n",
    "\n",
    "\"\"\"\n",
    "A supervisor that coordinates writing specialists.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class WritingState(TypedDict):\n",
    "    request: str\n",
    "    task_type: str\n",
    "    draft: str\n",
    "    final_output: str\n",
    "\n",
    "\n",
    "def email_writer(state: WritingState) -> dict:\n",
    "    \"\"\"Specialist for professional emails.\"\"\"\n",
    "    prompt = f\"\"\"Write a professional email for this request:\n",
    "    {state['request']}\n",
    "    \n",
    "    Use proper email format with subject line, greeting, body, and signature.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udce7 Email writer completed\")\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def blog_writer(state: WritingState) -> dict:\n",
    "    \"\"\"Specialist for blog posts.\"\"\"\n",
    "    prompt = f\"\"\"Write an engaging blog post for this request:\n",
    "    {state['request']}\n",
    "    \n",
    "    Include a catchy title, introduction, main points, and conclusion.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcdd Blog writer completed\")\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def summary_writer(state: WritingState) -> dict:\n",
    "    \"\"\"Specialist for summaries and briefs.\"\"\"\n",
    "    prompt = f\"\"\"Write a concise summary for this request:\n",
    "    {state['request']}\n",
    "    \n",
    "    Be brief but comprehensive. Use bullet points if helpful.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udccb Summary writer completed\")\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def supervisor(state: WritingState) -> dict:\n",
    "    \"\"\"Analyzes request and decides which specialist to use.\"\"\"\n",
    "    prompt = f\"\"\"Analyze this writing request and categorize it:\n",
    "    \n",
    "    Request: {state['request']}\n",
    "    \n",
    "    Categories:\n",
    "    - email: Professional correspondence, formal messages, business communication\n",
    "    - blog: Articles, posts, educational content, opinion pieces\n",
    "    - summary: Condensing information, briefs, overviews, TL;DR\n",
    "    \n",
    "    Reply with just the category name (email, blog, or summary).\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    task_type = response.content.strip().lower()\n",
    "    \n",
    "    # Normalize the response\n",
    "    if \"email\" in task_type:\n",
    "        task_type = \"email\"\n",
    "    elif \"blog\" in task_type:\n",
    "        task_type = \"blog\"\n",
    "    else:\n",
    "        task_type = \"summary\"\n",
    "    \n",
    "    print(f\"\ud83c\udfaf Supervisor assigned: {task_type}\")\n",
    "    return {\"task_type\": task_type}\n",
    "\n",
    "\n",
    "def finalizer(state: WritingState) -> dict:\n",
    "    \"\"\"Reviews and finalizes the draft.\"\"\"\n",
    "    prompt = f\"\"\"Review this {state['task_type']} draft and make minor improvements:\n",
    "    \n",
    "    {state['draft']}\n",
    "    \n",
    "    Fix any issues but preserve the style. Return the polished version.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"final_output\": response.content}\n",
    "\n",
    "\n",
    "def route_to_worker(state: WritingState) -> Literal[\"email\", \"blog\", \"summary\"]:\n",
    "    \"\"\"Routes to the appropriate specialist.\"\"\"\n",
    "    return state[\"task_type\"]\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(WritingState)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.add_node(\"email\", email_writer)\n",
    "workflow.add_node(\"blog\", blog_writer)\n",
    "workflow.add_node(\"summary\", summary_writer)\n",
    "workflow.add_node(\"finalizer\", finalizer)\n",
    "\n",
    "# Supervisor decides first\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# Route to appropriate worker\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_to_worker,\n",
    "    {\n",
    "        \"email\": \"email\",\n",
    "        \"blog\": \"blog\",\n",
    "        \"summary\": \"summary\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# All workers go to finalizer\n",
    "workflow.add_edge(\"email\", \"finalizer\")\n",
    "workflow.add_edge(\"blog\", \"finalizer\")\n",
    "workflow.add_edge(\"summary\", \"finalizer\")\n",
    "\n",
    "workflow.add_edge(\"finalizer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test it\n",
    "requests = [\n",
    "    \"Write to my boss asking for a day off next Friday\",\n",
    "    \"Create content about the benefits of remote work\",\n",
    "    \"Condense this 10-page report into key takeaways\"\n",
    "]\n",
    "\n",
    "for req in requests:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REQUEST: {req[:50]}...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"request\": req,\n",
    "        \"task_type\": \"\",\n",
    "        \"draft\": \"\",\n",
    "        \"final_output\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nOUTPUT:\\n{result['final_output'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: iterative_supervisor.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.3\n",
    "# File: iterative_supervisor.py\n",
    "\n",
    "\"\"\"\n",
    "Supervisor that can call workers multiple times.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    findings: Annotated[list[str], operator.add]  # Accumulates findings\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    needs_more: bool\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "def web_researcher(state: ResearchState) -> dict:\n",
    "    \"\"\"Simulates web research (in reality, would use search tools).\"\"\"\n",
    "    prompt = f\"\"\"For the query: {state['query']}\n",
    "    \n",
    "    Previous findings: {state['findings']}\n",
    "    \n",
    "    Provide ONE new finding that hasn't been mentioned yet.\n",
    "    Be specific and factual. Just the finding, no preamble.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\ud83d\udd0d Web researcher found: {response.content[:50]}...\")\n",
    "    return {\"findings\": [f\"[Web] {response.content}\"]}\n",
    "\n",
    "\n",
    "def academic_researcher(state: ResearchState) -> dict:\n",
    "    \"\"\"Simulates academic research.\"\"\"\n",
    "    prompt = f\"\"\"For the query: {state['query']}\n",
    "    \n",
    "    Previous findings: {state['findings']}\n",
    "    \n",
    "    Provide ONE academic or research-based finding not yet mentioned.\n",
    "    Cite a plausible source. Just the finding, no preamble.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\ud83d\udcda Academic researcher found: {response.content[:50]}...\")\n",
    "    return {\"findings\": [f\"[Academic] {response.content}\"]}\n",
    "\n",
    "\n",
    "def research_supervisor(state: ResearchState) -> dict:\n",
    "    \"\"\"Decides whether to continue research or compile results.\"\"\"\n",
    "    current_iteration = state.get(\"iteration\", 0) + 1\n",
    "    max_iter = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    # Check if we have enough findings\n",
    "    if len(state[\"findings\"]) >= 4 or current_iteration > max_iter:\n",
    "        print(f\"\ud83d\udcca Supervisor: Sufficient findings ({len(state['findings'])})\")\n",
    "        return {\"iteration\": current_iteration, \"needs_more\": False}\n",
    "    \n",
    "    print(f\"\ud83d\udcca Supervisor: Need more research (iteration {current_iteration})\")\n",
    "    return {\"iteration\": current_iteration, \"needs_more\": True}\n",
    "\n",
    "\n",
    "def choose_researcher(state: ResearchState) -> Literal[\"web\", \"academic\"]:\n",
    "    \"\"\"Alternates between research sources.\"\"\"\n",
    "    # Simple alternation - could be smarter based on query type\n",
    "    if len(state[\"findings\"]) % 2 == 0:\n",
    "        return \"web\"\n",
    "    return \"academic\"\n",
    "\n",
    "\n",
    "def report_compiler(state: ResearchState) -> dict:\n",
    "    \"\"\"Compiles all findings into a final report.\"\"\"\n",
    "    findings_text = \"\\n\".join(f\"- {f}\" for f in state[\"findings\"])\n",
    "    \n",
    "    prompt = f\"\"\"Compile these research findings into a coherent report:\n",
    "    \n",
    "    Query: {state['query']}\n",
    "    \n",
    "    Findings:\n",
    "    {findings_text}\n",
    "    \n",
    "    Write a 2-3 paragraph summary that synthesizes the findings.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcdd Report compiled\")\n",
    "    return {\"final_report\": response.content}\n",
    "\n",
    "\n",
    "def should_continue(state: ResearchState) -> Literal[\"research\", \"compile\"]:\n",
    "    \"\"\"Decides whether to continue researching or compile.\"\"\"\n",
    "    if state.get(\"needs_more\", True):\n",
    "        return \"research\"\n",
    "    return \"compile\"\n",
    "\n",
    "\n",
    "def route_researcher(state: ResearchState) -> Literal[\"web\", \"academic\"]:\n",
    "    \"\"\"Routes to specific researcher.\"\"\"\n",
    "    return choose_researcher(state)\n",
    "\n",
    "\n",
    "# Build the iterative workflow\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "workflow.add_node(\"supervisor\", research_supervisor)\n",
    "workflow.add_node(\"web\", web_researcher)\n",
    "workflow.add_node(\"academic\", academic_researcher)\n",
    "workflow.add_node(\"compiler\", report_compiler)\n",
    "\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "# Supervisor decides: more research or compile\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"research\": \"router\",  # Go get more findings\n",
    "        \"compile\": \"compiler\"   # Done, compile report\n",
    "    }\n",
    ")\n",
    "\n",
    "# Router node to pick researcher\n",
    "workflow.add_node(\"router\", lambda s: {})  # Pass-through\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route_researcher,\n",
    "    {\n",
    "        \"web\": \"web\",\n",
    "        \"academic\": \"academic\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Researchers loop back to supervisor\n",
    "workflow.add_edge(\"web\", \"supervisor\")\n",
    "workflow.add_edge(\"academic\", \"supervisor\")\n",
    "\n",
    "workflow.add_edge(\"compiler\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the iterative research\n",
    "result = app.invoke({\n",
    "    \"query\": \"What are the health effects of intermittent fasting?\",\n",
    "    \"findings\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"needs_more\": True,\n",
    "    \"final_report\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESEARCH REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFindings collected: {len(result['findings'])}\")\n",
    "print(f\"\\nReport:\\n{result['final_report']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: fault_tolerant_supervisor.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.3\n",
    "# File: fault_tolerant_supervisor.py\n",
    "\n",
    "\"\"\"\n",
    "Supervisor with error handling and fallbacks.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class TaskState(TypedDict):\n",
    "    task: str\n",
    "    worker_attempts: int\n",
    "    max_attempts: int\n",
    "    result: str\n",
    "    error: str\n",
    "    status: str  # \"pending\", \"success\", \"failed\"\n",
    "\n",
    "\n",
    "def unreliable_worker(state: TaskState) -> dict:\n",
    "    \"\"\"A worker that fails 40% of the time (for demonstration).\"\"\"\n",
    "    attempts = state.get(\"worker_attempts\", 0) + 1\n",
    "    \n",
    "    # Simulate random failures\n",
    "    if random.random() < 0.4:\n",
    "        print(f\"\u274c Worker failed (attempt {attempts})\")\n",
    "        return {\n",
    "            \"worker_attempts\": attempts,\n",
    "            \"error\": f\"Worker failed on attempt {attempts}\",\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "    \n",
    "    # Success case\n",
    "    prompt = f\"Complete this task briefly: {state['task']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\u2705 Worker succeeded (attempt {attempts})\")\n",
    "    \n",
    "    return {\n",
    "        \"worker_attempts\": attempts,\n",
    "        \"result\": response.content,\n",
    "        \"error\": \"\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "\n",
    "\n",
    "def fallback_worker(state: TaskState) -> dict:\n",
    "    \"\"\"Simpler, more reliable fallback.\"\"\"\n",
    "    print(\"\ud83d\udd04 Fallback worker activated\")\n",
    "    \n",
    "    prompt = f\"Provide a simple response for: {state['task']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"result\": f\"[Fallback] {response.content}\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "\n",
    "\n",
    "def supervisor_with_retry(state: TaskState) -> dict:\n",
    "    \"\"\"Supervisor that manages retries and fallbacks.\"\"\"\n",
    "    attempts = state.get(\"worker_attempts\", 0)\n",
    "    max_attempts = state.get(\"max_attempts\", 3)\n",
    "    status = state.get(\"status\", \"pending\")\n",
    "    \n",
    "    if status == \"success\":\n",
    "        print(\"\ud83d\udcca Supervisor: Task completed successfully\")\n",
    "    elif attempts >= max_attempts:\n",
    "        print(f\"\ud83d\udcca Supervisor: Max attempts ({max_attempts}) reached, using fallback\")\n",
    "    else:\n",
    "        print(f\"\ud83d\udcca Supervisor: Attempt {attempts + 1} of {max_attempts}\")\n",
    "    \n",
    "    return {}  # State already updated by worker\n",
    "\n",
    "\n",
    "def route_after_attempt(state: TaskState) -> Literal[\"worker\", \"fallback\", \"done\"]:\n",
    "    \"\"\"Decides next step based on worker result.\"\"\"\n",
    "    if state.get(\"status\") == \"success\":\n",
    "        return \"done\"\n",
    "    \n",
    "    attempts = state.get(\"worker_attempts\", 0)\n",
    "    max_attempts = state.get(\"max_attempts\", 3)\n",
    "    \n",
    "    if attempts >= max_attempts:\n",
    "        return \"fallback\"\n",
    "    \n",
    "    return \"worker\"\n",
    "\n",
    "\n",
    "# Build the fault-tolerant workflow\n",
    "workflow = StateGraph(TaskState)\n",
    "\n",
    "workflow.add_node(\"supervisor\", supervisor_with_retry)\n",
    "workflow.add_node(\"worker\", unreliable_worker)\n",
    "workflow.add_node(\"fallback\", fallback_worker)\n",
    "\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_edge(\"supervisor\", \"worker\")\n",
    "\n",
    "# After worker, decide what's next\n",
    "workflow.add_conditional_edges(\n",
    "    \"worker\",\n",
    "    route_after_attempt,\n",
    "    {\n",
    "        \"worker\": \"worker\",      # Retry\n",
    "        \"fallback\": \"fallback\",  # Give up, use fallback\n",
    "        \"done\": END              # Success!\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test reliability\n",
    "print(\"Testing fault-tolerant supervisor (results will vary):\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test run {i+1}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"task\": \"Explain what Python is in one sentence\",\n",
    "        \"worker_attempts\": 0,\n",
    "        \"max_attempts\": 3,\n",
    "        \"result\": \"\",\n",
    "        \"error\": \"\",\n",
    "        \"status\": \"pending\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFinal result: {result['result'][:100]}...\")\n",
    "    print(f\"Attempts used: {result['worker_attempts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: supervisor_hierarchy.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.3\n",
    "# File: supervisor_hierarchy.py\n",
    "\n",
    "\"\"\"\n",
    "Two-level supervisor hierarchy for content creation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class ContentState(TypedDict):\n",
    "    topic: str\n",
    "    stage: str  # \"research\", \"writing\", \"review\", \"done\"\n",
    "    research_notes: str\n",
    "    draft: str\n",
    "    review_feedback: str\n",
    "    final_content: str\n",
    "\n",
    "\n",
    "def research_worker(state: ContentState) -> dict:\n",
    "    \"\"\"Gathers information on the topic.\"\"\"\n",
    "    prompt = f\"List 3 key facts about: {state['topic']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"  \ud83d\udcda Research worker complete\")\n",
    "    return {\"research_notes\": response.content}\n",
    "\n",
    "\n",
    "def writing_worker(state: ContentState) -> dict:\n",
    "    \"\"\"Writes content based on research.\"\"\"\n",
    "    prompt = f\"\"\"Write a short piece about {state['topic']}.\n",
    "    Use these notes: {state['research_notes']}\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"  \u270d\ufe0f Writing worker complete\")\n",
    "    return {\"draft\": response.content}\n",
    "\n",
    "\n",
    "def review_worker(state: ContentState) -> dict:\n",
    "    \"\"\"Reviews and provides feedback.\"\"\"\n",
    "    prompt = f\"\"\"Review this draft and provide 2 improvement suggestions:\n",
    "    {state['draft']}\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"  \ud83d\udd0d Review worker complete\")\n",
    "    return {\"review_feedback\": response.content}\n",
    "\n",
    "\n",
    "def polish_worker(state: ContentState) -> dict:\n",
    "    \"\"\"Applies feedback to create final version.\"\"\"\n",
    "    prompt = f\"\"\"Improve this draft based on feedback:\n",
    "    \n",
    "    Draft: {state['draft']}\n",
    "    Feedback: {state['review_feedback']}\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"  \u2728 Polish worker complete\")\n",
    "    return {\"final_content\": response.content}\n",
    "\n",
    "\n",
    "def top_supervisor(state: ContentState) -> dict:\n",
    "    \"\"\"High-level supervisor that manages the content pipeline.\"\"\"\n",
    "    current_stage = state.get(\"stage\", \"research\")\n",
    "    \n",
    "    stage_order = [\"research\", \"writing\", \"review\", \"polish\", \"done\"]\n",
    "    current_index = stage_order.index(current_stage)\n",
    "    \n",
    "    if current_index < len(stage_order) - 1:\n",
    "        next_stage = stage_order[current_index + 1]\n",
    "    else:\n",
    "        next_stage = \"done\"\n",
    "    \n",
    "    print(f\"\ud83c\udfaf Top supervisor: {current_stage} \u2192 {next_stage}\")\n",
    "    return {\"stage\": next_stage}\n",
    "\n",
    "\n",
    "def route_by_stage(state: ContentState) -> Literal[\"research\", \"writing\", \"review\", \"polish\", \"done\"]:\n",
    "    \"\"\"Routes to appropriate stage.\"\"\"\n",
    "    return state[\"stage\"]\n",
    "\n",
    "\n",
    "# Build the hierarchy\n",
    "workflow = StateGraph(ContentState)\n",
    "\n",
    "workflow.add_node(\"top_supervisor\", top_supervisor)\n",
    "workflow.add_node(\"research\", research_worker)\n",
    "workflow.add_node(\"writing\", writing_worker)\n",
    "workflow.add_node(\"review\", review_worker)\n",
    "workflow.add_node(\"polish\", polish_worker)\n",
    "\n",
    "# Start with supervisor\n",
    "workflow.add_edge(START, \"top_supervisor\")\n",
    "\n",
    "# Route based on stage\n",
    "workflow.add_conditional_edges(\n",
    "    \"top_supervisor\",\n",
    "    route_by_stage,\n",
    "    {\n",
    "        \"research\": \"research\",\n",
    "        \"writing\": \"writing\",\n",
    "        \"review\": \"review\",\n",
    "        \"polish\": \"polish\",\n",
    "        \"done\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Each worker returns to supervisor for next decision\n",
    "workflow.add_edge(\"research\", \"top_supervisor\")\n",
    "workflow.add_edge(\"writing\", \"top_supervisor\")\n",
    "workflow.add_edge(\"review\", \"top_supervisor\")\n",
    "workflow.add_edge(\"polish\", \"top_supervisor\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the pipeline\n",
    "result = app.invoke({\n",
    "    \"topic\": \"The future of renewable energy\",\n",
    "    \"stage\": \"research\",\n",
    "    \"research_notes\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"review_feedback\": \"\",\n",
    "    \"final_content\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL CONTENT\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"final_content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.3.1: Customer Service Supervisor\n",
    "\n",
    "Build a supervisor-worker system for customer service with these workers:\n",
    "- **Greeter** - Welcomes customer and classifies their issue\n",
    "- **Billing Agent** - Handles payment and subscription issues\n",
    "- **Tech Support** - Handles technical problems\n",
    "- **Complaint Handler** - Handles complaints and escalations\n",
    "\n",
    "The supervisor should:\n",
    "1. Route to the appropriate specialist\n",
    "2. If the specialist can't fully resolve, route to a human handoff message\n",
    "3. Track the number of routing decisions made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.3.2: Document Processing Pipeline\n",
    "\n",
    "Create an iterative supervisor for document processing:\n",
    "- **Extractor** - Pulls out key information\n",
    "- **Validator** - Checks extracted info for completeness\n",
    "- **Enricher** - Adds additional context\n",
    "\n",
    "The supervisor should:\n",
    "1. Run extractor first\n",
    "2. Check validator\u2014if incomplete, run extractor again (max 2 retries)\n",
    "3. Only proceed to enricher when validator passes\n",
    "4. Compile final structured document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.3.3: Quality Control Supervisor\n",
    "\n",
    "Build a supervisor with quality control loops:\n",
    "- **Writer** - Creates content\n",
    "- **Critic** - Scores content 1-10 with feedback\n",
    "- **Improver** - Revises based on feedback\n",
    "\n",
    "The supervisor should:\n",
    "1. Start with writer\n",
    "2. Get critic score\n",
    "3. If score \\< 7, send to improver, then back to critic\n",
    "4. Loop until score \\>= 7 or max 3 improvement rounds\n",
    "5. Return final content with score history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.4: Collaborative agent systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: debate_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.4\n",
    "# File: debate_system.py\n",
    "\n",
    "\"\"\"\n",
    "Collaborative debate between agents with opposing viewpoints.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class DebateState(TypedDict):\n",
    "    topic: str\n",
    "    pro_arguments: Annotated[list[str], operator.add]\n",
    "    con_arguments: Annotated[list[str], operator.add]\n",
    "    current_round: int\n",
    "    max_rounds: int\n",
    "    synthesis: str\n",
    "\n",
    "\n",
    "def pro_debater(state: DebateState) -> dict:\n",
    "    \"\"\"Argues in favor of the topic.\"\"\"\n",
    "    existing_pro = state.get(\"pro_arguments\", [])\n",
    "    existing_con = state.get(\"con_arguments\", [])\n",
    "    round_num = state.get(\"current_round\", 1)\n",
    "    \n",
    "    prompt = f\"\"\"You are arguing IN FAVOR of: {state['topic']}\n",
    "    \n",
    "    Round {round_num} of the debate.\n",
    "    \n",
    "    Previous PRO arguments: {existing_pro}\n",
    "    Previous CON arguments: {existing_con}\n",
    "    \n",
    "    Provide ONE new compelling argument for your position.\n",
    "    If there are CON arguments, you may also rebut them.\n",
    "    Be concise but persuasive (2-3 sentences).\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\u2705 PRO (Round {round_num}): {response.content[:60]}...\")\n",
    "    \n",
    "    return {\"pro_arguments\": [f\"[R{round_num}] {response.content}\"]}\n",
    "\n",
    "\n",
    "def con_debater(state: DebateState) -> dict:\n",
    "    \"\"\"Argues against the topic.\"\"\"\n",
    "    existing_pro = state.get(\"pro_arguments\", [])\n",
    "    existing_con = state.get(\"con_arguments\", [])\n",
    "    round_num = state.get(\"current_round\", 1)\n",
    "    \n",
    "    prompt = f\"\"\"You are arguing AGAINST: {state['topic']}\n",
    "    \n",
    "    Round {round_num} of the debate.\n",
    "    \n",
    "    Previous PRO arguments: {existing_pro}\n",
    "    Previous CON arguments: {existing_con}\n",
    "    \n",
    "    Provide ONE new compelling argument against the position.\n",
    "    You may also rebut PRO arguments.\n",
    "    Be concise but persuasive (2-3 sentences).\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\u274c CON (Round {round_num}): {response.content[:60]}...\")\n",
    "    \n",
    "    return {\"con_arguments\": [f\"[R{round_num}] {response.content}\"]}\n",
    "\n",
    "\n",
    "def round_coordinator(state: DebateState) -> dict:\n",
    "    \"\"\"Advances to the next round.\"\"\"\n",
    "    current = state.get(\"current_round\", 0)\n",
    "    return {\"current_round\": current + 1}\n",
    "\n",
    "\n",
    "def judge(state: DebateState) -> dict:\n",
    "    \"\"\"Synthesizes arguments into a balanced conclusion.\"\"\"\n",
    "    pro_args = \"\\n\".join(state.get(\"pro_arguments\", []))\n",
    "    con_args = \"\\n\".join(state.get(\"con_arguments\", []))\n",
    "    \n",
    "    prompt = f\"\"\"As an impartial judge, synthesize this debate:\n",
    "    \n",
    "    TOPIC: {state['topic']}\n",
    "    \n",
    "    ARGUMENTS IN FAVOR:\n",
    "    {pro_args}\n",
    "    \n",
    "    ARGUMENTS AGAINST:\n",
    "    {con_args}\n",
    "    \n",
    "    Provide:\n",
    "    1. The strongest point from each side\n",
    "    2. A balanced conclusion\n",
    "    3. What additional information would help decide\n",
    "    \n",
    "    Be fair and analytical.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\u2696\ufe0f Judge has reached a conclusion\")\n",
    "    \n",
    "    return {\"synthesis\": response.content}\n",
    "\n",
    "\n",
    "def should_continue_debate(state: DebateState) -> str:\n",
    "    \"\"\"Checks if debate should continue.\"\"\"\n",
    "    current = state.get(\"current_round\", 0)\n",
    "    max_rounds = state.get(\"max_rounds\", 2)\n",
    "    \n",
    "    if current >= max_rounds:\n",
    "        return \"judge\"\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "# Build the debate workflow\n",
    "workflow = StateGraph(DebateState)\n",
    "\n",
    "workflow.add_node(\"coordinator\", round_coordinator)\n",
    "workflow.add_node(\"pro\", pro_debater)\n",
    "workflow.add_node(\"con\", con_debater)\n",
    "workflow.add_node(\"judge\", judge)\n",
    "\n",
    "# Start with coordinator to set round 1\n",
    "workflow.add_edge(START, \"coordinator\")\n",
    "\n",
    "# After coordinator, check if we should continue\n",
    "workflow.add_conditional_edges(\n",
    "    \"coordinator\",\n",
    "    should_continue_debate,\n",
    "    {\n",
    "        \"continue\": \"pro\",\n",
    "        \"judge\": \"judge\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Pro and Con take turns (both run each round)\n",
    "workflow.add_edge(\"pro\", \"con\")\n",
    "workflow.add_edge(\"con\", \"coordinator\")  # Back to coordinator for next round\n",
    "\n",
    "workflow.add_edge(\"judge\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run a debate\n",
    "result = app.invoke({\n",
    "    \"topic\": \"Remote work should be the default for knowledge workers\",\n",
    "    \"pro_arguments\": [],\n",
    "    \"con_arguments\": [],\n",
    "    \"current_round\": 0,\n",
    "    \"max_rounds\": 2,\n",
    "    \"synthesis\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBATE CONCLUSION\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"synthesis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: critique_refine.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.4\n",
    "# File: critique_refine.py\n",
    "\n",
    "\"\"\"\n",
    "Collaborative critique and refinement between agents.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class CritiqueState(TypedDict):\n",
    "    task: str\n",
    "    current_work: str\n",
    "    critique_history: Annotated[list[str], operator.add]\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "    is_approved: bool\n",
    "    final_work: str\n",
    "\n",
    "\n",
    "def creator(state: CritiqueState) -> dict:\n",
    "    \"\"\"Creates or revises the work based on critique.\"\"\"\n",
    "    revision = state.get(\"revision_count\", 0)\n",
    "    critique_history = state.get(\"critique_history\", [])\n",
    "    current = state.get(\"current_work\", \"\")\n",
    "    \n",
    "    if revision == 0:\n",
    "        # Initial creation\n",
    "        prompt = f\"\"\"Create a response for this task:\n",
    "        {state['task']}\n",
    "        \n",
    "        Be thorough but concise.\"\"\"\n",
    "    else:\n",
    "        # Revision based on critique\n",
    "        latest_critique = critique_history[-1] if critique_history else \"\"\n",
    "        prompt = f\"\"\"Revise your work based on this critique:\n",
    "        \n",
    "        TASK: {state['task']}\n",
    "        \n",
    "        YOUR PREVIOUS WORK:\n",
    "        {current}\n",
    "        \n",
    "        CRITIQUE:\n",
    "        {latest_critique}\n",
    "        \n",
    "        Address the concerns while keeping what works.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    action = \"Created\" if revision == 0 else f\"Revised (v{revision + 1})\"\n",
    "    print(f\"\u270d\ufe0f Creator: {action}\")\n",
    "    \n",
    "    return {\n",
    "        \"current_work\": response.content,\n",
    "        \"revision_count\": revision + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def critic(state: CritiqueState) -> dict:\n",
    "    \"\"\"Evaluates the work and provides constructive critique.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate this work critically but constructively:\n",
    "    \n",
    "    TASK: {state['task']}\n",
    "    \n",
    "    WORK TO EVALUATE:\n",
    "    {state['current_work']}\n",
    "    \n",
    "    Provide:\n",
    "    1. What works well (be specific)\n",
    "    2. What needs improvement (be specific)  \n",
    "    3. VERDICT: APPROVE if good enough, or REVISE if needs work\n",
    "    \n",
    "    Be fair but maintain high standards.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    is_approved = \"APPROVE\" in response.content.upper() and \"REVISE\" not in response.content.upper()\n",
    "    \n",
    "    verdict = \"APPROVED \u2713\" if is_approved else \"NEEDS REVISION\"\n",
    "    print(f\"\ud83d\udd0d Critic: {verdict}\")\n",
    "    \n",
    "    return {\n",
    "        \"critique_history\": [response.content],\n",
    "        \"is_approved\": is_approved\n",
    "    }\n",
    "\n",
    "\n",
    "def should_revise(state: CritiqueState) -> str:\n",
    "    \"\"\"Decides whether to revise or finalize.\"\"\"\n",
    "    if state.get(\"is_approved\", False):\n",
    "        return \"finalize\"\n",
    "    \n",
    "    revisions = state.get(\"revision_count\", 0)\n",
    "    max_rev = state.get(\"max_revisions\", 3)\n",
    "    \n",
    "    if revisions >= max_rev:\n",
    "        print(f\"\u26a0\ufe0f Max revisions ({max_rev}) reached\")\n",
    "        return \"finalize\"\n",
    "    \n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "def finalizer(state: CritiqueState) -> dict:\n",
    "    \"\"\"Packages the final approved work.\"\"\"\n",
    "    return {\"final_work\": state[\"current_work\"]}\n",
    "\n",
    "\n",
    "# Build the collaborative workflow\n",
    "workflow = StateGraph(CritiqueState)\n",
    "\n",
    "workflow.add_node(\"creator\", creator)\n",
    "workflow.add_node(\"critic\", critic)\n",
    "workflow.add_node(\"finalizer\", finalizer)\n",
    "\n",
    "workflow.add_edge(START, \"creator\")\n",
    "workflow.add_edge(\"creator\", \"critic\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_revise,\n",
    "    {\n",
    "        \"revise\": \"creator\",\n",
    "        \"finalize\": \"finalizer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"finalizer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the critique cycle\n",
    "result = app.invoke({\n",
    "    \"task\": \"Write a professional bio for a software engineer with 5 years experience\",\n",
    "    \"current_work\": \"\",\n",
    "    \"critique_history\": [],\n",
    "    \"revision_count\": 0,\n",
    "    \"max_revisions\": 3,\n",
    "    \"is_approved\": False,\n",
    "    \"final_work\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"FINAL WORK (after {result['revision_count']} revisions)\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"final_work\"])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CRITIQUE HISTORY\")\n",
    "print(\"=\" * 60)\n",
    "for i, critique in enumerate(result[\"critique_history\"], 1):\n",
    "    print(f\"\\n--- Critique {i} ---\")\n",
    "    print(critique[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ensemble_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.4\n",
    "# File: ensemble_system.py\n",
    "\n",
    "\"\"\"\n",
    "Ensemble pattern: Multiple agents solve independently, then merge.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)  # Higher temp for diversity\n",
    "\n",
    "\n",
    "class EnsembleState(TypedDict):\n",
    "    problem: str\n",
    "    solution_creative: str\n",
    "    solution_analytical: str\n",
    "    solution_practical: str\n",
    "    merged_solution: str\n",
    "\n",
    "\n",
    "def creative_solver(state: EnsembleState) -> dict:\n",
    "    \"\"\"Approaches problem creatively.\"\"\"\n",
    "    prompt = f\"\"\"Solve this problem with CREATIVE thinking:\n",
    "    \n",
    "    {state['problem']}\n",
    "    \n",
    "    Think outside the box. Consider unconventional approaches.\n",
    "    Be imaginative but still address the core problem.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83c\udfa8 Creative solver complete\")\n",
    "    return {\"solution_creative\": response.content}\n",
    "\n",
    "\n",
    "def analytical_solver(state: EnsembleState) -> dict:\n",
    "    \"\"\"Approaches problem analytically.\"\"\"\n",
    "    prompt = f\"\"\"Solve this problem with ANALYTICAL thinking:\n",
    "    \n",
    "    {state['problem']}\n",
    "    \n",
    "    Break it down systematically. Consider data and logic.\n",
    "    Be thorough and evidence-based.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udcca Analytical solver complete\")\n",
    "    return {\"solution_analytical\": response.content}\n",
    "\n",
    "\n",
    "def practical_solver(state: EnsembleState) -> dict:\n",
    "    \"\"\"Approaches problem practically.\"\"\"\n",
    "    prompt = f\"\"\"Solve this problem with PRACTICAL thinking:\n",
    "    \n",
    "    {state['problem']}\n",
    "    \n",
    "    Focus on what's actionable and realistic.\n",
    "    Consider constraints and implementation.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd27 Practical solver complete\")\n",
    "    return {\"solution_practical\": response.content}\n",
    "\n",
    "\n",
    "def solution_merger(state: EnsembleState) -> dict:\n",
    "    \"\"\"Synthesizes the three approaches into one solution.\"\"\"\n",
    "    prompt = f\"\"\"Synthesize these three approaches to the problem:\n",
    "    \n",
    "    PROBLEM: {state['problem']}\n",
    "    \n",
    "    CREATIVE APPROACH:\n",
    "    {state['solution_creative']}\n",
    "    \n",
    "    ANALYTICAL APPROACH:\n",
    "    {state['solution_analytical']}\n",
    "    \n",
    "    PRACTICAL APPROACH:\n",
    "    {state['solution_practical']}\n",
    "    \n",
    "    Create a unified solution that:\n",
    "    1. Takes the best ideas from each approach\n",
    "    2. Resolves any contradictions\n",
    "    3. Is both innovative AND actionable\n",
    "    \n",
    "    Provide a clear, integrated recommendation.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd00 Solutions merged\")\n",
    "    return {\"merged_solution\": response.content}\n",
    "\n",
    "\n",
    "# Build with parallel execution\n",
    "workflow = StateGraph(EnsembleState)\n",
    "\n",
    "workflow.add_node(\"creative\", creative_solver)\n",
    "workflow.add_node(\"analytical\", analytical_solver)\n",
    "workflow.add_node(\"practical\", practical_solver)\n",
    "workflow.add_node(\"merger\", solution_merger)\n",
    "\n",
    "# All three solvers start in parallel\n",
    "workflow.add_edge(START, \"creative\")\n",
    "workflow.add_edge(START, \"analytical\")\n",
    "workflow.add_edge(START, \"practical\")\n",
    "\n",
    "# All feed into merger\n",
    "workflow.add_edge(\"creative\", \"merger\")\n",
    "workflow.add_edge(\"analytical\", \"merger\")\n",
    "workflow.add_edge(\"practical\", \"merger\")\n",
    "\n",
    "workflow.add_edge(\"merger\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test ensemble solving\n",
    "result = app.invoke({\n",
    "    \"problem\": \"\"\"Our startup needs to acquire our first 1000 customers \n",
    "    with a limited budget of $5000. We're a B2B SaaS tool for project management.\"\"\",\n",
    "    \"solution_creative\": \"\",\n",
    "    \"solution_analytical\": \"\",\n",
    "    \"solution_practical\": \"\",\n",
    "    \"merged_solution\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ENSEMBLE SOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"merged_solution\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: consensus_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.4\n",
    "# File: consensus_system.py\n",
    "\n",
    "\"\"\"\n",
    "Consensus building among multiple agents.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "\n",
    "class ConsensusState(TypedDict):\n",
    "    question: str\n",
    "    proposals: Annotated[list[str], operator.add]\n",
    "    discussion: Annotated[list[str], operator.add]\n",
    "    round: int\n",
    "    max_rounds: int\n",
    "    consensus_reached: bool\n",
    "    final_decision: str\n",
    "\n",
    "\n",
    "def agent_alpha(state: ConsensusState) -> dict:\n",
    "    \"\"\"First perspective agent.\"\"\"\n",
    "    round_num = state.get(\"round\", 1)\n",
    "    existing_proposals = state.get(\"proposals\", [])\n",
    "    existing_discussion = state.get(\"discussion\", [])\n",
    "    \n",
    "    if round_num == 1:\n",
    "        # Initial proposal\n",
    "        prompt = f\"\"\"Propose your answer to: {state['question']}\n",
    "        \n",
    "        Give a clear, concise recommendation with brief reasoning.\"\"\"\n",
    "    else:\n",
    "        # Respond to discussion\n",
    "        prompt = f\"\"\"Question: {state['question']}\n",
    "        \n",
    "        Previous proposals: {existing_proposals}\n",
    "        Discussion so far: {existing_discussion}\n",
    "        \n",
    "        Based on the discussion, do you:\n",
    "        1. Maintain your position (explain why)\n",
    "        2. Modify your position (explain what changed)\n",
    "        3. Accept another proposal (say which one and why)\n",
    "        \n",
    "        Be constructive and aim for consensus.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    if round_num == 1:\n",
    "        return {\"proposals\": [f\"[Alpha] {response.content}\"]}\n",
    "    return {\"discussion\": [f\"[Alpha R{round_num}] {response.content}\"]}\n",
    "\n",
    "\n",
    "def agent_beta(state: ConsensusState) -> dict:\n",
    "    \"\"\"Second perspective agent.\"\"\"\n",
    "    round_num = state.get(\"round\", 1)\n",
    "    existing_proposals = state.get(\"proposals\", [])\n",
    "    existing_discussion = state.get(\"discussion\", [])\n",
    "    \n",
    "    if round_num == 1:\n",
    "        prompt = f\"\"\"Propose your answer to: {state['question']}\n",
    "        \n",
    "        Consider a different angle than others might take.\n",
    "        Give a clear recommendation with reasoning.\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Question: {state['question']}\n",
    "        \n",
    "        Previous proposals: {existing_proposals}\n",
    "        Discussion so far: {existing_discussion}\n",
    "        \n",
    "        Respond constructively. State if you agree, disagree, or want to modify.\n",
    "        Focus on finding common ground.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    if round_num == 1:\n",
    "        return {\"proposals\": [f\"[Beta] {response.content}\"]}\n",
    "    return {\"discussion\": [f\"[Beta R{round_num}] {response.content}\"]}\n",
    "\n",
    "\n",
    "def facilitator(state: ConsensusState) -> dict:\n",
    "    \"\"\"Checks if consensus has been reached.\"\"\"\n",
    "    proposals = state.get(\"proposals\", [])\n",
    "    discussion = state.get(\"discussion\", [])\n",
    "    round_num = state.get(\"round\", 0) + 1\n",
    "    \n",
    "    if round_num == 1:\n",
    "        # Just starting, no consensus check yet\n",
    "        print(f\"\ud83d\udde3\ufe0f Round {round_num}: Gathering initial proposals\")\n",
    "        return {\"round\": round_num, \"consensus_reached\": False}\n",
    "    \n",
    "    # Analyze discussion for consensus\n",
    "    all_content = \" \".join(proposals + discussion).lower()\n",
    "    \n",
    "    # Simple consensus detection (in production, use LLM for this)\n",
    "    agreement_signals = [\"agree\", \"accept\", \"consensus\", \"common ground\", \"align\"]\n",
    "    agreement_count = sum(1 for signal in agreement_signals if signal in all_content)\n",
    "    \n",
    "    consensus = agreement_count >= 2 or round_num > state.get(\"max_rounds\", 3)\n",
    "    \n",
    "    if consensus:\n",
    "        print(f\"\u2705 Consensus detected after {round_num} rounds\")\n",
    "    else:\n",
    "        print(f\"\ud83d\udde3\ufe0f Round {round_num}: Continuing discussion\")\n",
    "    \n",
    "    return {\"round\": round_num, \"consensus_reached\": consensus}\n",
    "\n",
    "\n",
    "def should_continue(state: ConsensusState) -> str:\n",
    "    \"\"\"Decides if discussion should continue.\"\"\"\n",
    "    if state.get(\"consensus_reached\", False):\n",
    "        return \"synthesize\"\n",
    "    if state.get(\"round\", 0) >= state.get(\"max_rounds\", 3):\n",
    "        return \"synthesize\"\n",
    "    return \"discuss\"\n",
    "\n",
    "\n",
    "def synthesizer(state: ConsensusState) -> dict:\n",
    "    \"\"\"Synthesizes discussion into final decision.\"\"\"\n",
    "    prompt = f\"\"\"Synthesize this group discussion into a final decision:\n",
    "    \n",
    "    QUESTION: {state['question']}\n",
    "    \n",
    "    INITIAL PROPOSALS:\n",
    "    {chr(10).join(state.get('proposals', []))}\n",
    "    \n",
    "    DISCUSSION:\n",
    "    {chr(10).join(state.get('discussion', []))}\n",
    "    \n",
    "    Provide:\n",
    "    1. The consensus decision (what the group agreed on)\n",
    "    2. Key points that led to agreement\n",
    "    3. Any remaining concerns or caveats\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udccb Final decision synthesized\")\n",
    "    \n",
    "    return {\"final_decision\": response.content}\n",
    "\n",
    "\n",
    "# Build the consensus workflow\n",
    "workflow = StateGraph(ConsensusState)\n",
    "\n",
    "workflow.add_node(\"facilitator\", facilitator)\n",
    "workflow.add_node(\"alpha\", agent_alpha)\n",
    "workflow.add_node(\"beta\", agent_beta)\n",
    "workflow.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "workflow.add_edge(START, \"facilitator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"facilitator\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"discuss\": \"alpha\",\n",
    "        \"synthesize\": \"synthesizer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Discussion flow: alpha -> beta -> facilitator\n",
    "workflow.add_edge(\"alpha\", \"beta\")\n",
    "workflow.add_edge(\"beta\", \"facilitator\")\n",
    "\n",
    "workflow.add_edge(\"synthesizer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test consensus building\n",
    "result = app.invoke({\n",
    "    \"question\": \"What programming language should a beginner learn first?\",\n",
    "    \"proposals\": [],\n",
    "    \"discussion\": [],\n",
    "    \"round\": 0,\n",
    "    \"max_rounds\": 3,\n",
    "    \"consensus_reached\": False,\n",
    "    \"final_decision\": \"\"\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONSENSUS DECISION\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"final_decision\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.4.1: Three-Way Debate\n",
    "\n",
    "Extend the debate pattern to include three agents:\n",
    "- **Optimist** - Focuses on opportunities and benefits\n",
    "- **Pessimist** - Focuses on risks and problems\n",
    "- **Realist** - Tries to find middle ground\n",
    "\n",
    "Each agent should respond to the others' arguments. The judge should identify where all three perspectives align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.4.2: Review Committee\n",
    "\n",
    "Build a collaborative review system with three reviewers:\n",
    "- **Technical Reviewer** - Checks accuracy and technical correctness\n",
    "- **Style Reviewer** - Checks clarity and readability\n",
    "- **Audience Reviewer** - Checks if it's appropriate for target audience\n",
    "\n",
    "They should each provide feedback, then collaboratively create a consolidated review with prioritized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.4.3: Negotiation Simulation\n",
    "\n",
    "Create two agents that negotiate a deal:\n",
    "- **Buyer Agent** - Wants lowest price, best terms\n",
    "- **Seller Agent** - Wants highest price, favorable terms\n",
    "\n",
    "They should:\n",
    "1. Each state their initial position\n",
    "2. Exchange counter-offers (max 3 rounds)\n",
    "3. Try to find a mutually acceptable deal\n",
    "4. Report final outcome (deal reached or impasse)\n",
    "\n",
    "Track concessions made by each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.5: Building a research assistant team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: research_team.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.5\n",
    "# File: research_team.py\n",
    "\n",
    "\"\"\"\n",
    "A multi-agent research assistant team.\n",
    "Basic sequential version: Planner -> Researcher -> Analyst -> Writer\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# We use a lower temperature for more focused, factual responses\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str                    # The research topic\n",
    "    questions: list[str]          # Research questions from planner\n",
    "    findings: list[str]           # Raw findings from researcher\n",
    "    insights: str                 # Analysis from analyst\n",
    "    report: str                   # Final report from writer\n",
    "    status: str                   # Current status for tracking\n",
    "\n",
    "\n",
    "def planner_agent(state: ResearchState) -> dict:\n",
    "    \"\"\"\n",
    "    Takes a topic and creates focused research questions.\n",
    "    Good research starts with good questions.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a research planner. Given a topic, create exactly 3 \n",
    "focused research questions that would help someone understand it thoroughly.\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "Requirements:\n",
    "- Each question should cover a different aspect\n",
    "- Questions should be specific and answerable\n",
    "- Together they should provide comprehensive coverage\n",
    "\n",
    "Format your response as:\n",
    "1. [First question]\n",
    "2. [Second question]\n",
    "3. [Third question]\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse the questions from the response\n",
    "    lines = response.content.strip().split('\\n')\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        # Remove numbering and clean up\n",
    "        cleaned = line.strip()\n",
    "        if cleaned and cleaned[0].isdigit():\n",
    "            # Remove \"1. \" or \"1) \" prefix\n",
    "            cleaned = cleaned[2:].strip() if len(cleaned) > 2 else cleaned\n",
    "            if cleaned.startswith('.') or cleaned.startswith(')'):\n",
    "                cleaned = cleaned[1:].strip()\n",
    "            questions.append(cleaned)\n",
    "    \n",
    "    # Ensure we have exactly 3 questions\n",
    "    questions = questions[:3] if len(questions) >= 3 else questions\n",
    "    \n",
    "    print(f\"\ud83d\udccb Planner created {len(questions)} research questions\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"   {i}. {q[:60]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"questions\": questions,\n",
    "        \"status\": \"questions_ready\"\n",
    "    }\n",
    "\n",
    "\n",
    "def researcher_agent(state: ResearchState) -> dict:\n",
    "    \"\"\"\n",
    "    Researches each question and gathers findings.\n",
    "    In production, this might call search APIs or databases.\n",
    "    \"\"\"\n",
    "    findings = []\n",
    "    \n",
    "    for i, question in enumerate(state['questions'], 1):\n",
    "        prompt = f\"\"\"You are a research assistant. Answer this research question \n",
    "with factual, specific information.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide 2-3 key facts or findings. Be specific and informative.\n",
    "Keep your response to 3-4 sentences.\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        finding = f\"Q{i}: {question}\\nFindings: {response.content}\"\n",
    "        findings.append(finding)\n",
    "        \n",
    "        print(f\"\ud83d\udd0d Researched question {i}/{len(state['questions'])}\")\n",
    "    \n",
    "    return {\n",
    "        \"findings\": findings,\n",
    "        \"status\": \"research_complete\"\n",
    "    }\n",
    "\n",
    "\n",
    "def analyst_agent(state: ResearchState) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes findings to extract key insights and patterns.\n",
    "    Moves from raw facts to deeper understanding.\n",
    "    \"\"\"\n",
    "    # Combine all findings into one text block\n",
    "    all_findings = \"\\n\\n\".join(state['findings'])\n",
    "    \n",
    "    prompt = f\"\"\"You are a research analyst. Review these research findings and \n",
    "extract key insights.\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "FINDINGS:\n",
    "{all_findings}\n",
    "\n",
    "Provide your analysis:\n",
    "1. What are the 2-3 most important takeaways?\n",
    "2. Are there any patterns or connections between findings?\n",
    "3. What's the overall picture that emerges?\n",
    "\n",
    "Be concise but insightful.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(\"\ud83d\udd2c Analysis complete\")\n",
    "    \n",
    "    return {\n",
    "        \"insights\": response.content,\n",
    "        \"status\": \"analysis_complete\"\n",
    "    }\n",
    "\n",
    "\n",
    "def writer_agent(state: ResearchState) -> dict:\n",
    "    \"\"\"\n",
    "    Compiles all research into a readable final report.\n",
    "    The output the user actually sees.\n",
    "    \"\"\"\n",
    "    all_findings = \"\\n\\n\".join(state['findings'])\n",
    "    \n",
    "    prompt = f\"\"\"You are a research report writer. Create a clear, well-organized \n",
    "report based on this research.\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "RESEARCH FINDINGS:\n",
    "{all_findings}\n",
    "\n",
    "KEY INSIGHTS:\n",
    "{state['insights']}\n",
    "\n",
    "Write a report with:\n",
    "1. A brief introduction (2-3 sentences)\n",
    "2. Main findings organized by theme (use bullet points)\n",
    "3. A conclusion with key takeaways (2-3 sentences)\n",
    "\n",
    "Keep it concise but comprehensive. Use clear, professional language.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(\"\u270d\ufe0f Report written\")\n",
    "    \n",
    "    return {\n",
    "        \"report\": response.content,\n",
    "        \"status\": \"complete\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Build the research workflow\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add all our agents as nodes\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"researcher\", researcher_agent)\n",
    "workflow.add_node(\"analyst\", analyst_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "\n",
    "# Connect them in sequence\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"writer\")\n",
    "workflow.add_edge(\"writer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "research_team = workflow.compile()\n",
    "\n",
    "\n",
    "def run_research(topic: str) -> str:\n",
    "    \"\"\"Run the research team on a topic and return the report.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"RESEARCHING: {topic}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    result = research_team.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"questions\": [],\n",
    "        \"findings\": [],\n",
    "        \"insights\": \"\",\n",
    "        \"report\": \"\",\n",
    "        \"status\": \"started\"\n",
    "    })\n",
    "    \n",
    "    return result[\"report\"]\n",
    "\n",
    "\n",
    "# Test it out!\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"The impact of artificial intelligence on healthcare\"\n",
    "    \n",
    "    report = run_research(topic)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RESEARCH REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: research_team_with_review.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.5\n",
    "# File: research_team_with_review.py\n",
    "\n",
    "\"\"\"\n",
    "Research team with quality review loop.\n",
    "Adds a reviewer that can request revisions before final output.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "\n",
    "class ResearchStateWithReview(TypedDict):\n",
    "    topic: str\n",
    "    questions: list[str]\n",
    "    findings: list[str]\n",
    "    insights: str\n",
    "    report: str\n",
    "    review_feedback: str\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "    approved: bool\n",
    "\n",
    "\n",
    "def planner_agent(state: ResearchStateWithReview) -> dict:\n",
    "    \"\"\"Creates research questions from topic.\"\"\"\n",
    "    prompt = f\"\"\"You are a research planner. Given a topic, create exactly 3 \n",
    "focused research questions.\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "Format: numbered list, one question per line.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    lines = response.content.strip().split('\\n')\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        cleaned = line.strip()\n",
    "        if cleaned and cleaned[0].isdigit():\n",
    "            cleaned = cleaned[2:].strip() if len(cleaned) > 2 else cleaned\n",
    "            if cleaned.startswith('.') or cleaned.startswith(')'):\n",
    "                cleaned = cleaned[1:].strip()\n",
    "            questions.append(cleaned)\n",
    "    \n",
    "    questions = questions[:3]\n",
    "    print(f\"\ud83d\udccb Planner created {len(questions)} research questions\")\n",
    "    return {\"questions\": questions}\n",
    "\n",
    "\n",
    "def researcher_agent(state: ResearchStateWithReview) -> dict:\n",
    "    \"\"\"Researches each question and gathers findings.\"\"\"\n",
    "    findings = []\n",
    "    \n",
    "    for i, question in enumerate(state['questions'], 1):\n",
    "        prompt = f\"\"\"Answer this research question with 2-3 key facts:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Be specific and informative. Keep to 3-4 sentences.\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        finding = f\"Q{i}: {question}\\nFindings: {response.content}\"\n",
    "        findings.append(finding)\n",
    "        print(f\"\ud83d\udd0d Researched question {i}/{len(state['questions'])}\")\n",
    "    \n",
    "    return {\"findings\": findings}\n",
    "\n",
    "\n",
    "def analyst_agent(state: ResearchStateWithReview) -> dict:\n",
    "    \"\"\"Analyzes findings to extract key insights.\"\"\"\n",
    "    all_findings = \"\\n\\n\".join(state['findings'])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these research findings:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "FINDINGS:\n",
    "{all_findings}\n",
    "\n",
    "What are the 2-3 most important takeaways? Be concise but insightful.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd2c Analysis complete\")\n",
    "    return {\"insights\": response.content}\n",
    "\n",
    "\n",
    "def writer_with_revision(state: ResearchStateWithReview) -> dict:\n",
    "    \"\"\"Writes or revises the report based on feedback.\"\"\"\n",
    "    all_findings = \"\\n\\n\".join(state['findings'])\n",
    "    \n",
    "    if state.get('review_feedback'):\n",
    "        # This is a revision\n",
    "        prompt = f\"\"\"Revise this research report based on feedback:\n",
    "\n",
    "CURRENT REPORT:\n",
    "{state['report']}\n",
    "\n",
    "FEEDBACK:\n",
    "{state['review_feedback']}\n",
    "\n",
    "Provide an improved version addressing the feedback.\"\"\"\n",
    "    else:\n",
    "        # Initial write\n",
    "        prompt = f\"\"\"Write a research report on: {state['topic']}\n",
    "\n",
    "FINDINGS:\n",
    "{all_findings}\n",
    "\n",
    "INSIGHTS:\n",
    "{state['insights']}\n",
    "\n",
    "Include: introduction, main findings, and conclusion.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\u270d\ufe0f Report \" + (\"revised\" if state.get('review_feedback') else \"written\"))\n",
    "    \n",
    "    return {\"report\": response.content}\n",
    "\n",
    "\n",
    "def reviewer_agent(state: ResearchStateWithReview) -> dict:\n",
    "    \"\"\"Reviews the report for quality and completeness.\"\"\"\n",
    "    prompt = f\"\"\"You are a research report reviewer. Evaluate this report:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "REPORT:\n",
    "{state['report']}\n",
    "\n",
    "Check for:\n",
    "1. Does it address the topic adequately?\n",
    "2. Is it well-organized and clear?\n",
    "3. Are claims supported by the findings?\n",
    "\n",
    "If the report is good, respond with: APPROVED\n",
    "\n",
    "If it needs improvement, respond with: REVISE: [specific feedback]\n",
    "\n",
    "Be reasonable - don't demand perfection.\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "    \n",
    "    approved = content.upper().startswith(\"APPROVED\")\n",
    "    revision_count = state.get(\"revision_count\", 0)\n",
    "    \n",
    "    if approved:\n",
    "        print(\"\u2705 Report approved!\")\n",
    "    else:\n",
    "        print(f\"\ud83d\udcdd Revision requested (attempt {revision_count + 1})\")\n",
    "    \n",
    "    return {\n",
    "        \"approved\": approved,\n",
    "        \"review_feedback\": content if not approved else \"\",\n",
    "        \"revision_count\": revision_count + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def should_revise(state: ResearchStateWithReview) -> Literal[\"revise\", \"done\"]:\n",
    "    \"\"\"Decide whether to revise or finish.\"\"\"\n",
    "    if state.get(\"approved\", False):\n",
    "        return \"done\"\n",
    "    \n",
    "    max_rev = state.get(\"max_revisions\", 2)\n",
    "    if state.get(\"revision_count\", 0) >= max_rev:\n",
    "        print(\"\u26a0\ufe0f Max revisions reached, accepting current version\")\n",
    "        return \"done\"\n",
    "    \n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "# Build the enhanced workflow\n",
    "workflow = StateGraph(ResearchStateWithReview)\n",
    "\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"researcher\", researcher_agent)\n",
    "workflow.add_node(\"analyst\", analyst_agent)\n",
    "workflow.add_node(\"writer\", writer_with_revision)\n",
    "workflow.add_node(\"reviewer\", reviewer_agent)\n",
    "\n",
    "# Main flow\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"writer\")\n",
    "workflow.add_edge(\"writer\", \"reviewer\")\n",
    "\n",
    "# Review loop\n",
    "workflow.add_conditional_edges(\n",
    "    \"reviewer\",\n",
    "    should_revise,\n",
    "    {\n",
    "        \"revise\": \"writer\",\n",
    "        \"done\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "research_team_v2 = workflow.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = research_team_v2.invoke({\n",
    "        \"topic\": \"Benefits of meditation for stress reduction\",\n",
    "        \"questions\": [],\n",
    "        \"findings\": [],\n",
    "        \"insights\": \"\",\n",
    "        \"report\": \"\",\n",
    "        \"review_feedback\": \"\",\n",
    "        \"revision_count\": 0,\n",
    "        \"max_revisions\": 2,\n",
    "        \"approved\": False\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(result[\"report\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: research_team_complete.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.5\n",
    "# File: research_team_complete.py\n",
    "\n",
    "\"\"\"\n",
    "Complete multi-agent research assistant team.\n",
    "Combines sequential pipeline with quality review loop.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "\n",
    "# === STATE ===\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    questions: list[str]\n",
    "    findings: list[str]\n",
    "    insights: str\n",
    "    report: str\n",
    "    feedback: str\n",
    "    revision_count: int\n",
    "    approved: bool\n",
    "\n",
    "\n",
    "# === AGENTS ===\n",
    "\n",
    "def planner(state: ResearchState) -> dict:\n",
    "    \"\"\"Creates research questions from topic.\"\"\"\n",
    "    prompt = f\"\"\"Create 3 research questions for: {state['topic']}\n",
    "    \n",
    "Format: numbered list, one question per line.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    questions = [line.strip()[3:] for line in response.content.split('\\n') \n",
    "                 if line.strip() and line.strip()[0].isdigit()][:3]\n",
    "    \n",
    "    print(f\"\ud83d\udccb Planner: {len(questions)} questions created\")\n",
    "    return {\"questions\": questions}\n",
    "\n",
    "\n",
    "def researcher(state: ResearchState) -> dict:\n",
    "    \"\"\"Gathers findings for each question.\"\"\"\n",
    "    findings = []\n",
    "    for i, q in enumerate(state['questions'], 1):\n",
    "        prompt = f\"Answer briefly with 2-3 facts: {q}\"\n",
    "        response = llm.invoke(prompt)\n",
    "        findings.append(f\"Q: {q}\\nA: {response.content}\")\n",
    "        print(f\"\ud83d\udd0d Researcher: question {i} done\")\n",
    "    return {\"findings\": findings}\n",
    "\n",
    "\n",
    "def analyst(state: ResearchState) -> dict:\n",
    "    \"\"\"Extracts insights from findings.\"\"\"\n",
    "    prompt = f\"\"\"Analyze these findings and give 3 key insights:\n",
    "    \n",
    "{chr(10).join(state['findings'])}\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"\ud83d\udd2c Analyst: insights extracted\")\n",
    "    return {\"insights\": response.content}\n",
    "\n",
    "\n",
    "def writer(state: ResearchState) -> dict:\n",
    "    \"\"\"Writes or revises the report.\"\"\"\n",
    "    if state.get('feedback'):\n",
    "        prompt = f\"\"\"Revise this report based on feedback:\n",
    "        \n",
    "REPORT: {state['report']}\n",
    "\n",
    "FEEDBACK: {state['feedback']}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Write a short report on: {state['topic']}\n",
    "\n",
    "INSIGHTS: {state['insights']}\n",
    "\n",
    "Include intro, findings, conclusion.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    action = \"revised\" if state.get('feedback') else \"written\"\n",
    "    print(f\"\u270d\ufe0f Writer: report {action}\")\n",
    "    return {\"report\": response.content, \"revision_count\": state.get('revision_count', 0) + 1}\n",
    "\n",
    "\n",
    "def reviewer(state: ResearchState) -> dict:\n",
    "    \"\"\"Reviews report quality.\"\"\"\n",
    "    prompt = f\"\"\"Review this report. Reply APPROVED if good, or REVISE: [feedback] if not.\n",
    "\n",
    "{state['report']}\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    approved = \"APPROVED\" in response.content.upper()\n",
    "    print(f\"{'\u2705 Approved' if approved else '\ud83d\udcdd Needs revision'}\")\n",
    "    \n",
    "    return {\n",
    "        \"approved\": approved,\n",
    "        \"feedback\": \"\" if approved else response.content\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: ResearchState) -> Literal[\"revise\", \"done\"]:\n",
    "    \"\"\"Check if we should revise or finish.\"\"\"\n",
    "    if state.get(\"approved\") or state.get(\"revision_count\", 0) >= 2:\n",
    "        return \"done\"\n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "# === BUILD GRAPH ===\n",
    "\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "workflow.add_node(\"planner\", planner)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"analyst\", analyst)\n",
    "workflow.add_node(\"writer\", writer)\n",
    "workflow.add_node(\"reviewer\", reviewer)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"analyst\")\n",
    "workflow.add_edge(\"analyst\", \"writer\")\n",
    "workflow.add_edge(\"writer\", \"reviewer\")\n",
    "\n",
    "workflow.add_conditional_edges(\"reviewer\", should_continue, {\n",
    "    \"revise\": \"writer\",\n",
    "    \"done\": END\n",
    "})\n",
    "\n",
    "research_team = workflow.compile()\n",
    "\n",
    "\n",
    "# === RUN ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = research_team.invoke({\n",
    "        \"topic\": \"Benefits of meditation for stress reduction\",\n",
    "        \"questions\": [],\n",
    "        \"findings\": [],\n",
    "        \"insights\": \"\",\n",
    "        \"report\": \"\",\n",
    "        \"feedback\": \"\",\n",
    "        \"revision_count\": 0,\n",
    "        \"approved\": False\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(result[\"report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.5.1: Add a Fact-Checker\n",
    "\n",
    "Add a fact-checker agent between the researcher and analyst that:\n",
    "- Reviews each finding for plausibility\n",
    "- Flags any claims that seem questionable\n",
    "- Adds confidence levels (high/medium/low) to findings\n",
    "\n",
    "The analyst should then weight its analysis based on confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.5.2: Parallel Research\n",
    "\n",
    "Modify the researcher to gather information from two different \"perspectives\":\n",
    "- **Academic perspective** - Focus on research and studies\n",
    "- **Practical perspective** - Focus on real-world applications\n",
    "\n",
    "Run both in parallel, then have the analyst synthesize both viewpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.5.3: Automatic Gap Detection\n",
    "\n",
    "Add a \"gap detector\" agent after the analyst that:\n",
    "- Reviews the insights and identifies topics that need deeper research\n",
    "- Automatically triggers additional research on weak areas\n",
    "- Limits to one round of additional research\n",
    "\n",
    "The gap detector should check if any of the original questions weren't fully answered and request targeted follow-up research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.6: Managing shared state between agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: state_patterns.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.6\n",
    "# File: state_patterns.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates the three state management patterns in LangGraph:\n",
    "1. Replace - Agent overwrites a field completely\n",
    "2. Accumulate - Agent adds to a list\n",
    "3. Conditional Update - Agent updates only if needed\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import operator\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 1: REPLACE\n",
    "# Agent overwrites a field completely\n",
    "# =============================================================================\n",
    "\n",
    "class ReplaceState(TypedDict):\n",
    "    input_text: str\n",
    "    processed_text: str  # Will be replaced each time\n",
    "\n",
    "\n",
    "def processor_replace(state: ReplaceState) -> dict:\n",
    "    \"\"\"This REPLACES whatever was in 'processed_text'\"\"\"\n",
    "    result = state[\"input_text\"].upper()\n",
    "    return {\"processed_text\": result}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 2: ACCUMULATE\n",
    "# Agent adds to a list using Annotated with operator.add\n",
    "# =============================================================================\n",
    "\n",
    "class AccumulateState(TypedDict):\n",
    "    items_to_process: list[str]\n",
    "    results: Annotated[list[str], operator.add]  # Will accumulate\n",
    "\n",
    "\n",
    "def processor_accumulate(state: AccumulateState) -> dict:\n",
    "    \"\"\"This APPENDS to the list, doesn't replace\"\"\"\n",
    "    new_results = [item.upper() for item in state[\"items_to_process\"]]\n",
    "    return {\"results\": new_results}  # Gets added to existing results\n",
    "\n",
    "\n",
    "def another_processor(state: AccumulateState) -> dict:\n",
    "    \"\"\"This also appends - both results are preserved\"\"\"\n",
    "    new_results = [f\"Processed: {item}\" for item in state[\"items_to_process\"]]\n",
    "    return {\"results\": new_results}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 3: CONDITIONAL UPDATE\n",
    "# Agent updates only if needed\n",
    "# =============================================================================\n",
    "\n",
    "class ConditionalState(TypedDict):\n",
    "    value: int\n",
    "    threshold: int\n",
    "    exceeded: bool\n",
    "    message: str\n",
    "\n",
    "\n",
    "def conditional_checker(state: ConditionalState) -> dict:\n",
    "    \"\"\"Only updates fields when condition is met\"\"\"\n",
    "    if state[\"value\"] > state[\"threshold\"]:\n",
    "        return {\n",
    "            \"exceeded\": True,\n",
    "            \"message\": f\"Value {state['value']} exceeds threshold {state['threshold']}\"\n",
    "        }\n",
    "    # Return empty dict if no update needed - fields keep previous values\n",
    "    return {}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "def demo_replace():\n",
    "    \"\"\"Demo the replace pattern\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PATTERN 1: REPLACE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    workflow = StateGraph(ReplaceState)\n",
    "    workflow.add_node(\"processor\", processor_replace)\n",
    "    workflow.add_edge(START, \"processor\")\n",
    "    workflow.add_edge(\"processor\", END)\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"input_text\": \"hello world\",\n",
    "        \"processed_text\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(f\"Input: 'hello world'\")\n",
    "    print(f\"Output: '{result['processed_text']}'\")\n",
    "    print(\"(Field was replaced with new value)\\n\")\n",
    "\n",
    "\n",
    "def demo_accumulate():\n",
    "    \"\"\"Demo the accumulate pattern\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PATTERN 2: ACCUMULATE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    workflow = StateGraph(AccumulateState)\n",
    "    workflow.add_node(\"proc1\", processor_accumulate)\n",
    "    workflow.add_node(\"proc2\", another_processor)\n",
    "    workflow.add_edge(START, \"proc1\")\n",
    "    workflow.add_edge(\"proc1\", \"proc2\")\n",
    "    workflow.add_edge(\"proc2\", END)\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"items_to_process\": [\"a\", \"b\", \"c\"],\n",
    "        \"results\": []\n",
    "    })\n",
    "    \n",
    "    print(f\"Input items: ['a', 'b', 'c']\")\n",
    "    print(f\"Accumulated results: {result['results']}\")\n",
    "    print(\"(Both processors' outputs are preserved)\\n\")\n",
    "\n",
    "\n",
    "def demo_conditional():\n",
    "    \"\"\"Demo the conditional update pattern\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"PATTERN 3: CONDITIONAL UPDATE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    workflow = StateGraph(ConditionalState)\n",
    "    workflow.add_node(\"checker\", conditional_checker)\n",
    "    workflow.add_edge(START, \"checker\")\n",
    "    workflow.add_edge(\"checker\", END)\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    # Test with value below threshold\n",
    "    result1 = app.invoke({\n",
    "        \"value\": 5,\n",
    "        \"threshold\": 10,\n",
    "        \"exceeded\": False,\n",
    "        \"message\": \"Initial\"\n",
    "    })\n",
    "    print(f\"Test 1: value=5, threshold=10\")\n",
    "    print(f\"  exceeded: {result1['exceeded']}, message: '{result1['message']}'\")\n",
    "    print(\"  (No update - condition not met)\")\n",
    "    \n",
    "    # Test with value above threshold\n",
    "    result2 = app.invoke({\n",
    "        \"value\": 15,\n",
    "        \"threshold\": 10,\n",
    "        \"exceeded\": False,\n",
    "        \"message\": \"Initial\"\n",
    "    })\n",
    "    print(f\"\\nTest 2: value=15, threshold=10\")\n",
    "    print(f\"  exceeded: {result2['exceeded']}, message: '{result2['message']}'\")\n",
    "    print(\"  (Updated - condition met)\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_replace()\n",
    "    demo_accumulate()\n",
    "    demo_conditional()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"KEY TAKEAWAYS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\u2022 REPLACE: Default behavior, last write wins\")\n",
    "    print(\"\u2022 ACCUMULATE: Use Annotated[list, operator.add]\")\n",
    "    print(\"\u2022 CONDITIONAL: Return empty dict {} to skip update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: parallel_state_handling.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.6\n",
    "# File: parallel_state_handling.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates how to handle state correctly when agents run in parallel.\n",
    "\n",
    "Problem: When two agents write to the same field, LangGraph raises an error.\n",
    "Solutions shown:\n",
    "1. Separate fields for each agent\n",
    "2. Accumulate into a shared list\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import operator\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# THE PROBLEM: Both agents write to same field\n",
    "# =============================================================================\n",
    "\n",
    "class BadState(TypedDict):\n",
    "    data: str\n",
    "    result: str  # PROBLEM: Both agents write here!\n",
    "\n",
    "\n",
    "def agent_a_bad(state: BadState) -> dict:\n",
    "    return {\"result\": f\"Agent A analyzed: {state['data'][:20]}\"}\n",
    "\n",
    "\n",
    "def agent_b_bad(state: BadState) -> dict:\n",
    "    return {\"result\": f\"Agent B analyzed: {state['data'][:20]}\"}  # Conflict!\n",
    "\n",
    "\n",
    "def demo_problem():\n",
    "    \"\"\"Show the problem with parallel agents writing to same field\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PROBLEM: Parallel agents writing to same field\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workflow = StateGraph(BadState)\n",
    "    workflow.add_node(\"agent_a\", agent_a_bad)\n",
    "    workflow.add_node(\"agent_b\", agent_b_bad)\n",
    "    \n",
    "    # Both run in parallel from START\n",
    "    workflow.add_edge(START, \"agent_a\")\n",
    "    workflow.add_edge(START, \"agent_b\")\n",
    "    workflow.add_edge(\"agent_a\", END)\n",
    "    workflow.add_edge(\"agent_b\", END)\n",
    "    \n",
    "    app = workflow.compile()\n",
    "    \n",
    "    try:\n",
    "        result = app.invoke({\n",
    "            \"data\": \"Some text to analyze\",\n",
    "            \"result\": \"\"\n",
    "        })\n",
    "        # Old behavior (silent overwrite)\n",
    "        print(f\"Result: '{result['result']}'\")\n",
    "        print(\"\u26a0\ufe0f  Only ONE agent's result is preserved!\")\n",
    "        print(\"   The other was overwritten.\\n\")\n",
    "    except Exception as e:\n",
    "        # New behavior (LangGraph catches the conflict)\n",
    "        print(f\"\\n\u274c LangGraph caught the conflict!\")\n",
    "        print(f\"   Error: {type(e).__name__}\")\n",
    "        print(f\"   Message: Can receive only one value per step.\")\n",
    "        print(\"\\n\u2705 This is actually GOOD - LangGraph prevents data loss!\")\n",
    "        print(\"   Let's see how to fix it properly...\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTION 1: Separate fields for each agent\n",
    "# =============================================================================\n",
    "\n",
    "class SeparateFieldsState(TypedDict):\n",
    "    data: str\n",
    "    result_a: str  # Agent A owns this\n",
    "    result_b: str  # Agent B owns this\n",
    "    combined: str  # Merger owns this\n",
    "\n",
    "\n",
    "def agent_a_separate(state: SeparateFieldsState) -> dict:\n",
    "    return {\"result_a\": f\"Agent A: {state['data'][:20]}...\"}\n",
    "\n",
    "\n",
    "def agent_b_separate(state: SeparateFieldsState) -> dict:\n",
    "    return {\"result_b\": f\"Agent B: {state['data'][:20]}...\"}\n",
    "\n",
    "\n",
    "def merger_separate(state: SeparateFieldsState) -> dict:\n",
    "    return {\"combined\": f\"{state['result_a']}\\n{state['result_b']}\"}\n",
    "\n",
    "\n",
    "def demo_solution_1():\n",
    "    \"\"\"Demo solution with separate fields\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SOLUTION 1: Separate fields for each agent\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workflow = StateGraph(SeparateFieldsState)\n",
    "    workflow.add_node(\"agent_a\", agent_a_separate)\n",
    "    workflow.add_node(\"agent_b\", agent_b_separate)\n",
    "    workflow.add_node(\"merger\", merger_separate)\n",
    "    \n",
    "    # Parallel execution\n",
    "    workflow.add_edge(START, \"agent_a\")\n",
    "    workflow.add_edge(START, \"agent_b\")\n",
    "    \n",
    "    # Both feed into merger\n",
    "    workflow.add_edge(\"agent_a\", \"merger\")\n",
    "    workflow.add_edge(\"agent_b\", \"merger\")\n",
    "    \n",
    "    workflow.add_edge(\"merger\", END)\n",
    "    \n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"data\": \"Some text to analyze for demonstration\",\n",
    "        \"result_a\": \"\",\n",
    "        \"result_b\": \"\",\n",
    "        \"combined\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(f\"Agent A result: '{result['result_a']}'\")\n",
    "    print(f\"Agent B result: '{result['result_b']}'\")\n",
    "    print(f\"Combined: '{result['combined']}'\")\n",
    "    print(\"\u2705 Both results preserved!\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTION 2: Accumulate into a shared list\n",
    "# =============================================================================\n",
    "\n",
    "class AccumulateState(TypedDict):\n",
    "    data: str\n",
    "    results: Annotated[list[str], operator.add]  # Both agents add to this\n",
    "\n",
    "\n",
    "def agent_a_accumulate(state: AccumulateState) -> dict:\n",
    "    return {\"results\": [f\"Agent A: {state['data'][:20]}...\"]}\n",
    "\n",
    "\n",
    "def agent_b_accumulate(state: AccumulateState) -> dict:\n",
    "    return {\"results\": [f\"Agent B: {state['data'][:20]}...\"]}\n",
    "\n",
    "\n",
    "def demo_solution_2():\n",
    "    \"\"\"Demo solution with accumulation\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SOLUTION 2: Accumulate into a shared list\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workflow = StateGraph(AccumulateState)\n",
    "    workflow.add_node(\"agent_a\", agent_a_accumulate)\n",
    "    workflow.add_node(\"agent_b\", agent_b_accumulate)\n",
    "    \n",
    "    # Parallel execution\n",
    "    workflow.add_edge(START, \"agent_a\")\n",
    "    workflow.add_edge(START, \"agent_b\")\n",
    "    workflow.add_edge(\"agent_a\", END)\n",
    "    workflow.add_edge(\"agent_b\", END)\n",
    "    \n",
    "    app = workflow.compile()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"data\": \"Some text to analyze for demonstration\",\n",
    "        \"results\": []\n",
    "    })\n",
    "    \n",
    "    print(f\"Accumulated results: {result['results']}\")\n",
    "    print(\"\u2705 Both results preserved in list!\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BEST PRACTICE: Clear field ownership\n",
    "# =============================================================================\n",
    "\n",
    "def show_ownership_pattern():\n",
    "    \"\"\"Show the ownership pattern we use\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BEST PRACTICE: Clear field ownership\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "In our research team (Section 16.5):\n",
    "\n",
    "    Agent          | Owns Field      | Reads From\n",
    "    ---------------|-----------------|------------------\n",
    "    Planner        | questions       | topic\n",
    "    Researcher     | findings        | questions\n",
    "    Analyst        | insights        | findings\n",
    "    Writer         | report          | insights, findings\n",
    "    Reviewer       | approved, feedback | report\n",
    "\n",
    "No conflicts because each agent writes to different fields!\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_problem()\n",
    "    demo_solution_1()\n",
    "    demo_solution_2()\n",
    "    show_ownership_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.6.1: State Audit\n",
    "\n",
    "Review the research team from Section 16.5. For each field in `ResearchState`:\n",
    "1. Which agent writes to it?\n",
    "2. Which agents read from it?\n",
    "3. Is it replaced or accumulated?\n",
    "\n",
    "Create a simple diagram showing the data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.6.2: Fix the Bug\n",
    "\n",
    "This state has a problem when used with parallel agents. Identify and fix it:\n",
    "\n",
    "```python\n",
    "class AnalysisState(TypedDict):\n",
    "    data: str\n",
    "    result: str  # Both analyzers write here!\n",
    "\n",
    "def technical_analyzer(state) -> dict:\n",
    "    return {\"result\": f\"Technical: {analyze(state['data'])}\"}\n",
    "\n",
    "def business_analyzer(state) -> dict:\n",
    "    return {\"result\": f\"Business: {analyze(state['data'])}\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.6.3: Design a State\n",
    "\n",
    "Design a state schema for a content moderation system with these agents:\n",
    "- **Toxicity Checker** - Scores content for toxic language\n",
    "- **Spam Detector** - Checks if content is spam\n",
    "- **PII Scanner** - Finds personal information\n",
    "- **Decision Maker** - Makes final allow/block decision\n",
    "\n",
    "Consider: What fields does each agent need? How should scores be stored? What does the decision maker need to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 16.7: Orchestration and coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: orchestrated_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16, Section 16.7\n",
    "# File: orchestrated_system.py\n",
    "\n",
    "\"\"\"\n",
    "Template for a well-orchestrated multi-agent system.\n",
    "\n",
    "Features:\n",
    "- Logging for observability\n",
    "- Error handling with retries\n",
    "- Fallback for graceful degradation\n",
    "- Clean state management\n",
    "- Clear routing logic\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"orchestrator\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE\n",
    "# =============================================================================\n",
    "\n",
    "class OrchestratedState(TypedDict):\n",
    "    task: str\n",
    "    stage: str\n",
    "    result: str\n",
    "    error: str\n",
    "    attempts: int\n",
    "    max_attempts: int\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AGENTS WITH ERROR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def worker_agent(state: OrchestratedState) -> dict:\n",
    "    \"\"\"Worker with built-in error handling.\"\"\"\n",
    "    logger.info(f\"Worker starting (attempt {state.get('attempts', 0) + 1})\")\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(f\"Complete this task briefly: {state['task']}\")\n",
    "        logger.info(\"Worker completed successfully\")\n",
    "        return {\n",
    "            \"result\": response.content,\n",
    "            \"stage\": \"complete\",\n",
    "            \"error\": \"\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Worker failed: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"attempts\": state.get(\"attempts\", 0) + 1\n",
    "        }\n",
    "\n",
    "\n",
    "def fallback_agent(state: OrchestratedState) -> dict:\n",
    "    \"\"\"Fallback when primary worker fails.\"\"\"\n",
    "    logger.warning(\"Using fallback agent\")\n",
    "    return {\n",
    "        \"result\": f\"Unable to fully complete: {state['task']}. Please try again.\",\n",
    "        \"stage\": \"fallback\"\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ROUTING\n",
    "# =============================================================================\n",
    "\n",
    "def check_result(state: OrchestratedState) -> Literal[\"done\", \"retry\", \"fallback\"]:\n",
    "    \"\"\"Decide next step based on result.\"\"\"\n",
    "    if state.get(\"result\") and not state.get(\"error\"):\n",
    "        return \"done\"\n",
    "    \n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "    max_attempts = state.get(\"max_attempts\", 3)\n",
    "    \n",
    "    if attempts >= max_attempts:\n",
    "        return \"fallback\"\n",
    "    \n",
    "    return \"retry\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD GRAPH\n",
    "# =============================================================================\n",
    "\n",
    "workflow = StateGraph(OrchestratedState)\n",
    "\n",
    "workflow.add_node(\"worker\", worker_agent)\n",
    "workflow.add_node(\"fallback\", fallback_agent)\n",
    "\n",
    "workflow.add_edge(START, \"worker\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"worker\",\n",
    "    check_result,\n",
    "    {\n",
    "        \"done\": END,\n",
    "        \"retry\": \"worker\",\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = app.invoke({\n",
    "        \"task\": \"Explain what an API is in one sentence\",\n",
    "        \"stage\": \"starting\",\n",
    "        \"result\": \"\",\n",
    "        \"error\": \"\",\n",
    "        \"attempts\": 0,\n",
    "        \"max_attempts\": 3\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nResult: {result['result']}\")\n",
    "    print(f\"Stage: {result['stage']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: content_creation_pipeline.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 16 Challenge Project\n",
    "# File: content_creation_pipeline.py\n",
    "\n",
    "\"\"\"\n",
    "Chapter 16 Challenge Project: Content Creation Pipeline\n",
    "\n",
    "Multi-agent system that produces blog posts with:\n",
    "- Topic Researcher: Gathers facts and background\n",
    "- Outline Creator: Structures content into sections\n",
    "- Draft Writer: Writes initial draft\n",
    "- Editor: Reviews and improves draft\n",
    "- SEO Optimizer: Adds keywords and improves searchability\n",
    "\n",
    "Features:\n",
    "- Pipeline flow: Research \u2192 Outline \u2192 Draft \u2192 Edit \u2192 SEO\n",
    "- Quality loop: Editor can send back to Writer (max 2 revisions)\n",
    "- Error handling with fallbacks\n",
    "- Metrics reporting\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"content_pipeline\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE\n",
    "# =============================================================================\n",
    "\n",
    "class ContentState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    outline: str\n",
    "    draft: str\n",
    "    edited_draft: str\n",
    "    final_content: str\n",
    "    feedback: str\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "    approved: bool\n",
    "    stage_times: Annotated[list[str], operator.add]  # Track time per stage\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER: Time tracker\n",
    "# =============================================================================\n",
    "\n",
    "def track_time(agent_name: str, start: datetime) -> str:\n",
    "    \"\"\"Create a time tracking entry.\"\"\"\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    return f\"{agent_name}: {duration:.2f}s\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AGENTS\n",
    "# =============================================================================\n",
    "\n",
    "def topic_researcher(state: ContentState) -> dict:\n",
    "    \"\"\"Gathers facts and background on the topic.\"\"\"\n",
    "    start = datetime.now()\n",
    "    logger.info(f\"[Researcher] Starting research on: {state['topic']}\")\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Research the topic: {state['topic']}\n",
    "\n",
    "Provide:\n",
    "1. Key facts (3-4 bullet points)\n",
    "2. Background context\n",
    "3. Current relevance/trends\n",
    "4. Target audience interests\n",
    "\n",
    "Keep it concise but informative.\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        logger.info(\"[Researcher] Research complete\")\n",
    "        return {\n",
    "            \"research\": response.content,\n",
    "            \"stage_times\": [track_time(\"Researcher\", start)]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Researcher] Error: {e}\")\n",
    "        return {\n",
    "            \"research\": f\"Basic research on {state['topic']}: A popular and relevant topic.\",\n",
    "            \"stage_times\": [track_time(\"Researcher (fallback)\", start)]\n",
    "        }\n",
    "\n",
    "\n",
    "def outline_creator(state: ContentState) -> dict:\n",
    "    \"\"\"Creates a structured outline for the content.\"\"\"\n",
    "    start = datetime.now()\n",
    "    logger.info(\"[Outline] Creating content structure\")\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Based on this research, create a blog post outline:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "RESEARCH:\n",
    "{state['research']}\n",
    "\n",
    "Create an outline with:\n",
    "- Catchy title\n",
    "- Introduction hook\n",
    "- 3-4 main sections with subpoints\n",
    "- Conclusion with call-to-action\n",
    "\n",
    "Format as a clear outline structure.\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        logger.info(\"[Outline] Structure created\")\n",
    "        return {\n",
    "            \"outline\": response.content,\n",
    "            \"stage_times\": [track_time(\"Outline Creator\", start)]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Outline] Error: {e}\")\n",
    "        return {\n",
    "            \"outline\": f\"I. Introduction\\nII. Main Points about {state['topic']}\\nIII. Conclusion\",\n",
    "            \"stage_times\": [track_time(\"Outline Creator (fallback)\", start)]\n",
    "        }\n",
    "\n",
    "\n",
    "def draft_writer(state: ContentState) -> dict:\n",
    "    \"\"\"Writes the initial draft based on outline.\"\"\"\n",
    "    start = datetime.now()\n",
    "    revision_note = f\" (revision {state['revision_count']})\" if state['revision_count'] > 0 else \"\"\n",
    "    logger.info(f\"[Writer] Writing draft{revision_note}\")\n",
    "    \n",
    "    try:\n",
    "        if state['revision_count'] > 0 and state['feedback']:\n",
    "            # Revision mode - incorporate feedback\n",
    "            prompt = f\"\"\"Revise this blog post based on the feedback:\n",
    "\n",
    "CURRENT DRAFT:\n",
    "{state['edited_draft'] or state['draft']}\n",
    "\n",
    "EDITOR FEEDBACK:\n",
    "{state['feedback']}\n",
    "\n",
    "Write an improved version that addresses all feedback points.\n",
    "Maintain the overall structure but improve quality.\"\"\"\n",
    "        else:\n",
    "            # Initial draft\n",
    "            prompt = f\"\"\"Write a blog post based on this outline:\n",
    "\n",
    "TOPIC: {state['topic']}\n",
    "\n",
    "OUTLINE:\n",
    "{state['outline']}\n",
    "\n",
    "RESEARCH:\n",
    "{state['research']}\n",
    "\n",
    "Write engaging, informative content:\n",
    "- Use conversational tone\n",
    "- Include specific facts from research\n",
    "- Make it 300-400 words\n",
    "- Use subheadings for sections\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        logger.info(\"[Writer] Draft complete\")\n",
    "        return {\n",
    "            \"draft\": response.content,\n",
    "            \"stage_times\": [track_time(f\"Writer{revision_note}\", start)]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Writer] Error: {e}\")\n",
    "        return {\n",
    "            \"draft\": f\"# {state['topic']}\\n\\nContent about {state['topic']}...\",\n",
    "            \"stage_times\": [track_time(\"Writer (fallback)\", start)]\n",
    "        }\n",
    "\n",
    "\n",
    "def editor(state: ContentState) -> dict:\n",
    "    \"\"\"Reviews and provides feedback on the draft.\"\"\"\n",
    "    start = datetime.now()\n",
    "    logger.info(f\"[Editor] Reviewing draft (revision {state['revision_count']})\")\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Review this blog post draft:\n",
    "\n",
    "{state['draft']}\n",
    "\n",
    "Evaluate:\n",
    "1. Content quality (facts, depth)\n",
    "2. Writing clarity\n",
    "3. Engagement level\n",
    "4. Structure and flow\n",
    "5. Length (should be 300+ words)\n",
    "\n",
    "If the draft is good (score 7+/10), respond with:\n",
    "APPROVED: Yes\n",
    "FEEDBACK: Brief praise\n",
    "\n",
    "If it needs work, respond with:\n",
    "APPROVED: No\n",
    "FEEDBACK: Specific improvements needed (bullet points)\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        result = response.content\n",
    "        \n",
    "        # Parse response\n",
    "        approved = \"APPROVED: Yes\" in result or \"approved: yes\" in result.lower()\n",
    "        \n",
    "        # Extract feedback\n",
    "        if \"FEEDBACK:\" in result:\n",
    "            feedback = result.split(\"FEEDBACK:\")[-1].strip()\n",
    "        else:\n",
    "            feedback = result\n",
    "        \n",
    "        logger.info(f\"[Editor] Review complete - Approved: {approved}\")\n",
    "        return {\n",
    "            \"edited_draft\": state['draft'],\n",
    "            \"feedback\": feedback,\n",
    "            \"approved\": approved,\n",
    "            \"revision_count\": state['revision_count'] + 1,\n",
    "            \"stage_times\": [track_time(\"Editor\", start)]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Editor] Error: {e}\")\n",
    "        return {\n",
    "            \"edited_draft\": state['draft'],\n",
    "            \"approved\": True,  # Approve on error to continue\n",
    "            \"feedback\": \"Auto-approved due to processing issue\",\n",
    "            \"revision_count\": state['revision_count'] + 1,\n",
    "            \"stage_times\": [track_time(\"Editor (fallback)\", start)]\n",
    "        }\n",
    "\n",
    "\n",
    "def seo_optimizer(state: ContentState) -> dict:\n",
    "    \"\"\"Adds SEO keywords and improves searchability.\"\"\"\n",
    "    start = datetime.now()\n",
    "    logger.info(\"[SEO] Optimizing for search\")\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"\"\"Optimize this blog post for SEO:\n",
    "\n",
    "{state['edited_draft'] or state['draft']}\n",
    "\n",
    "Add:\n",
    "1. SEO-friendly title (with main keyword)\n",
    "2. Meta description (150 chars)\n",
    "3. 3-5 relevant keywords naturally in text\n",
    "4. Internal/external link suggestions\n",
    "5. Alt text suggestions for potential images\n",
    "\n",
    "Return the optimized full article with SEO elements clearly marked.\"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        logger.info(\"[SEO] Optimization complete\")\n",
    "        return {\n",
    "            \"final_content\": response.content,\n",
    "            \"stage_times\": [track_time(\"SEO Optimizer\", start)]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[SEO] Error: {e}\")\n",
    "        return {\n",
    "            \"final_content\": state['edited_draft'] or state['draft'],\n",
    "            \"stage_times\": [track_time(\"SEO Optimizer (fallback)\", start)]\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ROUTING\n",
    "# =============================================================================\n",
    "\n",
    "def should_revise(state: ContentState) -> Literal[\"revise\", \"optimize\"]:\n",
    "    \"\"\"Decide if draft needs revision or can proceed to SEO.\"\"\"\n",
    "    if state['approved']:\n",
    "        logger.info(\"[Router] Draft approved -> SEO\")\n",
    "        return \"optimize\"\n",
    "    \n",
    "    if state['revision_count'] >= state['max_revisions']:\n",
    "        logger.info(f\"[Router] Max revisions ({state['max_revisions']}) reached -> SEO\")\n",
    "        return \"optimize\"\n",
    "    \n",
    "    logger.info(f\"[Router] Revision {state['revision_count']} requested -> Writer\")\n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD WORKFLOW\n",
    "# =============================================================================\n",
    "\n",
    "workflow = StateGraph(ContentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"researcher\", topic_researcher)\n",
    "workflow.add_node(\"outline\", outline_creator)\n",
    "workflow.add_node(\"writer\", draft_writer)\n",
    "workflow.add_node(\"editor\", editor)\n",
    "workflow.add_node(\"seo\", seo_optimizer)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"outline\")\n",
    "workflow.add_edge(\"outline\", \"writer\")\n",
    "workflow.add_edge(\"writer\", \"editor\")\n",
    "\n",
    "# Conditional edge for revision loop\n",
    "workflow.add_conditional_edges(\n",
    "    \"editor\",\n",
    "    should_revise,\n",
    "    {\n",
    "        \"revise\": \"writer\",\n",
    "        \"optimize\": \"seo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"seo\", END)\n",
    "\n",
    "content_pipeline = workflow.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# METRICS REPORT\n",
    "# =============================================================================\n",
    "\n",
    "def print_metrics(state: ContentState):\n",
    "    \"\"\"Print stage timing metrics.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PIPELINE METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_time = 0\n",
    "    for entry in state['stage_times']:\n",
    "        print(f\"  {entry}\")\n",
    "        # Extract time from entry like \"Writer: 1.23s\"\n",
    "        try:\n",
    "            time_str = entry.split(\":\")[-1].strip().replace(\"s\", \"\")\n",
    "            total_time += float(time_str)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Total: {total_time:.2f}s\")\n",
    "    print(f\"  Revisions: {state['revision_count']}\")\n",
    "    print(f\"  Approved: {state['approved']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONTENT CREATION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    topic = \"The Benefits of Learning Python in 2024\"\n",
    "    print(f\"\\nTopic: {topic}\\n\")\n",
    "    \n",
    "    result = content_pipeline.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"research\": \"\",\n",
    "        \"outline\": \"\",\n",
    "        \"draft\": \"\",\n",
    "        \"edited_draft\": \"\",\n",
    "        \"final_content\": \"\",\n",
    "        \"feedback\": \"\",\n",
    "        \"revision_count\": 0,\n",
    "        \"max_revisions\": 2,\n",
    "        \"approved\": False,\n",
    "        \"stage_times\": []\n",
    "    })\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(result)\n",
    "    \n",
    "    # Print final content\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL OPTIMIZED CONTENT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(result['final_content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 16.7 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.7.1: Add Monitoring\n",
    "\n",
    "Take the research team from Section 16.5 and add:\n",
    "- Logging for each agent (start/complete/duration)\n",
    "- A simple metrics report at the end\n",
    "- Error counting per agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.7.2: Implement Timeout\n",
    "\n",
    "Create a wrapper that:\n",
    "- Gives each agent a maximum time to complete\n",
    "- Returns a default response if timeout is exceeded\n",
    "- Logs timeout events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16.7.3: Build a Health Check\n",
    "\n",
    "Create a health check system that:\n",
    "- Tests each agent with a simple input\n",
    "- Reports which agents are working\n",
    "- Flags agents with high error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_16_multi_agent_solutions.ipynb**\n",
    "- Proceed to **Chapter 17**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}