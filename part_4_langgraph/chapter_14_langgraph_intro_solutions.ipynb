{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Introduction to LangGraph - Solutions\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "**Try the exercises in the main notebook first before viewing solutions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.1 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.1.1: Identify Chain Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_1_14_1_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.1.2: Flowchart Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_2_14_1_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.1.3: Analyze the Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_3_14_1_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.2.1: Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_1_14_2_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.2.2: Design a Recipe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_2_14_2_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.2.3: Identify the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_3_14_2_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.3 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.3.1: Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_1_14_3_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.3.2: API Key Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_2_14_3_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.3.3: Create a Setup Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_14_3_solution.py (Comprehensive Setup Checker)\n",
    "\n",
    "\"\"\"Comprehensive setup verification for LangGraph development.\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def print_header(title):\n",
    "    \"\"\"Print a formatted section header.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {title}\")\n",
    "    print('='*50)\n",
    "\n",
    "def check_python_version():\n",
    "    \"\"\"Check Python version is 3.9+.\"\"\"\n",
    "    version = sys.version_info\n",
    "    if version.major >= 3 and version.minor >= 9:\n",
    "        print(f\"‚úÖ Python {version.major}.{version.minor}.{version.micro}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå Python {version.major}.{version.minor} (need 3.9+)\")\n",
    "        return False\n",
    "\n",
    "def check_packages():\n",
    "    \"\"\"Check all required packages are installed.\"\"\"\n",
    "    packages = {\n",
    "        'langgraph': 'langgraph',\n",
    "        'langchain': 'langchain', \n",
    "        'langchain_openai': 'langchain-openai',\n",
    "        'dotenv': 'python-dotenv'\n",
    "    }\n",
    "    \n",
    "    all_good = True\n",
    "    for import_name, package_name in packages.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "            print(f\"‚úÖ {package_name}\")\n",
    "        except ImportError:\n",
    "            print(f\"‚ùå {package_name} - run: pip install {package_name}\")\n",
    "            all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "def check_api_key():\n",
    "    \"\"\"Check API key is configured.\"\"\"\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"‚ùå OPENAI_API_KEY not found\")\n",
    "        print(\"   Create a .env file with: OPENAI_API_KEY=sk-...\")\n",
    "        return False\n",
    "    \n",
    "    if not api_key.startswith(\"sk-\"):\n",
    "        print(\"‚ö†Ô∏è  API key format looks wrong (should start with 'sk-')\")\n",
    "        return False\n",
    "    \n",
    "    # Mask the key for display\n",
    "    masked = api_key[:7] + \"...\" + api_key[-4:]\n",
    "    print(f\"‚úÖ API key found ({masked})\")\n",
    "    return True\n",
    "\n",
    "def check_api_connection():\n",
    "    \"\"\"Test actual API connection.\"\"\"\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        \n",
    "        print(\"üîÑ Testing API connection...\")\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=10)\n",
    "        response = llm.invoke(\"Say 'OK'\")\n",
    "        print(f\"‚úÖ API connection working\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"authentication\" in error_msg.lower():\n",
    "            print(\"‚ùå API authentication failed - check your key\")\n",
    "        elif \"rate\" in error_msg.lower():\n",
    "            print(\"‚ùå Rate limited - wait a minute and try again\")\n",
    "        elif \"quota\" in error_msg.lower():\n",
    "            print(\"‚ùå No API quota - add credits to your OpenAI account\")\n",
    "        else:\n",
    "            print(f\"‚ùå API error: {error_msg[:100]}\")\n",
    "        return False\n",
    "\n",
    "def check_langgraph_imports():\n",
    "    \"\"\"Check LangGraph components are importable.\"\"\"\n",
    "    try:\n",
    "        from langgraph.graph import StateGraph, END\n",
    "        from langgraph.checkpoint.memory import MemorySaver\n",
    "        print(\"‚úÖ LangGraph components accessible\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå LangGraph import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all checks and report results.\"\"\"\n",
    "    print(\"\\nüîç LangGraph Setup Checker\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Check Python\n",
    "    print_header(\"Python Version\")\n",
    "    results['python'] = check_python_version()\n",
    "    \n",
    "    # Check packages\n",
    "    print_header(\"Required Packages\")\n",
    "    results['packages'] = check_packages()\n",
    "    \n",
    "    # Check LangGraph imports\n",
    "    print_header(\"LangGraph Components\")\n",
    "    results['langgraph'] = check_langgraph_imports()\n",
    "    \n",
    "    # Check API key\n",
    "    print_header(\"API Configuration\")\n",
    "    results['api_key'] = check_api_key()\n",
    "    \n",
    "    # Check API connection (only if key exists)\n",
    "    if results['api_key']:\n",
    "        print_header(\"API Connection Test\")\n",
    "        results['api_connection'] = check_api_connection()\n",
    "    else:\n",
    "        results['api_connection'] = False\n",
    "    \n",
    "    # Summary\n",
    "    print_header(\"Summary\")\n",
    "    \n",
    "    all_passed = all(results.values())\n",
    "    passed = sum(results.values())\n",
    "    total = len(results)\n",
    "    \n",
    "    for check, result in results.items():\n",
    "        status = \"‚úÖ\" if result else \"‚ùå\"\n",
    "        print(f\"  {status} {check.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\n  Result: {passed}/{total} checks passed\")\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\nüéâ All checks passed! You're ready to build with LangGraph!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some checks failed. Fix the issues above and run again.\")\n",
    "        print(\"   Need help? Check the troubleshooting section in 14.3\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    sys.exit(0 if success else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.4 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.4.1: Design a State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_1_14_4_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.4.2: Write the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution file not found: exercise_2_14_4_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.4.3: Draw the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_14_4_solution.py (Code Review Agent - combines exercises 1, 2, 3)\n",
    "\n",
    "\"\"\"\n",
    "Complete Code Review Agent demonstrating:\n",
    "- Exercise 1: State design with TypedDict\n",
    "- Exercise 2: Node implementations\n",
    "- Exercise 3: Graph construction with conditional edges\n",
    "\n",
    "This agent analyzes code, identifies issues, suggests fixes for each,\n",
    "and loops until all issues are addressed.\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXERCISE 1 SOLUTION: State Design\n",
    "# =============================================================================\n",
    "\n",
    "class CodeReviewState(TypedDict):\n",
    "    # Input\n",
    "    code: str                                    # The code to review\n",
    "    language: str                                # Programming language\n",
    "    \n",
    "    # Analysis results\n",
    "    issues: Annotated[list, add]                 # List of identified issues\n",
    "                                                 # Each issue: {\"id\": str, \"severity\": str, \n",
    "                                                 #              \"description\": str, \"line\": int}\n",
    "    \n",
    "    # Fix tracking  \n",
    "    suggested_fixes: Annotated[list, add]        # Fixes for issues\n",
    "                                                 # Each fix: {\"issue_id\": str, \"suggestion\": str,\n",
    "                                                 #            \"fixed_code\": str}\n",
    "    \n",
    "    addressed_issue_ids: Annotated[list, add]    # IDs of issues that have fixes\n",
    "    \n",
    "    # Control flow\n",
    "    current_issue_index: int                     # Which issue we're working on\n",
    "    review_complete: bool                        # Are we done?\n",
    "    \n",
    "    # Optional metadata\n",
    "    summary: Optional[str]                       # Final review summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXERCISE 2 SOLUTION: Node Implementations\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_code(state: CodeReviewState) -> dict:\n",
    "    \"\"\"Analyze the code and identify issues.\"\"\"\n",
    "    code = state[\"code\"]\n",
    "    language = state[\"language\"]\n",
    "    \n",
    "    # In reality, this would use an LLM or static analysis tool\n",
    "    # Pseudocode for the logic:\n",
    "    #\n",
    "    # prompt = f\"\"\"Analyze this {language} code for issues:\n",
    "    # {code}\n",
    "    # \n",
    "    # Return a list of issues with severity (high/medium/low),\n",
    "    # description, and line number.\"\"\"\n",
    "    # \n",
    "    # response = llm.invoke(prompt)\n",
    "    # issues = parse_issues(response)\n",
    "    \n",
    "    # For demo, pretend we found some issues:\n",
    "    issues = [\n",
    "        {\"id\": \"issue_1\", \"severity\": \"high\", \n",
    "         \"description\": \"Potential null pointer\", \"line\": 15},\n",
    "        {\"id\": \"issue_2\", \"severity\": \"medium\",\n",
    "         \"description\": \"Unused variable\", \"line\": 8},\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"issues\": issues,\n",
    "        \"current_issue_index\": 0,  # Start with first issue\n",
    "        \"review_complete\": False\n",
    "    }\n",
    "\n",
    "\n",
    "def suggest_fix(state: CodeReviewState) -> dict:\n",
    "    \"\"\"Suggest a fix for the current issue.\"\"\"\n",
    "    issues = state[\"issues\"]\n",
    "    current_index = state[\"current_issue_index\"]\n",
    "    code = state[\"code\"]\n",
    "    \n",
    "    # Get the current issue\n",
    "    current_issue = issues[current_index]\n",
    "    \n",
    "    # In reality, this would use an LLM:\n",
    "    # prompt = f\"\"\"Given this code:\n",
    "    # {code}\n",
    "    # \n",
    "    # Suggest a fix for this issue:\n",
    "    # {current_issue['description']} on line {current_issue['line']}\n",
    "    # \n",
    "    # Return the suggested fix and corrected code snippet.\"\"\"\n",
    "    #\n",
    "    # response = llm.invoke(prompt)\n",
    "    # fix = parse_fix(response)\n",
    "    \n",
    "    # For demo:\n",
    "    fix = {\n",
    "        \"issue_id\": current_issue[\"id\"],\n",
    "        \"suggestion\": f\"Fix for {current_issue['description']}\",\n",
    "        \"fixed_code\": \"# corrected code here\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"suggested_fixes\": [fix],\n",
    "        \"addressed_issue_ids\": [current_issue[\"id\"]],\n",
    "        \"current_issue_index\": current_index + 1  # Move to next issue\n",
    "    }\n",
    "\n",
    "\n",
    "def check_complete(state: CodeReviewState) -> dict:\n",
    "    \"\"\"Check if all issues have been addressed.\"\"\"\n",
    "    issues = state[\"issues\"]\n",
    "    addressed_ids = state[\"addressed_issue_ids\"]\n",
    "    \n",
    "    # Are all issues addressed?\n",
    "    all_issue_ids = {issue[\"id\"] for issue in issues}\n",
    "    addressed_set = set(addressed_ids)\n",
    "    \n",
    "    is_complete = all_issue_ids == addressed_set\n",
    "    \n",
    "    # If complete, generate summary\n",
    "    if is_complete:\n",
    "        summary = f\"Review complete. Found {len(issues)} issues, all addressed.\"\n",
    "    else:\n",
    "        summary = None  # Don't set summary until complete\n",
    "    \n",
    "    return {\n",
    "        \"review_complete\": is_complete,\n",
    "        \"summary\": summary if is_complete else state.get(\"summary\")\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXERCISE 3 SOLUTION: Graph Construction\n",
    "# =============================================================================\n",
    "\n",
    "def should_continue(state: CodeReviewState) -> str:\n",
    "    \"\"\"Decide whether to continue or finish.\"\"\"\n",
    "    if state[\"review_complete\"]:\n",
    "        return \"done\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "def build_code_review_graph():\n",
    "    \"\"\"Build and return the code review agent graph.\"\"\"\n",
    "    \n",
    "    # Create the graph with our state type\n",
    "    graph = StateGraph(CodeReviewState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    graph.add_node(\"analyze_code\", analyze_code)\n",
    "    graph.add_node(\"suggest_fix\", suggest_fix)\n",
    "    graph.add_node(\"check_complete\", check_complete)\n",
    "    \n",
    "    # Set the entry point\n",
    "    graph.set_entry_point(\"analyze_code\")\n",
    "    \n",
    "    # Add edges\n",
    "    graph.add_edge(\"analyze_code\", \"suggest_fix\")\n",
    "    graph.add_edge(\"suggest_fix\", \"check_complete\")\n",
    "    \n",
    "    # Conditional edge from check_complete\n",
    "    graph.add_conditional_edges(\n",
    "        \"check_complete\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"done\": END,\n",
    "            \"continue\": \"suggest_fix\"  # Loop back\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Compile and return\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main execution\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the graph\n",
    "    app = build_code_review_graph()\n",
    "    \n",
    "    # Example code to review\n",
    "    sample_code = \"\"\"\n",
    "def process_data(data):\n",
    "    unused_var = 42\n",
    "    result = data.get('value')\n",
    "    return result.upper()  # Potential None error\n",
    "\"\"\"\n",
    "    \n",
    "    # Run the review\n",
    "    result = app.invoke({\n",
    "        \"code\": sample_code,\n",
    "        \"language\": \"python\",\n",
    "        \"issues\": [],\n",
    "        \"suggested_fixes\": [],\n",
    "        \"addressed_issue_ids\": [],\n",
    "        \"current_issue_index\": 0,\n",
    "        \"review_complete\": False\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 50)\n",
    "    print(\"CODE REVIEW RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nüìã Issues Found: {len(result['issues'])}\")\n",
    "    for issue in result['issues']:\n",
    "        print(f\"   [{issue['severity'].upper()}] Line {issue['line']}: {issue['description']}\")\n",
    "    \n",
    "    print(f\"\\nüîß Fixes Suggested: {len(result['suggested_fixes'])}\")\n",
    "    for fix in result['suggested_fixes']:\n",
    "        print(f\"   Issue {fix['issue_id']}: {fix['suggestion']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {result['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.5 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.5.1: Add Draft History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_14_5_solution.py (Writer with Draft History)\n",
    "\n",
    "\"\"\"Self-improving writer that keeps history of all drafts.\n",
    "\n",
    "Exercise 1: Modify the writer to keep a history of all drafts using\n",
    "Annotated[list, add] so you can see how the writing evolved.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class WriterState(TypedDict):\n",
    "    topic: str\n",
    "    drafts: Annotated[list, add]     # History of all drafts\n",
    "    current_draft: str                # Most recent draft\n",
    "    critique: str\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "\n",
    "\n",
    "def write_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Write the initial draft.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    prompt = f\"\"\"Write a short, informative paragraph about: {topic}\n",
    "    Keep it concise but engaging. Aim for 3-4 sentences.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    draft = response.content\n",
    "    \n",
    "    print(f\"üìù Draft written ({len(draft)} chars)\")\n",
    "    \n",
    "    return {\n",
    "        \"current_draft\": draft,\n",
    "        \"drafts\": [{\"version\": 1, \"content\": draft}],  # Appends to list\n",
    "        \"revision_count\": 0\n",
    "    }\n",
    "\n",
    "\n",
    "def critique_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Critique the current draft.\"\"\"\n",
    "    draft = state[\"current_draft\"]\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    prompt = f\"\"\"Review this draft about \"{topic}\" and provide brief feedback.\n",
    "    \n",
    "    Draft:\n",
    "    {draft}\n",
    "    \n",
    "    If excellent, say \"EXCELLENT\" at the start. Otherwise give 2-3 suggestions.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"üîç Critique provided\")\n",
    "    \n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "\n",
    "def revise_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Revise based on feedback.\"\"\"\n",
    "    draft = state[\"current_draft\"]\n",
    "    critique = state[\"critique\"]\n",
    "    topic = state[\"topic\"]\n",
    "    revision_count = state[\"revision_count\"]\n",
    "    \n",
    "    prompt = f\"\"\"Revise this draft about \"{topic}\" based on feedback:\n",
    "    \n",
    "    Current draft: {draft}\n",
    "    Feedback: {critique}\n",
    "    \n",
    "    Write an improved version.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    new_draft = response.content\n",
    "    new_count = revision_count + 1\n",
    "    \n",
    "    print(f\"‚úèÔ∏è Revision {new_count} complete\")\n",
    "    \n",
    "    return {\n",
    "        \"current_draft\": new_draft,\n",
    "        \"drafts\": [{\"version\": new_count + 1, \"content\": new_draft}],  # Appends\n",
    "        \"revision_count\": new_count\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: WriterState) -> str:\n",
    "    \"\"\"Decide whether to continue revising.\"\"\"\n",
    "    if state[\"revision_count\"] >= state[\"max_revisions\"]:\n",
    "        return \"end\"\n",
    "    if \"EXCELLENT\" in state[\"critique\"].upper():\n",
    "        return \"end\"\n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "    graph = StateGraph(WriterState)\n",
    "    \n",
    "    graph.add_node(\"write_draft\", write_draft)\n",
    "    graph.add_node(\"critique\", critique_draft)\n",
    "    graph.add_node(\"revise\", revise_draft)\n",
    "    \n",
    "    graph.set_entry_point(\"write_draft\")\n",
    "    graph.add_edge(\"write_draft\", \"critique\")\n",
    "    graph.add_conditional_edges(\"critique\", should_continue, {\"revise\": \"revise\", \"end\": END})\n",
    "    graph.add_edge(\"revise\", \"critique\")\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def main():\n",
    "    app = create_graph()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"topic\": \"The benefits of reading books\",\n",
    "        \"drafts\": [],\n",
    "        \"current_draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"revision_count\": 0,\n",
    "        \"max_revisions\": 3\n",
    "    })\n",
    "    \n",
    "    # Display all versions\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìö DRAFT HISTORY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for draft in result[\"drafts\"]:\n",
    "        print(f\"\\n--- Version {draft['version']} ---\")\n",
    "        print(draft[\"content\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Total versions: {len(result['drafts'])}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.5.2: Quality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_14_5_solution.py (Writer with Quality Scoring)\n",
    "\n",
    "\"\"\"Self-improving writer with quality scoring.\n",
    "\n",
    "Exercise 2: Add a numeric quality score (1-10) to the process.\n",
    "The writer continues until the score reaches 8 or higher.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "class WriterState(TypedDict):\n",
    "    topic: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    quality_score: int               # Numeric quality score (1-10)\n",
    "    score_history: list              # Track scores over time\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "\n",
    "\n",
    "def write_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Write the initial draft.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    prompt = f\"\"\"Write a short, informative paragraph about: {topic}\n",
    "    Keep it concise but engaging. Aim for 3-4 sentences.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"üìù Draft written\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_count\": 0,\n",
    "        \"quality_score\": 0,\n",
    "        \"score_history\": []\n",
    "    }\n",
    "\n",
    "\n",
    "def critique_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Critique and score the draft.\"\"\"\n",
    "    draft = state[\"draft\"]\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    prompt = f\"\"\"Review this draft about \"{topic}\".\n",
    "    \n",
    "    Draft:\n",
    "    {draft}\n",
    "    \n",
    "    Provide:\n",
    "    1. A quality score from 1-10 (format: \"SCORE: X\")\n",
    "    2. Brief feedback for improvement\n",
    "    \n",
    "    Be a tough but fair critic. Only give 9-10 for truly excellent writing.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    critique_text = response.content\n",
    "    \n",
    "    # Extract score from response\n",
    "    score_match = re.search(r'SCORE:\\s*(\\d+)', critique_text, re.IGNORECASE)\n",
    "    score = int(score_match.group(1)) if score_match else 5\n",
    "    score = max(1, min(10, score))  # Clamp to 1-10\n",
    "    \n",
    "    # Update score history\n",
    "    new_history = state.get(\"score_history\", []) + [score]\n",
    "    \n",
    "    print(f\"üîç Critique: Score {score}/10\")\n",
    "    \n",
    "    return {\n",
    "        \"critique\": critique_text,\n",
    "        \"quality_score\": score,\n",
    "        \"score_history\": new_history\n",
    "    }\n",
    "\n",
    "\n",
    "def revise_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Revise based on feedback.\"\"\"\n",
    "    draft = state[\"draft\"]\n",
    "    critique = state[\"critique\"]\n",
    "    topic = state[\"topic\"]\n",
    "    revision_count = state[\"revision_count\"]\n",
    "    \n",
    "    prompt = f\"\"\"Revise this draft about \"{topic}\" based on feedback:\n",
    "    \n",
    "    Current draft: {draft}\n",
    "    Feedback: {critique}\n",
    "    \n",
    "    Write an improved version that addresses the feedback.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    new_count = revision_count + 1\n",
    "    print(f\"‚úèÔ∏è Revision {new_count} complete\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_count\": new_count\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: WriterState) -> str:\n",
    "    \"\"\"Decide based on score and revision count.\"\"\"\n",
    "    score = state[\"quality_score\"]\n",
    "    revision_count = state[\"revision_count\"]\n",
    "    max_revisions = state[\"max_revisions\"]\n",
    "    \n",
    "    # Stop if score is 8 or higher\n",
    "    if score >= 8:\n",
    "        print(f\"‚ú® Quality score {score}/10 - excellent!\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # Stop if max revisions reached\n",
    "    if revision_count >= max_revisions:\n",
    "        print(f\"üõë Max revisions reached (score: {score}/10)\")\n",
    "        return \"end\"\n",
    "    \n",
    "    print(f\"üîÑ Score {score}/10 - continuing...\")\n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "    graph = StateGraph(WriterState)\n",
    "    \n",
    "    graph.add_node(\"write_draft\", write_draft)\n",
    "    graph.add_node(\"critique\", critique_draft)\n",
    "    graph.add_node(\"revise\", revise_draft)\n",
    "    \n",
    "    graph.set_entry_point(\"write_draft\")\n",
    "    graph.add_edge(\"write_draft\", \"critique\")\n",
    "    graph.add_conditional_edges(\"critique\", should_continue, {\"revise\": \"revise\", \"end\": END})\n",
    "    graph.add_edge(\"revise\", \"critique\")\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def main():\n",
    "    app = create_graph()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"topic\": \"The importance of sleep for health\",\n",
    "        \"draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"quality_score\": 0,\n",
    "        \"score_history\": [],\n",
    "        \"revision_count\": 0,\n",
    "        \"max_revisions\": 3\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìÑ FINAL DRAFT:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result[\"draft\"])\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"üìä Score progression: {' ‚Üí '.join(map(str, result['score_history']))}\")\n",
    "    print(f\"üìä Final score: {result['quality_score']}/10\")\n",
    "    print(f\"üìä Total revisions: {result['revision_count']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.5.3: Different Writing Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_14_5_solution.py (Writer with Style Options)\n",
    "\n",
    "\"\"\"Self-improving writer with style options.\n",
    "\n",
    "Exercise 3: Add a style parameter (formal/casual/creative) that changes\n",
    "how the writer creates and evaluates content.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "# Style definitions\n",
    "STYLES = {\n",
    "    \"formal\": {\n",
    "        \"description\": \"professional, business-like, using industry terminology\",\n",
    "        \"tone\": \"authoritative and polished\",\n",
    "        \"example\": \"formal business report\"\n",
    "    },\n",
    "    \"casual\": {\n",
    "        \"description\": \"friendly, conversational, approachable\",\n",
    "        \"tone\": \"warm and relatable, like talking to a friend\",\n",
    "        \"example\": \"blog post or social media\"\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"description\": \"artistic, expressive, using vivid imagery\",\n",
    "        \"tone\": \"imaginative and evocative\",\n",
    "        \"example\": \"creative essay or storytelling\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class WriterState(TypedDict):\n",
    "    topic: str\n",
    "    style: str                       # Writing style (formal/casual/creative)\n",
    "    draft: str\n",
    "    critique: str\n",
    "    revision_count: int\n",
    "    max_revisions: int\n",
    "\n",
    "\n",
    "def get_style_prompt(style: str) -> str:\n",
    "    \"\"\"Get style instructions for prompts.\"\"\"\n",
    "    style_info = STYLES.get(style, STYLES[\"casual\"])\n",
    "    return f\"\"\"Style: {style_info['description']}\n",
    "Tone: {style_info['tone']}\n",
    "Write as if for: {style_info['example']}\"\"\"\n",
    "\n",
    "\n",
    "def write_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Write initial draft in the specified style.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    style = state[\"style\"]\n",
    "    style_instructions = get_style_prompt(style)\n",
    "    \n",
    "    prompt = f\"\"\"Write a short paragraph about: {topic}\n",
    "\n",
    "{style_instructions}\n",
    "\n",
    "Keep it to 3-4 sentences while maintaining the style throughout.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"üìù Draft written in {style} style\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_count\": 0\n",
    "    }\n",
    "\n",
    "\n",
    "def critique_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Critique with style considerations.\"\"\"\n",
    "    draft = state[\"draft\"]\n",
    "    topic = state[\"topic\"]\n",
    "    style = state[\"style\"]\n",
    "    style_info = STYLES.get(style, STYLES[\"casual\"])\n",
    "    \n",
    "    prompt = f\"\"\"Review this {style} draft about \"{topic}\".\n",
    "    \n",
    "    Draft:\n",
    "    {draft}\n",
    "    \n",
    "    The intended style is: {style_info['description']}\n",
    "    The intended tone is: {style_info['tone']}\n",
    "    \n",
    "    Evaluate:\n",
    "    1. Does it match the intended style and tone?\n",
    "    2. Is the content accurate and engaging?\n",
    "    3. What specific improvements would make it better?\n",
    "    \n",
    "    If it's excellent for the style, say \"EXCELLENT\" at the start.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(f\"üîç Critique for {style} style provided\")\n",
    "    \n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "\n",
    "def revise_draft(state: WriterState) -> dict:\n",
    "    \"\"\"Revise while maintaining style.\"\"\"\n",
    "    draft = state[\"draft\"]\n",
    "    critique = state[\"critique\"]\n",
    "    topic = state[\"topic\"]\n",
    "    style = state[\"style\"]\n",
    "    style_instructions = get_style_prompt(style)\n",
    "    revision_count = state[\"revision_count\"]\n",
    "    \n",
    "    prompt = f\"\"\"Revise this draft about \"{topic}\" based on feedback.\n",
    "\n",
    "{style_instructions}\n",
    "\n",
    "Current draft:\n",
    "{draft}\n",
    "\n",
    "Feedback:\n",
    "{critique}\n",
    "\n",
    "Write an improved version that addresses the feedback while maintaining the {style} style.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    new_count = revision_count + 1\n",
    "    print(f\"‚úèÔ∏è Revision {new_count} complete\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_count\": new_count\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: WriterState) -> str:\n",
    "    \"\"\"Decide whether to continue revising.\"\"\"\n",
    "    if state[\"revision_count\"] >= state[\"max_revisions\"]:\n",
    "        return \"end\"\n",
    "    if \"EXCELLENT\" in state[\"critique\"].upper():\n",
    "        return \"end\"\n",
    "    return \"revise\"\n",
    "\n",
    "\n",
    "def create_graph():\n",
    "    graph = StateGraph(WriterState)\n",
    "    \n",
    "    graph.add_node(\"write_draft\", write_draft)\n",
    "    graph.add_node(\"critique\", critique_draft)\n",
    "    graph.add_node(\"revise\", revise_draft)\n",
    "    \n",
    "    graph.set_entry_point(\"write_draft\")\n",
    "    graph.add_edge(\"write_draft\", \"critique\")\n",
    "    graph.add_conditional_edges(\"critique\", should_continue, {\"revise\": \"revise\", \"end\": END})\n",
    "    graph.add_edge(\"revise\", \"critique\")\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üé® Styled Self-Improving Writer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get topic\n",
    "    topic = input(\"\\nüìù Topic to write about:\\n> \").strip()\n",
    "    if not topic:\n",
    "        topic = \"The value of continuous learning\"\n",
    "    \n",
    "    # Get style\n",
    "    print(\"\\nüé® Available styles:\")\n",
    "    for style, info in STYLES.items():\n",
    "        print(f\"  - {style}: {info['description']}\")\n",
    "    \n",
    "    style = input(\"\\nChoose style (formal/casual/creative):\\n> \").strip().lower()\n",
    "    if style not in STYLES:\n",
    "        style = \"casual\"\n",
    "        print(f\"(Using default: {style})\")\n",
    "    \n",
    "    # Run for chosen style\n",
    "    app = create_graph()\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"style\": style,\n",
    "        \"draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"revision_count\": 0,\n",
    "        \"max_revisions\": 2\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"üìÑ FINAL DRAFT ({style.upper()} STYLE):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result[\"draft\"])\n",
    "    \n",
    "    # Bonus: Compare all three styles\n",
    "    compare = input(\"\\n\\nCompare all three styles on same topic? (y/n): \").strip().lower()\n",
    "    if compare == 'y':\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üé® STYLE COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for style_name in STYLES:\n",
    "            print(f\"\\n--- {style_name.upper()} ---\")\n",
    "            result = app.invoke({\n",
    "                \"topic\": topic,\n",
    "                \"style\": style_name,\n",
    "                \"draft\": \"\",\n",
    "                \"critique\": \"\",\n",
    "                \"revision_count\": 0,\n",
    "                \"max_revisions\": 1  # Just 1 revision for speed\n",
    "            })\n",
    "            print(result[\"draft\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.6 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.6.1: Email Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_14_6_solution.py\n",
    "\n",
    "\"\"\"Email classifier with multi-way routing.\n",
    "\n",
    "Exercise 1 Solution: Build a graph that classifies incoming emails\n",
    "and routes them to specialized handlers (5 categories).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "# === STATE ===\n",
    "\n",
    "class EmailState(TypedDict):\n",
    "    email_subject: str        # Subject line\n",
    "    email_body: str           # Full email content\n",
    "    sender: str               # Who sent it\n",
    "    category: str             # Classification result\n",
    "    action_taken: str         # What we did with it\n",
    "    extracted_info: dict      # Any info we pulled out\n",
    "\n",
    "\n",
    "# === CLASSIFICATION NODE ===\n",
    "\n",
    "def classify_email(state: EmailState) -> dict:\n",
    "    \"\"\"Classify the email into one of 5 categories.\n",
    "    \n",
    "    Uses the subject, body, and sender to determine the type.\n",
    "    \"\"\"\n",
    "    subject = state[\"email_subject\"]\n",
    "    body = state[\"email_body\"]\n",
    "    sender = state[\"sender\"]\n",
    "    \n",
    "    prompt = f\"\"\"Classify this email into exactly ONE category:\n",
    "\n",
    "    From: {sender}\n",
    "    Subject: {subject}\n",
    "    Body: {body}\n",
    "\n",
    "    Categories:\n",
    "    - URGENT: Time-sensitive, needs immediate response, emergencies\n",
    "    - MEETING: Calendar invites, scheduling, meeting requests\n",
    "    - NEWSLETTER: Marketing, promotions, subscriptions\n",
    "    - PERSONAL: From friends, family, personal contacts\n",
    "    - SPAM: Unwanted, suspicious, phishing attempts\n",
    "\n",
    "    Respond with only the category name.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    category = response.content.strip().upper()\n",
    "    \n",
    "    # Validate - default to NEWSLETTER if unrecognized\n",
    "    valid = [\"URGENT\", \"MEETING\", \"NEWSLETTER\", \"PERSONAL\", \"SPAM\"]\n",
    "    if category not in valid:\n",
    "        category = \"NEWSLETTER\"\n",
    "    \n",
    "    print(f\"üìß Classified as: {category}\")\n",
    "    return {\"category\": category}\n",
    "\n",
    "\n",
    "# === ROUTING FUNCTION ===\n",
    "\n",
    "def route_email(state: EmailState) -> str:\n",
    "    \"\"\"Route to the appropriate handler based on category.\"\"\"\n",
    "    category = state[\"category\"]\n",
    "    return f\"handle_{category.lower()}\"\n",
    "\n",
    "\n",
    "# === HANDLER NODES ===\n",
    "\n",
    "def handle_urgent(state: EmailState) -> dict:\n",
    "    \"\"\"Generate quick acknowledgment for urgent emails.\"\"\"\n",
    "    prompt = f\"\"\"Write a brief acknowledgment for this urgent email.\n",
    "    From: {state['sender']}\n",
    "    Subject: {state['email_subject']}\n",
    "    \n",
    "    Acknowledge receipt and promise quick response (2-3 sentences).\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"üö® Urgent: Acknowledgment generated\")\n",
    "    \n",
    "    return {\n",
    "        \"action_taken\": \"ACKNOWLEDGED\",\n",
    "        \"extracted_info\": {\"response_draft\": response.content}\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_meeting(state: EmailState) -> dict:\n",
    "    \"\"\"Extract meeting details from the email.\"\"\"\n",
    "    prompt = f\"\"\"Extract meeting information from this email:\n",
    "    Subject: {state['email_subject']}\n",
    "    Body: {state['email_body']}\n",
    "    \n",
    "    Extract: date/time, participants, location/link, purpose.\n",
    "    Format as a brief summary.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"üìÖ Meeting: Details extracted\")\n",
    "    \n",
    "    return {\n",
    "        \"action_taken\": \"MEETING_EXTRACTED\",\n",
    "        \"extracted_info\": {\"meeting_details\": response.content}\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_newsletter(state: EmailState) -> dict:\n",
    "    \"\"\"Archive newsletter emails.\"\"\"\n",
    "    print(\"üì∞ Newsletter: Archived\")\n",
    "    return {\n",
    "        \"action_taken\": \"ARCHIVED\",\n",
    "        \"extracted_info\": {\"folder\": \"Newsletters\", \"source\": state[\"sender\"]}\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_personal(state: EmailState) -> dict:\n",
    "    \"\"\"Flag personal emails for review.\"\"\"\n",
    "    print(\"üë§ Personal: Flagged for review\")\n",
    "    return {\n",
    "        \"action_taken\": \"FLAGGED_PERSONAL\",\n",
    "        \"extracted_info\": {\"flag\": \"Needs your attention\"}\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_spam(state: EmailState) -> dict:\n",
    "    \"\"\"Delete spam emails.\"\"\"\n",
    "    print(\"üóëÔ∏è Spam: Deleted\")\n",
    "    return {\n",
    "        \"action_taken\": \"DELETED\",\n",
    "        \"extracted_info\": {\"blocked_sender\": state[\"sender\"]}\n",
    "    }\n",
    "\n",
    "\n",
    "# === GRAPH BUILDER ===\n",
    "\n",
    "def create_email_graph():\n",
    "    \"\"\"Build the email classifier graph with 5-way routing.\"\"\"\n",
    "    graph = StateGraph(EmailState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"classify\", classify_email)\n",
    "    graph.add_node(\"handle_urgent\", handle_urgent)\n",
    "    graph.add_node(\"handle_meeting\", handle_meeting)\n",
    "    graph.add_node(\"handle_newsletter\", handle_newsletter)\n",
    "    graph.add_node(\"handle_personal\", handle_personal)\n",
    "    graph.add_node(\"handle_spam\", handle_spam)\n",
    "    \n",
    "    # Entry and routing\n",
    "    graph.set_entry_point(\"classify\")\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"classify\",\n",
    "        route_email,\n",
    "        {\n",
    "            \"handle_urgent\": \"handle_urgent\",\n",
    "            \"handle_meeting\": \"handle_meeting\",\n",
    "            \"handle_newsletter\": \"handle_newsletter\",\n",
    "            \"handle_personal\": \"handle_personal\",\n",
    "            \"handle_spam\": \"handle_spam\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All handlers end\n",
    "    for handler in [\"handle_urgent\", \"handle_meeting\", \"handle_newsletter\", \n",
    "                    \"handle_personal\", \"handle_spam\"]:\n",
    "        graph.add_edge(handler, END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "def main():\n",
    "    app = create_email_graph()\n",
    "    \n",
    "    test_emails = [\n",
    "        {\"sender\": \"boss@company.com\", \"subject\": \"URGENT: Server down!\", \n",
    "         \"body\": \"Production crashed. Need help immediately!\"},\n",
    "        {\"sender\": \"calendar@company.com\", \"subject\": \"Meeting: Q4 Planning\",\n",
    "         \"body\": \"Friday at 2pm in Conference Room A.\"},\n",
    "        {\"sender\": \"deals@store.com\", \"subject\": \"50% OFF Today Only!\",\n",
    "         \"body\": \"Our biggest sale of the year!\"},\n",
    "        {\"sender\": \"mom@email.com\", \"subject\": \"Sunday dinner?\",\n",
    "         \"body\": \"Are you coming for dinner? Love, Mom\"},\n",
    "        {\"sender\": \"prince@scam.com\", \"subject\": \"You won $1,000,000!\",\n",
    "         \"body\": \"Send your bank details to claim...\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üì¨ Email Classifier\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for email in test_emails:\n",
    "        print(f\"\\nüì© From: {email['sender']}\")\n",
    "        print(f\"   Subject: {email['subject']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        result = app.invoke({\n",
    "            \"email_subject\": email[\"subject\"],\n",
    "            \"email_body\": email[\"body\"],\n",
    "            \"sender\": email[\"sender\"],\n",
    "            \"category\": \"\",\n",
    "            \"action_taken\": \"\",\n",
    "            \"extracted_info\": {}\n",
    "        })\n",
    "        \n",
    "        print(f\"   Action: {result['action_taken']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.6.2: Multi-Stage Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_14_6_solution.py\n",
    "\n",
    "\"\"\"Multi-stage interview bot with role-based branching.\n",
    "\n",
    "Exercise 2 Solution: Create an interview bot with three stages:\n",
    "- Stage 1: Basic info (name, background)\n",
    "- Stage 2: Technical questions (different paths for engineer vs designer)\n",
    "- Stage 3: Behavioral questions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "# === STATE ===\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    candidate_name: str\n",
    "    role: str                            # \"engineer\" or \"designer\"\n",
    "    current_stage: int                   # 1, 2, or 3\n",
    "    questions_asked: Annotated[list, add]\n",
    "    responses: Annotated[list, add]\n",
    "    stage_complete: bool\n",
    "    interview_summary: str\n",
    "\n",
    "\n",
    "# === STAGE 1: BASIC INFO ===\n",
    "\n",
    "def stage1_basic_info(state: InterviewState) -> dict:\n",
    "    \"\"\"Stage 1: Gather basic information about the candidate.\"\"\"\n",
    "    name = state.get(\"candidate_name\", \"\")\n",
    "    \n",
    "    if not name:\n",
    "        question = \"Welcome! What's your name?\"\n",
    "        # Simulated response (real app would get user input)\n",
    "        return {\n",
    "            \"questions_asked\": [question],\n",
    "            \"responses\": [\"Alex\"],\n",
    "            \"candidate_name\": \"Alex\",\n",
    "            \"stage_complete\": False\n",
    "        }\n",
    "    else:\n",
    "        question = f\"Hi {name}! Tell me about your background.\"\n",
    "        return {\n",
    "            \"questions_asked\": [question],\n",
    "            \"responses\": [\"5 years experience...\"],\n",
    "            \"stage_complete\": True\n",
    "        }\n",
    "\n",
    "\n",
    "def check_stage1_complete(state: InterviewState) -> str:\n",
    "    \"\"\"Decide whether to continue stage 1 or advance.\"\"\"\n",
    "    if state[\"stage_complete\"] and state.get(\"candidate_name\"):\n",
    "        return \"advance_to_stage2\"\n",
    "    return \"continue_stage1\"\n",
    "\n",
    "\n",
    "# === STAGE 2: ROLE-BASED TECHNICAL QUESTIONS ===\n",
    "\n",
    "def advance_to_stage2(state: InterviewState) -> dict:\n",
    "    \"\"\"Transition to technical questions.\"\"\"\n",
    "    print(\"üìà Advancing to Stage 2: Technical Questions\")\n",
    "    return {\"current_stage\": 2, \"stage_complete\": False}\n",
    "\n",
    "\n",
    "def route_by_role(state: InterviewState) -> str:\n",
    "    \"\"\"Route to role-specific technical questions.\"\"\"\n",
    "    role = state.get(\"role\", \"engineer\").lower()\n",
    "    if \"design\" in role:\n",
    "        return \"stage2_designer\"\n",
    "    return \"stage2_engineer\"\n",
    "\n",
    "\n",
    "def stage2_engineer(state: InterviewState) -> dict:\n",
    "    \"\"\"Technical questions for engineering candidates.\"\"\"\n",
    "    questions = [\n",
    "        \"Describe a challenging technical problem you solved.\",\n",
    "        \"What's your experience with system design?\",\n",
    "        \"How do you approach debugging?\"\n",
    "    ]\n",
    "    \n",
    "    asked_count = len([q for q in state[\"questions_asked\"] \n",
    "                       if \"technical\" in q.lower() or \"system\" in q.lower()])\n",
    "    \n",
    "    if asked_count < len(questions):\n",
    "        q = questions[asked_count]\n",
    "        print(f\"üîß Engineer Q: {q[:40]}...\")\n",
    "        return {\n",
    "            \"questions_asked\": [q],\n",
    "            \"responses\": [f\"[Response to: {q[:20]}...]\"],\n",
    "            \"stage_complete\": asked_count >= len(questions) - 1\n",
    "        }\n",
    "    return {\"stage_complete\": True}\n",
    "\n",
    "\n",
    "def stage2_designer(state: InterviewState) -> dict:\n",
    "    \"\"\"Technical questions for design candidates.\"\"\"\n",
    "    questions = [\n",
    "        \"Walk me through your design process.\",\n",
    "        \"How do you incorporate user feedback?\",\n",
    "        \"What prototyping tools do you use?\"\n",
    "    ]\n",
    "    \n",
    "    asked_count = len([q for q in state[\"questions_asked\"] \n",
    "                       if \"design\" in q.lower() or \"user\" in q.lower()])\n",
    "    \n",
    "    if asked_count < len(questions):\n",
    "        q = questions[asked_count]\n",
    "        print(f\"üé® Designer Q: {q[:40]}...\")\n",
    "        return {\n",
    "            \"questions_asked\": [q],\n",
    "            \"responses\": [f\"[Response to: {q[:20]}...]\"],\n",
    "            \"stage_complete\": asked_count >= len(questions) - 1\n",
    "        }\n",
    "    return {\"stage_complete\": True}\n",
    "\n",
    "\n",
    "# === STAGE 3: BEHAVIORAL QUESTIONS ===\n",
    "\n",
    "def advance_to_stage3(state: InterviewState) -> dict:\n",
    "    \"\"\"Transition to behavioral questions.\"\"\"\n",
    "    print(\"üìà Advancing to Stage 3: Behavioral Questions\")\n",
    "    return {\"current_stage\": 3, \"stage_complete\": False}\n",
    "\n",
    "\n",
    "def stage3_behavioral(state: InterviewState) -> dict:\n",
    "    \"\"\"Behavioral questions for all candidates.\"\"\"\n",
    "    questions = [\n",
    "        \"Tell me about a time you worked with a difficult team member.\",\n",
    "        \"Describe meeting a tight deadline.\",\n",
    "        \"What motivates you?\"\n",
    "    ]\n",
    "    \n",
    "    asked_count = len([q for q in state[\"questions_asked\"] \n",
    "                       if \"tell me\" in q.lower() or \"describe\" in q.lower()])\n",
    "    \n",
    "    if asked_count < len(questions):\n",
    "        q = questions[asked_count]\n",
    "        print(f\"üí≠ Behavioral Q: {q[:40]}...\")\n",
    "        return {\n",
    "            \"questions_asked\": [q],\n",
    "            \"responses\": [f\"[Response to: {q[:20]}...]\"],\n",
    "            \"stage_complete\": asked_count >= len(questions) - 1\n",
    "        }\n",
    "    return {\"stage_complete\": True}\n",
    "\n",
    "\n",
    "# === SUMMARY ===\n",
    "\n",
    "def generate_summary(state: InterviewState) -> dict:\n",
    "    \"\"\"Generate final interview summary.\"\"\"\n",
    "    summary = f\"\"\"\n",
    "Interview Summary for {state['candidate_name']}\n",
    "Role: {state['role']}\n",
    "Questions Asked: {len(state['questions_asked'])}\n",
    "Stages Completed: 3/3\n",
    "\"\"\"\n",
    "    print(\"üìã Interview complete!\")\n",
    "    return {\"interview_summary\": summary.strip()}\n",
    "\n",
    "\n",
    "# === GRAPH BUILDER ===\n",
    "\n",
    "def create_interview_graph():\n",
    "    graph = StateGraph(InterviewState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    graph.add_node(\"stage1\", stage1_basic_info)\n",
    "    graph.add_node(\"advance_to_stage2\", advance_to_stage2)\n",
    "    graph.add_node(\"stage2_engineer\", stage2_engineer)\n",
    "    graph.add_node(\"stage2_designer\", stage2_designer)\n",
    "    graph.add_node(\"advance_to_stage3\", advance_to_stage3)\n",
    "    graph.add_node(\"stage3\", stage3_behavioral)\n",
    "    graph.add_node(\"summary\", generate_summary)\n",
    "    \n",
    "    graph.set_entry_point(\"stage1\")\n",
    "    \n",
    "    # Stage 1 loop or advance\n",
    "    graph.add_conditional_edges(\"stage1\", check_stage1_complete, {\n",
    "        \"continue_stage1\": \"stage1\",\n",
    "        \"advance_to_stage2\": \"advance_to_stage2\"\n",
    "    })\n",
    "    \n",
    "    # Stage 2 role branching\n",
    "    graph.add_conditional_edges(\"advance_to_stage2\", route_by_role, {\n",
    "        \"stage2_engineer\": \"stage2_engineer\",\n",
    "        \"stage2_designer\": \"stage2_designer\"\n",
    "    })\n",
    "    \n",
    "    # Stage 2 loops\n",
    "    def check_stage2(state):\n",
    "        return \"advance_to_stage3\" if state[\"stage_complete\"] else \"continue\"\n",
    "    \n",
    "    graph.add_conditional_edges(\"stage2_engineer\", check_stage2, {\n",
    "        \"continue\": \"stage2_engineer\",\n",
    "        \"advance_to_stage3\": \"advance_to_stage3\"\n",
    "    })\n",
    "    graph.add_conditional_edges(\"stage2_designer\", check_stage2, {\n",
    "        \"continue\": \"stage2_designer\", \n",
    "        \"advance_to_stage3\": \"advance_to_stage3\"\n",
    "    })\n",
    "    \n",
    "    # Stage 3\n",
    "    graph.add_edge(\"advance_to_stage3\", \"stage3\")\n",
    "    \n",
    "    def check_stage3(state):\n",
    "        return \"summary\" if state[\"stage_complete\"] else \"continue\"\n",
    "    \n",
    "    graph.add_conditional_edges(\"stage3\", check_stage3, {\n",
    "        \"continue\": \"stage3\",\n",
    "        \"summary\": \"summary\"\n",
    "    })\n",
    "    \n",
    "    graph.add_edge(\"summary\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "def main():\n",
    "    app = create_interview_graph()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üé§ Multi-Stage Interview Bot\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test with engineer\n",
    "    print(\"\\n--- Engineering Candidate ---\")\n",
    "    result = app.invoke({\n",
    "        \"candidate_name\": \"\",\n",
    "        \"role\": \"engineer\",\n",
    "        \"current_stage\": 1,\n",
    "        \"questions_asked\": [],\n",
    "        \"responses\": [],\n",
    "        \"stage_complete\": False,\n",
    "        \"interview_summary\": \"\"\n",
    "    })\n",
    "    print(result[\"interview_summary\"])\n",
    "    \n",
    "    # Test with designer\n",
    "    print(\"\\n--- Design Candidate ---\")\n",
    "    result = app.invoke({\n",
    "        \"candidate_name\": \"\",\n",
    "        \"role\": \"designer\",\n",
    "        \"current_stage\": 1,\n",
    "        \"questions_asked\": [],\n",
    "        \"responses\": [],\n",
    "        \"stage_complete\": False,\n",
    "        \"interview_summary\": \"\"\n",
    "    })\n",
    "    print(result[\"interview_summary\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.6.3: Retry with Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_14_6_solution.py\n",
    "\n",
    "\"\"\"Research assistant with retry logic for low-quality results.\n",
    "\n",
    "Exercise 3 Solution: Enhance a research assistant to handle poor-quality results:\n",
    "- If search quality is LOW, retry with a modified query\n",
    "- Track retries per search (max 2 retries)\n",
    "- If still low after retries, move on to next search\n",
    "\n",
    "Key insight: retry creates a mini-loop within the larger search loop.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "# === STATE ===\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    question: str\n",
    "    search_queries: Annotated[list, add]\n",
    "    findings: Annotated[list, add]\n",
    "    current_query: str            # The active query\n",
    "    current_quality: str          # HIGH, MEDIUM, LOW\n",
    "    retry_count: int              # Retries for THIS search\n",
    "    max_retries: int              # Limit per search (e.g., 2)\n",
    "    search_count: int             # Total searches done\n",
    "    max_searches: int             # Overall limit\n",
    "    has_enough_info: bool         # Do we have enough?\n",
    "    final_answer: str\n",
    "\n",
    "\n",
    "# === NODES ===\n",
    "\n",
    "def generate_search_query(state: ResearchState) -> dict:\n",
    "    \"\"\"Generate a search query based on the question and existing findings.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    findings = state.get(\"findings\", [])\n",
    "    \n",
    "    if not findings:\n",
    "        # First query - base it on the question\n",
    "        prompt = f\"Generate a concise search query for: {question}\"\n",
    "    else:\n",
    "        # Subsequent queries - look for gaps\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "        \n",
    "        Already found: {len(findings)} results.\n",
    "        \n",
    "        Generate a NEW search query to find additional information.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    query = response.content.strip()\n",
    "    \n",
    "    print(f\"üîç New query: {query}\")\n",
    "    \n",
    "    return {\n",
    "        \"current_query\": query,\n",
    "        \"search_queries\": [query],\n",
    "        \"search_count\": state.get(\"search_count\", 0) + 1,\n",
    "        \"retry_count\": 0  # Reset retry count for new search\n",
    "    }\n",
    "\n",
    "\n",
    "def perform_search(state: ResearchState) -> dict:\n",
    "    \"\"\"Search and assess result quality.\"\"\"\n",
    "    query = state[\"current_query\"]\n",
    "    \n",
    "    # Simulate search\n",
    "    prompt = f\"Simulate a search result for: {query}\\nProvide a brief finding.\"\n",
    "    response = llm.invoke(prompt)\n",
    "    finding = response.content.strip()\n",
    "    \n",
    "    # Simulate quality (higher retry = better chance)\n",
    "    # In real app, would actually assess the result\n",
    "    quality_score = random.random() + (state[\"retry_count\"] * 0.3)\n",
    "    if quality_score > 0.7:\n",
    "        quality = \"HIGH\"\n",
    "    elif quality_score > 0.4:\n",
    "        quality = \"MEDIUM\"\n",
    "    else:\n",
    "        quality = \"LOW\"\n",
    "    \n",
    "    print(f\"üìÑ Quality: {quality}\")\n",
    "    \n",
    "    return {\n",
    "        \"findings\": [{\"query\": query, \"result\": finding, \"quality\": quality}],\n",
    "        \"current_quality\": quality\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_search(state: ResearchState) -> str:\n",
    "    \"\"\"Decide: accept, retry, or move on.\"\"\"\n",
    "    quality = state[\"current_quality\"]\n",
    "    retry_count = state[\"retry_count\"]\n",
    "    max_retries = state[\"max_retries\"]\n",
    "    \n",
    "    # High quality: accept\n",
    "    if quality == \"HIGH\":\n",
    "        print(\"‚úÖ Good quality - accepting\")\n",
    "        return \"evaluate\"\n",
    "    \n",
    "    # Low quality with retries left: retry\n",
    "    if quality == \"LOW\" and retry_count < max_retries:\n",
    "        print(f\"üîÑ Low quality - retry {retry_count + 1}/{max_retries}\")\n",
    "        return \"retry_search\"\n",
    "    \n",
    "    # Medium or exhausted retries: accept and move on\n",
    "    print(\"‚ö†Ô∏è Accepting (medium quality or max retries)\")\n",
    "    return \"evaluate\"\n",
    "\n",
    "\n",
    "def retry_search(state: ResearchState) -> dict:\n",
    "    \"\"\"Modify query and increment retry count.\"\"\"\n",
    "    query = state[\"current_query\"]\n",
    "    \n",
    "    prompt = f'The search \"{query}\" gave poor results. Suggest a better query.'\n",
    "    response = llm.invoke(prompt)\n",
    "    new_query = response.content.strip()\n",
    "    \n",
    "    print(f\"üîÑ Retry with: {new_query}\")\n",
    "    \n",
    "    return {\n",
    "        \"current_query\": new_query,\n",
    "        \"retry_count\": state[\"retry_count\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_findings(state: ResearchState) -> dict:\n",
    "    \"\"\"Evaluate if we have enough information.\"\"\"\n",
    "    findings = state[\"findings\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Simple evaluation: do we have at least 2 high/medium quality findings?\n",
    "    good_findings = [f for f in findings if f.get(\"quality\") in [\"HIGH\", \"MEDIUM\"]]\n",
    "    has_enough = len(good_findings) >= 2\n",
    "    \n",
    "    print(f\"üìä Evaluation: {len(good_findings)} good findings, enough={has_enough}\")\n",
    "    \n",
    "    return {\"has_enough_info\": has_enough}\n",
    "\n",
    "\n",
    "def route_after_evaluate(state: ResearchState) -> str:\n",
    "    \"\"\"Decide: search more or synthesize answer.\"\"\"\n",
    "    has_enough = state[\"has_enough_info\"]\n",
    "    search_count = state[\"search_count\"]\n",
    "    max_searches = state[\"max_searches\"]\n",
    "    \n",
    "    # Safety valve: stop at max searches\n",
    "    if search_count >= max_searches:\n",
    "        print(f\"üõë Max searches ({max_searches}) reached\")\n",
    "        return \"synthesize\"\n",
    "    \n",
    "    # Enough info: done\n",
    "    if has_enough:\n",
    "        print(\"‚úÖ Enough info gathered\")\n",
    "        return \"synthesize\"\n",
    "    \n",
    "    # Otherwise, keep searching\n",
    "    return \"search_more\"\n",
    "\n",
    "\n",
    "def synthesize_answer(state: ResearchState) -> dict:\n",
    "    \"\"\"Synthesize final answer from findings.\"\"\"\n",
    "    findings = state[\"findings\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    findings_text = \"\\n\".join([f\"- {f['result']}\" for f in findings])\n",
    "    \n",
    "    prompt = f\"\"\"Based on these findings, answer the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Findings:\n",
    "    {findings_text}\n",
    "    \n",
    "    Provide a concise answer.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(\"üìù Answer synthesized\")\n",
    "    \n",
    "    return {\"final_answer\": response.content}\n",
    "\n",
    "\n",
    "# === GRAPH BUILDER ===\n",
    "\n",
    "def create_research_graph():\n",
    "    \"\"\"Build the research assistant graph with retry logic.\n",
    "    \n",
    "    Two levels of looping:\n",
    "    1. Outer loop: search ‚Üí evaluate ‚Üí maybe search again\n",
    "    2. Inner loop: search ‚Üí quality check ‚Üí maybe retry same search\n",
    "    \"\"\"\n",
    "    graph = StateGraph(ResearchState)\n",
    "    \n",
    "    graph.add_node(\"generate_query\", generate_search_query)\n",
    "    graph.add_node(\"search\", perform_search)\n",
    "    graph.add_node(\"retry_search\", retry_search)\n",
    "    graph.add_node(\"evaluate\", evaluate_findings)\n",
    "    graph.add_node(\"synthesize\", synthesize_answer)\n",
    "    \n",
    "    graph.set_entry_point(\"generate_query\")\n",
    "    \n",
    "    graph.add_edge(\"generate_query\", \"search\")\n",
    "    \n",
    "    # After search: accept, retry, or evaluate\n",
    "    graph.add_conditional_edges(\"search\", route_after_search, {\n",
    "        \"retry_search\": \"retry_search\",\n",
    "        \"evaluate\": \"evaluate\"\n",
    "    })\n",
    "    \n",
    "    # Retry loops back to search\n",
    "    graph.add_edge(\"retry_search\", \"search\")\n",
    "    \n",
    "    # After evaluate: more searching or synthesize\n",
    "    graph.add_conditional_edges(\"evaluate\", route_after_evaluate, {\n",
    "        \"search_more\": \"generate_query\",\n",
    "        \"synthesize\": \"synthesize\"\n",
    "    })\n",
    "    \n",
    "    graph.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "def main():\n",
    "    app = create_research_graph()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üî¨ Research Assistant with Retry Logic\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"question\": \"What are the main benefits of using TypeScript over JavaScript?\",\n",
    "        \"search_queries\": [],\n",
    "        \"findings\": [],\n",
    "        \"current_query\": \"\",\n",
    "        \"current_quality\": \"\",\n",
    "        \"retry_count\": 0,\n",
    "        \"max_retries\": 2,\n",
    "        \"search_count\": 0,\n",
    "        \"max_searches\": 4,\n",
    "        \"has_enough_info\": False,\n",
    "        \"final_answer\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã Final Answer:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(result[\"final_answer\"])\n",
    "    print(f\"\\nüìä Stats: {result['search_count']} searches, {len(result['findings'])} findings\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 14.7 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.7.1: Add Debugging to the Ticket Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_14_7_solution.py\n",
    "\n",
    "\"\"\"Ticket router with full debugging capabilities.\n",
    "\n",
    "Exercise 1 Solution: Add comprehensive debugging to the ticket router:\n",
    "- Debug output for every node\n",
    "- State tracking\n",
    "- Loop counter safety valve\n",
    "- Graph visualization at startup\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import TypedDict\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Debug flag - set to False in production\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "def debug_print(*args, **kwargs):\n",
    "    \"\"\"Print only when DEBUG is True.\"\"\"\n",
    "    if DEBUG:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "\n",
    "# === STATE (with safety counter) ===\n",
    "\n",
    "class TicketState(TypedDict):\n",
    "    ticket_text: str\n",
    "    category: str\n",
    "    priority: str\n",
    "    response: str\n",
    "    needs_human: bool\n",
    "    # Safety counter (even though this graph shouldn't loop)\n",
    "    node_visits: int\n",
    "\n",
    "\n",
    "# === STATE TRACKER ===\n",
    "\n",
    "class StateTracker:\n",
    "    \"\"\"Track state changes throughout execution.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def capture(self, node_name: str, state: dict, updates: dict = None):\n",
    "        import copy\n",
    "        self.history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"node\": node_name,\n",
    "            \"state\": copy.deepcopy(dict(state)),\n",
    "            \"updates\": copy.deepcopy(updates) if updates else None\n",
    "        })\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìú EXECUTION TRACE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, entry in enumerate(self.history):\n",
    "            print(f\"\\nStep {i+1}: {entry['node']}\")\n",
    "            if entry['updates']:\n",
    "                for k, v in entry['updates'].items():\n",
    "                    print(f\"  ‚Üí {k}: {str(v)[:50]}\")\n",
    "\n",
    "\n",
    "# Global tracker\n",
    "tracker = StateTracker()\n",
    "\n",
    "\n",
    "# === NODES (with debug output) ===\n",
    "\n",
    "def classify_ticket(state: TicketState) -> dict:\n",
    "    \"\"\"Classify with full debug output.\"\"\"\n",
    "    debug_print(f\"\\n{'='*50}\")\n",
    "    debug_print(f\"üîµ ENTERING: classify_ticket\")\n",
    "    debug_print(f\"   Ticket: {state['ticket_text'][:40]}...\")\n",
    "    \n",
    "    # Safety check\n",
    "    visits = state.get(\"node_visits\", 0) + 1\n",
    "    if visits > 10:\n",
    "        raise Exception(\"Safety limit: too many node visits!\")\n",
    "    \n",
    "    ticket = state[\"ticket_text\"]\n",
    "    \n",
    "    prompt = f\"\"\"Classify this support ticket.\n",
    "    \n",
    "    Ticket: {ticket}\n",
    "    \n",
    "    Categories: BILLING, TECHNICAL, ACCOUNT, GENERAL\n",
    "    Priority: HIGH, MEDIUM, LOW\n",
    "    \n",
    "    Format:\n",
    "    CATEGORY: <category>\n",
    "    PRIORITY: <priority>\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.upper()\n",
    "    \n",
    "    # Parse with defaults\n",
    "    category = \"GENERAL\"\n",
    "    for cat in [\"BILLING\", \"TECHNICAL\", \"ACCOUNT\"]:\n",
    "        if cat in content:\n",
    "            category = cat\n",
    "            break\n",
    "    \n",
    "    priority = \"MEDIUM\"\n",
    "    for pri in [\"HIGH\", \"LOW\"]:\n",
    "        if pri in content:\n",
    "            priority = pri\n",
    "            break\n",
    "    \n",
    "    updates = {\n",
    "        \"category\": category,\n",
    "        \"priority\": priority,\n",
    "        \"node_visits\": visits\n",
    "    }\n",
    "    \n",
    "    debug_print(f\"   Result: {category} ({priority})\")\n",
    "    tracker.capture(\"classify_ticket\", state, updates)\n",
    "    \n",
    "    return updates\n",
    "\n",
    "\n",
    "def route_by_category(state: TicketState) -> str:\n",
    "    \"\"\"Route with debug output.\"\"\"\n",
    "    category = state[\"category\"]\n",
    "    priority = state[\"priority\"]\n",
    "    \n",
    "    if priority == \"HIGH\":\n",
    "        decision = \"escalate\"\n",
    "    else:\n",
    "        routes = {\n",
    "            \"BILLING\": \"handle_billing\",\n",
    "            \"TECHNICAL\": \"handle_technical\", \n",
    "            \"ACCOUNT\": \"handle_account\",\n",
    "            \"GENERAL\": \"handle_general\"\n",
    "        }\n",
    "        decision = routes.get(category, \"handle_general\")\n",
    "    \n",
    "    debug_print(f\"üîÄ ROUTING: {decision}\")\n",
    "    debug_print(f\"   (category={category}, priority={priority})\")\n",
    "    \n",
    "    return decision\n",
    "\n",
    "\n",
    "# === HANDLER FACTORY ===\n",
    "\n",
    "def make_handler(name: str, emoji: str, specialty: str):\n",
    "    \"\"\"Factory to create debug-wrapped handlers.\"\"\"\n",
    "    \n",
    "    def handler(state: TicketState) -> dict:\n",
    "        debug_print(f\"\\n{'='*50}\")\n",
    "        debug_print(f\"üîµ ENTERING: {name}\")\n",
    "        \n",
    "        visits = state.get(\"node_visits\", 0) + 1\n",
    "        \n",
    "        prompt = f\"\"\"You are a {specialty} specialist. Help with:\n",
    "        {state['ticket_text']}\n",
    "        Keep response brief (2-3 sentences).\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        updates = {\n",
    "            \"response\": response.content,\n",
    "            \"needs_human\": False,\n",
    "            \"node_visits\": visits\n",
    "        }\n",
    "        \n",
    "        debug_print(f\"{emoji} Response generated\")\n",
    "        tracker.capture(name, state, updates)\n",
    "        \n",
    "        return updates\n",
    "    \n",
    "    return handler\n",
    "\n",
    "\n",
    "# Create handlers\n",
    "handle_billing = make_handler(\"handle_billing\", \"üí≥\", \"billing support\")\n",
    "handle_technical = make_handler(\"handle_technical\", \"üîß\", \"technical support\")\n",
    "handle_account = make_handler(\"handle_account\", \"üë§\", \"account support\")\n",
    "handle_general = make_handler(\"handle_general\", \"üìß\", \"general support\")\n",
    "\n",
    "\n",
    "def escalate_ticket(state: TicketState) -> dict:\n",
    "    debug_print(f\"\\n{'='*50}\")\n",
    "    debug_print(f\"üîµ ENTERING: escalate_ticket\")\n",
    "    \n",
    "    updates = {\n",
    "        \"response\": \"Escalated to senior agent. Response within 1 hour.\",\n",
    "        \"needs_human\": True,\n",
    "        \"node_visits\": state.get(\"node_visits\", 0) + 1\n",
    "    }\n",
    "    \n",
    "    debug_print(\"üö® Ticket escalated!\")\n",
    "    tracker.capture(\"escalate_ticket\", state, updates)\n",
    "    \n",
    "    return updates\n",
    "\n",
    "\n",
    "# === GRAPH (with visualization) ===\n",
    "\n",
    "def create_debug_router():\n",
    "    \"\"\"Build graph and show visualization.\"\"\"\n",
    "    graph = StateGraph(TicketState)\n",
    "    \n",
    "    graph.add_node(\"classify\", classify_ticket)\n",
    "    graph.add_node(\"handle_billing\", handle_billing)\n",
    "    graph.add_node(\"handle_technical\", handle_technical)\n",
    "    graph.add_node(\"handle_account\", handle_account)\n",
    "    graph.add_node(\"handle_general\", handle_general)\n",
    "    graph.add_node(\"escalate\", escalate_ticket)\n",
    "    \n",
    "    graph.set_entry_point(\"classify\")\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"classify\",\n",
    "        route_by_category,\n",
    "        {\n",
    "            \"handle_billing\": \"handle_billing\",\n",
    "            \"handle_technical\": \"handle_technical\",\n",
    "            \"handle_account\": \"handle_account\",\n",
    "            \"handle_general\": \"handle_general\",\n",
    "            \"escalate\": \"escalate\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for handler in [\"handle_billing\", \"handle_technical\", \n",
    "                    \"handle_account\", \"handle_general\", \"escalate\"]:\n",
    "        graph.add_edge(handler, END)\n",
    "    \n",
    "    app = graph.compile()\n",
    "    \n",
    "    # Show graph structure at startup\n",
    "    if DEBUG:\n",
    "        print(\"\\nüìä GRAPH STRUCTURE\")\n",
    "        print(\"-\" * 40)\n",
    "        print(app.get_graph().draw_mermaid())\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return app\n",
    "\n",
    "\n",
    "# === MAIN ===\n",
    "\n",
    "def main():\n",
    "    app = create_debug_router()\n",
    "    \n",
    "    test_tickets = [\n",
    "        \"I was charged twice!\",\n",
    "        \"App keeps crashing\",\n",
    "        \"THIS IS UNACCEPTABLE! FIX IT NOW!\"\n",
    "    ]\n",
    "    \n",
    "    for ticket in test_tickets:\n",
    "        tracker.history.clear()  # Reset for each ticket\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üì© Processing: {ticket}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        result = app.invoke({\n",
    "            \"ticket_text\": ticket,\n",
    "            \"category\": \"\",\n",
    "            \"priority\": \"\",\n",
    "            \"response\": \"\",\n",
    "            \"needs_human\": False,\n",
    "            \"node_visits\": 0\n",
    "        })\n",
    "        \n",
    "        # Print execution trace\n",
    "        tracker.print_summary()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Final: {result['category']} ‚Üí {result['response'][:50]}...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.7.2: Find the Bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_14_7_solution.py\n",
    "\n",
    "\"\"\"Fixed version of the buggy graph with debugging.\n",
    "\n",
    "Exercise 2 Solution: Find and fix all the bugs in the provided code.\n",
    "\n",
    "Bugs found and fixed:\n",
    "1. results: list ‚Üí results: Annotated[list, add] (so results accumulate)\n",
    "2. state[\"search_count\"] ‚Üí state.get(\"search_count\", 0) (KeyError fix)\n",
    "3. Routing function returns must match mapping keys exactly\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# BUG 1 FIX: Use Annotated[list, add] so results accumulate\n",
    "class FixedState(TypedDict):\n",
    "    query: str\n",
    "    results: Annotated[list, add]  # FIXED: Was just 'list'\n",
    "    search_count: int\n",
    "    max_searches: int\n",
    "\n",
    "\n",
    "def search(state: FixedState) -> dict:\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    # BUG 2 FIX: Use .get() with default value\n",
    "    count = state.get(\"search_count\", 0)  # FIXED: Was state[\"search_count\"]\n",
    "    \n",
    "    print(f\"üîç Search #{count + 1}: {query}\")\n",
    "    \n",
    "    result = f\"Result for: {query}\"\n",
    "    \n",
    "    return {\n",
    "        \"results\": [result],  # This now accumulates thanks to Annotated\n",
    "        \"search_count\": count + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: FixedState) -> str:\n",
    "    \"\"\"Fixed routing function with proper return values.\"\"\"\n",
    "    current = state.get(\"search_count\", 0)\n",
    "    maximum = state.get(\"max_searches\", 3)\n",
    "    \n",
    "    print(f\"üîÄ Checking: {current} < {maximum}?\")\n",
    "    \n",
    "    if current < maximum:\n",
    "        # BUG 3 FIX: Return value must match mapping keys\n",
    "        return \"continue\"  # FIXED: Was \"search\" which didn't match mapping\n",
    "    return \"done\"  # FIXED: Was \"end\" which didn't match mapping\n",
    "\n",
    "\n",
    "def create_fixed_graph():\n",
    "    graph = StateGraph(FixedState)\n",
    "    \n",
    "    graph.add_node(\"search\", search)\n",
    "    graph.set_entry_point(\"search\")\n",
    "    \n",
    "    # BUG 3 FIX: Mapping keys must match what routing function returns\n",
    "    graph.add_conditional_edges(\n",
    "        \"search\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"search\",  # Loops back\n",
    "            \"done\": END            # Exits\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def main():\n",
    "    app = create_fixed_graph()\n",
    "    \n",
    "    print(\"üêõ Running fixed graph...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = app.invoke({\n",
    "        \"query\": \"LangGraph tutorials\",\n",
    "        \"results\": [],          # Initialize empty list\n",
    "        \"search_count\": 0,      # Initialize counter\n",
    "        \"max_searches\": 3\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"‚úÖ Total searches: {result['search_count']}\")\n",
    "    print(f\"‚úÖ Results collected: {len(result['results'])}\")\n",
    "    for i, r in enumerate(result['results'], 1):\n",
    "        print(f\"   {i}. {r}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# === SUMMARY OF BUGS ===\n",
    "\"\"\"\n",
    "Bug 1: results: list should be results: Annotated[list, add]\n",
    "  - Without Annotated, list gets replaced each time\n",
    "  - With Annotated[list, add], lists accumulate across iterations\n",
    "\n",
    "Bug 2: state[\"search_count\"] throws KeyError if not initialized\n",
    "  - Fixed with state.get(\"search_count\", 0)\n",
    "  - Always use .get() with defaults for safety\n",
    "\n",
    "Bug 3: Routing function returned \"search\" and \"end\", but mapping had \"continue\" and \"done\"\n",
    "  - The return values must EXACTLY match the mapping keys\n",
    "  - This is a very common bug - always double-check your mapping!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14.7.3: Build a Debug Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_14_7_solution.py\n",
    "\n",
    "\"\"\"Debug dashboard for analyzing LangGraph executions.\n",
    "\n",
    "Exercise 3 Solution: Build a debug dashboard that produces:\n",
    "- Total nodes visited\n",
    "- Time spent\n",
    "- State changes for each field\n",
    "- Fields that never changed\n",
    "- Routing decisions made\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DebugDashboard:\n",
    "    \"\"\"Comprehensive execution analysis dashboard.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.executions = []\n",
    "        self.current_execution = None\n",
    "    \n",
    "    def start_execution(self, name: str = None):\n",
    "        \"\"\"Start tracking a new execution.\"\"\"\n",
    "        self.current_execution = {\n",
    "            \"name\": name or f\"Execution_{len(self.executions) + 1}\",\n",
    "            \"started\": datetime.now(),\n",
    "            \"ended\": None,\n",
    "            \"steps\": [],\n",
    "            \"routing_decisions\": [],\n",
    "            \"initial_state\": None,\n",
    "            \"final_state\": None\n",
    "        }\n",
    "    \n",
    "    def record_step(self, node_name: str, state_before: dict, updates: dict):\n",
    "        \"\"\"Record a single step in the execution.\"\"\"\n",
    "        if not self.current_execution:\n",
    "            self.start_execution()\n",
    "        \n",
    "        step = {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"node\": node_name,\n",
    "            \"state_before\": dict(state_before),\n",
    "            \"updates\": dict(updates) if updates else {}\n",
    "        }\n",
    "        self.current_execution[\"steps\"].append(step)\n",
    "        \n",
    "        # Track initial state\n",
    "        if self.current_execution[\"initial_state\"] is None:\n",
    "            self.current_execution[\"initial_state\"] = dict(state_before)\n",
    "    \n",
    "    def record_routing(self, router_name: str, decision: str, reason: str = None):\n",
    "        \"\"\"Record a routing decision.\"\"\"\n",
    "        if self.current_execution:\n",
    "            self.current_execution[\"routing_decisions\"].append({\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"router\": router_name,\n",
    "                \"decision\": decision,\n",
    "                \"reason\": reason\n",
    "            })\n",
    "    \n",
    "    def end_execution(self, final_state: dict):\n",
    "        \"\"\"End the current execution.\"\"\"\n",
    "        if self.current_execution:\n",
    "            self.current_execution[\"ended\"] = datetime.now()\n",
    "            self.current_execution[\"final_state\"] = dict(final_state)\n",
    "            self.executions.append(self.current_execution)\n",
    "            self.current_execution = None\n",
    "    \n",
    "    def generate_report(self, execution_index: int = -1) -> str:\n",
    "        \"\"\"Generate a comprehensive report for an execution.\"\"\"\n",
    "        if not self.executions:\n",
    "            return \"No executions recorded.\"\n",
    "        \n",
    "        exec_data = self.executions[execution_index]\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"=\" * 60)\n",
    "        lines.append(f\"üìä DEBUG REPORT: {exec_data['name']}\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        \n",
    "        # Timing\n",
    "        duration = (exec_data['ended'] - exec_data['started']).total_seconds()\n",
    "        lines.append(f\"\\n‚è±Ô∏è  TIMING\")\n",
    "        lines.append(f\"   Started: {exec_data['started'].strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "        lines.append(f\"   Ended: {exec_data['ended'].strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "        lines.append(f\"   Duration: {duration:.3f} seconds\")\n",
    "        \n",
    "        # Node visits\n",
    "        lines.append(f\"\\nüìç NODE VISITS ({len(exec_data['steps'])} total)\")\n",
    "        node_counts = defaultdict(int)\n",
    "        for step in exec_data['steps']:\n",
    "            node_counts[step['node']] += 1\n",
    "        for node, count in node_counts.items():\n",
    "            marker = \"‚ö†Ô∏è \" if count > 1 else \"   \"\n",
    "            lines.append(f\"{marker}{node}: {count} visit(s)\")\n",
    "        \n",
    "        # Routing decisions\n",
    "        lines.append(f\"\\nüîÄ ROUTING DECISIONS ({len(exec_data['routing_decisions'])})\")\n",
    "        for rd in exec_data['routing_decisions']:\n",
    "            reason = f\" ({rd['reason']})\" if rd['reason'] else \"\"\n",
    "            lines.append(f\"   {rd['router']} ‚Üí {rd['decision']}{reason}\")\n",
    "        \n",
    "        # State changes\n",
    "        lines.append(f\"\\nüìù STATE CHANGES\")\n",
    "        all_fields = set()\n",
    "        for step in exec_data['steps']:\n",
    "            all_fields.update(step['state_before'].keys())\n",
    "            all_fields.update(step['updates'].keys())\n",
    "        \n",
    "        initial = exec_data['initial_state'] or {}\n",
    "        final = exec_data['final_state'] or {}\n",
    "        \n",
    "        for field in sorted(all_fields):\n",
    "            initial_val = initial.get(field, \"<not set>\")\n",
    "            final_val = final.get(field, \"<not set>\")\n",
    "            \n",
    "            # Truncate long values\n",
    "            iv_str = str(initial_val)[:30]\n",
    "            fv_str = str(final_val)[:30]\n",
    "            \n",
    "            if initial_val == final_val:\n",
    "                lines.append(f\"   {field}: {fv_str} (unchanged)\")\n",
    "            else:\n",
    "                lines.append(f\"   {field}: {iv_str} ‚Üí {fv_str}\")\n",
    "        \n",
    "        # Fields that never changed\n",
    "        unchanged = []\n",
    "        for field in all_fields:\n",
    "            if initial.get(field) == final.get(field) and field in initial:\n",
    "                unchanged.append(field)\n",
    "        \n",
    "        if unchanged:\n",
    "            lines.append(f\"\\n‚ö†Ô∏è  NEVER CHANGED: {', '.join(unchanged)}\")\n",
    "        \n",
    "        lines.append(\"\\n\" + \"=\" * 60)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Demonstrate the dashboard.\"\"\"\n",
    "    dashboard = DebugDashboard()\n",
    "    \n",
    "    # Simulate an execution\n",
    "    dashboard.start_execution(\"Test Run\")\n",
    "    \n",
    "    state = {\"query\": \"test\", \"results\": [], \"count\": 0}\n",
    "    \n",
    "    dashboard.record_step(\"search\", state, {\"results\": [\"r1\"], \"count\": 1})\n",
    "    dashboard.record_routing(\"should_continue\", \"continue\", \"count < max\")\n",
    "    \n",
    "    state = {\"query\": \"test\", \"results\": [\"r1\"], \"count\": 1}\n",
    "    dashboard.record_step(\"search\", state, {\"results\": [\"r2\"], \"count\": 2})\n",
    "    dashboard.record_routing(\"should_continue\", \"done\", \"count >= max\")\n",
    "    \n",
    "    final = {\"query\": \"test\", \"results\": [\"r1\", \"r2\"], \"count\": 2}\n",
    "    dashboard.end_execution(final)\n",
    "    \n",
    "    print(dashboard.generate_report())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()\n",
    "\n",
    "\n",
    "# === HOW TO USE IN YOUR GRAPH ===\n",
    "\"\"\"\n",
    "from debug_dashboard import DebugDashboard\n",
    "\n",
    "dashboard = DebugDashboard()\n",
    "\n",
    "def my_node(state: MyState) -> dict:\n",
    "    # ... your logic ...\n",
    "    updates = {\"field\": \"value\"}\n",
    "    \n",
    "    dashboard.record_step(\"my_node\", state, updates)\n",
    "    return updates\n",
    "\n",
    "def my_router(state: MyState) -> str:\n",
    "    decision = \"some_path\"\n",
    "    dashboard.record_routing(\"my_router\", decision, f\"score={state.get('score')}\")\n",
    "    return decision\n",
    "\n",
    "# In main:\n",
    "dashboard.start_execution(\"My Test\")\n",
    "result = app.invoke(initial_state)\n",
    "dashboard.end_execution(result)\n",
    "print(dashboard.generate_report())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Return to **Chapter 15: Next Topic**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}