{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: Advanced LangGraph Patterns\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- Implementing human-in-the-loop workflows\n",
    "- Streaming and real-time updates\n",
    "- Parallel execution and map-reduce patterns\n",
    "- Subgraphs and modular design\n",
    "- Dynamic graph construction\n",
    "- Implementing feedback loops\n",
    "- Production deployment considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.1: Implementing human-in-the-loop workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: setup_check.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.1\n",
    "# File: setup_check.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is available\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"\u2705 OpenAI API key found\")\n",
    "else:\n",
    "    print(\"\u274c Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "    from langgraph.types import interrupt, Command\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    print(\"\u2705 All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Import error: {e}\")\n",
    "    print(\"Run: pip install langgraph langchain-openai python-dotenv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_approval.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.1\n",
    "# File: simple_approval.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Our workflow state\n",
    "class ContentState(TypedDict):\n",
    "    topic: str                    # What to write about\n",
    "    draft: str                    # The generated content\n",
    "    status: str                   # Current status\n",
    "\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "def draft_content(state: ContentState) -> dict:\n",
    "    \"\"\"Generate initial content draft.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcdd Drafting content about: {state['topic']}\")\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Write a short, engaging paragraph about: {state['topic']}\"\n",
    "    )\n",
    "    \n",
    "    draft = response.content\n",
    "    print(f\"\u2705 Draft created ({len(draft)} characters)\")\n",
    "    \n",
    "    return {\n",
    "        \"draft\": draft,\n",
    "        \"status\": \"draft_complete\"\n",
    "    }\n",
    "\n",
    "\n",
    "def human_review(state: ContentState) -> dict:\n",
    "    \"\"\"Pause for human review and get approval decision.\"\"\"\n",
    "    \n",
    "    print(\"\\n\u23f8\ufe0f  Pausing for human review...\")\n",
    "    \n",
    "    # This is where the magic happens!\n",
    "    # interrupt() pauses execution and returns this value to the caller\n",
    "    # When resumed, it returns whatever was passed to Command(resume=...)\n",
    "    human_decision = interrupt({\n",
    "        \"type\": \"approval_request\",\n",
    "        \"draft\": state[\"draft\"],\n",
    "        \"message\": \"Please review this content. Approve or provide feedback.\"\n",
    "    })\n",
    "    \n",
    "    # This code only runs AFTER the human responds\n",
    "    print(f\"\\n\ud83d\udcec Received human decision: {human_decision}\")\n",
    "    \n",
    "    if human_decision.get(\"approved\"):\n",
    "        return {\"status\": \"approved\"}\n",
    "    else:\n",
    "        # Human rejected - we need to revise\n",
    "        return {\n",
    "            \"status\": \"needs_revision\",\n",
    "            \"draft\": \"\"  # Clear draft so we regenerate\n",
    "        }\n",
    "\n",
    "\n",
    "def revise_content(state: ContentState) -> dict:\n",
    "    \"\"\"Revise content based on rejection.\"\"\"\n",
    "    print(f\"\\n\ud83d\udd04 Revising content...\")\n",
    "    \n",
    "    # In a real app, you'd include the feedback in the prompt\n",
    "    response = llm.invoke(\n",
    "        f\"Write a different, improved paragraph about: {state['topic']}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"status\": \"draft_complete\"\n",
    "    }\n",
    "\n",
    "def publish_content(state: ContentState) -> dict:\n",
    "    \"\"\"Publish the approved content.\"\"\"\n",
    "    print(\"\\n\ud83d\ude80 Publishing content...\")\n",
    "    print(f\"Published: {state['draft'][:100]}...\")\n",
    "    \n",
    "    return {\"status\": \"published\"}\n",
    "\n",
    "\n",
    "def route_after_review(state: ContentState) -> str:\n",
    "    \"\"\"Route based on review outcome.\"\"\"\n",
    "    if state[\"status\"] == \"approved\":\n",
    "        return \"publish\"\n",
    "    elif state[\"status\"] == \"needs_revision\":\n",
    "        return \"revise\"\n",
    "    else:\n",
    "        return \"review\"  # Stay in review\n",
    "\n",
    "def build_approval_workflow():\n",
    "    \"\"\"Build the content approval workflow.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(ContentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"draft\", draft_content)\n",
    "    workflow.add_node(\"review\", human_review)\n",
    "    workflow.add_node(\"revise\", revise_content)\n",
    "    workflow.add_node(\"publish\", publish_content)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.add_edge(START, \"draft\")\n",
    "    workflow.add_edge(\"draft\", \"review\")\n",
    "    \n",
    "    # After review, route based on decision\n",
    "    workflow.add_conditional_edges(\n",
    "        \"review\",\n",
    "        route_after_review,\n",
    "        {\n",
    "            \"publish\": \"publish\",\n",
    "            \"revise\": \"revise\",\n",
    "            \"review\": \"review\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After revision, go back to review\n",
    "    workflow.add_edge(\"revise\", \"review\")\n",
    "    \n",
    "    # After publish, we're done\n",
    "    workflow.add_edge(\"publish\", END)\n",
    "    \n",
    "    # Compile with checkpointer (required for interrupts!)\n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def run_with_approval():\n",
    "    \"\"\"Run the workflow with human-in-the-loop approval.\"\"\"\n",
    "    \n",
    "    app = build_approval_workflow()\n",
    "    \n",
    "    # Thread ID identifies this specific workflow instance\n",
    "    config = {\"configurable\": {\"thread_id\": \"content-1\"}}\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"\ud83c\udfac Starting content creation workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = {\n",
    "        \"topic\": \"Why learning to code is like learning to cook\",\n",
    "        \"draft\": \"\",\n",
    "        \"status\": \"starting\"\n",
    "    }\n",
    "    \n",
    "    # Run until interrupt\n",
    "    result = app.invoke(initial_state, config)\n",
    "\n",
    "    # Check if we hit an interrupt\n",
    "    while \"__interrupt__\" in result or hasattr(result, '__interrupt__'):\n",
    "        # Get the interrupt payload\n",
    "        interrupt_info = result.get(\"__interrupt__\", [])\n",
    "        if interrupt_info:\n",
    "            payload = interrupt_info[0].value  # Get the first interrupt's value\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"\ud83d\udccb CONTENT FOR REVIEW:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(payload.get(\"draft\", result.get(\"draft\", \"No draft available\")))\n",
    "            print(\"-\" * 50)\n",
    "            print(payload.get(\"message\", \"Please review\"))\n",
    "            \n",
    "            # Get human decision\n",
    "            print(\"\\nOptions: [a]pprove, [r]eject, [c]ancel\")\n",
    "            decision = input(\"Your decision: \").strip().lower()\n",
    "            \n",
    "            if decision == 'a':\n",
    "                # Resume with approval\n",
    "                result = app.invoke(\n",
    "                    Command(resume={\"approved\": True}),\n",
    "                    config\n",
    "                )\n",
    "            elif decision == 'r':\n",
    "                # Resume with rejection\n",
    "                feedback = input(\"Feedback (optional): \").strip()\n",
    "                result = app.invoke(\n",
    "                    Command(resume={\"approved\": False, \"feedback\": feedback}),\n",
    "                    config\n",
    "                )\n",
    "            elif decision == 'c':\n",
    "                print(\"\\n\u274c Workflow cancelled by user\")\n",
    "                return None\n",
    "            else:\n",
    "                print(\"Invalid option, please try again\")\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"\u2705 Workflow completed with status: {result.get('status', 'unknown')}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_with_approval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: order_approval.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.1\n",
    "# File: order_approval.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class OrderState(TypedDict):\n",
    "    item: str\n",
    "    quantity: int\n",
    "    price: float\n",
    "    total: float\n",
    "    order_id: str\n",
    "    status: str\n",
    "    rejection_reason: str\n",
    "    messages: Annotated[list[str], operator.add]  # Audit trail\n",
    "\n",
    "\n",
    "def calculate_order(state: OrderState) -> dict:\n",
    "    \"\"\"Calculate order details.\"\"\"\n",
    "    total = state[\"quantity\"] * state[\"price\"]\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcb0 Order calculated: {state['quantity']}x {state['item']} = ${total:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"status\": \"pending_approval\",\n",
    "        \"messages\": [f\"Order {state['order_id']} created: ${total:.2f}\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def request_approval(state: OrderState) -> dict:\n",
    "    \"\"\"Request human approval for the order.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\u23f8\ufe0f  Requesting approval for order {state['order_id']}...\")\n",
    "    \n",
    "    # Interrupt with full order details\n",
    "    decision = interrupt({\n",
    "        \"type\": \"order_approval\",\n",
    "        \"order_id\": state[\"order_id\"],\n",
    "        \"item\": state[\"item\"],\n",
    "        \"quantity\": state[\"quantity\"],\n",
    "        \"unit_price\": state[\"price\"],\n",
    "        \"total\": state[\"total\"],\n",
    "        \"options\": [\"approve\", \"reject\", \"modify\"]\n",
    "    })\n",
    "    \n",
    "    # Process the decision\n",
    "    action = decision.get(\"action\", \"reject\")\n",
    "    \n",
    "    if action == \"approve\":\n",
    "        return {\n",
    "            \"status\": \"approved\",\n",
    "            \"messages\": [f\"Order approved by {decision.get('approver', 'unknown')}\"]\n",
    "        }\n",
    "    elif action == \"modify\":\n",
    "        # Human wants to change the quantity\n",
    "        new_qty = decision.get(\"new_quantity\", state[\"quantity\"])\n",
    "        return {\n",
    "            \"quantity\": new_qty,\n",
    "            \"status\": \"modified\",\n",
    "            \"messages\": [f\"Order modified: quantity changed to {new_qty}\"]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"status\": \"rejected\",\n",
    "            \"rejection_reason\": decision.get(\"reason\", \"No reason provided\"),\n",
    "            \"messages\": [f\"Order rejected: {decision.get('reason', 'No reason')}\"]\n",
    "        }\n",
    "\n",
    "\n",
    "def process_order(state: OrderState) -> dict:\n",
    "    \"\"\"Process the approved order.\"\"\"\n",
    "    print(f\"\\n\u2705 Processing order {state['order_id']}...\")\n",
    "    return {\n",
    "        \"status\": \"processed\",\n",
    "        \"messages\": [\"Order processed and sent to fulfillment\"]\n",
    "    }\n",
    "\n",
    "def cancel_order(state: OrderState) -> dict:\n",
    "    \"\"\"Cancel the rejected order.\"\"\"\n",
    "    print(f\"\\n\u274c Cancelling order {state['order_id']}: {state['rejection_reason']}\")\n",
    "    return {\n",
    "        \"status\": \"cancelled\",\n",
    "        \"messages\": [f\"Order cancelled: {state['rejection_reason']}\"]\n",
    "    }\n",
    "\n",
    "def recalculate_order(state: OrderState) -> dict:\n",
    "    \"\"\"Recalculate after modification.\"\"\"\n",
    "    new_total = state[\"quantity\"] * state[\"price\"]\n",
    "    print(f\"\\n\ud83d\udd04 Recalculating: {state['quantity']}x {state['item']} = ${new_total:.2f}\")\n",
    "    return {\n",
    "        \"total\": new_total,\n",
    "        \"status\": \"pending_approval\",\n",
    "        \"messages\": [f\"Order recalculated: ${new_total:.2f}\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_approval(state: OrderState) -> str:\n",
    "    \"\"\"Route based on approval decision.\"\"\"\n",
    "    status = state[\"status\"]\n",
    "    if status == \"approved\":\n",
    "        return \"process\"\n",
    "    elif status == \"modified\":\n",
    "        return \"recalculate\"\n",
    "    elif status == \"rejected\":\n",
    "        return \"cancel\"\n",
    "    else:\n",
    "        return \"approve\"  # Stay in approval\n",
    "\n",
    "def build_order_workflow():\n",
    "    workflow = StateGraph(OrderState)\n",
    "    \n",
    "    workflow.add_node(\"calculate\", calculate_order)\n",
    "    workflow.add_node(\"approve\", request_approval)\n",
    "    workflow.add_node(\"process\", process_order)\n",
    "    workflow.add_node(\"cancel\", cancel_order)\n",
    "    workflow.add_node(\"recalculate\", recalculate_order)\n",
    "    \n",
    "    workflow.add_edge(START, \"calculate\")\n",
    "    workflow.add_edge(\"calculate\", \"approve\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"approve\",\n",
    "        route_after_approval,\n",
    "        {\n",
    "            \"process\": \"process\",\n",
    "            \"cancel\": \"cancel\",\n",
    "            \"recalculate\": \"recalculate\",\n",
    "            \"approve\": \"approve\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After recalculation, go back for approval\n",
    "    workflow.add_edge(\"recalculate\", \"approve\")\n",
    "    \n",
    "    workflow.add_edge(\"process\", END)\n",
    "    workflow.add_edge(\"cancel\", END)\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = build_order_workflow()\n",
    "    config = {\"configurable\": {\"thread_id\": \"order-001\"}}\n",
    "    \n",
    "    initial_state = {\n",
    "        \"item\": \"Laptop\",\n",
    "        \"quantity\": 5,\n",
    "        \"price\": 999.99,\n",
    "        \"total\": 0.0,\n",
    "        \"order_id\": \"ORD-001\",\n",
    "        \"status\": \"new\",\n",
    "        \"rejection_reason\": \"\",\n",
    "        \"messages\": []\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state, config)\n",
    "    print(f\"Final status: {result.get('status')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_stage_approval.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.1\n",
    "# File: multi_stage_approval.py\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    title: str\n",
    "    content: str\n",
    "    current_stage: str\n",
    "    author_approved: bool\n",
    "    editor_approved: bool\n",
    "    legal_approved: bool\n",
    "    revision_notes: str\n",
    "    revision_count: int\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "def create_stage_review(stage_name: str, approver_title: str):\n",
    "    \"\"\"Factory function to create review nodes for different stages.\"\"\"\n",
    "    \n",
    "    def review_node(state: DocumentState) -> dict:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"\ud83d\udccb {approver_title} Review Stage\")\n",
    "        print('='*50)\n",
    "        \n",
    "        # Request approval from this stage's reviewer\n",
    "        decision = interrupt({\n",
    "            \"stage\": stage_name,\n",
    "            \"approver\": approver_title,\n",
    "            \"document_title\": state[\"title\"],\n",
    "            \"content_preview\": state[\"content\"][:500] + \"...\" if len(state[\"content\"]) > 500 else state[\"content\"],\n",
    "            \"message\": f\"{approver_title}: Please review this document.\"\n",
    "        })\n",
    "        \n",
    "        approved = decision.get(\"approved\", False)\n",
    "        \n",
    "        if approved:\n",
    "            print(f\"\u2705 {approver_title} approved\")\n",
    "            return {\n",
    "                f\"{stage_name}_approved\": True,\n",
    "                \"current_stage\": f\"{stage_name}_complete\"\n",
    "            }\n",
    "        else:\n",
    "            print(f\"\u274c {approver_title} requested changes\")\n",
    "            return {\n",
    "                f\"{stage_name}_approved\": False,\n",
    "                \"current_stage\": \"needs_revision\",\n",
    "                \"revision_notes\": decision.get(\"notes\", \"Please revise\")\n",
    "            }\n",
    "    \n",
    "    return review_node\n",
    "\n",
    "# Create the three review stages\n",
    "author_review = create_stage_review(\"author\", \"Author\")\n",
    "editor_review = create_stage_review(\"editor\", \"Editor\")\n",
    "legal_review = create_stage_review(\"legal\", \"Legal Team\")\n",
    "\n",
    "\n",
    "def generate_document(state: DocumentState) -> dict:\n",
    "    \"\"\"Generate initial document content.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcc4 Generating document: {state['title']}\")\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Write a brief professional document titled '{state['title']}'\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.content,\n",
    "        \"current_stage\": \"author_review\"\n",
    "    }\n",
    "\n",
    "def revise_document(state: DocumentState) -> dict:\n",
    "    \"\"\"Revise document based on feedback.\"\"\"\n",
    "    print(f\"\\n\ud83d\udd04 Revising document (revision #{state['revision_count'] + 1})\")\n",
    "    print(f\"   Notes: {state['revision_notes']}\")\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Revise this document based on the feedback.\\n\\n\"\n",
    "        f\"Current content:\\n{state['content']}\\n\\n\"\n",
    "        f\"Feedback: {state['revision_notes']}\\n\\n\"\n",
    "        f\"Revised document:\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.content,\n",
    "        \"revision_count\": state[\"revision_count\"] + 1,\n",
    "        \"revision_notes\": \"\",\n",
    "        \"current_stage\": \"author_review\",  # Start over from author\n",
    "        # Reset all approvals since content changed\n",
    "        \"author_approved\": False,\n",
    "        \"editor_approved\": False,\n",
    "        \"legal_approved\": False\n",
    "    }\n",
    "\n",
    "def finalize_document(state: DocumentState) -> dict:\n",
    "    \"\"\"Document is fully approved.\"\"\"\n",
    "    print(\"\\n\ud83c\udf89 Document approved by all reviewers!\")\n",
    "    return {\"current_stage\": \"finalized\"}\n",
    "\n",
    "\n",
    "def route_document(state: DocumentState) -> str:\n",
    "    \"\"\"Route based on current stage and approvals.\"\"\"\n",
    "    stage = state[\"current_stage\"]\n",
    "    \n",
    "    if stage == \"needs_revision\":\n",
    "        return \"revise\"\n",
    "    elif stage == \"author_review\" or (stage == \"author_complete\" and not state[\"editor_approved\"]):\n",
    "        if state[\"author_approved\"]:\n",
    "            return \"editor\"\n",
    "        return \"author\"\n",
    "    elif stage == \"editor_complete\" or state[\"editor_approved\"]:\n",
    "        if state[\"legal_approved\"]:\n",
    "            return \"finalize\"\n",
    "        return \"legal\"\n",
    "    elif state[\"author_approved\"] and state[\"editor_approved\"] and state[\"legal_approved\"]:\n",
    "        return \"finalize\"\n",
    "    \n",
    "    return \"author\"  # Default to start of review chain\n",
    "\n",
    "def build_multi_stage_workflow():\n",
    "    workflow = StateGraph(DocumentState)\n",
    "    \n",
    "    workflow.add_node(\"generate\", generate_document)\n",
    "    workflow.add_node(\"author\", author_review)\n",
    "    workflow.add_node(\"editor\", editor_review)\n",
    "    workflow.add_node(\"legal\", legal_review)\n",
    "    workflow.add_node(\"revise\", revise_document)\n",
    "    workflow.add_node(\"finalize\", finalize_document)\n",
    "    \n",
    "    workflow.add_edge(START, \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"author\")\n",
    "    \n",
    "    # Author can approve (-> editor) or reject (-> revise)\n",
    "    workflow.add_conditional_edges(\n",
    "        \"author\",\n",
    "        lambda s: \"editor\" if s[\"author_approved\"] else \"revise\",\n",
    "        {\"editor\": \"editor\", \"revise\": \"revise\"}\n",
    "    )\n",
    "    \n",
    "    # Editor can approve (-> legal) or reject (-> revise)\n",
    "    workflow.add_conditional_edges(\n",
    "        \"editor\",\n",
    "        lambda s: \"legal\" if s[\"editor_approved\"] else \"revise\",\n",
    "        {\"legal\": \"legal\", \"revise\": \"revise\"}\n",
    "    )\n",
    "    \n",
    "    # Legal can approve (-> finalize) or reject (-> revise)\n",
    "    workflow.add_conditional_edges(\n",
    "        \"legal\",\n",
    "        lambda s: \"finalize\" if s[\"legal_approved\"] else \"revise\",\n",
    "        {\"finalize\": \"finalize\", \"revise\": \"revise\"}\n",
    "    )\n",
    "    \n",
    "    # After revision, start the review chain over\n",
    "    workflow.add_edge(\"revise\", \"author\")\n",
    "    \n",
    "    workflow.add_edge(\"finalize\", END)\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = build_multi_stage_workflow()\n",
    "    config = {\"configurable\": {\"thread_id\": \"doc-001\"}}\n",
    "    \n",
    "    initial_state = {\n",
    "        \"title\": \"Q4 Product Launch Announcement\",\n",
    "        \"content\": \"\",\n",
    "        \"current_stage\": \"starting\",\n",
    "        \"author_approved\": False,\n",
    "        \"editor_approved\": False,\n",
    "        \"legal_approved\": False,\n",
    "        \"revision_notes\": \"\",\n",
    "        \"revision_count\": 0\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state, config)\n",
    "    print(f\"\\nFinal stage: {result.get('current_stage')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.1.1: Expense Approval System\n",
    "\n",
    "Build a workflow where:\n",
    "- Expenses under $100 are auto-approved (no interrupt)\n",
    "- Expenses $100-$1000 need manager approval\n",
    "- Expenses over $1000 need manager AND finance approval\n",
    "- Rejected expenses can be revised and resubmitted\n",
    "\n",
    "Requirements:\n",
    "- Track who approved at each stage\n",
    "- Allow adding notes with approval/rejection\n",
    "- Limit to 3 revision attempts\n",
    "- Use `interrupt()` for human approval points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.1.2: Content Moderation Pipeline\n",
    "\n",
    "Create a content moderation system that:\n",
    "- AI first screens content for obvious violations\n",
    "- Borderline content triggers `interrupt()` for human review\n",
    "- Humans can approve, reject, or escalate\n",
    "- Escalated content goes to senior moderators (another interrupt)\n",
    "\n",
    "Requirements:\n",
    "- Different severity levels (warning, removal, ban)\n",
    "- Show AI's confidence score in the interrupt payload\n",
    "- Allow moderators to override AI recommendations\n",
    "- Track all decisions in an audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.1.3: Interview Scheduling Assistant\n",
    "\n",
    "Build an interview scheduling workflow where:\n",
    "- AI proposes time slots\n",
    "- `interrupt()` for candidate to select preferred times\n",
    "- `interrupt()` for interviewer to confirm\n",
    "- Either party can request rescheduling\n",
    "\n",
    "Requirements:\n",
    "- Handle conflicts and unavailability\n",
    "- Use interrupt payloads to present options clearly\n",
    "- Allow up to 2 reschedule requests\n",
    "- Track the full scheduling history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.2: Streaming and real-time updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: setup_streaming.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: setup_streaming.py\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify setup\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"\u2705 OpenAI API key found\")\n",
    "else:\n",
    "    print(\"\u274c Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    print(\"\u2705 All imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Import error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: stream_updates.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: stream_updates.py\n",
    "\n",
    "import asyncio\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    analysis: str\n",
    "    summary: str\n",
    "    current_step: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "async def research_topic(state: ResearchState) -> dict:\n",
    "    \"\"\"Gather information about the topic.\"\"\"\n",
    "    print(\"\ud83d\udcda Researching...\")\n",
    "    \n",
    "    response = await llm.ainvoke(\n",
    "        f\"Provide 3 key facts about: {state['topic']}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"research\": response.content,\n",
    "        \"current_step\": \"research_complete\"\n",
    "    }\n",
    "\n",
    "async def analyze_research(state: ResearchState) -> dict:\n",
    "    \"\"\"Analyze the gathered research.\"\"\"\n",
    "    print(\"\ud83d\udd0d Analyzing...\")\n",
    "    \n",
    "    response = await llm.ainvoke(\n",
    "        f\"Analyze these facts and identify the main theme:\\n{state['research']}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"analysis\": response.content,\n",
    "        \"current_step\": \"analysis_complete\"\n",
    "    }\n",
    "\n",
    "async def write_summary(state: ResearchState) -> dict:\n",
    "    \"\"\"Write a final summary.\"\"\"\n",
    "    print(\"\u270d\ufe0f Summarizing...\")\n",
    "    \n",
    "    response = await llm.ainvoke(\n",
    "        f\"Write a one-paragraph summary based on:\\n{state['analysis']}\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"summary\": response.content,\n",
    "        \"current_step\": \"complete\"\n",
    "    }\n",
    "\n",
    "def build_research_graph():\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    workflow.add_node(\"research\", research_topic)\n",
    "    workflow.add_node(\"analyze\", analyze_research)\n",
    "    workflow.add_node(\"summarize\", write_summary)\n",
    "    \n",
    "    workflow.add_edge(START, \"research\")\n",
    "    workflow.add_edge(\"research\", \"analyze\")\n",
    "    workflow.add_edge(\"analyze\", \"summarize\")\n",
    "    workflow.add_edge(\"summarize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "async def run_with_streaming():\n",
    "    \"\"\"Run the workflow and stream state updates.\"\"\"\n",
    "    \n",
    "    graph = build_research_graph()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"topic\": \"The history of coffee\",\n",
    "        \"research\": \"\",\n",
    "        \"analysis\": \"\",\n",
    "        \"summary\": \"\",\n",
    "        \"current_step\": \"starting\"\n",
    "    }\n",
    "    \n",
    "    print(\"\ud83d\ude80 Starting research workflow...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # stream_mode=\"updates\" gives us just the changes from each node\n",
    "    async for event in graph.astream(initial_state, stream_mode=\"updates\"):\n",
    "        # event is a dict: {node_name: {state_updates}}\n",
    "        for node_name, updates in event.items():\n",
    "            print(f\"\\n\u2705 Node '{node_name}' completed\")\n",
    "            print(f\"   Step: {updates.get('current_step', 'N/A')}\")\n",
    "            \n",
    "            # Show preview of any text content\n",
    "            for key, value in updates.items():\n",
    "                if key != \"current_step\" and isinstance(value, str) and value:\n",
    "                    preview = value[:100] + \"...\" if len(value) > 100 else value\n",
    "                    print(f\"   {key}: {preview}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83c\udf89 Workflow complete!\")\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_with_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: stream_tokens.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: stream_tokens.py\n",
    "\n",
    "import asyncio\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, streaming=True)\n",
    "\n",
    "async def chat_node(state: ChatState) -> dict:\n",
    "    \"\"\"Generate a response to the user's message.\"\"\"\n",
    "    response = await llm.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def build_chat_graph():\n",
    "    workflow = StateGraph(ChatState)\n",
    "    workflow.add_node(\"chat\", chat_node)\n",
    "    workflow.add_edge(START, \"chat\")\n",
    "    workflow.add_edge(\"chat\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "async def chat_with_streaming(user_message: str):\n",
    "    \"\"\"Chat with token-by-token streaming.\"\"\"\n",
    "    \n",
    "    graph = build_chat_graph()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_message}]\n",
    "    }\n",
    "    \n",
    "    print(f\"You: {user_message}\")\n",
    "    print(\"AI: \", end=\"\", flush=True)\n",
    "    \n",
    "    # astream_events gives us fine-grained events including tokens\n",
    "    async for event in graph.astream_events(initial_state, version=\"v2\"):\n",
    "        # We're looking for chat model stream events\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            # Extract the token content\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            if hasattr(chunk, \"content\") and chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # New line after response\n",
    "\n",
    "async def main():\n",
    "    print(\"\ud83d\udcac Streaming Chat Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    await chat_with_streaming(\"Explain quantum computing in simple terms.\")\n",
    "    print()\n",
    "    await chat_with_streaming(\"What are its practical applications?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: custom_streaming.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: custom_streaming.py\n",
    "\n",
    "import asyncio\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import StreamWriter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ProcessingState(TypedDict):\n",
    "    items: list[str]\n",
    "    results: list[str]\n",
    "    status: str\n",
    "\n",
    "async def process_items(state: ProcessingState, writer: StreamWriter) -> dict:\n",
    "    \"\"\"Process items with progress updates.\"\"\"\n",
    "    \n",
    "    items = state[\"items\"]\n",
    "    results = []\n",
    "    \n",
    "    # Send initial status\n",
    "    writer({\"status\": \"starting\", \"message\": f\"Processing {len(items)} items...\"})\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        # Send progress update for each item\n",
    "        progress = (i + 1) / len(items) * 100\n",
    "        writer({\n",
    "            \"status\": \"processing\",\n",
    "            \"current_item\": item,\n",
    "            \"progress\": f\"{progress:.0f}%\",\n",
    "            \"message\": f\"Processing item {i + 1}/{len(items)}: {item}\"\n",
    "        })\n",
    "        \n",
    "        # Simulate processing time\n",
    "        await asyncio.sleep(0.5)\n",
    "        \n",
    "        # Process the item (just uppercase for demo)\n",
    "        results.append(item.upper())\n",
    "    \n",
    "    # Send completion status\n",
    "    writer({\"status\": \"complete\", \"message\": \"All items processed!\"})\n",
    "    \n",
    "    return {\"results\": results, \"status\": \"complete\"}\n",
    "\n",
    "def build_processing_graph():\n",
    "    workflow = StateGraph(ProcessingState)\n",
    "    workflow.add_node(\"process\", process_items)\n",
    "    workflow.add_edge(START, \"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "async def run_with_progress():\n",
    "    \"\"\"Run workflow with custom progress streaming.\"\"\"\n",
    "    \n",
    "    graph = build_processing_graph()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"items\": [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"],\n",
    "        \"results\": [],\n",
    "        \"status\": \"pending\"\n",
    "    }\n",
    "    \n",
    "    print(\"\ud83d\udd04 Processing with live progress...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use stream_mode=\"custom\" to receive our custom data\n",
    "    async for chunk in graph.astream(initial_state, stream_mode=\"custom\"):\n",
    "        # chunk contains our custom data from writer()\n",
    "        status = chunk.get(\"status\", \"\")\n",
    "        message = chunk.get(\"message\", \"\")\n",
    "        progress = chunk.get(\"progress\", \"\")\n",
    "        \n",
    "        if progress:\n",
    "            print(f\"  [{progress}] {message}\")\n",
    "        else:\n",
    "            print(f\"  {message}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"\u2705 Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_with_progress())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: combined_streaming.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: combined_streaming.py\n",
    "\n",
    "import asyncio\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import StreamWriter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    query: str\n",
    "    result: str\n",
    "    status: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "async def analyze_query(state: AnalysisState, writer: StreamWriter) -> dict:\n",
    "    \"\"\"Analyze a query with progress updates.\"\"\"\n",
    "    \n",
    "    writer({\"phase\": \"starting\", \"message\": \"Beginning analysis...\"})\n",
    "    \n",
    "    await asyncio.sleep(0.3)\n",
    "    \n",
    "    writer({\"phase\": \"thinking\", \"message\": \"Formulating response...\"})\n",
    "    \n",
    "    response = await llm.ainvoke(f\"Briefly analyze: {state['query']}\")\n",
    "    \n",
    "    writer({\"phase\": \"complete\", \"message\": \"Analysis complete!\"})\n",
    "    \n",
    "    return {\n",
    "        \"result\": response.content,\n",
    "        \"status\": \"done\"\n",
    "    }\n",
    "\n",
    "def build_analysis_graph():\n",
    "    workflow = StateGraph(AnalysisState)\n",
    "    workflow.add_node(\"analyze\", analyze_query)\n",
    "    workflow.add_edge(START, \"analyze\")\n",
    "    workflow.add_edge(\"analyze\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "async def run_combined_streaming():\n",
    "    \"\"\"Demonstrate combined streaming modes.\"\"\"\n",
    "    \n",
    "    graph = build_analysis_graph()\n",
    "    \n",
    "    initial_state = {\n",
    "        \"query\": \"What makes Python popular for AI development?\",\n",
    "        \"result\": \"\",\n",
    "        \"status\": \"pending\"\n",
    "    }\n",
    "    \n",
    "    print(\"\ud83d\udd0d Running analysis with combined streaming...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Combine \"updates\" and \"custom\" modes\n",
    "    async for mode, chunk in graph.astream(\n",
    "        initial_state, \n",
    "        stream_mode=[\"updates\", \"custom\"]\n",
    "    ):\n",
    "        if mode == \"custom\":\n",
    "            # Our custom progress updates\n",
    "            print(f\"  \ud83d\udce1 {chunk.get('message', chunk)}\")\n",
    "        elif mode == \"updates\":\n",
    "            # State updates from nodes\n",
    "            for node_name, updates in chunk.items():\n",
    "                print(f\"  \u2705 Node '{node_name}' updated state\")\n",
    "                if updates.get(\"result\"):\n",
    "                    preview = updates[\"result\"][:80] + \"...\"\n",
    "                    print(f\"     Result: {preview}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_combined_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: streaming_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.2\n",
    "# Save as: streaming_chat.py\n",
    "\n",
    "import asyncio\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, streaming=True)\n",
    "\n",
    "async def respond(state: ConversationState) -> dict:\n",
    "    \"\"\"Generate a response.\"\"\"\n",
    "    response = await llm.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def build_conversation_graph():\n",
    "    workflow = StateGraph(ConversationState)\n",
    "    workflow.add_node(\"respond\", respond)\n",
    "    workflow.add_edge(START, \"respond\")\n",
    "    workflow.add_edge(\"respond\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "async def stream_response(graph, messages: list) -> str:\n",
    "    \"\"\"Stream a response and return the complete text.\"\"\"\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    async for event in graph.astream_events(\n",
    "        {\"messages\": messages}, \n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        if event[\"event\"] == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            if hasattr(chunk, \"content\") and chunk.content:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response += chunk.content\n",
    "    \n",
    "    print()  # New line\n",
    "    return full_response\n",
    "\n",
    "async def chat_loop():\n",
    "    \"\"\"Interactive chat with streaming responses.\"\"\"\n",
    "    \n",
    "    graph = build_conversation_graph()\n",
    "    messages = []\n",
    "    \n",
    "    print(\"\ud83d\udcac Streaming Chat\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Type 'quit' to exit, 'clear' to reset conversation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"\ud83d\udc4b Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'clear':\n",
    "            messages = []\n",
    "            print(\"\ud83d\udd04 Conversation cleared!\")\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Stream the response\n",
    "        print(\"AI: \", end=\"\", flush=True)\n",
    "        response = await stream_response(graph, messages)\n",
    "        \n",
    "        # Add AI response to history\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(chat_loop())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.2.1: Research Assistant with Progress\n",
    "\n",
    "Build a research workflow that:\n",
    "- Takes a topic and generates 3 questions about it\n",
    "- Researches each question (simulated with LLM calls)\n",
    "- Streams progress for each research step (\"Researching question 1/3...\")\n",
    "- Compiles findings into a final report\n",
    "- Streams the final report token-by-token\n",
    "\n",
    "Requirements:\n",
    "- Use `StreamWriter` for progress updates\n",
    "- Use `astream_events` for token streaming\n",
    "- Show percentage completion during research phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.2.2: File Processor Simulation\n",
    "\n",
    "Create a workflow that simulates processing multiple files:\n",
    "- Accept a list of \"filenames\"\n",
    "- Process each file with a progress bar\n",
    "- Show status updates: \"Validating...\", \"Processing...\", \"Saving...\"\n",
    "- Report any \"errors\" encountered (simulate randomly)\n",
    "- Stream a summary report at the end\n",
    "\n",
    "Requirements:\n",
    "- Use custom streaming for progress bar effect\n",
    "- Show `[\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591] 40%` style progress\n",
    "- Handle simulated errors gracefully\n",
    "- Final summary shows success/failure counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.2.3: Multi-Step Form Validator\n",
    "\n",
    "Build a form validation workflow that:\n",
    "- Takes form data (name, email, phone, address)\n",
    "- Validates each field sequentially\n",
    "- Streams validation status for each field\n",
    "- Uses LLM to check if address looks valid\n",
    "- Returns validation results with helpful messages\n",
    "\n",
    "Requirements:\n",
    "- Stream \"Validating email...\" type messages\n",
    "- Show \u2714 or [X] for each field as validated\n",
    "- Stream the LLM's address analysis token-by-token\n",
    "- Final summary shows all validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.3: Parallel execution and map-reduce patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: parallel_research.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.3\n",
    "# Save as: parallel_research.py\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    # This field will receive results from parallel nodes\n",
    "    # operator.add means: combine by extending the list\n",
    "    findings: Annotated[list[str], operator.add]\n",
    "    summary: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "def search_wikipedia(state: ResearchState) -> dict:\n",
    "    \"\"\"Simulate searching Wikipedia.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"You are Wikipedia. Give 2 key facts about: {state['topic']}\"\n",
    "    )\n",
    "    return {\"findings\": [f\"[Wikipedia] {response.content}\"]}\n",
    "\n",
    "def search_academic(state: ResearchState) -> dict:\n",
    "    \"\"\"Simulate searching academic papers.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"You are an academic database. Give 2 scholarly insights about: {state['topic']}\"\n",
    "    )\n",
    "    return {\"findings\": [f\"[Academic] {response.content}\"]}\n",
    "\n",
    "def search_news(state: ResearchState) -> dict:\n",
    "    \"\"\"Simulate searching recent news.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"You are a news service. Give 2 recent developments about: {state['topic']}\"\n",
    "    )\n",
    "    return {\"findings\": [f\"[News] {response.content}\"]}\n",
    "\n",
    "def synthesize_findings(state: ResearchState) -> dict:\n",
    "    \"\"\"Combine all findings into a summary.\"\"\"\n",
    "    all_findings = \"\\n\\n\".join(state[\"findings\"])\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Synthesize these research findings into a brief summary:\\n\\n{all_findings}\"\n",
    "    )\n",
    "    return {\"summary\": response.content}\n",
    "\n",
    "def build_parallel_research_graph():\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"wikipedia\", search_wikipedia)\n",
    "    workflow.add_node(\"academic\", search_academic)\n",
    "    workflow.add_node(\"news\", search_news)\n",
    "    workflow.add_node(\"synthesize\", synthesize_findings)\n",
    "    \n",
    "    # Fan-out: START connects to all three search nodes\n",
    "    workflow.add_edge(START, \"wikipedia\")\n",
    "    workflow.add_edge(START, \"academic\")\n",
    "    workflow.add_edge(START, \"news\")\n",
    "    \n",
    "    # Fan-in: All search nodes connect to synthesize\n",
    "    workflow.add_edge(\"wikipedia\", \"synthesize\")\n",
    "    workflow.add_edge(\"academic\", \"synthesize\")\n",
    "    workflow.add_edge(\"news\", \"synthesize\")\n",
    "    \n",
    "    # End after synthesis\n",
    "    workflow.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_parallel_research():\n",
    "    graph = build_parallel_research_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"topic\": \"renewable energy\",\n",
    "        \"findings\": [],\n",
    "        \"summary\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udd2c Parallel Research Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nTopic: {result['topic']}\")\n",
    "    print(f\"\\nFindings from {len(result['findings'])} sources:\")\n",
    "    for finding in result[\"findings\"]:\n",
    "        print(f\"\\n{finding[:200]}...\")\n",
    "    print(f\"\\n\ud83d\udcdd Summary:\\n{result['summary']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_parallel_research()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: dynamic_parallel.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.3\n",
    "# Save as: dynamic_parallel.py\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Main graph state\n",
    "class MainState(TypedDict):\n",
    "    topic: str\n",
    "    questions: list[str]\n",
    "    # Results from parallel research will be collected here\n",
    "    answers: Annotated[list[str], operator.add]\n",
    "    final_report: str\n",
    "\n",
    "# State for each parallel research task\n",
    "class QuestionState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "def generate_questions(state: MainState) -> dict:\n",
    "    \"\"\"Generate research questions about the topic.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Generate exactly 3 specific research questions about: {state['topic']}. \"\n",
    "        \"Return only the questions, one per line.\"\n",
    "    )\n",
    "    \n",
    "    questions = [q.strip() for q in response.content.strip().split('\\n') if q.strip()]\n",
    "    print(f\"\ud83d\udccb Generated {len(questions)} questions\")\n",
    "    \n",
    "    return {\"questions\": questions}\n",
    "\n",
    "def route_to_research(state: MainState) -> list[Send]:\n",
    "    \"\"\"Create a parallel research task for each question.\"\"\"\n",
    "    \n",
    "    # Return a list of Send objects\n",
    "    # Each Send creates a parallel branch to the \"research\" node\n",
    "    return [\n",
    "        Send(\"research\", {\"question\": q}) \n",
    "        for q in state[\"questions\"]\n",
    "    ]\n",
    "\n",
    "def research_question(state: QuestionState) -> dict:\n",
    "    \"\"\"Research a single question.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Briefly answer this question in 2-3 sentences: {state['question']}\"\n",
    "    )\n",
    "    \n",
    "    answer = f\"Q: {state['question']}\\nA: {response.content}\"\n",
    "    print(f\"\u2705 Researched: {state['question'][:50]}...\")\n",
    "    \n",
    "    # This returns to the MAIN state's \"answers\" field\n",
    "    return {\"answers\": [answer]}\n",
    "\n",
    "def compile_report(state: MainState) -> dict:\n",
    "    \"\"\"Compile all answers into a final report.\"\"\"\n",
    "    all_answers = \"\\n\\n\".join(state[\"answers\"])\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Create a brief research report about '{state['topic']}' \"\n",
    "        f\"based on these Q&As:\\n\\n{all_answers}\"\n",
    "    )\n",
    "    \n",
    "    return {\"final_report\": response.content}\n",
    "\n",
    "def build_dynamic_parallel_graph():\n",
    "    workflow = StateGraph(MainState)\n",
    "    \n",
    "    workflow.add_node(\"generate\", generate_questions)\n",
    "    workflow.add_node(\"research\", research_question)\n",
    "    workflow.add_node(\"compile\", compile_report)\n",
    "    \n",
    "    # Start by generating questions\n",
    "    workflow.add_edge(START, \"generate\")\n",
    "    \n",
    "    # Use conditional edge with Send for dynamic fan-out\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        route_to_research,\n",
    "        [\"research\"]  # Possible destinations\n",
    "    )\n",
    "    \n",
    "    # All research tasks feed into compile\n",
    "    workflow.add_edge(\"research\", \"compile\")\n",
    "    workflow.add_edge(\"compile\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_dynamic_research():\n",
    "    graph = build_dynamic_parallel_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"topic\": \"artificial intelligence ethics\",\n",
    "        \"questions\": [],\n",
    "        \"answers\": [],\n",
    "        \"final_report\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udcca Dynamic Parallel Research Complete!\")\n",
    "    print(f\"\\nQuestions researched: {len(result['questions'])}\")\n",
    "    print(f\"\\n\ud83d\udcdd Final Report:\\n{result['final_report']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_dynamic_research()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: map_reduce_docs.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.3\n",
    "# Save as: map_reduce_docs.py\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "class MapReduceState(TypedDict):\n",
    "    documents: list[str]\n",
    "    summaries: Annotated[list[str], operator.add]\n",
    "    final_summary: str\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    document: str\n",
    "    doc_index: int\n",
    "\n",
    "def distribute_documents(state: MapReduceState) -> list[Send]:\n",
    "    \"\"\"Send each document to a parallel summarization node.\"\"\"\n",
    "    return [\n",
    "        Send(\"summarize_doc\", {\"document\": doc, \"doc_index\": i})\n",
    "        for i, doc in enumerate(state[\"documents\"])\n",
    "    ]\n",
    "\n",
    "def summarize_document(state: DocumentState) -> dict:\n",
    "    \"\"\"Summarize a single document.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Summarize this text in 2-3 sentences:\\n\\n{state['document']}\"\n",
    "    )\n",
    "    \n",
    "    summary = f\"[Doc {state['doc_index'] + 1}] {response.content}\"\n",
    "    print(f\"\ud83d\udcc4 Summarized document {state['doc_index'] + 1}\")\n",
    "    \n",
    "    return {\"summaries\": [summary]}\n",
    "\n",
    "def combine_summaries(state: MapReduceState) -> dict:\n",
    "    \"\"\"Reduce: combine all summaries into a final summary.\"\"\"\n",
    "    all_summaries = \"\\n\\n\".join(state[\"summaries\"])\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Combine these document summaries into one coherent summary:\\n\\n{all_summaries}\"\n",
    "    )\n",
    "    \n",
    "    return {\"final_summary\": response.content}\n",
    "\n",
    "def build_map_reduce_graph():\n",
    "    workflow = StateGraph(MapReduceState)\n",
    "    \n",
    "    # We'll use a dummy \"start\" node to trigger the map\n",
    "    workflow.add_node(\"start_map\", lambda state: {})\n",
    "    workflow.add_node(\"summarize_doc\", summarize_document)\n",
    "    workflow.add_node(\"reduce\", combine_summaries)\n",
    "    \n",
    "    workflow.add_edge(START, \"start_map\")\n",
    "    \n",
    "    # Map: distribute to parallel workers\n",
    "    workflow.add_conditional_edges(\n",
    "        \"start_map\",\n",
    "        distribute_documents,\n",
    "        [\"summarize_doc\"]\n",
    "    )\n",
    "    \n",
    "    # Reduce: combine results\n",
    "    workflow.add_edge(\"summarize_doc\", \"reduce\")\n",
    "    workflow.add_edge(\"reduce\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_map_reduce():\n",
    "    graph = build_map_reduce_graph()\n",
    "    \n",
    "    # Sample documents (in real use, these could be much longer)\n",
    "    documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity. \"\n",
    "        \"It was created by Guido van Rossum and first released in 1991. \"\n",
    "        \"Python emphasizes code readability and allows programmers to express concepts in fewer lines.\",\n",
    "        \n",
    "        \"Machine learning is a subset of artificial intelligence. \"\n",
    "        \"It enables computers to learn from data without being explicitly programmed. \"\n",
    "        \"Common applications include image recognition and natural language processing.\",\n",
    "        \n",
    "        \"Climate change refers to long-term shifts in global temperatures. \"\n",
    "        \"Human activities, particularly burning fossil fuels, are the primary cause. \"\n",
    "        \"Effects include rising sea levels, extreme weather, and ecosystem disruption.\",\n",
    "        \n",
    "        \"The Internet of Things connects everyday devices to the internet. \"\n",
    "        \"Smart homes, wearables, and industrial sensors are common examples. \"\n",
    "        \"IoT is expected to have 75 billion connected devices by 2025.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\ud83d\udcda Processing {len(documents)} documents in parallel...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"documents\": documents,\n",
    "        \"summaries\": [],\n",
    "        \"final_summary\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"\u2705 Processed {len(result['summaries'])} documents\")\n",
    "    print(f\"\\n\ud83d\udcdd Final Combined Summary:\\n{result['final_summary']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_map_reduce()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: parallel_voting.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.3\n",
    "# Save as: parallel_voting.py\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class VotingState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    votes: Annotated[list[dict], operator.add]\n",
    "    final_decision: str\n",
    "\n",
    "def create_voter(name: str, perspective: str):\n",
    "    \"\"\"Factory function to create voter nodes.\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "    \n",
    "    def vote(state: VotingState) -> dict:\n",
    "        response = llm.invoke(\n",
    "            f\"You are an expert in {perspective}. \"\n",
    "            f\"Question: {state['question']}\\n\"\n",
    "            f\"Context: {state['context']}\\n\\n\"\n",
    "            f\"Give your opinion in 2-3 sentences, then vote YES or NO on the last line.\"\n",
    "        )\n",
    "        \n",
    "        content = response.content\n",
    "        vote_value = \"YES\" if \"YES\" in content.upper().split('\\n')[-1] else \"NO\"\n",
    "        \n",
    "        print(f\"\ud83d\uddf3\ufe0f  {name} voted: {vote_value}\")\n",
    "        \n",
    "        return {\"votes\": [{\n",
    "            \"voter\": name,\n",
    "            \"perspective\": perspective,\n",
    "            \"reasoning\": content,\n",
    "            \"vote\": vote_value\n",
    "        }]}\n",
    "    \n",
    "    return vote\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "def aggregate_votes(state: VotingState) -> dict:\n",
    "    \"\"\"Count votes and make final decision.\"\"\"\n",
    "    yes_votes = sum(1 for v in state[\"votes\"] if v[\"vote\"] == \"YES\")\n",
    "    no_votes = len(state[\"votes\"]) - yes_votes\n",
    "    \n",
    "    # Compile reasoning\n",
    "    all_reasoning = \"\\n\\n\".join([\n",
    "        f\"{v['voter']} ({v['perspective']}): {v['reasoning']}\"\n",
    "        for v in state[\"votes\"]\n",
    "    ])\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"Based on these expert opinions:\\n\\n{all_reasoning}\\n\\n\"\n",
    "        f\"Votes: {yes_votes} YES, {no_votes} NO\\n\\n\"\n",
    "        f\"Provide a final decision with brief justification.\"\n",
    "    )\n",
    "    \n",
    "    return {\"final_decision\": response.content}\n",
    "\n",
    "def build_voting_graph():\n",
    "    workflow = StateGraph(VotingState)\n",
    "    \n",
    "    # Create diverse voter nodes\n",
    "    workflow.add_node(\"technical\", create_voter(\"Technical Expert\", \"technology and engineering\"))\n",
    "    workflow.add_node(\"business\", create_voter(\"Business Analyst\", \"business strategy and ROI\"))\n",
    "    workflow.add_node(\"ethics\", create_voter(\"Ethics Advisor\", \"ethics and social impact\"))\n",
    "    workflow.add_node(\"aggregate\", aggregate_votes)\n",
    "    \n",
    "    # Fan-out to all voters\n",
    "    workflow.add_edge(START, \"technical\")\n",
    "    workflow.add_edge(START, \"business\")\n",
    "    workflow.add_edge(START, \"ethics\")\n",
    "    \n",
    "    # Fan-in to aggregation\n",
    "    workflow.add_edge(\"technical\", \"aggregate\")\n",
    "    workflow.add_edge(\"business\", \"aggregate\")\n",
    "    workflow.add_edge(\"ethics\", \"aggregate\")\n",
    "    \n",
    "    workflow.add_edge(\"aggregate\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def run_voting():\n",
    "    graph = build_voting_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"question\": \"Should we implement AI-powered customer service chatbots?\",\n",
    "        \"context\": \"Our company handles 10,000 customer inquiries daily. \"\n",
    "                   \"Current wait times average 15 minutes. \"\n",
    "                   \"Implementation cost is $500,000 with ongoing costs of $50,000/year.\",\n",
    "        \"votes\": [],\n",
    "        \"final_decision\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udcca Voting Results\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for vote in result[\"votes\"]:\n",
    "        print(f\"\\n{vote['voter']}: {vote['vote']}\")\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfc6 Final Decision:\\n{result['final_decision']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_voting()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.3.1: Multi-Source Fact Checker\n",
    "\n",
    "Build a fact-checking system that:\n",
    "- Takes a claim to verify\n",
    "- Searches 4 different \"sources\" in parallel (simulate with different LLM prompts)\n",
    "- Each source rates the claim's accuracy (1-10) with reasoning\n",
    "- Aggregates ratings and provides a final verdict with confidence level\n",
    "\n",
    "Requirements:\n",
    "- Use fan-out/fan-in for parallel searches\n",
    "- Calculate average rating and standard deviation\n",
    "- Final verdict should note if sources disagree significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.3.2: Parallel Document Analyzer\n",
    "\n",
    "Create a document analysis pipeline that:\n",
    "- Takes a list of documents\n",
    "- For each document in parallel: extract key themes, identify sentiment, find action items\n",
    "- Combines all results into a comprehensive report\n",
    "- Groups similar themes across documents\n",
    "\n",
    "Requirements:\n",
    "- Use `Send` for dynamic parallelization\n",
    "- Each document analysis should return structured data\n",
    "- Final report should deduplicate and rank themes by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.3.3: Competitive Analysis System\n",
    "\n",
    "Build a system that analyzes competitors in parallel:\n",
    "- Takes a company name and list of competitors\n",
    "- For each competitor in parallel: analyze strengths, weaknesses, market position\n",
    "- Generates a comparative matrix\n",
    "- Produces strategic recommendations\n",
    "\n",
    "Requirements:\n",
    "- Use `Send` to handle variable number of competitors\n",
    "- Each analysis should follow a consistent structure\n",
    "- Final output should include a ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.4: Subgraphs and modular design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: shared_state_subgraph.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.4\n",
    "# Save as: shared_state_subgraph.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "class SharedState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    analysis: str\n",
    "    final_report: str\n",
    "\n",
    "def build_research_subgraph():\n",
    "    \"\"\"Build a reusable research subgraph.\"\"\"\n",
    "    \n",
    "    def gather_info(state: SharedState) -> dict:\n",
    "        \"\"\"Gather information about the topic.\"\"\"\n",
    "        response = llm.invoke(\n",
    "            f\"Provide 3 key facts about: {state['topic']}\"\n",
    "        )\n",
    "        return {\"research\": response.content}\n",
    "    \n",
    "    def analyze_info(state: SharedState) -> dict:\n",
    "        \"\"\"Analyze the gathered information.\"\"\"\n",
    "        response = llm.invoke(\n",
    "            f\"Analyze these facts and identify patterns:\\n{state['research']}\"\n",
    "        )\n",
    "        return {\"analysis\": response.content}\n",
    "    \n",
    "    # Build the subgraph\n",
    "    subgraph = StateGraph(SharedState)\n",
    "    subgraph.add_node(\"gather\", gather_info)\n",
    "    subgraph.add_node(\"analyze\", analyze_info)\n",
    "    \n",
    "    subgraph.add_edge(START, \"gather\")\n",
    "    subgraph.add_edge(\"gather\", \"analyze\")\n",
    "    subgraph.add_edge(\"analyze\", END)\n",
    "    \n",
    "    return subgraph.compile()\n",
    "\n",
    "def build_parent_graph():\n",
    "    \"\"\"Build parent graph that uses the research subgraph.\"\"\"\n",
    "    \n",
    "    # Get our compiled subgraph\n",
    "    research_subgraph = build_research_subgraph()\n",
    "    \n",
    "    def write_report(state: SharedState) -> dict:\n",
    "        \"\"\"Write final report based on research and analysis.\"\"\"\n",
    "        response = llm.invoke(\n",
    "            f\"Write a brief report about {state['topic']}.\\n\"\n",
    "            f\"Research: {state['research']}\\n\"\n",
    "            f\"Analysis: {state['analysis']}\"\n",
    "        )\n",
    "        return {\"final_report\": response.content}\n",
    "    \n",
    "    # Build parent graph\n",
    "    parent = StateGraph(SharedState)\n",
    "    \n",
    "    # Add the subgraph as a node!\n",
    "    parent.add_node(\"research\", research_subgraph)\n",
    "    parent.add_node(\"report\", write_report)\n",
    "    \n",
    "    parent.add_edge(START, \"research\")\n",
    "    parent.add_edge(\"research\", \"report\")\n",
    "    parent.add_edge(\"report\", END)\n",
    "    \n",
    "    return parent.compile()\n",
    "\n",
    "def run_shared_state_example():\n",
    "    graph = build_parent_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"topic\": \"renewable energy\",\n",
    "        \"research\": \"\",\n",
    "        \"analysis\": \"\",\n",
    "        \"final_report\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcca Subgraph Example: Shared State\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nTopic: {result['topic']}\")\n",
    "    print(f\"\\n\ud83d\udcda Research:\\n{result['research'][:200]}...\")\n",
    "    print(f\"\\n\ud83d\udd0d Analysis:\\n{result['analysis'][:200]}...\")\n",
    "    print(f\"\\n\ud83d\udcdd Final Report:\\n{result['final_report']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_shared_state_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: different_state_subgraph.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.4\n",
    "# Save as: different_state_subgraph.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Parent uses these field names\n",
    "class ParentState(TypedDict):\n",
    "    user_query: str\n",
    "    validated_query: str\n",
    "    search_results: str\n",
    "    final_answer: str\n",
    "\n",
    "# Subgraph uses different field names\n",
    "class ValidationState(TypedDict):\n",
    "    input_text: str\n",
    "    is_valid: bool\n",
    "    cleaned_text: str\n",
    "    validation_notes: str\n",
    "\n",
    "def build_validation_subgraph():\n",
    "    \"\"\"Build a validation subgraph with its own state schema.\"\"\"\n",
    "    \n",
    "    def check_validity(state: ValidationState) -> dict:\n",
    "        \"\"\"Check if input is valid.\"\"\"\n",
    "        text = state[\"input_text\"]\n",
    "        \n",
    "        # Simple validation rules\n",
    "        is_valid = len(text) > 3 and not text.isdigit()\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": is_valid,\n",
    "            \"validation_notes\": \"Valid query\" if is_valid else \"Query too short or invalid\"\n",
    "        }\n",
    "    \n",
    "    def clean_text(state: ValidationState) -> dict:\n",
    "        \"\"\"Clean and normalize the text.\"\"\"\n",
    "        if not state[\"is_valid\"]:\n",
    "            return {\"cleaned_text\": \"\"}\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned = state[\"input_text\"].strip().lower()\n",
    "        return {\"cleaned_text\": cleaned}\n",
    "    \n",
    "    subgraph = StateGraph(ValidationState)\n",
    "    subgraph.add_node(\"check\", check_validity)\n",
    "    subgraph.add_node(\"clean\", clean_text)\n",
    "    \n",
    "    subgraph.add_edge(START, \"check\")\n",
    "    subgraph.add_edge(\"check\", \"clean\")\n",
    "    subgraph.add_edge(\"clean\", END)\n",
    "    \n",
    "    return subgraph.compile()\n",
    "\n",
    "# Compile subgraph once\n",
    "validation_subgraph = build_validation_subgraph()\n",
    "\n",
    "def call_validation_subgraph(state: ParentState) -> dict:\n",
    "    \"\"\"Wrapper that transforms state for the validation subgraph.\"\"\"\n",
    "    \n",
    "    # Transform: Parent state \u2192 Subgraph state\n",
    "    subgraph_input = {\n",
    "        \"input_text\": state[\"user_query\"],\n",
    "        \"is_valid\": False,\n",
    "        \"cleaned_text\": \"\",\n",
    "        \"validation_notes\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Call the subgraph\n",
    "    subgraph_output = validation_subgraph.invoke(subgraph_input)\n",
    "    \n",
    "    # Transform: Subgraph state \u2192 Parent state\n",
    "    return {\n",
    "        \"validated_query\": subgraph_output[\"cleaned_text\"]\n",
    "    }\n",
    "\n",
    "def search_and_answer(state: ParentState) -> dict:\n",
    "    \"\"\"Search and generate answer.\"\"\"\n",
    "    query = state[\"validated_query\"]\n",
    "    \n",
    "    if not query:\n",
    "        return {\n",
    "            \"search_results\": \"\",\n",
    "            \"final_answer\": \"Sorry, I couldn't understand your query.\"\n",
    "        }\n",
    "    \n",
    "    # Simulate search\n",
    "    response = llm.invoke(f\"Answer this query briefly: {query}\")\n",
    "    \n",
    "    return {\n",
    "        \"search_results\": f\"Found results for: {query}\",\n",
    "        \"final_answer\": response.content\n",
    "    }\n",
    "\n",
    "def build_parent_with_transform():\n",
    "    \"\"\"Build parent graph with state transformation.\"\"\"\n",
    "    \n",
    "    parent = StateGraph(ParentState)\n",
    "    \n",
    "    # Use the wrapper function as a node\n",
    "    parent.add_node(\"validate\", call_validation_subgraph)\n",
    "    parent.add_node(\"search\", search_and_answer)\n",
    "    \n",
    "    parent.add_edge(START, \"validate\")\n",
    "    parent.add_edge(\"validate\", \"search\")\n",
    "    parent.add_edge(\"search\", END)\n",
    "    \n",
    "    return parent.compile()\n",
    "\n",
    "def run_different_state_example():\n",
    "    graph = build_parent_with_transform()\n",
    "    \n",
    "    # Test with valid query\n",
    "    result = graph.invoke({\n",
    "        \"user_query\": \"What is machine learning?\",\n",
    "        \"validated_query\": \"\",\n",
    "        \"search_results\": \"\",\n",
    "        \"final_answer\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcca Subgraph Example: Different State Schemas\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nOriginal Query: {result['user_query']}\")\n",
    "    print(f\"Validated Query: {result['validated_query']}\")\n",
    "    print(f\"Answer: {result['final_answer'][:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_different_state_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: component_library.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.4\n",
    "# Save as: component_library.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SHARED STATE\n",
    "# =============================================================================\n",
    "\n",
    "class WorkflowState(TypedDict):\n",
    "    input_text: str\n",
    "    summary: str\n",
    "    sentiment: str\n",
    "    keywords: list[str]\n",
    "    translation: str\n",
    "    final_output: str\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CORE FUNCTIONS (defined once, reused everywhere)\n",
    "# =============================================================================\n",
    "\n",
    "def summarize(state: WorkflowState) -> dict:\n",
    "    \"\"\"Summarize input text.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Summarize this in 2 sentences:\\n{state['input_text']}\"\n",
    "    )\n",
    "    return {\"summary\": response.content}\n",
    "\n",
    "\n",
    "def analyze_sentiment(state: WorkflowState) -> dict:\n",
    "    \"\"\"Analyze sentiment of input text.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"What is the sentiment of this text? \"\n",
    "        f\"Answer POSITIVE, NEGATIVE, or NEUTRAL:\\n{state['input_text']}\"\n",
    "    )\n",
    "    sentiment = response.content.strip().upper()\n",
    "    if sentiment not in [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"]:\n",
    "        sentiment = \"NEUTRAL\"\n",
    "    return {\"sentiment\": sentiment}\n",
    "\n",
    "\n",
    "def extract_keywords(state: WorkflowState) -> dict:\n",
    "    \"\"\"Extract keywords from input text.\"\"\"\n",
    "    response = llm.invoke(\n",
    "        f\"Extract 5 keywords from this text as a comma-separated list:\\n{state['input_text']}\"\n",
    "    )\n",
    "    keywords = [k.strip() for k in response.content.split(\",\")]\n",
    "    return {\"keywords\": keywords[:5]}\n",
    "\n",
    "\n",
    "def compile_results(state: WorkflowState) -> dict:\n",
    "    \"\"\"Compile all analysis results into final output.\"\"\"\n",
    "    output = (\n",
    "        f\"\ud83d\udcdd Summary: {state['summary']}\\n\"\n",
    "        f\"\ud83d\ude0a Sentiment: {state['sentiment']}\\n\"\n",
    "        f\"\ud83d\udd11 Keywords: {', '.join(state['keywords'])}\"\n",
    "    )\n",
    "    return {\"final_output\": output}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUBGRAPH WRAPPERS (for modular/sequential composition)\n",
    "# =============================================================================\n",
    "\n",
    "def build_summarizer():\n",
    "    \"\"\"Reusable summarization component.\"\"\"\n",
    "    graph = StateGraph(WorkflowState)\n",
    "    graph.add_node(\"summarize\", summarize)  # Reuse core function\n",
    "    graph.add_edge(START, \"summarize\")\n",
    "    graph.add_edge(\"summarize\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def build_sentiment_analyzer():\n",
    "    \"\"\"Reusable sentiment analysis component.\"\"\"\n",
    "    graph = StateGraph(WorkflowState)\n",
    "    graph.add_node(\"sentiment\", analyze_sentiment)  # Reuse core function\n",
    "    graph.add_edge(START, \"sentiment\")\n",
    "    graph.add_edge(\"sentiment\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "def build_keyword_extractor():\n",
    "    \"\"\"Reusable keyword extraction component.\"\"\"\n",
    "    graph = StateGraph(WorkflowState)\n",
    "    graph.add_node(\"keywords\", extract_keywords)  # Reuse core function\n",
    "    graph.add_edge(START, \"keywords\")\n",
    "    graph.add_edge(\"keywords\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SEQUENTIAL PIPELINE (uses subgraphs)\n",
    "# =============================================================================\n",
    "\n",
    "def build_analysis_pipeline():\n",
    "    \"\"\"Compose components into a sequential analysis pipeline.\n",
    "    \n",
    "    Uses compiled subgraphs - works great for sequential flows.\n",
    "    \"\"\"\n",
    "    summarizer = build_summarizer()\n",
    "    sentiment_analyzer = build_sentiment_analyzer()\n",
    "    keyword_extractor = build_keyword_extractor()\n",
    "    \n",
    "    pipeline = StateGraph(WorkflowState)\n",
    "    \n",
    "    # Add subgraphs as nodes\n",
    "    pipeline.add_node(\"summarize\", summarizer)\n",
    "    pipeline.add_node(\"sentiment\", sentiment_analyzer)\n",
    "    pipeline.add_node(\"keywords\", keyword_extractor)\n",
    "    pipeline.add_node(\"compile\", compile_results)\n",
    "    \n",
    "    # Sequential flow\n",
    "    pipeline.add_edge(START, \"summarize\")\n",
    "    pipeline.add_edge(\"summarize\", \"sentiment\")\n",
    "    pipeline.add_edge(\"sentiment\", \"keywords\")\n",
    "    pipeline.add_edge(\"keywords\", \"compile\")\n",
    "    pipeline.add_edge(\"compile\", END)\n",
    "    \n",
    "    return pipeline.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PARALLEL PIPELINE (uses functions directly)\n",
    "# =============================================================================\n",
    "\n",
    "def build_parallel_analysis():\n",
    "    \"\"\"Run analysis components in parallel.\n",
    "    \n",
    "    Uses functions directly instead of subgraphs. Compiled subgraphs\n",
    "    can't run in parallel because they process the full state, causing\n",
    "    concurrent update conflicts on shared input fields.\n",
    "    \"\"\"\n",
    "    pipeline = StateGraph(WorkflowState)\n",
    "    \n",
    "    # Use core functions directly for parallel execution\n",
    "    pipeline.add_node(\"summarize\", summarize)\n",
    "    pipeline.add_node(\"sentiment\", analyze_sentiment)\n",
    "    pipeline.add_node(\"keywords\", extract_keywords)\n",
    "    pipeline.add_node(\"compile\", compile_results)\n",
    "    \n",
    "    # Parallel: all three start from START\n",
    "    pipeline.add_edge(START, \"summarize\")\n",
    "    pipeline.add_edge(START, \"sentiment\")\n",
    "    pipeline.add_edge(START, \"keywords\")\n",
    "    \n",
    "    # All converge to compile\n",
    "    pipeline.add_edge(\"summarize\", \"compile\")\n",
    "    pipeline.add_edge(\"sentiment\", \"compile\")\n",
    "    pipeline.add_edge(\"keywords\", \"compile\")\n",
    "    \n",
    "    pipeline.add_edge(\"compile\", END)\n",
    "    \n",
    "    return pipeline.compile()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMO\n",
    "# =============================================================================\n",
    "\n",
    "def run_component_library_example():\n",
    "    sample_text = \"\"\"\n",
    "    Artificial intelligence is transforming healthcare in remarkable ways.\n",
    "    From early disease detection to personalized treatment plans, AI is\n",
    "    helping doctors provide better care. However, concerns about privacy\n",
    "    and the need for human oversight remain important considerations.\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"input_text\": sample_text,\n",
    "        \"summary\": \"\",\n",
    "        \"sentiment\": \"\",\n",
    "        \"keywords\": [],\n",
    "        \"translation\": \"\",\n",
    "        \"final_output\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Test sequential pipeline\n",
    "    print(\"\ud83d\udcca Sequential Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    sequential = build_analysis_pipeline()\n",
    "    result = sequential.invoke(initial_state)\n",
    "    print(result[\"final_output\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    # Test parallel pipeline\n",
    "    print(\"\u26a1 Parallel Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    parallel = build_parallel_analysis()\n",
    "    result = parallel.invoke(initial_state)\n",
    "    print(result[\"final_output\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_component_library_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: nested_subgraphs.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.4\n",
    "# Save as: nested_subgraphs.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    document: str\n",
    "    spell_checked: str\n",
    "    grammar_fixed: str\n",
    "    style_improved: str\n",
    "    final_document: str\n",
    "\n",
    "# Level 3 (deepest): Individual editing operations\n",
    "\n",
    "def build_spell_checker():\n",
    "    \"\"\"Deepest level: spell checking.\"\"\"\n",
    "    \n",
    "    def check_spelling(state: DocumentState) -> dict:\n",
    "        response = llm.invoke(\n",
    "            f\"Fix any spelling errors in this text. Return only the corrected text:\\n{state['document']}\"\n",
    "        )\n",
    "        return {\"spell_checked\": response.content}\n",
    "    \n",
    "    graph = StateGraph(DocumentState)\n",
    "    graph.add_node(\"spell\", check_spelling)\n",
    "    graph.add_edge(START, \"spell\")\n",
    "    graph.add_edge(\"spell\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "def build_grammar_fixer():\n",
    "    \"\"\"Deepest level: grammar fixing.\"\"\"\n",
    "    \n",
    "    def fix_grammar(state: DocumentState) -> dict:\n",
    "        text = state.get(\"spell_checked\") or state[\"document\"]\n",
    "        response = llm.invoke(\n",
    "            f\"Fix any grammar errors. Return only the corrected text:\\n{text}\"\n",
    "        )\n",
    "        return {\"grammar_fixed\": response.content}\n",
    "    \n",
    "    graph = StateGraph(DocumentState)\n",
    "    graph.add_node(\"grammar\", fix_grammar)\n",
    "    graph.add_edge(START, \"grammar\")\n",
    "    graph.add_edge(\"grammar\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "# Level 2: Editing subgraph that contains spell and grammar\n",
    "\n",
    "def build_editing_subgraph():\n",
    "    \"\"\"Middle level: combines spell check and grammar fix.\"\"\"\n",
    "    \n",
    "    spell_checker = build_spell_checker()\n",
    "    grammar_fixer = build_grammar_fixer()\n",
    "    \n",
    "    editing = StateGraph(DocumentState)\n",
    "    editing.add_node(\"spell\", spell_checker)\n",
    "    editing.add_node(\"grammar\", grammar_fixer)\n",
    "    \n",
    "    editing.add_edge(START, \"spell\")\n",
    "    editing.add_edge(\"spell\", \"grammar\")\n",
    "    editing.add_edge(\"grammar\", END)\n",
    "    \n",
    "    return editing.compile()\n",
    "\n",
    "# Level 1 (top): Full document processor\n",
    "\n",
    "def build_document_processor():\n",
    "    \"\"\"Top level: full document processing pipeline.\"\"\"\n",
    "    \n",
    "    editing_subgraph = build_editing_subgraph()\n",
    "    \n",
    "    def improve_style(state: DocumentState) -> dict:\n",
    "        text = state.get(\"grammar_fixed\") or state[\"document\"]\n",
    "        response = llm.invoke(\n",
    "            f\"Improve the writing style of this text. Make it clearer and more engaging:\\n{text}\"\n",
    "        )\n",
    "        return {\"style_improved\": response.content}\n",
    "    \n",
    "    def finalize(state: DocumentState) -> dict:\n",
    "        return {\"final_document\": state[\"style_improved\"]}\n",
    "    \n",
    "    processor = StateGraph(DocumentState)\n",
    "    \n",
    "    # Editing subgraph (which contains spell + grammar)\n",
    "    processor.add_node(\"edit\", editing_subgraph)\n",
    "    processor.add_node(\"style\", improve_style)\n",
    "    processor.add_node(\"finalize\", finalize)\n",
    "    \n",
    "    processor.add_edge(START, \"edit\")\n",
    "    processor.add_edge(\"edit\", \"style\")\n",
    "    processor.add_edge(\"style\", \"finalize\")\n",
    "    processor.add_edge(\"finalize\", END)\n",
    "    \n",
    "    return processor.compile()\n",
    "\n",
    "def run_nested_example():\n",
    "    graph = build_document_processor()\n",
    "    \n",
    "    messy_text = \"\"\"\n",
    "    Their going to the store becuase they need supplys.\n",
    "    The weather is real nice today and me and him wants to go outside.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"document\": messy_text,\n",
    "        \"spell_checked\": \"\",\n",
    "        \"grammar_fixed\": \"\",\n",
    "        \"style_improved\": \"\",\n",
    "        \"final_document\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcc4 Nested Subgraph Document Processor\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\n\u274c Original:\\n{messy_text}\")\n",
    "    print(f\"\\n\u2705 Final:\\n{result['final_document']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_nested_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.4.1: Content Processing Pipeline\n",
    "\n",
    "Build a modular content processing system with these subgraphs:\n",
    "- **InputValidator** - Checks content length, detects language\n",
    "- **ContentEnricher** - Adds metadata, extracts entities\n",
    "- **OutputFormatter** - Formats for different outputs (JSON, Markdown, plain text)\n",
    "\n",
    "Requirements:\n",
    "- Each subgraph should be independently testable\n",
    "- Parent graph should compose them sequentially\n",
    "- Use shared state schema\n",
    "- Add a configuration option to skip enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.4.2: Multi-Format Translator\n",
    "\n",
    "Create a translation system using subgraphs with different state schemas:\n",
    "- **Parent state**: `source_text`, `source_lang`, `target_langs`, `translations`\n",
    "- **Translation subgraph state**: `text`, `from_lang`, `to_lang`, `result`\n",
    "- Translate to multiple languages using dynamic Send + subgraph\n",
    "\n",
    "Requirements:\n",
    "- Subgraph has different state schema than parent\n",
    "- Create wrapper function for state transformation\n",
    "- Support translating to 3+ languages in parallel\n",
    "- Combine all translations in parent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.4.3: Document Review System\n",
    "\n",
    "Build a hierarchical document review system:\n",
    "- **Level 1**: DocumentReviewer (orchestrates everything)\n",
    "- **Level 2**: TechnicalReview, EditorialReview subgraphs\n",
    "- **Level 3**: Within TechnicalReview: FactChecker, CodeValidator\n",
    "\n",
    "Requirements:\n",
    "- At least 3 levels of nesting\n",
    "- Technical and Editorial reviews run in parallel\n",
    "- Each subgraph should have clear input/output\n",
    "- Final output combines all review feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.5: Dynamic graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: graph_factory.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.5\n",
    "# Save as: graph_factory.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "class ProcessingState(TypedDict):\n",
    "    text: str\n",
    "    cleaned: str\n",
    "    summarized: str\n",
    "    translated: str\n",
    "    final_output: str\n",
    "\n",
    "def clean_text(state: ProcessingState) -> dict:\n",
    "    \"\"\"Remove extra whitespace and normalize text.\"\"\"\n",
    "    cleaned = \" \".join(state[\"text\"].split())\n",
    "    return {\"cleaned\": cleaned}\n",
    "\n",
    "def summarize_text(state: ProcessingState) -> dict:\n",
    "    \"\"\"Summarize the text.\"\"\"\n",
    "    text = state.get(\"cleaned\") or state[\"text\"]\n",
    "    response = llm.invoke(f\"Summarize in 2 sentences:\\n{text}\")\n",
    "    return {\"summarized\": response.content}\n",
    "\n",
    "def translate_text(state: ProcessingState) -> dict:\n",
    "    \"\"\"Translate to Spanish.\"\"\"\n",
    "    text = state.get(\"summarized\") or state.get(\"cleaned\") or state[\"text\"]\n",
    "    response = llm.invoke(f\"Translate to Spanish:\\n{text}\")\n",
    "    return {\"translated\": response.content}\n",
    "\n",
    "def format_output(state: ProcessingState) -> dict:\n",
    "    \"\"\"Compile final output.\"\"\"\n",
    "    parts = []\n",
    "    if state.get(\"cleaned\"):\n",
    "        parts.append(f\"Cleaned: {state['cleaned'][:100]}...\")\n",
    "    if state.get(\"summarized\"):\n",
    "        parts.append(f\"Summary: {state['summarized']}\")\n",
    "    if state.get(\"translated\"):\n",
    "        parts.append(f\"Spanish: {state['translated']}\")\n",
    "    return {\"final_output\": \"\\n\\n\".join(parts)}\n",
    "\n",
    "# Map of available nodes\n",
    "AVAILABLE_NODES = {\n",
    "    \"clean\": clean_text,\n",
    "    \"summarize\": summarize_text,\n",
    "    \"translate\": translate_text,\n",
    "    \"format\": format_output\n",
    "}\n",
    "\n",
    "def build_pipeline_from_config(config: dict):\n",
    "    \"\"\"\n",
    "    Build a graph dynamically from configuration.\n",
    "    \n",
    "    Config format:\n",
    "    {\n",
    "        \"steps\": [\"clean\", \"summarize\", \"translate\"],\n",
    "        \"include_format\": True\n",
    "    }\n",
    "    \"\"\"\n",
    "    steps = config.get(\"steps\", [])\n",
    "    include_format = config.get(\"include_format\", True)\n",
    "    \n",
    "    if not steps:\n",
    "        raise ValueError(\"Config must include at least one step\")\n",
    "    \n",
    "    # Validate all steps exist\n",
    "    for step in steps:\n",
    "        if step not in AVAILABLE_NODES:\n",
    "            raise ValueError(f\"Unknown step: {step}\")\n",
    "    \n",
    "    # Build the graph\n",
    "    graph = StateGraph(ProcessingState)\n",
    "    \n",
    "    # Add requested nodes\n",
    "    for step in steps:\n",
    "        graph.add_node(step, AVAILABLE_NODES[step])\n",
    "    \n",
    "    if include_format:\n",
    "        graph.add_node(\"format\", AVAILABLE_NODES[\"format\"])\n",
    "    \n",
    "    # Connect nodes in sequence\n",
    "    graph.add_edge(START, steps[0])\n",
    "    \n",
    "    for i in range(len(steps) - 1):\n",
    "        graph.add_edge(steps[i], steps[i + 1])\n",
    "    \n",
    "    # Connect last step to format or END\n",
    "    if include_format:\n",
    "        graph.add_edge(steps[-1], \"format\")\n",
    "        graph.add_edge(\"format\", END)\n",
    "    else:\n",
    "        graph.add_edge(steps[-1], END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_graph_factory():\n",
    "    sample_text = \"\"\"\n",
    "    Artificial   intelligence   is transforming    how we work.\n",
    "    Machine learning models can now understand and generate human language\n",
    "    with remarkable accuracy. This has implications for many industries.\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"text\": sample_text,\n",
    "        \"cleaned\": \"\",\n",
    "        \"summarized\": \"\",\n",
    "        \"translated\": \"\",\n",
    "        \"final_output\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Config 1: Full pipeline\n",
    "    print(\"\ud83d\udcca Config 1: Full Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    full_pipeline = build_pipeline_from_config({\n",
    "        \"steps\": [\"clean\", \"summarize\", \"translate\"],\n",
    "        \"include_format\": True\n",
    "    })\n",
    "    result = full_pipeline.invoke(initial_state)\n",
    "    print(result[\"final_output\"])\n",
    "    \n",
    "    # Config 2: Just clean and summarize\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udcca Config 2: Clean + Summarize Only\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_only = build_pipeline_from_config({\n",
    "        \"steps\": [\"clean\", \"summarize\"],\n",
    "        \"include_format\": True\n",
    "    })\n",
    "    result = summary_only.invoke(initial_state)\n",
    "    print(result[\"final_output\"])\n",
    "    \n",
    "    # Config 3: Minimal - just clean\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\ud83d\udcca Config 3: Clean Only\")\n",
    "    print(\"=\" * 50)\n",
    "    clean_only = build_pipeline_from_config({\n",
    "        \"steps\": [\"clean\"],\n",
    "        \"include_format\": False\n",
    "    })\n",
    "    result = clean_only.invoke(initial_state)\n",
    "    print(f\"Cleaned: {result['cleaned']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_graph_factory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: json_config_graph.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.5\n",
    "# Save as: json_config_graph.py\n",
    "\n",
    "import json\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "class FlexibleState(TypedDict):\n",
    "    input_data: str\n",
    "    results: Annotated[list[dict], operator.add]\n",
    "    final_output: str\n",
    "\n",
    "def create_llm_node(prompt_template: str, output_key: str):\n",
    "    \"\"\"Factory to create LLM-powered nodes from config.\"\"\"\n",
    "    \n",
    "    def node_function(state: FlexibleState) -> dict:\n",
    "        # Fill in the template with input data\n",
    "        prompt = prompt_template.format(input=state[\"input_data\"])\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"results\": [{\n",
    "                \"step\": output_key,\n",
    "                \"output\": response.content\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    return node_function\n",
    "\n",
    "def create_transform_node(transform_type: str):\n",
    "    \"\"\"Factory to create transform nodes from config.\"\"\"\n",
    "    \n",
    "    def node_function(state: FlexibleState) -> dict:\n",
    "        text = state[\"input_data\"]\n",
    "        \n",
    "        if transform_type == \"uppercase\":\n",
    "            result = text.upper()\n",
    "        elif transform_type == \"lowercase\":\n",
    "            result = text.lower()\n",
    "        elif transform_type == \"word_count\":\n",
    "            result = f\"Word count: {len(text.split())}\"\n",
    "        else:\n",
    "            result = text\n",
    "        \n",
    "        return {\n",
    "            \"results\": [{\n",
    "                \"step\": transform_type,\n",
    "                \"output\": result\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    return node_function\n",
    "\n",
    "def build_from_json(config_json: str):\n",
    "    \"\"\"\n",
    "    Build a graph from JSON configuration.\n",
    "    \n",
    "    JSON format:\n",
    "    {\n",
    "        \"nodes\": [\n",
    "            {\"name\": \"step1\", \"type\": \"llm\", \"prompt\": \"...\", \"output_key\": \"...\"},\n",
    "            {\"name\": \"step2\", \"type\": \"transform\", \"transform\": \"uppercase\"}\n",
    "        ],\n",
    "        \"edges\": [\n",
    "            {\"from\": \"START\", \"to\": \"step1\"},\n",
    "            {\"from\": \"step1\", \"to\": \"step2\"},\n",
    "            {\"from\": \"step2\", \"to\": \"END\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    config = json.loads(config_json)\n",
    "    \n",
    "    graph = StateGraph(FlexibleState)\n",
    "    \n",
    "    # Create and add nodes\n",
    "    for node_config in config[\"nodes\"]:\n",
    "        name = node_config[\"name\"]\n",
    "        node_type = node_config[\"type\"]\n",
    "        \n",
    "        if node_type == \"llm\":\n",
    "            node_fn = create_llm_node(\n",
    "                node_config[\"prompt\"],\n",
    "                node_config.get(\"output_key\", name)\n",
    "            )\n",
    "        elif node_type == \"transform\":\n",
    "            node_fn = create_transform_node(node_config[\"transform\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown node type: {node_type}\")\n",
    "        \n",
    "        graph.add_node(name, node_fn)\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in config[\"edges\"]:\n",
    "        from_node = edge[\"from\"]\n",
    "        to_node = edge[\"to\"]\n",
    "        \n",
    "        if from_node == \"START\":\n",
    "            graph.add_edge(START, to_node)\n",
    "        elif to_node == \"END\":\n",
    "            graph.add_edge(from_node, END)\n",
    "        else:\n",
    "            graph.add_edge(from_node, to_node)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_json_config():\n",
    "    # Define workflow as JSON\n",
    "    config = json.dumps({\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"analyze\",\n",
    "                \"type\": \"llm\",\n",
    "                \"prompt\": \"Analyze the sentiment of this text: {input}\",\n",
    "                \"output_key\": \"sentiment\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"extract\",\n",
    "                \"type\": \"llm\", \n",
    "                \"prompt\": \"Extract 3 key topics from: {input}\",\n",
    "                \"output_key\": \"topics\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"count\",\n",
    "                \"type\": \"transform\",\n",
    "                \"transform\": \"word_count\"\n",
    "            }\n",
    "        ],\n",
    "        \"edges\": [\n",
    "            {\"from\": \"START\", \"to\": \"analyze\"},\n",
    "            {\"from\": \"START\", \"to\": \"extract\"},\n",
    "            {\"from\": \"START\", \"to\": \"count\"},\n",
    "            {\"from\": \"analyze\", \"to\": \"END\"},\n",
    "            {\"from\": \"extract\", \"to\": \"END\"},\n",
    "            {\"from\": \"count\", \"to\": \"END\"}\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcca Graph Built from JSON Config\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Config creates 3 parallel nodes!\")\n",
    "    \n",
    "    graph = build_from_json(config)\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"input_data\": \"The new AI product launch exceeded expectations. \"\n",
    "                      \"Customers loved the intuitive interface and powerful features. \"\n",
    "                      \"Sales grew 150% in the first quarter.\",\n",
    "        \"results\": [],\n",
    "        \"final_output\": \"\"\n",
    "    })\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    for r in result[\"results\"]:\n",
    "        print(f\"\\n{r['step'].upper()}:\")\n",
    "        print(f\"  {r['output']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_json_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: llm_driven_graph.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.5\n",
    "# Save as: llm_driven_graph.py\n",
    "\n",
    "import json\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class TaskState(TypedDict):\n",
    "    user_request: str\n",
    "    plan: list[str]\n",
    "    results: Annotated[list[str], operator.add]\n",
    "    final_answer: str\n",
    "\n",
    "# Available operations the LLM can choose from\n",
    "OPERATIONS = {\n",
    "    \"research\": lambda topic: f\"Researched: Found key facts about {topic}\",\n",
    "    \"summarize\": lambda text: f\"Summary: Condensed the information\",\n",
    "    \"compare\": lambda items: f\"Comparison: Analyzed differences between items\",\n",
    "    \"recommend\": lambda criteria: f\"Recommendation: Based on criteria, suggest option A\",\n",
    "    \"validate\": lambda claim: f\"Validation: Verified the claim is accurate\",\n",
    "    \"format\": lambda style: f\"Formatted: Output structured as {style}\"\n",
    "}\n",
    "\n",
    "def create_operation_node(operation: str):\n",
    "    \"\"\"Create a node for a specific operation.\"\"\"\n",
    "    \n",
    "    def node_fn(state: TaskState) -> dict:\n",
    "        # In real implementation, this would do actual work\n",
    "        op_func = OPERATIONS.get(operation, lambda x: f\"Executed {operation}\")\n",
    "        result = op_func(state[\"user_request\"])\n",
    "        return {\"results\": [f\"[{operation}] {result}\"]}\n",
    "    \n",
    "    return node_fn\n",
    "\n",
    "def plan_workflow(user_request: str) -> list[str]:\n",
    "    \"\"\"Use LLM to decide what steps are needed.\"\"\"\n",
    "    \n",
    "    available_ops = list(OPERATIONS.keys())\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        f\"\"\"Given this user request, decide what operations are needed.\n",
    "        \n",
    "User request: {user_request}\n",
    "\n",
    "Available operations: {available_ops}\n",
    "\n",
    "Return a JSON array of operation names in the order they should execute.\n",
    "Example: [\"research\", \"summarize\", \"format\"]\n",
    "\n",
    "Only return the JSON array, nothing else.\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        plan = json.loads(response.content)\n",
    "        # Validate operations exist\n",
    "        plan = [op for op in plan if op in OPERATIONS]\n",
    "        return plan if plan else [\"research\"]  # Default fallback\n",
    "    except json.JSONDecodeError:\n",
    "        return [\"research\"]  # Fallback\n",
    "\n",
    "def build_llm_planned_graph(user_request: str):\n",
    "    \"\"\"Build a graph based on LLM's workflow plan.\"\"\"\n",
    "    \n",
    "    # Get LLM's plan\n",
    "    plan = plan_workflow(user_request)\n",
    "    print(f\"\ud83e\udd16 LLM planned these steps: {plan}\")\n",
    "    \n",
    "    # Build graph dynamically\n",
    "    graph = StateGraph(TaskState)\n",
    "    \n",
    "    # Add nodes for each planned step\n",
    "    for operation in plan:\n",
    "        graph.add_node(operation, create_operation_node(operation))\n",
    "    \n",
    "    # Add final compilation node\n",
    "    def compile_results(state: TaskState) -> dict:\n",
    "        final = \"Final Answer:\\n\" + \"\\n\".join(state[\"results\"])\n",
    "        return {\"final_answer\": final}\n",
    "    \n",
    "    graph.add_node(\"compile\", compile_results)\n",
    "    \n",
    "    # Connect in sequence\n",
    "    graph.add_edge(START, plan[0])\n",
    "    \n",
    "    for i in range(len(plan) - 1):\n",
    "        graph.add_edge(plan[i], plan[i + 1])\n",
    "    \n",
    "    graph.add_edge(plan[-1], \"compile\")\n",
    "    graph.add_edge(\"compile\", END)\n",
    "    \n",
    "    return graph.compile(), plan\n",
    "\n",
    "def test_llm_driven():\n",
    "    requests = [\n",
    "        \"Compare Python and JavaScript for web development\",\n",
    "        \"Research the latest AI trends and summarize them\",\n",
    "        \"Validate if quantum computing will replace classical computers soon\"\n",
    "    ]\n",
    "    \n",
    "    for request in requests:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"\ud83d\udcdd Request: {request}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        graph, plan = build_llm_planned_graph(request)\n",
    "        \n",
    "        result = graph.invoke({\n",
    "            \"user_request\": request,\n",
    "            \"plan\": plan,\n",
    "            \"results\": [],\n",
    "            \"final_answer\": \"\"\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{result['final_answer']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_llm_driven()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: dynamic_subgraph.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.5\n",
    "# Save as: dynamic_subgraph.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "class MainState(TypedDict):\n",
    "    documents: list[str]\n",
    "    processing_type: str  # \"quick\", \"thorough\", \"deep\"\n",
    "    results: Annotated[list[str], operator.add]\n",
    "    summary: str\n",
    "\n",
    "def build_processing_subgraph(processing_type: str):\n",
    "    \"\"\"Build different subgraphs based on processing type.\"\"\"\n",
    "    \n",
    "    class SubState(TypedDict):\n",
    "        doc: str\n",
    "        processed: str\n",
    "    \n",
    "    def basic_process(state: SubState) -> dict:\n",
    "        return {\"processed\": f\"[Basic] {state['doc'][:50]}...\"}\n",
    "    \n",
    "    def detailed_process(state: SubState) -> dict:\n",
    "        response = llm.invoke(f\"Summarize: {state['doc']}\")\n",
    "        return {\"processed\": f\"[Detailed] {response.content}\"}\n",
    "    \n",
    "    def deep_analysis(state: SubState) -> dict:\n",
    "        response = llm.invoke(\n",
    "            f\"Provide deep analysis with insights: {state['doc']}\"\n",
    "        )\n",
    "        return {\"processed\": f\"[Deep] {response.content}\"}\n",
    "    \n",
    "    def quality_check(state: SubState) -> dict:\n",
    "        return {\"processed\": state[\"processed\"] + \" \u2713 Verified\"}\n",
    "    \n",
    "    subgraph = StateGraph(SubState)\n",
    "    \n",
    "    if processing_type == \"quick\":\n",
    "        # Quick: just basic processing\n",
    "        subgraph.add_node(\"process\", basic_process)\n",
    "        subgraph.add_edge(START, \"process\")\n",
    "        subgraph.add_edge(\"process\", END)\n",
    "    \n",
    "    elif processing_type == \"thorough\":\n",
    "        # Thorough: detailed + quality check\n",
    "        subgraph.add_node(\"process\", detailed_process)\n",
    "        subgraph.add_node(\"verify\", quality_check)\n",
    "        subgraph.add_edge(START, \"process\")\n",
    "        subgraph.add_edge(\"process\", \"verify\")\n",
    "        subgraph.add_edge(\"verify\", END)\n",
    "    \n",
    "    else:  # deep\n",
    "        # Deep: analysis + quality check\n",
    "        subgraph.add_node(\"analyze\", deep_analysis)\n",
    "        subgraph.add_node(\"verify\", quality_check)\n",
    "        subgraph.add_edge(START, \"analyze\")\n",
    "        subgraph.add_edge(\"analyze\", \"verify\")\n",
    "        subgraph.add_edge(\"verify\", END)\n",
    "    \n",
    "    return subgraph.compile()\n",
    "\n",
    "def process_documents(state: MainState) -> dict:\n",
    "    \"\"\"Process each document using dynamically created subgraph.\"\"\"\n",
    "    \n",
    "    # Build subgraph based on processing type\n",
    "    subgraph = build_processing_subgraph(state[\"processing_type\"])\n",
    "    \n",
    "    results = []\n",
    "    for i, doc in enumerate(state[\"documents\"]):\n",
    "        print(f\"  Processing document {i + 1}...\")\n",
    "        \n",
    "        # Run subgraph for each document\n",
    "        sub_result = subgraph.invoke({\n",
    "            \"doc\": doc,\n",
    "            \"processed\": \"\"\n",
    "        })\n",
    "        \n",
    "        results.append(sub_result[\"processed\"])\n",
    "    \n",
    "    return {\"results\": results}\n",
    "\n",
    "def summarize_all(state: MainState) -> dict:\n",
    "    \"\"\"Summarize all results.\"\"\"\n",
    "    summary = f\"Processed {len(state['results'])} documents:\\n\"\n",
    "    for i, r in enumerate(state[\"results\"], 1):\n",
    "        summary += f\"\\n{i}. {r[:100]}...\"\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "def build_main_graph():\n",
    "    \"\"\"Build the main orchestrating graph.\"\"\"\n",
    "    \n",
    "    graph = StateGraph(MainState)\n",
    "    \n",
    "    graph.add_node(\"process\", process_documents)\n",
    "    graph.add_node(\"summarize\", summarize_all)\n",
    "    \n",
    "    graph.add_edge(START, \"process\")\n",
    "    graph.add_edge(\"process\", \"summarize\")\n",
    "    graph.add_edge(\"summarize\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_dynamic_subgraph():\n",
    "    docs = [\n",
    "        \"AI is transforming healthcare with better diagnostics.\",\n",
    "        \"Renewable energy adoption is accelerating globally.\",\n",
    "        \"Remote work has changed corporate culture permanently.\"\n",
    "    ]\n",
    "    \n",
    "    main_graph = build_main_graph()\n",
    "    \n",
    "    for proc_type in [\"quick\", \"thorough\", \"deep\"]:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"\ud83d\udcca Processing Type: {proc_type.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        result = main_graph.invoke({\n",
    "            \"documents\": docs,\n",
    "            \"processing_type\": proc_type,\n",
    "            \"results\": [],\n",
    "            \"summary\": \"\"\n",
    "        })\n",
    "        \n",
    "        print(result[\"summary\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dynamic_subgraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: conditional_assembly.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.5\n",
    "# Save as: conditional_assembly.py\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "class OrderState(TypedDict):\n",
    "    order_type: str  # \"standard\", \"express\", \"international\"\n",
    "    items: list[str]\n",
    "    requires_approval: bool\n",
    "    total_amount: float\n",
    "    status: str\n",
    "    audit_log: list[str]\n",
    "\n",
    "def validate_order(state: OrderState) -> dict:\n",
    "    return {\"status\": \"validated\", \"audit_log\": [\"Order validated\"]}\n",
    "\n",
    "def check_inventory(state: OrderState) -> dict:\n",
    "    return {\"audit_log\": [\"Inventory checked - all items available\"]}\n",
    "\n",
    "def calculate_shipping(state: OrderState) -> dict:\n",
    "    return {\"audit_log\": [\"Shipping calculated\"]}\n",
    "\n",
    "def apply_express_handling(state: OrderState) -> dict:\n",
    "    return {\"audit_log\": [\"Express handling applied - priority queue\"]}\n",
    "\n",
    "def customs_declaration(state: OrderState) -> dict:\n",
    "    return {\"audit_log\": [\"Customs declaration prepared\"]}\n",
    "\n",
    "def manager_approval(state: OrderState) -> dict:\n",
    "    return {\"audit_log\": [\"Manager approval obtained\"]}\n",
    "\n",
    "def process_payment(state: OrderState) -> dict:\n",
    "    return {\"status\": \"paid\", \"audit_log\": [\"Payment processed\"]}\n",
    "\n",
    "def finalize_order(state: OrderState) -> dict:\n",
    "    return {\"status\": \"complete\", \"audit_log\": [\"Order finalized\"]}\n",
    "\n",
    "def build_order_graph(\n",
    "    order_type: str,\n",
    "    requires_approval: bool,\n",
    "    amount: float\n",
    "):\n",
    "    \"\"\"Build order processing graph based on conditions.\"\"\"\n",
    "    \n",
    "    graph = StateGraph(OrderState)\n",
    "    \n",
    "    # Always include these\n",
    "    graph.add_node(\"validate\", validate_order)\n",
    "    graph.add_node(\"inventory\", check_inventory)\n",
    "    graph.add_node(\"shipping\", calculate_shipping)\n",
    "    graph.add_node(\"payment\", process_payment)\n",
    "    graph.add_node(\"finalize\", finalize_order)\n",
    "    \n",
    "    # Conditional nodes\n",
    "    if order_type == \"express\":\n",
    "        graph.add_node(\"express\", apply_express_handling)\n",
    "    \n",
    "    if order_type == \"international\":\n",
    "        graph.add_node(\"customs\", customs_declaration)\n",
    "    \n",
    "    if requires_approval or amount > 1000:\n",
    "        graph.add_node(\"approval\", manager_approval)\n",
    "    \n",
    "    # Build edges based on what nodes exist\n",
    "    graph.add_edge(START, \"validate\")\n",
    "    graph.add_edge(\"validate\", \"inventory\")\n",
    "    \n",
    "    # After inventory, handle express\n",
    "    if order_type == \"express\":\n",
    "        graph.add_edge(\"inventory\", \"express\")\n",
    "        next_after_inventory = \"express\"\n",
    "    else:\n",
    "        next_after_inventory = \"shipping\"\n",
    "        graph.add_edge(\"inventory\", \"shipping\")\n",
    "    \n",
    "    # Connect express to shipping if it exists\n",
    "    if order_type == \"express\":\n",
    "        graph.add_edge(\"express\", \"shipping\")\n",
    "    \n",
    "    # After shipping, handle customs for international\n",
    "    if order_type == \"international\":\n",
    "        graph.add_edge(\"shipping\", \"customs\")\n",
    "        pre_payment = \"customs\"\n",
    "    else:\n",
    "        pre_payment = \"shipping\"\n",
    "    \n",
    "    # Handle approval if needed\n",
    "    if requires_approval or amount > 1000:\n",
    "        graph.add_edge(pre_payment, \"approval\")\n",
    "        graph.add_edge(\"approval\", \"payment\")\n",
    "    else:\n",
    "        graph.add_edge(pre_payment, \"payment\")\n",
    "    \n",
    "    graph.add_edge(\"payment\", \"finalize\")\n",
    "    graph.add_edge(\"finalize\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_conditional_assembly():\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"name\": \"Standard small order\",\n",
    "            \"order_type\": \"standard\",\n",
    "            \"requires_approval\": False,\n",
    "            \"amount\": 50.0\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Express order\",\n",
    "            \"order_type\": \"express\", \n",
    "            \"requires_approval\": False,\n",
    "            \"amount\": 200.0\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"International high-value\",\n",
    "            \"order_type\": \"international\",\n",
    "            \"requires_approval\": False,\n",
    "            \"amount\": 2500.0\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Standard with approval\",\n",
    "            \"order_type\": \"standard\",\n",
    "            \"requires_approval\": True,\n",
    "            \"amount\": 100.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"\ud83d\udce6 Scenario: {scenario['name']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        graph = build_order_graph(\n",
    "            scenario[\"order_type\"],\n",
    "            scenario[\"requires_approval\"],\n",
    "            scenario[\"amount\"]\n",
    "        )\n",
    "        \n",
    "        result = graph.invoke({\n",
    "            \"order_type\": scenario[\"order_type\"],\n",
    "            \"items\": [\"Widget A\", \"Gadget B\"],\n",
    "            \"requires_approval\": scenario[\"requires_approval\"],\n",
    "            \"total_amount\": scenario[\"amount\"],\n",
    "            \"status\": \"new\",\n",
    "            \"audit_log\": []\n",
    "        })\n",
    "        \n",
    "        print(f\"Status: {result['status']}\")\n",
    "        print(\"Audit trail:\")\n",
    "        for log in result[\"audit_log\"]:\n",
    "            print(f\"  \u2713 {log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_conditional_assembly()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.5.1: Configurable Report Generator\n",
    "\n",
    "Build a report generator where users configure which sections to include:\n",
    "- Executive summary (optional)\n",
    "- Data analysis (always included)\n",
    "- Visualizations (optional)  \n",
    "- Recommendations (optional)\n",
    "- Appendix (optional)\n",
    "\n",
    "Requirements:\n",
    "- Accept config: `{\"sections\": [\"summary\", \"analysis\", \"recommendations\"]}`\n",
    "- Build graph with only requested sections\n",
    "- Each section is a node that processes input data\n",
    "- Handle empty config gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.5.2: Dynamic Approval Workflow\n",
    "\n",
    "Create an approval system that builds different chains based on request type:\n",
    "- \"vacation\": Employee \u2192 Manager \u2192 END\n",
    "- \"expense\" (\\< $100): Employee \u2192 Manager \u2192 END\n",
    "- \"expense\" (\\>= $100): Employee \u2192 Manager \u2192 Finance \u2192 END\n",
    "- \"expense\" (\\>= $1000): Employee \u2192 Manager \u2192 Finance \u2192 Director \u2192 END\n",
    "- \"hiring\": HR \u2192 Manager \u2192 Director \u2192 VP \u2192 END\n",
    "\n",
    "Requirements:\n",
    "- Factory function takes request type and amount\n",
    "- Build minimal approval chain needed\n",
    "- Track who approved at each step\n",
    "- Return final approval status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.5.3: LLM-Designed Research Assistant\n",
    "\n",
    "Build a research assistant where the LLM designs its own workflow:\n",
    "- User provides research question\n",
    "- LLM decides what steps are needed (search, analyze, compare, synthesize, etc.)\n",
    "- System builds and executes the planned graph\n",
    "- LLM can include 2-5 steps based on complexity\n",
    "\n",
    "Requirements:\n",
    "- LLM outputs JSON plan with step names and descriptions\n",
    "- Graph is built from LLM's plan\n",
    "- Each step uses LLM with appropriate prompt\n",
    "- Final step synthesizes all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.6: Implementing feedback loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: basic_reflection.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.6\n",
    "# Save as: basic_reflection.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "class ReflectionState(TypedDict):\n",
    "    topic: str\n",
    "    current_draft: str\n",
    "    critique: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    history: Annotated[list[str], operator.add]\n",
    "\n",
    "def generate_draft(state: ReflectionState) -> dict:\n",
    "    \"\"\"Generate initial draft or revision.\"\"\"\n",
    "    \n",
    "    if state[\"iteration\"] == 0:\n",
    "        # First draft\n",
    "        prompt = f\"Write a short paragraph about: {state['topic']}\"\n",
    "    else:\n",
    "        # Revision based on critique\n",
    "        prompt = f\"\"\"Improve this draft based on the critique.\n",
    "\n",
    "Draft:\n",
    "{state['current_draft']}\n",
    "\n",
    "Critique:\n",
    "{state['critique']}\n",
    "\n",
    "Write an improved version:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"current_draft\": response.content,\n",
    "        \"history\": [f\"Draft {state['iteration'] + 1}: {response.content[:100]}...\"]\n",
    "    }\n",
    "\n",
    "def critique_draft(state: ReflectionState) -> dict:\n",
    "    \"\"\"Critique the current draft.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Critique this draft. Be specific about what could be improved.\n",
    "Focus on: clarity, engagement, accuracy, and completeness.\n",
    "\n",
    "Draft:\n",
    "{state['current_draft']}\n",
    "\n",
    "Provide 2-3 specific suggestions for improvement:\"\"\"\n",
    "    \n",
    "    response = critic_llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"critique\": response.content,\n",
    "        \"iteration\": state[\"iteration\"] + 1,\n",
    "        \"history\": [f\"Critique {state['iteration'] + 1}: {response.content[:100]}...\"]\n",
    "    }\n",
    "\n",
    "def should_continue(state: ReflectionState) -> str:\n",
    "    \"\"\"Decide whether to continue refining or finish.\"\"\"\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Check if critique suggests the draft is good\n",
    "    critique_lower = state[\"critique\"].lower()\n",
    "    positive_indicators = [\"excellent\", \"well-written\", \"no major issues\", \"good as is\"]\n",
    "    \n",
    "    if any(indicator in critique_lower for indicator in positive_indicators):\n",
    "        return \"end\"\n",
    "    \n",
    "    return \"revise\"\n",
    "\n",
    "def build_reflection_graph():\n",
    "    graph = StateGraph(ReflectionState)\n",
    "    \n",
    "    graph.add_node(\"generate\", generate_draft)\n",
    "    graph.add_node(\"critique\", critique_draft)\n",
    "    \n",
    "    # Start with generation\n",
    "    graph.add_edge(START, \"generate\")\n",
    "    \n",
    "    # After generation, always critique\n",
    "    graph.add_edge(\"generate\", \"critique\")\n",
    "    \n",
    "    # After critique, decide whether to continue\n",
    "    graph.add_conditional_edges(\n",
    "        \"critique\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"revise\": \"generate\",  # Loop back to improve\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_reflection():\n",
    "    graph = build_reflection_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"topic\": \"The importance of sleep for productivity\",\n",
    "        \"current_draft\": \"\",\n",
    "        \"critique\": \"\",\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"history\": []\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udd04 Reflection Loop Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Iterations completed: {result['iteration']}\")\n",
    "    print(\"\\n\ud83d\udcdc History:\")\n",
    "    for entry in result[\"history\"]:\n",
    "        print(f\"  \u2022 {entry}\")\n",
    "    print(f\"\\n\ud83d\udcdd Final Draft:\\n{result['current_draft']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_reflection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: quality_scoring.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.6\n",
    "# Save as: quality_scoring.py\n",
    "\n",
    "import json\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "scorer_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class ScoredState(TypedDict):\n",
    "    task: str\n",
    "    current_output: str\n",
    "    scores: dict  # {\"clarity\": 8, \"accuracy\": 7, ...}\n",
    "    overall_score: float\n",
    "    feedback: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    target_score: float\n",
    "    score_history: Annotated[list[dict], operator.add]\n",
    "\n",
    "def score_output(state: ScoredState) -> dict:\n",
    "    \"\"\"Score the output on multiple dimensions.\"\"\"\n",
    "    \n",
    "    scoring_prompt = f\"\"\"Score this output on a scale of 1-10 for each criterion.\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Output:\n",
    "{state['current_output']}\n",
    "\n",
    "Score each criterion and provide brief feedback.\n",
    "Return as JSON:\n",
    "{{\n",
    "    \"clarity\": <1-10>,\n",
    "    \"accuracy\": <1-10>,\n",
    "    \"completeness\": <1-10>,\n",
    "    \"engagement\": <1-10>,\n",
    "    \"feedback\": \"<specific suggestions for improvement>\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON:\"\"\"\n",
    "\n",
    "    response = scorer_llm.invoke(scoring_prompt)\n",
    "    \n",
    "    try:\n",
    "        scores = json.loads(response.content)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        score_values = [\n",
    "            scores.get(\"clarity\", 5),\n",
    "            scores.get(\"accuracy\", 5),\n",
    "            scores.get(\"completeness\", 5),\n",
    "            scores.get(\"engagement\", 5)\n",
    "        ]\n",
    "        overall = sum(score_values) / len(score_values)\n",
    "        \n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"overall_score\": overall,\n",
    "            \"feedback\": scores.get(\"feedback\", \"\"),\n",
    "            \"score_history\": [{\n",
    "                \"iteration\": state[\"iteration\"],\n",
    "                \"overall\": overall,\n",
    "                \"scores\": scores\n",
    "            }]\n",
    "        }\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if parsing fails\n",
    "        return {\n",
    "            \"scores\": {},\n",
    "            \"overall_score\": 5.0,\n",
    "            \"feedback\": \"Could not parse scores\",\n",
    "            \"score_history\": []\n",
    "        }\n",
    "\n",
    "def generate_output(state: ScoredState) -> dict:\n",
    "    \"\"\"Generate or improve output.\"\"\"\n",
    "    \n",
    "    if state[\"iteration\"] == 0:\n",
    "        prompt = f\"Complete this task:\\n{state['task']}\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Improve this output based on feedback.\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current output:\n",
    "{state['current_output']}\n",
    "\n",
    "Feedback (current score: {state['overall_score']:.1f}/10):\n",
    "{state['feedback']}\n",
    "\n",
    "Write an improved version that addresses the feedback:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"current_output\": response.content,\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n",
    "\n",
    "def check_quality(state: ScoredState) -> str:\n",
    "    \"\"\"Check if quality threshold is met.\"\"\"\n",
    "    \n",
    "    # Stop if target score reached\n",
    "    if state[\"overall_score\"] >= state[\"target_score\"]:\n",
    "        return \"done\"\n",
    "    \n",
    "    # Stop if max iterations reached\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return \"done\"\n",
    "    \n",
    "    return \"improve\"\n",
    "\n",
    "def build_scoring_graph():\n",
    "    graph = StateGraph(ScoredState)\n",
    "    \n",
    "    graph.add_node(\"generate\", generate_output)\n",
    "    graph.add_node(\"score\", score_output)\n",
    "    \n",
    "    graph.add_edge(START, \"generate\")\n",
    "    graph.add_edge(\"generate\", \"score\")\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"score\",\n",
    "        check_quality,\n",
    "        {\n",
    "            \"improve\": \"generate\",\n",
    "            \"done\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_quality_scoring():\n",
    "    graph = build_scoring_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"task\": \"Write a compelling product description for a smart water bottle that tracks hydration\",\n",
    "        \"current_output\": \"\",\n",
    "        \"scores\": {},\n",
    "        \"overall_score\": 0.0,\n",
    "        \"feedback\": \"\",\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": 4,\n",
    "        \"target_score\": 8.0,\n",
    "        \"score_history\": []\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcca Quality Scoring Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final Score: {result['overall_score']:.1f}/10\")\n",
    "    print(f\"Target Score: {result['target_score']}/10\")\n",
    "    print(f\"Iterations: {result['iteration']}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc8 Score History:\")\n",
    "    for entry in result[\"score_history\"]:\n",
    "        print(f\"  Iteration {entry['iteration']}: {entry['overall']:.1f}/10\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcdd Final Output:\\n{result['current_output']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_quality_scoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: self_correcting_code.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.6\n",
    "# Save as: self_correcting_code.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "import traceback\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "class CodeState(TypedDict):\n",
    "    task: str\n",
    "    code: str\n",
    "    test_result: str\n",
    "    error_message: str\n",
    "    is_working: bool\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    attempt_history: Annotated[list[str], operator.add]\n",
    "\n",
    "def generate_code(state: CodeState) -> dict:\n",
    "    \"\"\"Generate or fix code.\"\"\"\n",
    "    \n",
    "    if state[\"iteration\"] == 0:\n",
    "        prompt = f\"\"\"Write Python code to accomplish this task.\n",
    "Return ONLY the code, no explanations.\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Code:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Fix this Python code based on the error.\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current code:\n",
    "```python\n",
    "{state['code']}\n",
    "```\n",
    "\n",
    "Error:\n",
    "{state['error_message']}\n",
    "\n",
    "Write the corrected code only:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Extract code from response (handle markdown code blocks)\n",
    "    code = response.content\n",
    "    if \"```python\" in code:\n",
    "        code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code:\n",
    "        code = code.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    return {\n",
    "        \"code\": code.strip(),\n",
    "        \"iteration\": state[\"iteration\"] + 1,\n",
    "        \"attempt_history\": [f\"Attempt {state['iteration'] + 1}\"]\n",
    "    }\n",
    "\n",
    "def test_code(state: CodeState) -> dict:\n",
    "    \"\"\"Execute the code and capture results.\"\"\"\n",
    "    \n",
    "    code = state[\"code\"]\n",
    "    \n",
    "    try:\n",
    "        # Create a restricted execution environment\n",
    "        exec_globals = {\"__builtins__\": __builtins__}\n",
    "        exec_locals = {}\n",
    "        \n",
    "        # Execute the code\n",
    "        exec(code, exec_globals, exec_locals)\n",
    "        \n",
    "        # If we get here, code ran without errors\n",
    "        return {\n",
    "            \"is_working\": True,\n",
    "            \"test_result\": \"Code executed successfully\",\n",
    "            \"error_message\": \"\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
    "        return {\n",
    "            \"is_working\": False,\n",
    "            \"test_result\": \"Code failed\",\n",
    "            \"error_message\": error_msg\n",
    "        }\n",
    "\n",
    "def check_code_status(state: CodeState) -> str:\n",
    "    \"\"\"Check if code works or needs fixing.\"\"\"\n",
    "    \n",
    "    if state[\"is_working\"]:\n",
    "        return \"success\"\n",
    "    \n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return \"give_up\"\n",
    "    \n",
    "    return \"fix\"\n",
    "\n",
    "def build_code_graph():\n",
    "    graph = StateGraph(CodeState)\n",
    "    \n",
    "    graph.add_node(\"generate\", generate_code)\n",
    "    graph.add_node(\"test\", test_code)\n",
    "    \n",
    "    graph.add_edge(START, \"generate\")\n",
    "    graph.add_edge(\"generate\", \"test\")\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"test\",\n",
    "        check_code_status,\n",
    "        {\n",
    "            \"fix\": \"generate\",\n",
    "            \"success\": END,\n",
    "            \"give_up\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_code_generation():\n",
    "    graph = build_code_graph()\n",
    "    \n",
    "    tasks = [\n",
    "        \"Write a function called 'fibonacci' that returns the nth fibonacci number\",\n",
    "        \"Write a function called 'is_palindrome' that checks if a string is a palindrome\",\n",
    "        \"Write a function called 'flatten' that flattens a nested list\"\n",
    "    ]\n",
    "    \n",
    "    for task in tasks:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"\ud83c\udfaf Task: {task}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        result = graph.invoke({\n",
    "            \"task\": task,\n",
    "            \"code\": \"\",\n",
    "            \"test_result\": \"\",\n",
    "            \"error_message\": \"\",\n",
    "            \"is_working\": False,\n",
    "            \"iteration\": 0,\n",
    "            \"max_iterations\": 3,\n",
    "            \"attempt_history\": []\n",
    "        })\n",
    "        \n",
    "        status = \"\u2705 Success\" if result[\"is_working\"] else \"\u274c Failed\"\n",
    "        print(f\"\\n{status} after {result['iteration']} attempts\")\n",
    "        print(f\"\\n\ud83d\udcdd Final Code:\\n{result['code']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_code_generation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: reflexion_agent.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.6\n",
    "# Save as: reflexion_agent.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "validator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class ReflexionState(TypedDict):\n",
    "    question: str\n",
    "    current_answer: str\n",
    "    # External validation\n",
    "    fact_checks: list[dict]\n",
    "    # Self-reflection\n",
    "    reflection: str\n",
    "    missing_info: list[str]\n",
    "    # Control\n",
    "    is_satisfactory: bool\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    revision_history: Annotated[list[str], operator.add]\n",
    "\n",
    "def generate_answer(state: ReflexionState) -> dict:\n",
    "    \"\"\"Generate or revise answer.\"\"\"\n",
    "    \n",
    "    if state[\"iteration\"] == 0:\n",
    "        prompt = f\"Answer this question thoroughly:\\n{state['question']}\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Revise your answer based on reflection and fact checks.\n",
    "\n",
    "Question: {state['question']}\n",
    "\n",
    "Previous answer:\n",
    "{state['current_answer']}\n",
    "\n",
    "Reflection:\n",
    "{state['reflection']}\n",
    "\n",
    "Missing information to address:\n",
    "{', '.join(state['missing_info']) if state['missing_info'] else 'None identified'}\n",
    "\n",
    "Write an improved, more accurate answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"current_answer\": response.content,\n",
    "        \"revision_history\": [f\"Revision {state['iteration'] + 1}\"]\n",
    "    }\n",
    "\n",
    "def validate_claims(state: ReflexionState) -> dict:\n",
    "    \"\"\"Extract and validate claims in the answer.\"\"\"\n",
    "    \n",
    "    validation_prompt = f\"\"\"Analyze this answer for factual claims.\n",
    "\n",
    "Question: {state['question']}\n",
    "\n",
    "Answer:\n",
    "{state['current_answer']}\n",
    "\n",
    "For each major claim, assess if it appears accurate.\n",
    "Return a brief assessment of overall factual reliability (HIGH, MEDIUM, or LOW)\n",
    "and list any claims that seem questionable or need verification.\n",
    "\n",
    "Format:\n",
    "Reliability: <HIGH/MEDIUM/LOW>\n",
    "Questionable claims: <list or \"None\">\n",
    "Missing important information: <list or \"None\">\"\"\"\n",
    "\n",
    "    response = validator_llm.invoke(validation_prompt)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse response (simplified)\n",
    "    reliability = \"MEDIUM\"\n",
    "    if \"Reliability: HIGH\" in content:\n",
    "        reliability = \"HIGH\"\n",
    "    elif \"Reliability: LOW\" in content:\n",
    "        reliability = \"LOW\"\n",
    "    \n",
    "    # Extract missing info\n",
    "    missing = []\n",
    "    if \"Missing\" in content:\n",
    "        missing_section = content.split(\"Missing\")[1].split(\"\\n\")[0]\n",
    "        if \"None\" not in missing_section:\n",
    "            missing = [m.strip() for m in missing_section.split(\",\") if m.strip()]\n",
    "    \n",
    "    return {\n",
    "        \"fact_checks\": [{\"reliability\": reliability, \"details\": content}],\n",
    "        \"missing_info\": missing[:3]  # Limit to top 3\n",
    "    }\n",
    "\n",
    "def reflect_on_answer(state: ReflexionState) -> dict:\n",
    "    \"\"\"Reflect on the answer quality.\"\"\"\n",
    "    \n",
    "    reflection_prompt = f\"\"\"Critically evaluate this answer.\n",
    "\n",
    "Question: {state['question']}\n",
    "\n",
    "Answer:\n",
    "{state['current_answer']}\n",
    "\n",
    "Validation feedback:\n",
    "{state['fact_checks'][-1]['details'] if state['fact_checks'] else 'No validation yet'}\n",
    "\n",
    "Reflect on:\n",
    "1. Is the answer complete and accurate?\n",
    "2. What could be improved?\n",
    "3. Are there any gaps or weaknesses?\n",
    "\n",
    "Provide honest self-reflection:\"\"\"\n",
    "\n",
    "    response = validator_llm.invoke(reflection_prompt)\n",
    "    \n",
    "    # Determine if satisfactory\n",
    "    reflection = response.content.lower()\n",
    "    is_good = (\n",
    "        state[\"fact_checks\"] and \n",
    "        state[\"fact_checks\"][-1][\"reliability\"] == \"HIGH\" and\n",
    "        \"complete\" in reflection and\n",
    "        \"no major\" in reflection\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"reflection\": response.content,\n",
    "        \"is_satisfactory\": is_good,\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n",
    "\n",
    "def should_revise(state: ReflexionState) -> str:\n",
    "    \"\"\"Decide whether to revise or finish.\"\"\"\n",
    "    \n",
    "    if state[\"is_satisfactory\"]:\n",
    "        return \"done\"\n",
    "    \n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return \"done\"\n",
    "    \n",
    "    return \"revise\"\n",
    "\n",
    "def build_reflexion_graph():\n",
    "    graph = StateGraph(ReflexionState)\n",
    "    \n",
    "    graph.add_node(\"generate\", generate_answer)\n",
    "    graph.add_node(\"validate\", validate_claims)\n",
    "    graph.add_node(\"reflect\", reflect_on_answer)\n",
    "    \n",
    "    graph.add_edge(START, \"generate\")\n",
    "    graph.add_edge(\"generate\", \"validate\")\n",
    "    graph.add_edge(\"validate\", \"reflect\")\n",
    "    \n",
    "    graph.add_conditional_edges(\n",
    "        \"reflect\",\n",
    "        should_revise,\n",
    "        {\n",
    "            \"revise\": \"generate\",\n",
    "            \"done\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_reflexion():\n",
    "    graph = build_reflexion_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"question\": \"What are the main causes and effects of deforestation?\",\n",
    "        \"current_answer\": \"\",\n",
    "        \"fact_checks\": [],\n",
    "        \"reflection\": \"\",\n",
    "        \"missing_info\": [],\n",
    "        \"is_satisfactory\": False,\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"revision_history\": []\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udd0d Reflexion Agent Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Iterations: {result['iteration']}\")\n",
    "    print(f\"Satisfactory: {'Yes' if result['is_satisfactory'] else 'No'}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Final Validation:\")\n",
    "    if result[\"fact_checks\"]:\n",
    "        print(f\"  Reliability: {result['fact_checks'][-1]['reliability']}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcdd Final Answer:\\n{result['current_answer']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_reflexion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_aspect_feedback.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.6\n",
    "# Save as: multi_aspect_feedback.py\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "evaluator = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class MultiAspectState(TypedDict):\n",
    "    task: str\n",
    "    content: str\n",
    "    # Aspect scores (1-10)\n",
    "    style_score: int\n",
    "    accuracy_score: int\n",
    "    structure_score: int\n",
    "    # Feedback per aspect\n",
    "    style_feedback: str\n",
    "    accuracy_feedback: str\n",
    "    structure_feedback: str\n",
    "    # Control\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    all_pass: bool\n",
    "    threshold: int\n",
    "\n",
    "def evaluate_style(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Evaluate writing style.\"\"\"\n",
    "    prompt = f\"\"\"Score the STYLE of this content (1-10).\n",
    "Consider: tone, readability, engagement, word choice.\n",
    "\n",
    "Content:\n",
    "{state['content']}\n",
    "\n",
    "Return format:\n",
    "Score: <number>\n",
    "Feedback: <brief feedback>\"\"\"\n",
    "    \n",
    "    response = evaluator.invoke(prompt)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse score\n",
    "    score = 5\n",
    "    try:\n",
    "        score_line = [l for l in content.split('\\n') if 'Score' in l][0]\n",
    "        score = int(''.join(filter(str.isdigit, score_line)))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        \"style_score\": min(10, max(1, score)),\n",
    "        \"style_feedback\": content\n",
    "    }\n",
    "\n",
    "def evaluate_accuracy(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Evaluate factual accuracy.\"\"\"\n",
    "    prompt = f\"\"\"Score the ACCURACY of this content (1-10).\n",
    "Consider: factual correctness, precision, reliability.\n",
    "\n",
    "Content:\n",
    "{state['content']}\n",
    "\n",
    "Return format:\n",
    "Score: <number>\n",
    "Feedback: <brief feedback>\"\"\"\n",
    "    \n",
    "    response = evaluator.invoke(prompt)\n",
    "    content = response.content\n",
    "    \n",
    "    score = 5\n",
    "    try:\n",
    "        score_line = [l for l in content.split('\\n') if 'Score' in l][0]\n",
    "        score = int(''.join(filter(str.isdigit, score_line)))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_score\": min(10, max(1, score)),\n",
    "        \"accuracy_feedback\": content\n",
    "    }\n",
    "\n",
    "def evaluate_structure(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Evaluate content structure.\"\"\"\n",
    "    prompt = f\"\"\"Score the STRUCTURE of this content (1-10).\n",
    "Consider: organization, flow, completeness, logical order.\n",
    "\n",
    "Content:\n",
    "{state['content']}\n",
    "\n",
    "Return format:\n",
    "Score: <number>\n",
    "Feedback: <brief feedback>\"\"\"\n",
    "    \n",
    "    response = evaluator.invoke(prompt)\n",
    "    content = response.content\n",
    "    \n",
    "    score = 5\n",
    "    try:\n",
    "        score_line = [l for l in content.split('\\n') if 'Score' in l][0]\n",
    "        score = int(''.join(filter(str.isdigit, score_line)))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        \"structure_score\": min(10, max(1, score)),\n",
    "        \"structure_feedback\": content\n",
    "    }\n",
    "\n",
    "def aggregate_and_decide(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Aggregate scores and decide if all pass threshold.\"\"\"\n",
    "    scores = [\n",
    "        state[\"style_score\"],\n",
    "        state[\"accuracy_score\"],\n",
    "        state[\"structure_score\"]\n",
    "    ]\n",
    "    \n",
    "    all_pass = all(s >= state[\"threshold\"] for s in scores)\n",
    "    \n",
    "    return {\n",
    "        \"all_pass\": all_pass,\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n",
    "\n",
    "def improve_content(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Improve content based on all feedback.\"\"\"\n",
    "    \n",
    "    # Find lowest scoring aspect\n",
    "    scores = {\n",
    "        \"style\": (state[\"style_score\"], state[\"style_feedback\"]),\n",
    "        \"accuracy\": (state[\"accuracy_score\"], state[\"accuracy_feedback\"]),\n",
    "        \"structure\": (state[\"structure_score\"], state[\"structure_feedback\"])\n",
    "    }\n",
    "    \n",
    "    weakest = min(scores.items(), key=lambda x: x[1][0])\n",
    "    \n",
    "    prompt = f\"\"\"Improve this content, focusing especially on {weakest[0]}.\n",
    "\n",
    "Task: {state['task']}\n",
    "\n",
    "Current content:\n",
    "{state['content']}\n",
    "\n",
    "Feedback to address:\n",
    "- Style ({state['style_score']}/10): {state['style_feedback'][:100]}\n",
    "- Accuracy ({state['accuracy_score']}/10): {state['accuracy_feedback'][:100]}\n",
    "- Structure ({state['structure_score']}/10): {state['structure_feedback'][:100]}\n",
    "\n",
    "Priority: Improve {weakest[0]} (lowest score: {weakest[1][0]}/10)\n",
    "\n",
    "Write improved content:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"content\": response.content}\n",
    "\n",
    "def generate_initial(state: MultiAspectState) -> dict:\n",
    "    \"\"\"Generate initial content.\"\"\"\n",
    "    response = llm.invoke(f\"Complete this task:\\n{state['task']}\")\n",
    "    return {\"content\": response.content}\n",
    "\n",
    "def check_quality(state: MultiAspectState) -> str:\n",
    "    if state[\"all_pass\"]:\n",
    "        return \"done\"\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return \"done\"\n",
    "    return \"improve\"\n",
    "\n",
    "def build_multi_aspect_graph():\n",
    "    graph = StateGraph(MultiAspectState)\n",
    "    \n",
    "    graph.add_node(\"generate\", generate_initial)\n",
    "    graph.add_node(\"eval_style\", evaluate_style)\n",
    "    graph.add_node(\"eval_accuracy\", evaluate_accuracy)\n",
    "    graph.add_node(\"eval_structure\", evaluate_structure)\n",
    "    graph.add_node(\"aggregate\", aggregate_and_decide)\n",
    "    graph.add_node(\"improve\", improve_content)\n",
    "    \n",
    "    # Initial generation\n",
    "    graph.add_edge(START, \"generate\")\n",
    "    \n",
    "    # Parallel evaluation\n",
    "    graph.add_edge(\"generate\", \"eval_style\")\n",
    "    graph.add_edge(\"generate\", \"eval_accuracy\")\n",
    "    graph.add_edge(\"generate\", \"eval_structure\")\n",
    "    \n",
    "    # All evaluations feed into aggregate\n",
    "    graph.add_edge(\"eval_style\", \"aggregate\")\n",
    "    graph.add_edge(\"eval_accuracy\", \"aggregate\")\n",
    "    graph.add_edge(\"eval_structure\", \"aggregate\")\n",
    "    \n",
    "    # Conditional improvement loop\n",
    "    graph.add_conditional_edges(\n",
    "        \"aggregate\",\n",
    "        check_quality,\n",
    "        {\n",
    "            \"improve\": \"improve\",\n",
    "            \"done\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After improvement, re-evaluate\n",
    "    graph.add_edge(\"improve\", \"eval_style\")\n",
    "    graph.add_edge(\"improve\", \"eval_accuracy\")\n",
    "    graph.add_edge(\"improve\", \"eval_structure\")\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "def test_multi_aspect():\n",
    "    graph = build_multi_aspect_graph()\n",
    "    \n",
    "    result = graph.invoke({\n",
    "        \"task\": \"Write a brief guide on effective time management for students\",\n",
    "        \"content\": \"\",\n",
    "        \"style_score\": 0,\n",
    "        \"accuracy_score\": 0,\n",
    "        \"structure_score\": 0,\n",
    "        \"style_feedback\": \"\",\n",
    "        \"accuracy_feedback\": \"\",\n",
    "        \"structure_feedback\": \"\",\n",
    "        \"iteration\": 0,\n",
    "        \"max_iterations\": 3,\n",
    "        \"all_pass\": False,\n",
    "        \"threshold\": 7\n",
    "    })\n",
    "    \n",
    "    print(\"\ud83d\udcca Multi-Aspect Feedback Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Iterations: {result['iteration']}\")\n",
    "    print(f\"All aspects \u2265 {result['threshold']}? {'Yes \u2705' if result['all_pass'] else 'No'}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc8 Final Scores:\")\n",
    "    print(f\"  Style: {result['style_score']}/10\")\n",
    "    print(f\"  Accuracy: {result['accuracy_score']}/10\")\n",
    "    print(f\"  Structure: {result['structure_score']}/10\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcdd Final Content:\\n{result['content'][:500]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_multi_aspect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 17.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.6.1: Essay Improver\n",
    "\n",
    "Build an essay improvement system that:\n",
    "- Takes a rough draft as input\n",
    "- Scores it on: thesis clarity, evidence quality, writing style, conclusion strength\n",
    "- Iterates until all scores \u2265 7 OR max 4 iterations\n",
    "- Shows score progression across iterations\n",
    "- Produces final improved essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.6.2: Bug-Fixing Agent\n",
    "\n",
    "Create a code debugging agent that:\n",
    "- Takes buggy code and expected behavior as input\n",
    "- Attempts to run the code\n",
    "- If it fails, analyzes the error and attempts a fix\n",
    "- Tracks what fixes were attempted\n",
    "- Stops when code runs correctly OR after 5 attempts\n",
    "- Reports what bug was found and how it was fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17.6.3: Fact-Checked Article Generator\n",
    "\n",
    "Build an article generator with fact-checking:\n",
    "- Generate article on given topic\n",
    "- Extract 3-5 key claims from the article\n",
    "- \"Verify\" each claim (simulate with LLM evaluation)\n",
    "- If any claims are questionable, revise the article\n",
    "- Track which claims were revised\n",
    "- Continue until all claims pass OR max 3 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 17.7: Production deployment considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: production_patterns.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 17, Section 17.7\n",
    "# Save as: production_patterns.py\n",
    "# Reference patterns for production LangGraph deployment\n",
    "# Note: These are illustrative patterns, not a complete runnable application\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import TypedDict\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 1: RETRY WITH BACKOFF\n",
    "# =============================================================================\n",
    "# Use tenacity for robust retry logic with transient failures\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def call_llm_with_retry(llm, prompt):\n",
    "    \"\"\"Retry LLM calls with exponential backoff.\n",
    "    \n",
    "    - Retries up to 3 times\n",
    "    - Waits 2s, then 4s, then 8s between retries (capped at 10s)\n",
    "    - Handles transient API failures gracefully\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 2: ASYNC TIMEOUT\n",
    "# =============================================================================\n",
    "# Prevent hanging nodes with async timeouts\n",
    "\n",
    "from asyncio import timeout\n",
    "\n",
    "async def node_with_timeout(state, llm, timeout_seconds=30):\n",
    "    \"\"\"Execute node with timeout to prevent hanging.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state\n",
    "        llm: Language model instance\n",
    "        timeout_seconds: Maximum execution time\n",
    "        \n",
    "    Raises:\n",
    "        TimeoutError: If execution exceeds timeout\n",
    "    \"\"\"\n",
    "    async with timeout(timeout_seconds):\n",
    "        return await llm.ainvoke(state[\"messages\"])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 3: SAFE NODE WITH FALLBACK\n",
    "# =============================================================================\n",
    "# Always provide fallback responses\n",
    "\n",
    "def safe_node(state, process_fn, logger):\n",
    "    \"\"\"Wrap node processing with error handling and fallback.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state\n",
    "        process_fn: The actual processing function\n",
    "        logger: Logger instance for error tracking\n",
    "        \n",
    "    Returns:\n",
    "        Either the processed result or a fallback error response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return process_fn(state)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Node failed: {e}\", exc_info=True)\n",
    "        return {\"error\": \"I encountered an issue. Please try again.\"}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 4: TOKEN USAGE TRACKING\n",
    "# =============================================================================\n",
    "# Track and limit token usage per session\n",
    "\n",
    "MAX_TOKENS_PER_SESSION = 50000  # Example limit\n",
    "\n",
    "def track_usage(state):\n",
    "    \"\"\"Check token usage and enforce limits.\n",
    "    \n",
    "    Add this as a node or check within nodes to prevent\n",
    "    runaway costs from long conversations or loops.\n",
    "    \"\"\"\n",
    "    usage = state.get(\"total_tokens\", 0)\n",
    "    if usage > MAX_TOKENS_PER_SESSION:\n",
    "        return {\"error\": \"Session token limit reached. Please start a new session.\"}\n",
    "    return state\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 5: ITERATION LIMITS\n",
    "# =============================================================================\n",
    "# Always cap loops to prevent infinite execution\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "def should_continue(state):\n",
    "    \"\"\"Route function with mandatory iteration cap.\n",
    "    \n",
    "    Always include iteration checks in feedback loops\n",
    "    to prevent infinite execution and runaway costs.\n",
    "    \"\"\"\n",
    "    if state[\"iteration\"] >= MAX_ITERATIONS:\n",
    "        return \"end\"  # Force exit regardless of other conditions\n",
    "    \n",
    "    # Your other routing logic here\n",
    "    if state.get(\"task_complete\"):\n",
    "        return \"end\"\n",
    "    \n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 6: MODEL TIERING\n",
    "# =============================================================================\n",
    "# Use cheaper models for simple tasks, expensive for complex\n",
    "\n",
    "def get_appropriate_model(task_type):\n",
    "    \"\"\"Select model based on task complexity.\n",
    "    \n",
    "    Saves costs by using cheaper models for simple tasks\n",
    "    while reserving expensive models for complex reasoning.\n",
    "    \"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    if task_type in [\"classification\", \"extraction\", \"simple_qa\"]:\n",
    "        return ChatOpenAI(model=\"gpt-3.5-turbo\")  # Cheaper\n",
    "    elif task_type in [\"complex_reasoning\", \"code_generation\", \"analysis\"]:\n",
    "        return ChatOpenAI(model=\"gpt-4\")  # More capable\n",
    "    else:\n",
    "        return ChatOpenAI(model=\"gpt-3.5-turbo\")  # Default to cheaper\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 7: STRUCTURED LOGGING\n",
    "# =============================================================================\n",
    "# Essential logging for production observability\n",
    "\n",
    "logger = logging.getLogger(\"agent\")\n",
    "\n",
    "def logged_node(state, node_name, process_fn):\n",
    "    \"\"\"Wrap node with comprehensive logging.\n",
    "    \n",
    "    Logs:\n",
    "    - Node start with context (thread_id, iteration)\n",
    "    - Execution duration\n",
    "    - Errors with full stack traces\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    \n",
    "    logger.info(f\"[{node_name}] Starting\", extra={\n",
    "        \"thread_id\": state.get(\"thread_id\"),\n",
    "        \"iteration\": state.get(\"iteration\"),\n",
    "        \"node\": node_name\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        result = process_fn(state)\n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        \n",
    "        logger.info(f\"[{node_name}] Completed in {duration:.2f}s\", extra={\n",
    "            \"thread_id\": state.get(\"thread_id\"),\n",
    "            \"duration\": duration,\n",
    "            \"node\": node_name\n",
    "        })\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"[{node_name}] Failed: {e}\", exc_info=True, extra={\n",
    "            \"thread_id\": state.get(\"thread_id\"),\n",
    "            \"node\": node_name,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        raise\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 8: INPUT VALIDATION\n",
    "# =============================================================================\n",
    "# Never trust user input\n",
    "\n",
    "MAX_MESSAGE_LENGTH = 10000\n",
    "BLOCKED_PATTERNS = [\"ignore previous instructions\", \"system prompt\"]\n",
    "\n",
    "def validate_input(user_message: str) -> str:\n",
    "    \"\"\"Validate and sanitize user input.\n",
    "    \n",
    "    Checks:\n",
    "    - Message length limits\n",
    "    - Blocked patterns (basic prompt injection defense)\n",
    "    - Empty input\n",
    "    \n",
    "    Returns:\n",
    "        Sanitized message\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    if not user_message or not user_message.strip():\n",
    "        raise ValueError(\"Empty message\")\n",
    "    \n",
    "    if len(user_message) > MAX_MESSAGE_LENGTH:\n",
    "        raise ValueError(f\"Message too long (max {MAX_MESSAGE_LENGTH} chars)\")\n",
    "    \n",
    "    # Basic prompt injection check\n",
    "    message_lower = user_message.lower()\n",
    "    for pattern in BLOCKED_PATTERNS:\n",
    "        if pattern in message_lower:\n",
    "            raise ValueError(\"Invalid input detected\")\n",
    "    \n",
    "    return user_message.strip()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 9: OUTPUT FILTERING\n",
    "# =============================================================================\n",
    "# Filter sensitive information from responses\n",
    "\n",
    "SENSITIVE_PATTERNS = [\"API_KEY\", \"password\", \"secret\"]\n",
    "\n",
    "def filter_output(response: str) -> str:\n",
    "    \"\"\"Filter potentially sensitive content from responses.\n",
    "    \n",
    "    Production systems should implement more sophisticated\n",
    "    content filtering based on their specific requirements.\n",
    "    \"\"\"\n",
    "    filtered = response\n",
    "    \n",
    "    # Remove any accidentally leaked sensitive patterns\n",
    "    for pattern in SENSITIVE_PATTERNS:\n",
    "        if pattern.lower() in filtered.lower():\n",
    "            filtered = filtered.replace(pattern, \"[REDACTED]\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 10: RATE LIMITING\n",
    "# =============================================================================\n",
    "# Protect against abuse with rate limits\n",
    "\n",
    "request_counts = defaultdict(list)\n",
    "\n",
    "def rate_limit(user_id: str, max_requests: int = 10, window_seconds: int = 60):\n",
    "    \"\"\"Enforce per-user rate limits.\n",
    "    \n",
    "    Args:\n",
    "        user_id: Unique identifier for the user\n",
    "        max_requests: Maximum requests allowed in window\n",
    "        window_seconds: Time window in seconds\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If rate limit exceeded\n",
    "    \"\"\"\n",
    "    now = time.time()\n",
    "    \n",
    "    # Clean old requests outside the window\n",
    "    request_counts[user_id] = [\n",
    "        t for t in request_counts[user_id] \n",
    "        if now - t < window_seconds\n",
    "    ]\n",
    "    \n",
    "    if len(request_counts[user_id]) >= max_requests:\n",
    "        raise Exception(f\"Rate limit exceeded. Max {max_requests} requests per {window_seconds}s\")\n",
    "    \n",
    "    request_counts[user_id].append(now)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 11: PRODUCTION CHECKPOINTER\n",
    "# =============================================================================\n",
    "# Use external storage for scalable state persistence\n",
    "\n",
    "def get_production_checkpointer(connection_string: str):\n",
    "    \"\"\"Create a production-ready checkpointer.\n",
    "    \n",
    "    For production, use PostgreSQL or another persistent store\n",
    "    instead of in-memory checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        connection_string: Database connection string\n",
    "        \n",
    "    Returns:\n",
    "        PostgresSaver checkpointer instance\n",
    "    \"\"\"\n",
    "    from langgraph.checkpoint.postgres import PostgresSaver\n",
    "    \n",
    "    return PostgresSaver.from_conn_string(connection_string)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# checkpointer = get_production_checkpointer(\n",
    "#     \"postgresql://user:pass@host:5432/db\"\n",
    "# )\n",
    "# graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PATTERN 12: SIMPLE TEST EXAMPLE\n",
    "# =============================================================================\n",
    "# Basic test pattern for agents\n",
    "\n",
    "def test_agent_handles_greeting(graph):\n",
    "    \"\"\"Example test: agent handles basic greeting.\n",
    "    \n",
    "    Production agents should have comprehensive test suites:\n",
    "    - Unit tests for individual nodes\n",
    "    - Integration tests for full flows\n",
    "    - End-to-end tests for conversations\n",
    "    - Load tests for performance\n",
    "    - Adversarial tests for security\n",
    "    \"\"\"\n",
    "    result = graph.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "    })\n",
    "    \n",
    "    assert \"error\" not in result, \"Agent should not return error for greeting\"\n",
    "    assert len(result.get(\"messages\", [])) > 1, \"Agent should respond to greeting\"\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMBINED EXAMPLE: Production-Ready Node\n",
    "# =============================================================================\n",
    "\n",
    "def create_production_node(node_name, process_fn, llm):\n",
    "    \"\"\"Factory for creating production-ready nodes.\n",
    "    \n",
    "    Combines multiple patterns:\n",
    "    - Retry logic\n",
    "    - Timeout handling\n",
    "    - Logging\n",
    "    - Error handling with fallback\n",
    "    - Usage tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "    def production_node(state):\n",
    "        start = datetime.now()\n",
    "        \n",
    "        logger.info(f\"[{node_name}] Starting\", extra={\n",
    "            \"thread_id\": state.get(\"thread_id\"),\n",
    "            \"iteration\": state.get(\"iteration\")\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Check token limits\n",
    "            if state.get(\"total_tokens\", 0) > MAX_TOKENS_PER_SESSION:\n",
    "                return {\"error\": \"Token limit reached\"}\n",
    "            \n",
    "            # Process\n",
    "            result = process_fn(state, llm)\n",
    "            \n",
    "            # Log success\n",
    "            duration = (datetime.now() - start).total_seconds()\n",
    "            logger.info(f\"[{node_name}] Completed in {duration:.2f}s\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{node_name}] Failed: {e}\", exc_info=True)\n",
    "            return {\"error\": f\"Node {node_name} failed. Please try again.\"}\n",
    "    \n",
    "    return production_node\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRODUCTION CHECKLIST SUMMARY\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Production Deployment Checklist:\n",
    "\n",
    "RELIABILITY:\n",
    "\u25a1 Retry logic with exponential backoff\n",
    "\u25a1 Timeouts on all external calls\n",
    "\u25a1 Fallback responses for failures\n",
    "\u25a1 Graceful degradation\n",
    "\n",
    "COST MANAGEMENT:\n",
    "\u25a1 Token usage tracking\n",
    "\u25a1 Iteration limits on all loops\n",
    "\u25a1 Model tiering (cheap vs expensive)\n",
    "\u25a1 Session duration limits\n",
    "\n",
    "OBSERVABILITY:\n",
    "\u25a1 Structured logging (JSON format)\n",
    "\u25a1 Request tracing (thread_id)\n",
    "\u25a1 Duration tracking\n",
    "\u25a1 Error tracking with stack traces\n",
    "\u25a1 Consider LangSmith for traces\n",
    "\n",
    "SECURITY:\n",
    "\u25a1 Input validation and length limits\n",
    "\u25a1 Output filtering for sensitive data\n",
    "\u25a1 Rate limiting per user\n",
    "\u25a1 Prompt injection defenses\n",
    "\n",
    "SCALING:\n",
    "\u25a1 External checkpointer (PostgreSQL)\n",
    "\u25a1 Async nodes for concurrency\n",
    "\u25a1 Connection pooling\n",
    "\u25a1 Load balancer ready\n",
    "\n",
    "TESTING:\n",
    "\u25a1 Unit tests for nodes\n",
    "\u25a1 Integration tests for flows\n",
    "\u25a1 End-to-end conversation tests\n",
    "\u25a1 Load tests\n",
    "\u25a1 Adversarial/security tests\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Production Patterns Reference File\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"This file contains reference patterns for production deployment.\")\n",
    "    print(\"Import and adapt these patterns for your specific use case.\")\n",
    "    print(\"\\nPatterns included:\")\n",
    "    print(\"  1. Retry with backoff\")\n",
    "    print(\"  2. Async timeout\")\n",
    "    print(\"  3. Safe node with fallback\")\n",
    "    print(\"  4. Token usage tracking\")\n",
    "    print(\"  5. Iteration limits\")\n",
    "    print(\"  6. Model tiering\")\n",
    "    print(\"  7. Structured logging\")\n",
    "    print(\"  8. Input validation\")\n",
    "    print(\"  9. Output filtering\")\n",
    "    print(\"  10. Rate limiting\")\n",
    "    print(\"  11. Production checkpointer\")\n",
    "    print(\"  12. Test example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_17_advanced_patterns_solutions.ipynb**\n",
    "- Proceed to **Chapter 18**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}