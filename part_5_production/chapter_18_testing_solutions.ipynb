{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Testing and Evaluation - Solutions\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "**Try the exercises in the main notebook first before viewing solutions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.1 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.1: Testing a Validation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_18_1_solution.py\n",
    "# Description: Exercise 1 Solution - Email validation tool with comprehensive tests\n",
    "\n",
    "import pytest\n",
    "import re\n",
    "\n",
    "\n",
    "# The tool function\n",
    "def validate_email(email: str) -> dict:\n",
    "    \"\"\"Validate an email address and return detailed results.\"\"\"\n",
    "    if not email or not isinstance(email, str):\n",
    "        return {\n",
    "            \"valid\": False,\n",
    "            \"error\": \"Email must be a non-empty string\",\n",
    "            \"normalized\": None\n",
    "        }\n",
    "    \n",
    "    # Normalize the email\n",
    "    normalized = email.strip().lower()\n",
    "    \n",
    "    # Basic regex pattern for email validation\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    if not re.match(pattern, email.strip()):\n",
    "        return {\n",
    "            \"valid\": False,\n",
    "            \"error\": \"Invalid email format\",\n",
    "            \"normalized\": normalized\n",
    "        }\n",
    "    \n",
    "    # Check for common issues\n",
    "    if \"..\" in normalized:\n",
    "        return {\n",
    "            \"valid\": False,\n",
    "            \"error\": \"Email cannot contain consecutive dots\",\n",
    "            \"normalized\": normalized\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"valid\": True,\n",
    "        \"error\": None,\n",
    "        \"normalized\": normalized\n",
    "    }\n",
    "\n",
    "\n",
    "# The tests\n",
    "class TestValidateEmail:\n",
    "    \"\"\"Tests for the validate_email function.\"\"\"\n",
    "    \n",
    "    def test_valid_simple_email(self):\n",
    "        \"\"\"Test a standard valid email.\"\"\"\n",
    "        result = validate_email(\"user@example.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "        assert result[\"error\"] is None\n",
    "        assert result[\"normalized\"] == \"user@example.com\"\n",
    "    \n",
    "    def test_valid_email_with_plus(self):\n",
    "        \"\"\"Test email with plus addressing.\"\"\"\n",
    "        result = validate_email(\"user+tag@example.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "        assert result[\"normalized\"] == \"user+tag@example.com\"\n",
    "    \n",
    "    def test_valid_email_with_dots(self):\n",
    "        \"\"\"Test email with dots in local part.\"\"\"\n",
    "        result = validate_email(\"first.last@example.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "    \n",
    "    def test_valid_email_normalized_to_lowercase(self):\n",
    "        \"\"\"Test that emails are normalized to lowercase.\"\"\"\n",
    "        result = validate_email(\"User@EXAMPLE.COM\")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "        assert result[\"normalized\"] == \"user@example.com\"\n",
    "    \n",
    "    def test_valid_email_with_subdomain(self):\n",
    "        \"\"\"Test email with subdomain.\"\"\"\n",
    "        result = validate_email(\"user@mail.example.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "    \n",
    "    def test_invalid_no_at_symbol(self):\n",
    "        \"\"\"Test email without @ symbol.\"\"\"\n",
    "        result = validate_email(\"userexample.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "        assert result[\"error\"] == \"Invalid email format\"\n",
    "    \n",
    "    def test_invalid_no_domain(self):\n",
    "        \"\"\"Test email without domain.\"\"\"\n",
    "        result = validate_email(\"user@\")\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "    \n",
    "    def test_invalid_no_tld(self):\n",
    "        \"\"\"Test email without TLD.\"\"\"\n",
    "        result = validate_email(\"user@example\")\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "    \n",
    "    def test_invalid_consecutive_dots(self):\n",
    "        \"\"\"Test email with consecutive dots.\"\"\"\n",
    "        result = validate_email(\"user..name@example.com\")\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "        assert \"consecutive dots\" in result[\"error\"]\n",
    "    \n",
    "    def test_empty_string(self):\n",
    "        \"\"\"Test empty string input.\"\"\"\n",
    "        result = validate_email(\"\")\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "        assert \"non-empty string\" in result[\"error\"]\n",
    "        assert result[\"normalized\"] is None\n",
    "    \n",
    "    def test_none_input(self):\n",
    "        \"\"\"Test None input.\"\"\"\n",
    "        result = validate_email(None)\n",
    "        \n",
    "        assert result[\"valid\"] is False\n",
    "        assert result[\"normalized\"] is None\n",
    "    \n",
    "    def test_whitespace_trimmed(self):\n",
    "        \"\"\"Test that whitespace is trimmed.\"\"\"\n",
    "        result = validate_email(\"  user@example.com  \")\n",
    "        \n",
    "        assert result[\"valid\"] is True\n",
    "        assert result[\"normalized\"] == \"user@example.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.2: Testing State Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_18_1_solution.py\n",
    "# Description: Exercise 2 Solution - Shopping cart state helpers with tests\n",
    "\n",
    "import pytest\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# State definitions\n",
    "class CartItem(TypedDict):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: float\n",
    "\n",
    "\n",
    "class CartState(TypedDict):\n",
    "    items: list[CartItem]\n",
    "    total: float\n",
    "    coupon_code: str | None\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def add_item(state: CartState, name: str, quantity: int, price: float) -> dict:\n",
    "    \"\"\"Add an item to the cart. Returns state update.\"\"\"\n",
    "    # Check if item already exists\n",
    "    for item in state[\"items\"]:\n",
    "        if item[\"name\"] == name:\n",
    "            # Update quantity of existing item\n",
    "            new_items = []\n",
    "            for i in state[\"items\"]:\n",
    "                if i[\"name\"] == name:\n",
    "                    new_items.append({\n",
    "                        \"name\": name,\n",
    "                        \"quantity\": i[\"quantity\"] + quantity,\n",
    "                        \"price\": price\n",
    "                    })\n",
    "                else:\n",
    "                    new_items.append(i)\n",
    "            return {\"items\": new_items}\n",
    "    \n",
    "    # Add new item\n",
    "    new_items = state[\"items\"] + [{\"name\": name, \"quantity\": quantity, \"price\": price}]\n",
    "    return {\"items\": new_items}\n",
    "\n",
    "\n",
    "def remove_item(state: CartState, name: str) -> dict:\n",
    "    \"\"\"Remove an item from the cart. Returns state update.\"\"\"\n",
    "    new_items = [item for item in state[\"items\"] if item[\"name\"] != name]\n",
    "    \n",
    "    if len(new_items) == len(state[\"items\"]):\n",
    "        # Item wasn't found - return unchanged\n",
    "        return {\"items\": state[\"items\"]}\n",
    "    \n",
    "    return {\"items\": new_items}\n",
    "\n",
    "\n",
    "def apply_coupon(state: CartState, code: str, valid_coupons: dict[str, float]) -> dict:\n",
    "    \"\"\"Apply a coupon code. Returns state update with discount info.\"\"\"\n",
    "    if code not in valid_coupons:\n",
    "        return {\"coupon_code\": None, \"coupon_error\": \"Invalid coupon code\"}\n",
    "    \n",
    "    return {\"coupon_code\": code, \"coupon_error\": None}\n",
    "\n",
    "\n",
    "def calculate_total(state: CartState, valid_coupons: dict[str, float] = None) -> dict:\n",
    "    \"\"\"Calculate cart total. Returns state update.\"\"\"\n",
    "    valid_coupons = valid_coupons or {}\n",
    "    \n",
    "    subtotal = sum(item[\"price\"] * item[\"quantity\"] for item in state[\"items\"])\n",
    "    \n",
    "    discount = 0.0\n",
    "    if state.get(\"coupon_code\") and state[\"coupon_code\"] in valid_coupons:\n",
    "        discount = subtotal * valid_coupons[state[\"coupon_code\"]]\n",
    "    \n",
    "    total = subtotal - discount\n",
    "    return {\"total\": round(total, 2)}\n",
    "\n",
    "\n",
    "# Tests\n",
    "class TestAddItem:\n",
    "    def test_add_new_item(self):\n",
    "        state = {\"items\": [], \"total\": 0, \"coupon_code\": None}\n",
    "        \n",
    "        result = add_item(state, \"Widget\", 2, 9.99)\n",
    "        \n",
    "        assert len(result[\"items\"]) == 1\n",
    "        assert result[\"items\"][0][\"name\"] == \"Widget\"\n",
    "        assert result[\"items\"][0][\"quantity\"] == 2\n",
    "        assert result[\"items\"][0][\"price\"] == 9.99\n",
    "    \n",
    "    def test_add_item_increases_quantity_if_exists(self):\n",
    "        state = {\n",
    "            \"items\": [{\"name\": \"Widget\", \"quantity\": 2, \"price\": 9.99}],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": None\n",
    "        }\n",
    "        \n",
    "        result = add_item(state, \"Widget\", 3, 9.99)\n",
    "        \n",
    "        assert len(result[\"items\"]) == 1\n",
    "        assert result[\"items\"][0][\"quantity\"] == 5\n",
    "    \n",
    "    def test_add_different_items(self):\n",
    "        state = {\n",
    "            \"items\": [{\"name\": \"Widget\", \"quantity\": 2, \"price\": 9.99}],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": None\n",
    "        }\n",
    "        \n",
    "        result = add_item(state, \"Gadget\", 1, 19.99)\n",
    "        \n",
    "        assert len(result[\"items\"]) == 2\n",
    "\n",
    "\n",
    "class TestRemoveItem:\n",
    "    def test_remove_existing_item(self):\n",
    "        state = {\n",
    "            \"items\": [\n",
    "                {\"name\": \"Widget\", \"quantity\": 2, \"price\": 9.99},\n",
    "                {\"name\": \"Gadget\", \"quantity\": 1, \"price\": 19.99}\n",
    "            ],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": None\n",
    "        }\n",
    "        \n",
    "        result = remove_item(state, \"Widget\")\n",
    "        \n",
    "        assert len(result[\"items\"]) == 1\n",
    "        assert result[\"items\"][0][\"name\"] == \"Gadget\"\n",
    "    \n",
    "    def test_remove_nonexistent_item_returns_unchanged(self):\n",
    "        state = {\n",
    "            \"items\": [{\"name\": \"Widget\", \"quantity\": 2, \"price\": 9.99}],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": None\n",
    "        }\n",
    "        \n",
    "        result = remove_item(state, \"Nonexistent\")\n",
    "        \n",
    "        assert len(result[\"items\"]) == 1\n",
    "        assert result[\"items\"][0][\"name\"] == \"Widget\"\n",
    "    \n",
    "    def test_remove_from_empty_cart(self):\n",
    "        state = {\"items\": [], \"total\": 0, \"coupon_code\": None}\n",
    "        \n",
    "        result = remove_item(state, \"Widget\")\n",
    "        \n",
    "        assert result[\"items\"] == []\n",
    "\n",
    "\n",
    "class TestApplyCoupon:\n",
    "    def test_apply_valid_coupon(self):\n",
    "        state = {\"items\": [], \"total\": 0, \"coupon_code\": None}\n",
    "        valid_coupons = {\"SAVE10\": 0.10, \"SAVE20\": 0.20}\n",
    "        \n",
    "        result = apply_coupon(state, \"SAVE10\", valid_coupons)\n",
    "        \n",
    "        assert result[\"coupon_code\"] == \"SAVE10\"\n",
    "        assert result.get(\"coupon_error\") is None\n",
    "    \n",
    "    def test_apply_invalid_coupon(self):\n",
    "        state = {\"items\": [], \"total\": 0, \"coupon_code\": None}\n",
    "        valid_coupons = {\"SAVE10\": 0.10}\n",
    "        \n",
    "        result = apply_coupon(state, \"INVALID\", valid_coupons)\n",
    "        \n",
    "        assert result[\"coupon_code\"] is None\n",
    "        assert \"Invalid\" in result[\"coupon_error\"]\n",
    "\n",
    "\n",
    "class TestCalculateTotal:\n",
    "    def test_calculate_total_no_discount(self):\n",
    "        state = {\n",
    "            \"items\": [\n",
    "                {\"name\": \"Widget\", \"quantity\": 2, \"price\": 10.00},\n",
    "                {\"name\": \"Gadget\", \"quantity\": 1, \"price\": 20.00}\n",
    "            ],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": None\n",
    "        }\n",
    "        \n",
    "        result = calculate_total(state)\n",
    "        \n",
    "        assert result[\"total\"] == 40.00\n",
    "    \n",
    "    def test_calculate_total_with_discount(self):\n",
    "        state = {\n",
    "            \"items\": [{\"name\": \"Widget\", \"quantity\": 1, \"price\": 100.00}],\n",
    "            \"total\": 0,\n",
    "            \"coupon_code\": \"SAVE10\"\n",
    "        }\n",
    "        valid_coupons = {\"SAVE10\": 0.10}\n",
    "        \n",
    "        result = calculate_total(state, valid_coupons)\n",
    "        \n",
    "        assert result[\"total\"] == 90.00\n",
    "    \n",
    "    def test_calculate_total_empty_cart(self):\n",
    "        state = {\"items\": [], \"total\": 0, \"coupon_code\": None}\n",
    "        \n",
    "        result = calculate_total(state)\n",
    "        \n",
    "        assert result[\"total\"] == 0.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.3: Mocking an API Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_18_1_solution.py\n",
    "# Description: Exercise 3 Solution - Weather API tool with async mocking tests\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import AsyncMock, patch, MagicMock\n",
    "import httpx\n",
    "\n",
    "\n",
    "# The weather tool function\n",
    "async def get_weather(city: str) -> dict:\n",
    "    \"\"\"Fetch current weather for a city from the weather API.\"\"\"\n",
    "    if not city or not city.strip():\n",
    "        return {\"error\": \"City name is required\", \"temperature\": None}\n",
    "    \n",
    "    city = city.strip()\n",
    "    \n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=10.0) as client:\n",
    "            response = await client.get(\n",
    "                f\"https://api.weather.example.com/current\",\n",
    "                params={\"city\": city}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 404:\n",
    "                return {\"error\": f\"City '{city}' not found\", \"temperature\": None}\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"city\": data[\"location\"][\"name\"],\n",
    "                \"temperature\": data[\"current\"][\"temp_c\"],\n",
    "                \"condition\": data[\"current\"][\"condition\"][\"text\"],\n",
    "                \"error\": None\n",
    "            }\n",
    "    \n",
    "    except httpx.TimeoutException:\n",
    "        return {\"error\": \"Request timed out\", \"temperature\": None}\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return {\"error\": f\"API error: {e.response.status_code}\", \"temperature\": None}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {str(e)}\", \"temperature\": None}\n",
    "\n",
    "\n",
    "# Tests\n",
    "class TestGetWeather:\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_successful_response(self):\n",
    "        \"\"\"Test correct parsing of successful API response.\"\"\"\n",
    "        mock_api_response = {\n",
    "            \"location\": {\"name\": \"London\"},\n",
    "            \"current\": {\n",
    "                \"temp_c\": 15.5,\n",
    "                \"condition\": {\"text\": \"Partly cloudy\"}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with patch(\"httpx.AsyncClient\") as MockClient:\n",
    "            mock_client = AsyncMock()\n",
    "            mock_response = MagicMock()\n",
    "            mock_response.status_code = 200\n",
    "            mock_response.json.return_value = mock_api_response\n",
    "            mock_response.raise_for_status = MagicMock()\n",
    "            mock_client.get.return_value = mock_response\n",
    "            MockClient.return_value.__aenter__.return_value = mock_client\n",
    "            \n",
    "            result = await get_weather(\"London\")\n",
    "            \n",
    "            assert result[\"city\"] == \"London\"\n",
    "            assert result[\"temperature\"] == 15.5\n",
    "            assert result[\"condition\"] == \"Partly cloudy\"\n",
    "            assert result[\"error\"] is None\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_city_not_found_404(self):\n",
    "        \"\"\"Test handling of 404 response.\"\"\"\n",
    "        with patch(\"httpx.AsyncClient\") as MockClient:\n",
    "            mock_client = AsyncMock()\n",
    "            mock_response = MagicMock()\n",
    "            mock_response.status_code = 404\n",
    "            mock_client.get.return_value = mock_response\n",
    "            MockClient.return_value.__aenter__.return_value = mock_client\n",
    "            \n",
    "            result = await get_weather(\"Nonexistentville\")\n",
    "            \n",
    "            assert result[\"temperature\"] is None\n",
    "            assert \"not found\" in result[\"error\"]\n",
    "            assert \"Nonexistentville\" in result[\"error\"]\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_timeout_handling(self):\n",
    "        \"\"\"Test handling of network timeout.\"\"\"\n",
    "        with patch(\"httpx.AsyncClient\") as MockClient:\n",
    "            mock_client = AsyncMock()\n",
    "            mock_client.get.side_effect = httpx.TimeoutException(\"Connection timed out\")\n",
    "            MockClient.return_value.__aenter__.return_value = mock_client\n",
    "            \n",
    "            result = await get_weather(\"Tokyo\")\n",
    "            \n",
    "            assert result[\"temperature\"] is None\n",
    "            assert \"timed out\" in result[\"error\"].lower()\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_correct_city_passed_to_api(self):\n",
    "        \"\"\"Test that the city name is correctly passed to the API.\"\"\"\n",
    "        mock_api_response = {\n",
    "            \"location\": {\"name\": \"Paris\"},\n",
    "            \"current\": {\"temp_c\": 20.0, \"condition\": {\"text\": \"Sunny\"}}\n",
    "        }\n",
    "        \n",
    "        with patch(\"httpx.AsyncClient\") as MockClient:\n",
    "            mock_client = AsyncMock()\n",
    "            mock_response = MagicMock()\n",
    "            mock_response.status_code = 200\n",
    "            mock_response.json.return_value = mock_api_response\n",
    "            mock_response.raise_for_status = MagicMock()\n",
    "            mock_client.get.return_value = mock_response\n",
    "            MockClient.return_value.__aenter__.return_value = mock_client\n",
    "            \n",
    "            await get_weather(\"Paris\")\n",
    "            \n",
    "            # Verify the API was called with correct parameters\n",
    "            mock_client.get.assert_called_once()\n",
    "            call_kwargs = mock_client.get.call_args\n",
    "            assert call_kwargs.kwargs[\"params\"][\"city\"] == \"Paris\"\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_empty_city_name(self):\n",
    "        \"\"\"Test handling of empty city name.\"\"\"\n",
    "        result = await get_weather(\"\")\n",
    "        \n",
    "        assert result[\"temperature\"] is None\n",
    "        assert \"required\" in result[\"error\"].lower()\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_whitespace_city_name_trimmed(self):\n",
    "        \"\"\"Test that whitespace is trimmed from city name.\"\"\"\n",
    "        mock_api_response = {\n",
    "            \"location\": {\"name\": \"Berlin\"},\n",
    "            \"current\": {\"temp_c\": 18.0, \"condition\": {\"text\": \"Clear\"}}\n",
    "        }\n",
    "        \n",
    "        with patch(\"httpx.AsyncClient\") as MockClient:\n",
    "            mock_client = AsyncMock()\n",
    "            mock_response = MagicMock()\n",
    "            mock_response.status_code = 200\n",
    "            mock_response.json.return_value = mock_api_response\n",
    "            mock_response.raise_for_status = MagicMock()\n",
    "            mock_client.get.return_value = mock_response\n",
    "            MockClient.return_value.__aenter__.return_value = mock_client\n",
    "            \n",
    "            await get_weather(\"  Berlin  \")\n",
    "            \n",
    "            call_kwargs = mock_client.get.call_args\n",
    "            assert call_kwargs.kwargs[\"params\"][\"city\"] == \"Berlin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.1: Testing a Multi-Node Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_18_2_solution.py\n",
    "# Description: Exercise 1 Solution - Order processing workflow with integration tests\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import MagicMock\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "# State definition\n",
    "class OrderState(TypedDict):\n",
    "    order_id: str\n",
    "    items: list[dict]\n",
    "    subtotal: float | None\n",
    "    tax: float | None\n",
    "    total: float | None\n",
    "    is_valid: bool | None\n",
    "    error: str | None\n",
    "    confirmation_message: str | None\n",
    "\n",
    "\n",
    "# Node functions\n",
    "def validate_order(state: OrderState) -> dict:\n",
    "    \"\"\"Validate order data.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    if not state.get(\"order_id\"):\n",
    "        errors.append(\"Order ID is required\")\n",
    "    \n",
    "    if not state.get(\"items\") or len(state[\"items\"]) == 0:\n",
    "        errors.append(\"Order must have at least one item\")\n",
    "    \n",
    "    for item in state.get(\"items\", []):\n",
    "        if item.get(\"quantity\", 0) <= 0:\n",
    "            errors.append(f\"Invalid quantity for {item.get('name', 'unknown item')}\")\n",
    "        if item.get(\"price\", 0) <= 0:\n",
    "            errors.append(f\"Invalid price for {item.get('name', 'unknown item')}\")\n",
    "    \n",
    "    if errors:\n",
    "        return {\"is_valid\": False, \"error\": \"; \".join(errors)}\n",
    "    \n",
    "    return {\"is_valid\": True, \"error\": None}\n",
    "\n",
    "\n",
    "def calculate_total(state: OrderState) -> dict:\n",
    "    \"\"\"Calculate order total with tax.\"\"\"\n",
    "    subtotal = sum(\n",
    "        item[\"price\"] * item[\"quantity\"] \n",
    "        for item in state[\"items\"]\n",
    "    )\n",
    "    tax_rate = 0.08  # 8% tax\n",
    "    tax = round(subtotal * tax_rate, 2)\n",
    "    total = round(subtotal + tax, 2)\n",
    "    \n",
    "    return {\n",
    "        \"subtotal\": subtotal,\n",
    "        \"tax\": tax,\n",
    "        \"total\": total\n",
    "    }\n",
    "\n",
    "\n",
    "def confirm_order(state: OrderState) -> dict:\n",
    "    \"\"\"Generate order confirmation.\"\"\"\n",
    "    message = (\n",
    "        f\"Order {state['order_id']} confirmed! \"\n",
    "        f\"Total: ${state['total']:.2f} (includes ${state['tax']:.2f} tax)\"\n",
    "    )\n",
    "    return {\"confirmation_message\": message}\n",
    "\n",
    "\n",
    "def route_after_validation(state: OrderState) -> str:\n",
    "    \"\"\"Route based on validation result.\"\"\"\n",
    "    if state.get(\"is_valid\"):\n",
    "        return \"calculate_total\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "\n",
    "def build_order_graph():\n",
    "    \"\"\"Build the order processing graph.\"\"\"\n",
    "    graph = StateGraph(OrderState)\n",
    "    \n",
    "    graph.add_node(\"validate_order\", validate_order)\n",
    "    graph.add_node(\"calculate_total\", calculate_total)\n",
    "    graph.add_node(\"confirm_order\", confirm_order)\n",
    "    \n",
    "    graph.add_edge(START, \"validate_order\")\n",
    "    graph.add_conditional_edges(\"validate_order\", route_after_validation)\n",
    "    graph.add_edge(\"calculate_total\", \"confirm_order\")\n",
    "    graph.add_edge(\"confirm_order\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Integration tests\n",
    "class TestOrderProcessingWorkflow:\n",
    "    \"\"\"Integration tests for the complete order processing workflow.\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def graph(self):\n",
    "        return build_order_graph()\n",
    "    \n",
    "    def test_valid_order_flows_through_all_nodes(self, graph):\n",
    "        \"\"\"Test that valid orders complete the full workflow.\"\"\"\n",
    "        initial_state = {\n",
    "            \"order_id\": \"ORD-001\",\n",
    "            \"items\": [\n",
    "                {\"name\": \"Widget\", \"price\": 10.00, \"quantity\": 2},\n",
    "                {\"name\": \"Gadget\", \"price\": 25.00, \"quantity\": 1}\n",
    "            ],\n",
    "            \"subtotal\": None,\n",
    "            \"tax\": None,\n",
    "            \"total\": None,\n",
    "            \"is_valid\": None,\n",
    "            \"error\": None,\n",
    "            \"confirmation_message\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Verify validation passed\n",
    "        assert result[\"is_valid\"] is True\n",
    "        assert result[\"error\"] is None\n",
    "        \n",
    "        # Verify calculation happened\n",
    "        assert result[\"subtotal\"] == 45.00  # (10*2) + (25*1)\n",
    "        assert result[\"tax\"] == 3.60  # 45 * 0.08\n",
    "        assert result[\"total\"] == 48.60\n",
    "        \n",
    "        # Verify confirmation generated\n",
    "        assert result[\"confirmation_message\"] is not None\n",
    "        assert \"ORD-001\" in result[\"confirmation_message\"]\n",
    "        assert \"48.60\" in result[\"confirmation_message\"]\n",
    "    \n",
    "    def test_invalid_order_stops_at_validation(self, graph):\n",
    "        \"\"\"Test that invalid orders don't proceed to calculation.\"\"\"\n",
    "        initial_state = {\n",
    "            \"order_id\": \"ORD-002\",\n",
    "            \"items\": [],  # Empty items - invalid!\n",
    "            \"subtotal\": None,\n",
    "            \"tax\": None,\n",
    "            \"total\": None,\n",
    "            \"is_valid\": None,\n",
    "            \"error\": None,\n",
    "            \"confirmation_message\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Validation should fail\n",
    "        assert result[\"is_valid\"] is False\n",
    "        assert result[\"error\"] is not None\n",
    "        assert \"at least one item\" in result[\"error\"]\n",
    "        \n",
    "        # Calculation should NOT have happened\n",
    "        assert result[\"subtotal\"] is None\n",
    "        assert result[\"total\"] is None\n",
    "        \n",
    "        # Confirmation should NOT have happened\n",
    "        assert result[\"confirmation_message\"] is None\n",
    "    \n",
    "    def test_state_accumulates_through_workflow(self, graph):\n",
    "        \"\"\"Test that state is correctly accumulated at each step.\"\"\"\n",
    "        initial_state = {\n",
    "            \"order_id\": \"ORD-003\",\n",
    "            \"items\": [{\"name\": \"Test Item\", \"price\": 100.00, \"quantity\": 1}],\n",
    "            \"subtotal\": None,\n",
    "            \"tax\": None,\n",
    "            \"total\": None,\n",
    "            \"is_valid\": None,\n",
    "            \"error\": None,\n",
    "            \"confirmation_message\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Original data should be preserved\n",
    "        assert result[\"order_id\"] == \"ORD-003\"\n",
    "        assert result[\"items\"] == initial_state[\"items\"]\n",
    "        \n",
    "        # Each stage should have added its data\n",
    "        assert result[\"is_valid\"] is not None  # From validate_order\n",
    "        assert result[\"subtotal\"] is not None  # From calculate_total\n",
    "        assert result[\"confirmation_message\"] is not None  # From confirm_order\n",
    "    \n",
    "    def test_missing_order_id_fails_validation(self, graph):\n",
    "        \"\"\"Test validation catches missing order ID.\"\"\"\n",
    "        initial_state = {\n",
    "            \"order_id\": \"\",  # Empty order ID\n",
    "            \"items\": [{\"name\": \"Item\", \"price\": 10.00, \"quantity\": 1}],\n",
    "            \"subtotal\": None,\n",
    "            \"tax\": None,\n",
    "            \"total\": None,\n",
    "            \"is_valid\": None,\n",
    "            \"error\": None,\n",
    "            \"confirmation_message\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"is_valid\"] is False\n",
    "        assert \"Order ID\" in result[\"error\"]\n",
    "    \n",
    "    def test_invalid_item_quantity_fails_validation(self, graph):\n",
    "        \"\"\"Test validation catches invalid quantities.\"\"\"\n",
    "        initial_state = {\n",
    "            \"order_id\": \"ORD-004\",\n",
    "            \"items\": [{\"name\": \"Item\", \"price\": 10.00, \"quantity\": -1}],\n",
    "            \"subtotal\": None,\n",
    "            \"tax\": None,\n",
    "            \"total\": None,\n",
    "            \"is_valid\": None,\n",
    "            \"error\": None,\n",
    "            \"confirmation_message\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"is_valid\"] is False\n",
    "        assert \"Invalid quantity\" in result[\"error\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.2: Testing Branching Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_18_2_solution.py\n",
    "# Description: Exercise 2 Solution - Sentiment routing with integration tests\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import MagicMock, AsyncMock\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class SentimentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    sentiment: str | None\n",
    "    handler_invoked: str | None\n",
    "    response: str | None\n",
    "\n",
    "\n",
    "def classify_sentiment(state: SentimentState, llm) -> dict:\n",
    "    \"\"\"Classify the sentiment of the last message.\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=f\"Classify sentiment: {last_message}\")])\n",
    "    sentiment = response.content.strip().lower()\n",
    "    \n",
    "    # Normalize to expected values\n",
    "    if sentiment not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        sentiment = \"neutral\"\n",
    "    \n",
    "    return {\"sentiment\": sentiment}\n",
    "\n",
    "\n",
    "def route_by_sentiment(state: SentimentState) -> str:\n",
    "    \"\"\"Route to appropriate handler based on sentiment.\"\"\"\n",
    "    sentiment = state.get(\"sentiment\", \"neutral\")\n",
    "    \n",
    "    if sentiment == \"positive\":\n",
    "        return \"handle_positive\"\n",
    "    elif sentiment == \"negative\":\n",
    "        return \"handle_negative\"\n",
    "    else:\n",
    "        return \"handle_neutral\"\n",
    "\n",
    "\n",
    "def handle_positive(state: SentimentState) -> dict:\n",
    "    \"\"\"Handle positive sentiment messages.\"\"\"\n",
    "    return {\n",
    "        \"handler_invoked\": \"positive\",\n",
    "        \"response\": \"Thank you for your positive feedback! We're glad you're happy.\",\n",
    "        \"messages\": [AIMessage(content=\"Thank you for your positive feedback!\")]\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_negative(state: SentimentState) -> dict:\n",
    "    \"\"\"Handle negative sentiment messages.\"\"\"\n",
    "    return {\n",
    "        \"handler_invoked\": \"negative\",\n",
    "        \"response\": \"We're sorry to hear you're unhappy. Let us help resolve this.\",\n",
    "        \"messages\": [AIMessage(content=\"We're sorry to hear that. How can we help?\")]\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_neutral(state: SentimentState) -> dict:\n",
    "    \"\"\"Handle neutral sentiment messages.\"\"\"\n",
    "    return {\n",
    "        \"handler_invoked\": \"neutral\",\n",
    "        \"response\": \"Thank you for reaching out. How can we assist you today?\",\n",
    "        \"messages\": [AIMessage(content=\"How can we assist you today?\")]\n",
    "    }\n",
    "\n",
    "\n",
    "def build_sentiment_graph(llm):\n",
    "    \"\"\"Build the sentiment routing graph.\"\"\"\n",
    "    graph = StateGraph(SentimentState)\n",
    "    \n",
    "    graph.add_node(\"classify_sentiment\", lambda s: classify_sentiment(s, llm))\n",
    "    graph.add_node(\"handle_positive\", handle_positive)\n",
    "    graph.add_node(\"handle_negative\", handle_negative)\n",
    "    graph.add_node(\"handle_neutral\", handle_neutral)\n",
    "    \n",
    "    graph.add_edge(START, \"classify_sentiment\")\n",
    "    graph.add_conditional_edges(\"classify_sentiment\", route_by_sentiment)\n",
    "    graph.add_edge(\"handle_positive\", END)\n",
    "    graph.add_edge(\"handle_negative\", END)\n",
    "    graph.add_edge(\"handle_neutral\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Integration tests\n",
    "class TestSentimentRouting:\n",
    "    \"\"\"Integration tests for sentiment-based routing.\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def mock_llm_positive(self):\n",
    "        \"\"\"Mock LLM that classifies as positive.\"\"\"\n",
    "        llm = MagicMock()\n",
    "        llm.invoke.return_value = MagicMock(content=\"positive\")\n",
    "        return llm\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def mock_llm_negative(self):\n",
    "        \"\"\"Mock LLM that classifies as negative.\"\"\"\n",
    "        llm = MagicMock()\n",
    "        llm.invoke.return_value = MagicMock(content=\"negative\")\n",
    "        return llm\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def mock_llm_neutral(self):\n",
    "        \"\"\"Mock LLM that classifies as neutral.\"\"\"\n",
    "        llm = MagicMock()\n",
    "        llm.invoke.return_value = MagicMock(content=\"neutral\")\n",
    "        return llm\n",
    "    \n",
    "    def test_positive_sentiment_routes_to_positive_handler(self, mock_llm_positive):\n",
    "        \"\"\"Test that positive sentiment routes correctly.\"\"\"\n",
    "        graph = build_sentiment_graph(mock_llm_positive)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"I love your product!\")],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"sentiment\"] == \"positive\"\n",
    "        assert result[\"handler_invoked\"] == \"positive\"\n",
    "        assert \"positive feedback\" in result[\"response\"].lower()\n",
    "    \n",
    "    def test_negative_sentiment_routes_to_negative_handler(self, mock_llm_negative):\n",
    "        \"\"\"Test that negative sentiment routes correctly.\"\"\"\n",
    "        graph = build_sentiment_graph(mock_llm_negative)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"This is terrible!\")],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"sentiment\"] == \"negative\"\n",
    "        assert result[\"handler_invoked\"] == \"negative\"\n",
    "        assert \"sorry\" in result[\"response\"].lower()\n",
    "    \n",
    "    def test_neutral_sentiment_routes_to_neutral_handler(self, mock_llm_neutral):\n",
    "        \"\"\"Test that neutral sentiment routes correctly.\"\"\"\n",
    "        graph = build_sentiment_graph(mock_llm_neutral)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"What are your hours?\")],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"sentiment\"] == \"neutral\"\n",
    "        assert result[\"handler_invoked\"] == \"neutral\"\n",
    "        assert \"assist\" in result[\"response\"].lower()\n",
    "    \n",
    "    def test_unknown_sentiment_defaults_to_neutral(self):\n",
    "        \"\"\"Test that unrecognized sentiment defaults to neutral.\"\"\"\n",
    "        mock_llm = MagicMock()\n",
    "        mock_llm.invoke.return_value = MagicMock(content=\"confused\")\n",
    "        \n",
    "        graph = build_sentiment_graph(mock_llm)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"...\")],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Should normalize to neutral\n",
    "        assert result[\"sentiment\"] == \"neutral\"\n",
    "        assert result[\"handler_invoked\"] == \"neutral\"\n",
    "    \n",
    "    def test_llm_is_called_with_user_message(self, mock_llm_positive):\n",
    "        \"\"\"Verify the LLM receives the user's message for classification.\"\"\"\n",
    "        graph = build_sentiment_graph(mock_llm_positive)\n",
    "        \n",
    "        test_message = \"This is my specific test message\"\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=test_message)],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        graph.invoke(initial_state)\n",
    "        \n",
    "        # Verify LLM was called\n",
    "        mock_llm_positive.invoke.assert_called_once()\n",
    "        \n",
    "        # Verify the message content was passed\n",
    "        call_args = mock_llm_positive.invoke.call_args[0][0]\n",
    "        assert any(test_message in str(msg) for msg in call_args)\n",
    "    \n",
    "    def test_response_message_added_to_state(self, mock_llm_positive):\n",
    "        \"\"\"Verify handler adds response to messages.\"\"\"\n",
    "        graph = build_sentiment_graph(mock_llm_positive)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"Great service!\")],\n",
    "            \"sentiment\": None,\n",
    "            \"handler_invoked\": None,\n",
    "            \"response\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Should have original message plus AI response\n",
    "        assert len(result[\"messages\"]) >= 2\n",
    "        \n",
    "        # Last message should be from AI\n",
    "        ai_messages = [m for m in result[\"messages\"] if isinstance(m, AIMessage)]\n",
    "        assert len(ai_messages) >= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.3: Testing Conversation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_18_2_solution.py\n",
    "# Description: Exercise 3 Solution - FAQ agent with context maintenance\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import MagicMock\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "class FAQState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    topics_discussed: list[str]\n",
    "    current_topic: str | None\n",
    "\n",
    "\n",
    "def detect_topic(message: str, previous_topics: list[str]) -> str:\n",
    "    \"\"\"Detect the topic of a message, considering context.\"\"\"\n",
    "    message_lower = message.lower()\n",
    "    \n",
    "    # Direct topic mentions\n",
    "    if \"pricing\" in message_lower or \"cost\" in message_lower or \"price\" in message_lower:\n",
    "        return \"pricing\"\n",
    "    elif \"shipping\" in message_lower or \"delivery\" in message_lower:\n",
    "        return \"shipping\"\n",
    "    elif \"return\" in message_lower or \"refund\" in message_lower:\n",
    "        return \"returns\"\n",
    "    elif \"discount\" in message_lower or \"coupon\" in message_lower or \"sale\" in message_lower:\n",
    "        # Context-aware: \"discount\" after \"pricing\" is about pricing discounts\n",
    "        if \"pricing\" in previous_topics:\n",
    "            return \"pricing\"\n",
    "        return \"discounts\"\n",
    "    elif \"hours\" in message_lower or \"open\" in message_lower:\n",
    "        return \"hours\"\n",
    "    \n",
    "    # If no direct match, continue previous topic\n",
    "    if previous_topics:\n",
    "        return previous_topics[-1]\n",
    "    \n",
    "    return \"general\"\n",
    "\n",
    "\n",
    "def answer_question(state: FAQState, llm) -> dict:\n",
    "    \"\"\"Generate an answer based on topic and context.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content\n",
    "    topics_discussed = state.get(\"topics_discussed\", [])\n",
    "    \n",
    "    # Detect topic with context awareness\n",
    "    topic = detect_topic(last_message, topics_discussed)\n",
    "    \n",
    "    # Build context-aware prompt\n",
    "    context = \"\"\n",
    "    if topics_discussed:\n",
    "        context = f\"Previously discussed topics: {', '.join(topics_discussed)}. \"\n",
    "    \n",
    "    prompt = f\"{context}User question about {topic}: {last_message}\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"You are answering a question about {topic}.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ])\n",
    "    \n",
    "    # Update topics discussed\n",
    "    new_topics = topics_discussed.copy()\n",
    "    if topic not in new_topics:\n",
    "        new_topics.append(topic)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"topics_discussed\": new_topics,\n",
    "        \"current_topic\": topic\n",
    "    }\n",
    "\n",
    "\n",
    "def build_faq_graph(llm):\n",
    "    \"\"\"Build the FAQ agent graph.\"\"\"\n",
    "    graph = StateGraph(FAQState)\n",
    "    \n",
    "    graph.add_node(\"answer\", lambda s: answer_question(s, llm))\n",
    "    \n",
    "    graph.add_edge(START, \"answer\")\n",
    "    graph.add_edge(\"answer\", END)\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# Integration tests\n",
    "class TestFAQContextMaintenance:\n",
    "    \"\"\"Integration tests for FAQ context maintenance.\"\"\"\n",
    "    \n",
    "    @pytest.fixture\n",
    "    def mock_llm(self):\n",
    "        \"\"\"Create a mock LLM that echoes the topic.\"\"\"\n",
    "        llm = MagicMock()\n",
    "        # Return response that includes topic for verification\n",
    "        def dynamic_response(messages):\n",
    "            # Extract topic from system message\n",
    "            system_msg = next((m for m in messages if isinstance(m, SystemMessage)), None)\n",
    "            topic = \"unknown\"\n",
    "            if system_msg:\n",
    "                topic = system_msg.content.split(\"about \")[-1].rstrip(\".\")\n",
    "            return MagicMock(content=f\"Here's information about {topic}\")\n",
    "        \n",
    "        llm.invoke.side_effect = dynamic_response\n",
    "        return llm\n",
    "    \n",
    "    def test_first_question_starts_fresh_context(self, mock_llm):\n",
    "        \"\"\"Test that first question has no prior context.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"What is your pricing?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        assert result[\"current_topic\"] == \"pricing\"\n",
    "        assert \"pricing\" in result[\"topics_discussed\"]\n",
    "        assert len(result[\"topics_discussed\"]) == 1\n",
    "    \n",
    "    def test_context_maintained_across_questions(self, mock_llm):\n",
    "        \"\"\"Test that context accumulates across questions.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        # First question about pricing\n",
    "        state_1 = {\n",
    "            \"messages\": [HumanMessage(content=\"What is your pricing?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        result_1 = graph.invoke(state_1)\n",
    "        \n",
    "        assert result_1[\"current_topic\"] == \"pricing\"\n",
    "        assert result_1[\"topics_discussed\"] == [\"pricing\"]\n",
    "        \n",
    "        # Second question - simulate continuing conversation\n",
    "        state_2 = {\n",
    "            \"messages\": result_1[\"messages\"] + [HumanMessage(content=\"What about shipping?\")],\n",
    "            \"topics_discussed\": result_1[\"topics_discussed\"],\n",
    "            \"current_topic\": result_1[\"current_topic\"]\n",
    "        }\n",
    "        result_2 = graph.invoke(state_2)\n",
    "        \n",
    "        assert result_2[\"current_topic\"] == \"shipping\"\n",
    "        assert \"pricing\" in result_2[\"topics_discussed\"]\n",
    "        assert \"shipping\" in result_2[\"topics_discussed\"]\n",
    "    \n",
    "    def test_discount_question_uses_pricing_context(self, mock_llm):\n",
    "        \"\"\"Test that 'discounts' after 'pricing' stays in pricing context.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        # First establish pricing context\n",
    "        state_1 = {\n",
    "            \"messages\": [HumanMessage(content=\"What are your prices?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        result_1 = graph.invoke(state_1)\n",
    "        \n",
    "        # Now ask about discounts - should relate to pricing\n",
    "        state_2 = {\n",
    "            \"messages\": result_1[\"messages\"] + [HumanMessage(content=\"Any discounts available?\")],\n",
    "            \"topics_discussed\": result_1[\"topics_discussed\"],\n",
    "            \"current_topic\": result_1[\"current_topic\"]\n",
    "        }\n",
    "        result_2 = graph.invoke(state_2)\n",
    "        \n",
    "        # Because pricing was discussed, discount should be treated as pricing-related\n",
    "        assert result_2[\"current_topic\"] == \"pricing\"\n",
    "    \n",
    "    def test_discount_question_without_context(self, mock_llm):\n",
    "        \"\"\"Test that 'discounts' without context is its own topic.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        # Ask about discounts first (no pricing context)\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=\"Do you have any discounts?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Without pricing context, should be its own topic\n",
    "        assert result[\"current_topic\"] == \"discounts\"\n",
    "    \n",
    "    def test_ambiguous_followup_uses_previous_topic(self, mock_llm):\n",
    "        \"\"\"Test that ambiguous follow-ups use the previous topic.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        # Establish shipping topic\n",
    "        state_1 = {\n",
    "            \"messages\": [HumanMessage(content=\"How does shipping work?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        result_1 = graph.invoke(state_1)\n",
    "        \n",
    "        # Ask ambiguous follow-up\n",
    "        state_2 = {\n",
    "            \"messages\": result_1[\"messages\"] + [HumanMessage(content=\"What about international?\")],\n",
    "            \"topics_discussed\": result_1[\"topics_discussed\"],\n",
    "            \"current_topic\": result_1[\"current_topic\"]\n",
    "        }\n",
    "        result_2 = graph.invoke(state_2)\n",
    "        \n",
    "        # Should continue with shipping topic\n",
    "        assert result_2[\"current_topic\"] == \"shipping\"\n",
    "    \n",
    "    def test_messages_preserved_through_conversation(self, mock_llm):\n",
    "        \"\"\"Test that all messages are preserved in conversation.\"\"\"\n",
    "        graph = build_faq_graph(mock_llm)\n",
    "        \n",
    "        # Build up a multi-turn conversation\n",
    "        state = {\n",
    "            \"messages\": [HumanMessage(content=\"Question 1: What are your hours?\")],\n",
    "            \"topics_discussed\": [],\n",
    "            \"current_topic\": None\n",
    "        }\n",
    "        result = graph.invoke(state)\n",
    "        \n",
    "        # Add second question\n",
    "        state = {\n",
    "            \"messages\": result[\"messages\"] + [HumanMessage(content=\"Question 2: What about weekends?\")],\n",
    "            \"topics_discussed\": result[\"topics_discussed\"],\n",
    "            \"current_topic\": result[\"current_topic\"]\n",
    "        }\n",
    "        result = graph.invoke(state)\n",
    "        \n",
    "        # All messages should be present\n",
    "        all_content = \" \".join(m.content for m in result[\"messages\"])\n",
    "        assert \"Question 1\" in all_content\n",
    "        assert \"Question 2\" in all_content\n",
    "        \n",
    "        # Should have both user questions and AI responses\n",
    "        human_messages = [m for m in result[\"messages\"] if isinstance(m, HumanMessage)]\n",
    "        ai_messages = [m for m in result[\"messages\"] if isinstance(m, AIMessage)]\n",
    "        \n",
    "        assert len(human_messages) == 2\n",
    "        assert len(ai_messages) == 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.3 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.1: Custom Evaluation Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_18_3_solution.py\n",
    "# Description: Exercise 1 Solution - Customer service evaluator with 4 criteria\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class CustomerServiceEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate customer service agent responses across four criteria:\n",
    "    - Accuracy: Factual correctness of information provided\n",
    "    - Empathy: Emotional tone and understanding\n",
    "    - Actionability: Does it help solve the problem?\n",
    "    - Policy Compliance: Stays within company guidelines\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0)\n",
    "        \n",
    "        # Define the rubric for each criterion\n",
    "        self.rubrics = {\n",
    "            \"accuracy\": \"\"\"\n",
    "Evaluate the ACCURACY of the response - is the information factually correct?\n",
    "\n",
    "1 = Contains significant factual errors or misinformation\n",
    "2 = Contains some inaccuracies that could mislead the customer\n",
    "3 = Mostly accurate with minor errors that don't affect the outcome\n",
    "4 = Accurate information with only trivial imprecisions\n",
    "5 = Completely accurate, all facts verified and correct\n",
    "\"\"\",\n",
    "            \"empathy\": \"\"\"\n",
    "Evaluate the EMPATHY of the response - does it show understanding and care?\n",
    "\n",
    "1 = Cold, dismissive, or robotic with no acknowledgment of customer feelings\n",
    "2 = Minimal acknowledgment, feels scripted and impersonal\n",
    "3 = Adequate acknowledgment but could be warmer\n",
    "4 = Warm and understanding, makes customer feel heard\n",
    "5 = Exceptional empathy, perfectly balances professionalism with genuine care\n",
    "\"\"\",\n",
    "            \"actionability\": \"\"\"\n",
    "Evaluate the ACTIONABILITY of the response - does it help solve the problem?\n",
    "\n",
    "1 = No clear next steps, customer left confused about what to do\n",
    "2 = Vague suggestions that don't directly address the issue\n",
    "3 = Some helpful guidance but incomplete or unclear\n",
    "4 = Clear, specific steps that should resolve the issue\n",
    "5 = Comprehensive solution with alternatives and proactive help\n",
    "\"\"\",\n",
    "            \"policy_compliance\": \"\"\"\n",
    "Evaluate POLICY COMPLIANCE - does the response stay within appropriate boundaries?\n",
    "\n",
    "1 = Violates policies (unauthorized promises, shares restricted info, etc.)\n",
    "2 = Bends rules inappropriately or makes questionable commitments\n",
    "3 = Follows policies but rigidly, missing opportunities to help within bounds\n",
    "4 = Good policy adherence while still being helpful\n",
    "5 = Perfect balance of policy compliance and customer advocacy\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        customer_query: str,\n",
    "        agent_response: str,\n",
    "        context: dict | None = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate a customer service response across all criteria.\n",
    "        \n",
    "        Args:\n",
    "            customer_query: The customer's original message\n",
    "            agent_response: The agent's response to evaluate\n",
    "            context: Optional context (e.g., company policies, customer history)\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive evaluation with scores and feedback\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"customer_query\": customer_query,\n",
    "            \"agent_response\": agent_response[:300] + \"...\" if len(agent_response) > 300 else agent_response,\n",
    "            \"criteria_scores\": {},\n",
    "            \"detailed_feedback\": {},\n",
    "            \"summary\": {}\n",
    "        }\n",
    "        \n",
    "        context_str = \"\"\n",
    "        if context:\n",
    "            context_str = f\"\\n\\nCONTEXT:\\n{self._format_context(context)}\"\n",
    "        \n",
    "        # Evaluate each criterion\n",
    "        for criterion, rubric in self.rubrics.items():\n",
    "            score, feedback = self._evaluate_criterion(\n",
    "                customer_query=customer_query,\n",
    "                agent_response=agent_response,\n",
    "                criterion=criterion,\n",
    "                rubric=rubric,\n",
    "                context_str=context_str\n",
    "            )\n",
    "            results[\"criteria_scores\"][criterion] = score\n",
    "            results[\"detailed_feedback\"][criterion] = feedback\n",
    "        \n",
    "        # Calculate summary\n",
    "        scores = list(results[\"criteria_scores\"].values())\n",
    "        results[\"summary\"] = {\n",
    "            \"average_score\": sum(scores) / len(scores),\n",
    "            \"lowest_criterion\": min(results[\"criteria_scores\"], key=results[\"criteria_scores\"].get),\n",
    "            \"highest_criterion\": max(results[\"criteria_scores\"], key=results[\"criteria_scores\"].get),\n",
    "            \"passed\": all(s >= 3 for s in scores),\n",
    "            \"needs_improvement\": [c for c, s in results[\"criteria_scores\"].items() if s < 3]\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_criterion(\n",
    "        self,\n",
    "        customer_query: str,\n",
    "        agent_response: str,\n",
    "        criterion: str,\n",
    "        rubric: str,\n",
    "        context_str: str\n",
    "    ) -> tuple[int, str]:\n",
    "        \"\"\"Evaluate a single criterion using LLM.\"\"\"\n",
    "        \n",
    "        system_prompt = f\"\"\"You are an expert evaluator for customer service interactions.\n",
    "Your task is to evaluate responses based on specific criteria.\n",
    "\n",
    "{rubric}\n",
    "\n",
    "Respond in exactly this format:\n",
    "SCORE: [1-5]\n",
    "FEEDBACK: [Specific feedback explaining the score with examples from the response]\n",
    "SUGGESTION: [One specific way to improve, if score < 5]\"\"\"\n",
    "\n",
    "        evaluation_prompt = f\"\"\"CUSTOMER QUERY:\n",
    "{customer_query}\n",
    "\n",
    "AGENT RESPONSE:\n",
    "{agent_response}\n",
    "{context_str}\n",
    "\n",
    "Evaluate this response for {criterion.upper()}.\"\"\"\n",
    "\n",
    "        result = self.llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=evaluation_prompt)\n",
    "        ])\n",
    "        \n",
    "        # Parse response\n",
    "        content = result.content\n",
    "        score = self._extract_score(content)\n",
    "        feedback = self._extract_section(content, \"FEEDBACK:\")\n",
    "        suggestion = self._extract_section(content, \"SUGGESTION:\")\n",
    "        \n",
    "        full_feedback = feedback\n",
    "        if suggestion and score < 5:\n",
    "            full_feedback += f\"\\n\\nSuggestion: {suggestion}\"\n",
    "        \n",
    "        return score, full_feedback\n",
    "    \n",
    "    def _extract_score(self, content: str) -> int:\n",
    "        \"\"\"Extract numeric score from response.\"\"\"\n",
    "        for line in content.split('\\n'):\n",
    "            if line.strip().startswith('SCORE:'):\n",
    "                try:\n",
    "                    return int(line.split(':')[1].strip().split()[0])\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "        return 3\n",
    "    \n",
    "    def _extract_section(self, content: str, header: str) -> str:\n",
    "        \"\"\"Extract a section from the response.\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        capture = False\n",
    "        result = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.strip().startswith(header):\n",
    "                capture = True\n",
    "                result.append(line.split(':', 1)[1].strip() if ':' in line else '')\n",
    "            elif capture and line.strip().startswith(('SCORE:', 'FEEDBACK:', 'SUGGESTION:')):\n",
    "                break\n",
    "            elif capture:\n",
    "                result.append(line)\n",
    "        \n",
    "        return ' '.join(result).strip()\n",
    "    \n",
    "    def _format_context(self, context: dict) -> str:\n",
    "        \"\"\"Format context dictionary as string.\"\"\"\n",
    "        lines = []\n",
    "        for key, value in context.items():\n",
    "            lines.append(f\"- {key}: {value}\")\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# Example usage and test\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = CustomerServiceEvaluator()\n",
    "    \n",
    "    # Test case\n",
    "    result = evaluator.evaluate(\n",
    "        customer_query=\"I ordered a laptop 2 weeks ago and it still hasn't arrived. This is ridiculous! I need it for work!\",\n",
    "        agent_response=\"I understand how frustrating this must be, especially when you need the laptop for work. Let me look into this right away. I can see your order #12345 was shipped on the 5th. It appears there's been a delay at the carrier's hub. I'm going to: 1) Contact the carrier directly to expedite delivery, 2) Send you tracking updates every 24 hours, and 3) If it's not delivered by Friday, I'll arrange for a replacement with overnight shipping at no extra cost. Does that work for you?\",\n",
    "        context={\n",
    "            \"company_policy\": \"Can offer replacement or refund after 14 days of delay\",\n",
    "            \"customer_tier\": \"Premium member\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Scores: {result['criteria_scores']}\")\n",
    "    print(f\"Average: {result['summary']['average_score']:.2f}\")\n",
    "    print(f\"Passed: {result['summary']['passed']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.2: Comparative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_18_3_solution.py\n",
    "# Description: Exercise 2 Solution - Comparative evaluator for A/B testing\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ComparativeEvaluator:\n",
    "    \"\"\"\n",
    "    Compare two agent responses and determine which is better.\n",
    "    Useful for A/B testing different agent versions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    def compare(\n",
    "        self,\n",
    "        question: str,\n",
    "        response_a: str,\n",
    "        response_b: str,\n",
    "        criteria: list[str] | None = None,\n",
    "        context: str | None = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Compare two responses and determine which is better.\n",
    "        \n",
    "        Args:\n",
    "            question: The original question/query\n",
    "            response_a: First response (e.g., from agent version A)\n",
    "            response_b: Second response (e.g., from agent version B)\n",
    "            criteria: Specific criteria to compare on\n",
    "            context: Additional context for evaluation\n",
    "        \n",
    "        Returns:\n",
    "            Comparison result with winner and reasoning\n",
    "        \"\"\"\n",
    "        criteria = criteria or [\"accuracy\", \"helpfulness\", \"clarity\", \"completeness\"]\n",
    "        criteria_str = \", \".join(criteria)\n",
    "        \n",
    "        system_prompt = f\"\"\"You are an expert evaluator comparing two AI assistant responses.\n",
    "\n",
    "Your task is to determine which response is better based on these criteria: {criteria_str}\n",
    "\n",
    "Be objective and thorough. Consider all criteria equally unless one response has a critical flaw.\n",
    "\n",
    "Respond in exactly this format:\n",
    "WINNER: [A or B or TIE]\n",
    "CONFIDENCE: [HIGH, MEDIUM, or LOW]\n",
    "SUMMARY: [One sentence summary of the decision]\n",
    "\n",
    "RESPONSE_A_STRENGTHS:\n",
    "- [strength 1]\n",
    "- [strength 2]\n",
    "\n",
    "RESPONSE_A_WEAKNESSES:\n",
    "- [weakness 1]\n",
    "- [weakness 2]\n",
    "\n",
    "RESPONSE_B_STRENGTHS:\n",
    "- [strength 1]\n",
    "- [strength 2]\n",
    "\n",
    "RESPONSE_B_WEAKNESSES:\n",
    "- [weakness 1]\n",
    "- [weakness 2]\n",
    "\n",
    "DETAILED_REASONING:\n",
    "[Paragraph explaining the decision, comparing specific aspects]\"\"\"\n",
    "\n",
    "        comparison_prompt = f\"\"\"QUESTION:\n",
    "{question}\n",
    "\n",
    "RESPONSE A:\n",
    "{response_a}\n",
    "\n",
    "RESPONSE B:\n",
    "{response_b}\n",
    "\"\"\"\n",
    "        if context:\n",
    "            comparison_prompt += f\"\\nCONTEXT:\\n{context}\"\n",
    "        \n",
    "        comparison_prompt += f\"\\n\\nCompare these responses based on: {criteria_str}\"\n",
    "        \n",
    "        result = self.llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=comparison_prompt)\n",
    "        ])\n",
    "        \n",
    "        return self._parse_comparison(result.content, response_a, response_b)\n",
    "    \n",
    "    def _parse_comparison(self, content: str, response_a: str, response_b: str) -> dict:\n",
    "        \"\"\"Parse the comparison result.\"\"\"\n",
    "        result = {\n",
    "            \"response_a_preview\": response_a[:150] + \"...\" if len(response_a) > 150 else response_a,\n",
    "            \"response_b_preview\": response_b[:150] + \"...\" if len(response_b) > 150 else response_b,\n",
    "            \"winner\": \"TIE\",\n",
    "            \"confidence\": \"MEDIUM\",\n",
    "            \"summary\": \"\",\n",
    "            \"response_a_strengths\": [],\n",
    "            \"response_a_weaknesses\": [],\n",
    "            \"response_b_strengths\": [],\n",
    "            \"response_b_weaknesses\": [],\n",
    "            \"detailed_reasoning\": \"\"\n",
    "        }\n",
    "        \n",
    "        lines = content.split('\\n')\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('WINNER:'):\n",
    "                winner = line.split(':')[1].strip().upper()\n",
    "                if winner in ['A', 'B', 'TIE']:\n",
    "                    result[\"winner\"] = winner\n",
    "            elif line.startswith('CONFIDENCE:'):\n",
    "                conf = line.split(':')[1].strip().upper()\n",
    "                if conf in ['HIGH', 'MEDIUM', 'LOW']:\n",
    "                    result[\"confidence\"] = conf\n",
    "            elif line.startswith('SUMMARY:'):\n",
    "                result[\"summary\"] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('RESPONSE_A_STRENGTHS:'):\n",
    "                current_section = \"response_a_strengths\"\n",
    "            elif line.startswith('RESPONSE_A_WEAKNESSES:'):\n",
    "                current_section = \"response_a_weaknesses\"\n",
    "            elif line.startswith('RESPONSE_B_STRENGTHS:'):\n",
    "                current_section = \"response_b_strengths\"\n",
    "            elif line.startswith('RESPONSE_B_WEAKNESSES:'):\n",
    "                current_section = \"response_b_weaknesses\"\n",
    "            elif line.startswith('DETAILED_REASONING:'):\n",
    "                current_section = \"detailed_reasoning\"\n",
    "            elif line.startswith('- ') and current_section in [\n",
    "                \"response_a_strengths\", \"response_a_weaknesses\",\n",
    "                \"response_b_strengths\", \"response_b_weaknesses\"\n",
    "            ]:\n",
    "                result[current_section].append(line[2:])\n",
    "            elif current_section == \"detailed_reasoning\" and line:\n",
    "                result[\"detailed_reasoning\"] += line + \" \"\n",
    "        \n",
    "        result[\"detailed_reasoning\"] = result[\"detailed_reasoning\"].strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_compare(\n",
    "        self,\n",
    "        comparisons: list[dict]\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Run multiple comparisons and aggregate results.\n",
    "        \n",
    "        Args:\n",
    "            comparisons: List of dicts with 'question', 'response_a', 'response_b'\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated comparison results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        a_wins = 0\n",
    "        b_wins = 0\n",
    "        ties = 0\n",
    "        \n",
    "        for comp in comparisons:\n",
    "            result = self.compare(\n",
    "                question=comp[\"question\"],\n",
    "                response_a=comp[\"response_a\"],\n",
    "                response_b=comp[\"response_b\"],\n",
    "                criteria=comp.get(\"criteria\"),\n",
    "                context=comp.get(\"context\")\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            if result[\"winner\"] == \"A\":\n",
    "                a_wins += 1\n",
    "            elif result[\"winner\"] == \"B\":\n",
    "                b_wins += 1\n",
    "            else:\n",
    "                ties += 1\n",
    "        \n",
    "        total = len(comparisons)\n",
    "        \n",
    "        return {\n",
    "            \"individual_results\": results,\n",
    "            \"aggregate\": {\n",
    "                \"total_comparisons\": total,\n",
    "                \"a_wins\": a_wins,\n",
    "                \"b_wins\": b_wins,\n",
    "                \"ties\": ties,\n",
    "                \"a_win_rate\": a_wins / total if total > 0 else 0,\n",
    "                \"b_win_rate\": b_wins / total if total > 0 else 0,\n",
    "                \"tie_rate\": ties / total if total > 0 else 0,\n",
    "                \"recommendation\": self._get_recommendation(a_wins, b_wins, ties)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_recommendation(self, a_wins: int, b_wins: int, ties: int) -> str:\n",
    "        \"\"\"Generate recommendation based on results.\"\"\"\n",
    "        total = a_wins + b_wins + ties\n",
    "        if total == 0:\n",
    "            return \"No comparisons to analyze\"\n",
    "        \n",
    "        a_rate = a_wins / total\n",
    "        b_rate = b_wins / total\n",
    "        \n",
    "        if a_rate > 0.6:\n",
    "            return \"Strong preference for Response A (version A recommended)\"\n",
    "        elif b_rate > 0.6:\n",
    "            return \"Strong preference for Response B (version B recommended)\"\n",
    "        elif a_rate > b_rate + 0.1:\n",
    "            return \"Slight preference for Response A (more testing recommended)\"\n",
    "        elif b_rate > a_rate + 0.1:\n",
    "            return \"Slight preference for Response B (more testing recommended)\"\n",
    "        else:\n",
    "            return \"No clear winner (responses are comparable)\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = ComparativeEvaluator()\n",
    "    \n",
    "    result = evaluator.compare(\n",
    "        question=\"How do I reset my password?\",\n",
    "        response_a=\"Go to settings and click reset password.\",\n",
    "        response_b=\"To reset your password: 1) Click your profile icon in the top right, 2) Select 'Settings', 3) Under 'Security', click 'Reset Password', 4) Check your email for a reset link. The link expires in 24 hours. Need help? Reply to this message!\",\n",
    "        criteria=[\"completeness\", \"clarity\", \"helpfulness\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Winner: {result['winner']} (Confidence: {result['confidence']})\")\n",
    "    print(f\"Summary: {result['summary']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.3: Evaluation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_18_3_solution.py\n",
    "# Description: Exercise 3 Solution - Evaluation dashboard with summary reports\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class EvaluationDashboard:\n",
    "    \"\"\"\n",
    "    Generate summary reports from evaluation results.\n",
    "    Provides insights for debugging and tracking improvements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, history: list[dict] | None = None):\n",
    "        \"\"\"\n",
    "        Initialize with optional historical data.\n",
    "        \n",
    "        Args:\n",
    "            history: List of past evaluation results for trend analysis\n",
    "        \"\"\"\n",
    "        self.history = history or []\n",
    "    \n",
    "    def generate_report(\n",
    "        self,\n",
    "        evaluation_results: dict,\n",
    "        version: str = \"current\",\n",
    "        include_details: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a formatted evaluation report.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_results: Results from an evaluation pipeline run\n",
    "            version: Version identifier for tracking\n",
    "            include_details: Whether to include detailed case analysis\n",
    "        \n",
    "        Returns:\n",
    "            Formatted report string\n",
    "        \"\"\"\n",
    "        report_lines = []\n",
    "        \n",
    "        # Header\n",
    "        report_lines.append(\"=\" * 60)\n",
    "        report_lines.append(\"AGENT EVALUATION REPORT\")\n",
    "        report_lines.append(\"=\" * 60)\n",
    "        report_lines.append(f\"Version: {version}\")\n",
    "        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_lines.append(f\"Total Test Cases: {evaluation_results.get('total_cases', 0)}\")\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Overall Summary\n",
    "        aggregate = evaluation_results.get(\"aggregate\", {})\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"OVERALL SUMMARY\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        \n",
    "        pass_rate = aggregate.get(\"pass_rate\", 0) * 100\n",
    "        avg_score = aggregate.get(\"average_score\", 0) * 100\n",
    "        \n",
    "        report_lines.append(f\"Pass Rate: {pass_rate:.1f}% ({aggregate.get('passed_count', 0)}/{evaluation_results.get('total_cases', 0)})\")\n",
    "        report_lines.append(f\"Average Score: {avg_score:.1f}%\")\n",
    "        report_lines.append(self._get_grade(avg_score))\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Breakdown by Criterion\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"BREAKDOWN BY CRITERION\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        \n",
    "        criterion_stats = self._calculate_criterion_stats(evaluation_results.get(\"cases\", []))\n",
    "        \n",
    "        for criterion, stats in sorted(criterion_stats.items(), key=lambda x: x[1][\"avg\"], reverse=True):\n",
    "            bar = self._score_bar(stats[\"avg\"])\n",
    "            report_lines.append(f\"{criterion:20} {bar} {stats['avg']*100:.1f}% (pass: {stats['pass_rate']*100:.0f}%)\")\n",
    "        \n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Worst Performing Cases\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        report_lines.append(\"CASES NEEDING ATTENTION\")\n",
    "        report_lines.append(\"-\" * 40)\n",
    "        \n",
    "        worst_cases = self._get_worst_cases(evaluation_results.get(\"cases\", []), n=5)\n",
    "        \n",
    "        if worst_cases:\n",
    "            for i, case in enumerate(worst_cases, 1):\n",
    "                report_lines.append(f\"\\n{i}. Case: {case.get('case_id', 'unknown')}\")\n",
    "                report_lines.append(f\"   Score: {case['summary']['average_score']*100:.1f}%\")\n",
    "                report_lines.append(f\"   Question: {case.get('question', 'N/A')[:60]}...\")\n",
    "                \n",
    "                failed = [c for c, e in case.get('evaluations', {}).items() \n",
    "                         if not e.get('passed', True)]\n",
    "                if failed:\n",
    "                    report_lines.append(f\"   Failed criteria: {', '.join(failed)}\")\n",
    "        else:\n",
    "            report_lines.append(\"All cases passed! \")\n",
    "        \n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Trend Analysis\n",
    "        if self.history:\n",
    "            report_lines.append(\"-\" * 40)\n",
    "            report_lines.append(\"TREND ANALYSIS\")\n",
    "            report_lines.append(\"-\" * 40)\n",
    "            \n",
    "            trend = self._analyze_trend()\n",
    "            report_lines.append(f\"Evaluations in history: {len(self.history)}\")\n",
    "            report_lines.append(f\"Score trend: {trend['direction']}\")\n",
    "            \n",
    "            if trend['recent_change'] is not None:\n",
    "                sign = \"+\" if trend['recent_change'] > 0 else \"\"\n",
    "                report_lines.append(f\"Recent change: {sign}{trend['recent_change']*100:.1f}%\")\n",
    "        \n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Detailed Case Analysis (optional)\n",
    "        if include_details and evaluation_results.get(\"cases\"):\n",
    "            report_lines.append(\"-\" * 40)\n",
    "            report_lines.append(\"DETAILED CASE RESULTS\")\n",
    "            report_lines.append(\"-\" * 40)\n",
    "            \n",
    "            for case in evaluation_results[\"cases\"][:10]:  # Limit to first 10\n",
    "                report_lines.append(f\"\\nCase: {case.get('case_id', 'unknown')}\")\n",
    "                report_lines.append(f\"  Overall: {'PASS' if case['summary']['all_passed'] else 'FAIL'} ({case['summary']['average_score']*100:.1f}%)\")\n",
    "                \n",
    "                for criterion, eval_data in case.get(\"evaluations\", {}).items():\n",
    "                    status = \"\" if eval_data.get(\"passed\", True) else \"\"\n",
    "                    score = eval_data.get(\"normalized_score\", eval_data.get(\"score\", 0))\n",
    "                    if isinstance(score, float) and score <= 1:\n",
    "                        score_str = f\"{score*100:.0f}%\"\n",
    "                    else:\n",
    "                        score_str = str(score)\n",
    "                    report_lines.append(f\"  {status} {criterion}: {score_str}\")\n",
    "        \n",
    "        # Footer\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(\"=\" * 60)\n",
    "        report_lines.append(\"END OF REPORT\")\n",
    "        report_lines.append(\"=\" * 60)\n",
    "        \n",
    "        return \"\\n\".join(report_lines)\n",
    "    \n",
    "    def _calculate_criterion_stats(self, cases: list[dict]) -> dict:\n",
    "        \"\"\"Calculate statistics for each evaluation criterion.\"\"\"\n",
    "        criterion_scores = {}\n",
    "        \n",
    "        for case in cases:\n",
    "            for criterion, eval_data in case.get(\"evaluations\", {}).items():\n",
    "                if criterion not in criterion_scores:\n",
    "                    criterion_scores[criterion] = {\"scores\": [], \"passed\": []}\n",
    "                \n",
    "                score = eval_data.get(\"normalized_score\", eval_data.get(\"score\", 0))\n",
    "                if isinstance(score, (int, float)):\n",
    "                    if score > 1:  # Assume it's a 1-5 scale\n",
    "                        score = score / 5.0\n",
    "                    criterion_scores[criterion][\"scores\"].append(score)\n",
    "                    criterion_scores[criterion][\"passed\"].append(eval_data.get(\"passed\", True))\n",
    "        \n",
    "        stats = {}\n",
    "        for criterion, data in criterion_scores.items():\n",
    "            scores = data[\"scores\"]\n",
    "            passed = data[\"passed\"]\n",
    "            stats[criterion] = {\n",
    "                \"avg\": sum(scores) / len(scores) if scores else 0,\n",
    "                \"min\": min(scores) if scores else 0,\n",
    "                \"max\": max(scores) if scores else 0,\n",
    "                \"pass_rate\": sum(passed) / len(passed) if passed else 0\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _get_worst_cases(self, cases: list[dict], n: int = 5) -> list[dict]:\n",
    "        \"\"\"Get the n worst performing cases.\"\"\"\n",
    "        sorted_cases = sorted(\n",
    "            cases,\n",
    "            key=lambda c: c.get(\"summary\", {}).get(\"average_score\", 1)\n",
    "        )\n",
    "        return sorted_cases[:n]\n",
    "    \n",
    "    def _get_grade(self, score: float) -> str:\n",
    "        \"\"\"Convert score to letter grade with emoji.\"\"\"\n",
    "        if score >= 90:\n",
    "            return \"Grade: A \"\n",
    "        elif score >= 80:\n",
    "            return \"Grade: B \"\n",
    "        elif score >= 70:\n",
    "            return \"Grade: C \"\n",
    "        elif score >= 60:\n",
    "            return \"Grade: D \"\n",
    "        else:\n",
    "            return \"Grade: F \"\n",
    "    \n",
    "    def _score_bar(self, score: float, width: int = 20) -> str:\n",
    "        \"\"\"Create a visual score bar.\"\"\"\n",
    "        filled = int(score * width)\n",
    "        empty = width - filled\n",
    "        return f\"[{'' * filled}{'' * empty}]\"\n",
    "    \n",
    "    def _analyze_trend(self) -> dict:\n",
    "        \"\"\"Analyze score trends from history.\"\"\"\n",
    "        if len(self.history) < 2:\n",
    "            return {\"direction\": \"Not enough data\", \"recent_change\": None}\n",
    "        \n",
    "        scores = [r.get(\"aggregate\", {}).get(\"average_score\", 0) for r in self.history]\n",
    "        \n",
    "        recent_change = scores[-1] - scores[-2] if len(scores) >= 2 else None\n",
    "        \n",
    "        # Simple trend direction\n",
    "        if len(scores) >= 3:\n",
    "            recent_avg = sum(scores[-3:]) / 3\n",
    "            older_avg = sum(scores[:-3]) / (len(scores) - 3) if len(scores) > 3 else scores[0]\n",
    "            \n",
    "            if recent_avg > older_avg + 0.05:\n",
    "                direction = \" Improving\"\n",
    "            elif recent_avg < older_avg - 0.05:\n",
    "                direction = \" Declining\"\n",
    "            else:\n",
    "                direction = \" Stable\"\n",
    "        else:\n",
    "            direction = \" Stable\"\n",
    "        \n",
    "        return {\n",
    "            \"direction\": direction,\n",
    "            \"recent_change\": recent_change\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample evaluation results\n",
    "    sample_results = {\n",
    "        \"total_cases\": 10,\n",
    "        \"aggregate\": {\n",
    "            \"pass_rate\": 0.8,\n",
    "            \"average_score\": 0.75,\n",
    "            \"passed_count\": 8,\n",
    "            \"failed_count\": 2\n",
    "        },\n",
    "        \"cases\": [\n",
    "            {\n",
    "                \"case_id\": \"test_001\",\n",
    "                \"question\": \"How do I reset my password?\",\n",
    "                \"summary\": {\"average_score\": 0.9, \"all_passed\": True},\n",
    "                \"evaluations\": {\n",
    "                    \"accuracy\": {\"normalized_score\": 0.9, \"passed\": True},\n",
    "                    \"helpfulness\": {\"normalized_score\": 0.85, \"passed\": True}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"case_id\": \"test_002\",\n",
    "                \"question\": \"What is your refund policy for digital products?\",\n",
    "                \"summary\": {\"average_score\": 0.5, \"all_passed\": False},\n",
    "                \"evaluations\": {\n",
    "                    \"accuracy\": {\"normalized_score\": 0.4, \"passed\": False},\n",
    "                    \"helpfulness\": {\"normalized_score\": 0.6, \"passed\": True}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Sample history for trend analysis\n",
    "    history = [\n",
    "        {\"aggregate\": {\"average_score\": 0.65}},\n",
    "        {\"aggregate\": {\"average_score\": 0.70}},\n",
    "        {\"aggregate\": {\"average_score\": 0.75}},\n",
    "    ]\n",
    "    \n",
    "    dashboard = EvaluationDashboard(history=history)\n",
    "    report = dashboard.generate_report(sample_results, version=\"v2.1.0\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Return to **Chapter 19: Next Topic**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}