{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20: Capstone Project - Building a Complete Agent System\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This capstone project guides you through building **CASPAR** - a production-ready Customer Service AI Agent using everything you've learned in this book.\n",
    "\n",
    "**CASPAR** = **C**ustomer **A**ssistant for **S**upport, **P**roblem-solving, **A**nd **R**esolution\n",
    "\n",
    "### What CASPAR Does\n",
    "- Answers customer questions using a knowledge base (RAG)\n",
    "- Looks up orders and provides status updates\n",
    "- Detects frustrated customers and escalates appropriately\n",
    "- Creates support tickets when issues need follow-up\n",
    "- Transfers to human agents when needed\n",
    "- Remembers conversation context with PostgreSQL persistence\n",
    "\n",
    "### Project Structure\n",
    "The complete CASPAR project is in the `caspar/` folder alongside this notebook. The code below is for **reference** - to run CASPAR, use the actual project files:\n",
    "\n",
    "```bash\n",
    "cd caspar\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # or venv\\Scripts\\activate on Windows\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "docker compose up -d  # Start PostgreSQL\n",
    "python scripts/verify_setup.py\n",
    "```\n",
    "\n",
    "### Sections\n",
    "- 20.1: Project setup and configuration\n",
    "- 20.2: Agent architecture design (LangGraph)\n",
    "- 20.3: Knowledge retrieval (RAG with ChromaDB)\n",
    "- 20.4: Conversation flow and tools\n",
    "- 20.5: Human handoff system\n",
    "- 20.6: Testing and evaluation\n",
    "- 20.7: Deployment with Docker and FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for REFERENCE only\n",
    "# To run CASPAR, navigate to the caspar/ folder and follow the setup instructions above\n",
    "\n",
    "# Quick check that the caspar folder exists\n",
    "import os\n",
    "caspar_path = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), \"caspar\")\n",
    "if os.path.exists(caspar_path):\n",
    "    print(f\"\u2713 CASPAR project found at: {caspar_path}\")\n",
    "    print(f\"  Files: {len(os.listdir(caspar_path))} items\")\n",
    "else:\n",
    "    print(\"\u26a0 CASPAR folder not found - please ensure caspar/ is in the same directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.1: Project overview: Customer service automation agent\n",
    "\n",
    "This section sets up the CASPAR project - **C**ustomer **A**ssistant for **S**upport, **P**roblem-solving, **A**nd **R**esolution.\n",
    "\n",
    "**Key files:**\n",
    "- `requirements.txt` - Project dependencies\n",
    "- `pyproject.toml` - Package configuration\n",
    "- `docker-compose.yml` - PostgreSQL database setup\n",
    "- `src/caspar/config/settings.py` - Configuration management\n",
    "- `src/caspar/config/logging.py` - Structured logging\n",
    "- `scripts/verify_setup.py` - Setup verification script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: requirements.txt\n",
    "\n",
    "# Core LLM and Agent frameworks\n",
    "langchain==1.1.1\n",
    "langchain-core==1.1.0\n",
    "langchain-openai==1.1.0\n",
    "langchain-text-splitters==1.0.0\n",
    "langgraph==1.0.4\n",
    "langgraph-checkpoint-postgres==3.0.1\n",
    "\n",
    "# Vector database for RAG\n",
    "chromadb==1.3.5\n",
    "langchain-chroma==1.0.0\n",
    "\n",
    "# API framework\n",
    "fastapi==0.123.5\n",
    "uvicorn==0.38.0\n",
    "python-multipart==0.0.20\n",
    "\n",
    "# Database\n",
    "psycopg==3.3.1\n",
    "psycopg-binary==3.3.1\n",
    "psycopg-pool==3.3.0\n",
    "asyncpg==0.31.0\n",
    "\n",
    "# Configuration and utilities\n",
    "pydantic==2.12.5\n",
    "pydantic-settings==2.12.0\n",
    "python-dotenv==1.2.1\n",
    "\n",
    "# Logging and monitoring\n",
    "structlog==25.5.0\n",
    "\n",
    "# Testing\n",
    "pytest==9.0.1\n",
    "pytest-asyncio==1.3.0\n",
    "pytest-cov==7.0.0\n",
    "pytest-timeout==2.4.0\n",
    "httpx==0.28.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyproject.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[build-system]\n",
    "requires = [\"setuptools>=61.0\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"caspar\"\n",
    "version = \"1.0.0\"\n",
    "description = \"CASPAR - Customer Assistance System for Product and Account Resolution\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.11\"\n",
    "dependencies = [\n",
    "    # Core LLM and Agent frameworks\n",
    "    \"langchain==1.1.1\",\n",
    "    \"langchain-core==1.1.0\",\n",
    "    \"langchain-openai==1.1.0\",\n",
    "    \"langchain-text-splitters==1.0.0\",\n",
    "    \"langgraph==1.0.4\",\n",
    "    \"langgraph-checkpoint-postgres==3.0.1\",\n",
    "    \n",
    "    # Vector database for RAG\n",
    "    \"chromadb==1.3.5\",\n",
    "    \"langchain-chroma==1.0.0\",\n",
    "    \n",
    "    # API framework\n",
    "    \"fastapi==0.123.5\",\n",
    "    \"uvicorn==0.38.0\",\n",
    "    \"python-multipart==0.0.20\",\n",
    "    \n",
    "    # Database\n",
    "    \"psycopg==3.3.1\",\n",
    "    \"psycopg-binary==3.3.1\",\n",
    "    \"psycopg-pool==3.3.0\",\n",
    "    \"asyncpg==0.31.0\",\n",
    "    \n",
    "    # Configuration and utilities\n",
    "    \"pydantic==2.12.5\",\n",
    "    \"pydantic-settings==2.12.0\",\n",
    "    \"python-dotenv==1.2.1\",\n",
    "    \n",
    "    # Logging and monitoring\n",
    "    \"structlog==25.5.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest==9.0.1\",\n",
    "    \"pytest-asyncio==1.3.0\",\n",
    "    \"pytest-cov==7.0.0\",\n",
    "    \"pytest-timeout==2.4.0\",\n",
    "    \"httpx==0.28.1\",\n",
    "]\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "asyncio_mode = \"auto\"\n",
    "testpaths = [\"tests\"]\n",
    "addopts = \"-v --tb=short\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker-compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: docker-compose.yml\n",
    "\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  caspar:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
    "      - DATABASE_URL=postgresql://caspar:caspar_secret@postgres:5432/caspar_db\n",
    "      - LOG_LEVEL=INFO\n",
    "      - ENVIRONMENT=development\n",
    "    volumes:\n",
    "      # Mount for development hot-reload\n",
    "      - ./src:/app/src:ro\n",
    "      - ./data:/app/data:ro\n",
    "    depends_on:\n",
    "      postgres:\n",
    "        condition: service_healthy\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      start_period: 40s\n",
    "      retries: 3\n",
    "\n",
    "  # PostgreSQL for conversation persistence\n",
    "  postgres:\n",
    "    image: postgres:16-alpine\n",
    "    environment:\n",
    "      POSTGRES_USER: caspar\n",
    "      POSTGRES_PASSWORD: caspar_secret\n",
    "      POSTGRES_DB: caspar_db\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U caspar -d caspar_db\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "  # Optional: Redis for horizontal scaling session management\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: .env.example\n",
    "\n",
    "# OpenAI API key (required)\n",
    "OPENAI_API_KEY=your-openai-api-key-here\n",
    "\n",
    "# PostgreSQL database URL for conversation persistence (required for production)\n",
    "# Format: postgresql://user:password@host:port/database\n",
    "DATABASE_URL=postgresql://caspar:caspar_secret@localhost:5432/caspar_db\n",
    "\n",
    "# Logging level (DEBUG, INFO, WARNING, ERROR)\n",
    "LOG_LEVEL=INFO\n",
    "\n",
    "# Environment (development, production)\n",
    "ENVIRONMENT=development\n",
    "\n",
    "# Optional: Model configuration\n",
    "OPENAI_MODEL=gpt-4o-mini\n",
    "OPENAI_TEMPERATURE=0.1\n",
    "\n",
    "# Optional: API port (Railway sets this automatically)\n",
    "PORT=8000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.1\n",
    "# File: src/caspar/config/settings.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR Configuration Settings\n",
    "\n",
    "This module provides centralized configuration management using Pydantic Settings.\n",
    "All configuration is loaded from environment variables with sensible defaults.\n",
    "\"\"\"\n",
    "\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"Find the project root directory (where .env lives).\"\"\"\n",
    "    # Start from this file's directory and go up until we find .env or pyproject.toml\n",
    "    current = Path(__file__).resolve().parent\n",
    "    \n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \".env\").exists() or (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    \n",
    "    # Fallback to current working directory\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "# Get path to .env file\n",
    "PROJECT_ROOT = get_project_root()\n",
    "ENV_FILE = PROJECT_ROOT / \".env\"\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Application settings loaded from environment variables.\"\"\"\n",
    "    \n",
    "    # Pydantic Settings v2 configuration\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=str(ENV_FILE),\n",
    "        env_file_encoding=\"utf-8\",\n",
    "        case_sensitive=False,\n",
    "        extra=\"ignore\",  # Ignore extra env vars\n",
    "    )\n",
    "    \n",
    "    # OpenAI Configuration\n",
    "    openai_api_key: str = Field(..., description=\"OpenAI API key\")\n",
    "    default_model: str = Field(\n",
    "        default=\"gpt-4o-mini\",\n",
    "        description=\"Default LLM model for most operations\"\n",
    "    )\n",
    "    smart_model: str = Field(\n",
    "        default=\"gpt-4o\",\n",
    "        description=\"Smarter model for complex reasoning\"\n",
    "    )\n",
    "    \n",
    "    # Database Configuration\n",
    "    database_url: str = Field(\n",
    "        default=\"postgresql://caspar:caspar_secret@localhost:5432/caspar_db\",\n",
    "        description=\"PostgreSQL connection string\"\n",
    "    )\n",
    "    \n",
    "    # Application Settings\n",
    "    environment: str = Field(\n",
    "        default=\"development\",\n",
    "        description=\"Environment (development, staging, production)\"\n",
    "    )\n",
    "    debug: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Enable debug mode\"\n",
    "    )\n",
    "    log_level: str = Field(\n",
    "        default=\"INFO\",\n",
    "        description=\"Logging level\"\n",
    "    )\n",
    "    \n",
    "    # Agent Configuration\n",
    "    max_conversation_turns: int = Field(\n",
    "        default=50,\n",
    "        description=\"Maximum turns before suggesting human handoff\"\n",
    "    )\n",
    "    sentiment_threshold: float = Field(\n",
    "        default=-0.5,\n",
    "        description=\"Sentiment score below which to escalate\"\n",
    "    )\n",
    "    \n",
    "    # RAG Configuration\n",
    "    chroma_persist_directory: str = Field(\n",
    "        default=\"./chroma_data\",\n",
    "        description=\"Directory for ChromaDB persistence\"\n",
    "    )\n",
    "    retrieval_k: int = Field(\n",
    "        default=4,\n",
    "        description=\"Number of documents to retrieve for RAG\"\n",
    "    )\n",
    "    \n",
    "    # API Configuration\n",
    "    api_host: str = Field(default=\"0.0.0.0\", description=\"API host\")\n",
    "    api_port: int = Field(default=8000, description=\"API port\")\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def get_settings() -> Settings:\n",
    "    \"\"\"\n",
    "    Get cached settings instance.\n",
    "    \n",
    "    Using lru_cache ensures we only load settings once,\n",
    "    improving performance and consistency.\n",
    "    \"\"\"\n",
    "    return Settings()\n",
    "\n",
    "\n",
    "# Convenience function for quick access\n",
    "settings = get_settings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config/logging.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.1\n",
    "# File: src/caspar/config/logging.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR Logging Configuration\n",
    "\n",
    "Provides structured logging using structlog for better\n",
    "observability in production environments.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import structlog\n",
    "from .settings import settings\n",
    "\n",
    "\n",
    "def setup_logging() -> None:\n",
    "    \"\"\"Configure structured logging for the application.\"\"\"\n",
    "    \n",
    "    # Set the log level based on settings\n",
    "    log_level = getattr(logging, settings.log_level.upper(), logging.INFO)\n",
    "    \n",
    "    # Configure structlog\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.contextvars.merge_contextvars,\n",
    "            structlog.processors.add_log_level,\n",
    "            structlog.processors.StackInfoRenderer(),\n",
    "            structlog.dev.set_exc_info,\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            # Use console renderer in development, JSON in production\n",
    "            structlog.dev.ConsoleRenderer()\n",
    "            if settings.environment == \"development\"\n",
    "            else structlog.processors.JSONRenderer(),\n",
    "        ],\n",
    "        wrapper_class=structlog.make_filtering_bound_logger(log_level),\n",
    "        context_class=dict,\n",
    "        logger_factory=structlog.PrintLoggerFactory(),\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "    \n",
    "    # Also configure standard logging for third-party libraries\n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        level=log_level,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_logger(name: str) -> structlog.BoundLogger:\n",
    "    \"\"\"Get a logger instance with the given name.\"\"\"\n",
    "    return structlog.get_logger(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts/verify_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.1\n",
    "# File: scripts/verify_setup.py\n",
    "\n",
    "\"\"\"\n",
    "Setup Verification Script\n",
    "\n",
    "Run this to ensure your CASPAR development environment is properly configured.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_python_version():\n",
    "    \"\"\"Verify Python version is 3.11+\"\"\"\n",
    "    version = sys.version_info\n",
    "    if version.major < 3 or (version.major == 3 and version.minor < 11):\n",
    "        print(f\"\u274c Python 3.11+ required, found {version.major}.{version.minor}\")\n",
    "        return False\n",
    "    print(f\"\u2705 Python {version.major}.{version.minor}.{version.micro}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Verify all required packages are installed.\"\"\"\n",
    "    required = [\n",
    "        (\"langchain\", \"langchain\"),\n",
    "        (\"langchain_openai\", \"langchain-openai\"),\n",
    "        (\"langchain_text_splitters\", \"langchain-text-splitters\"),\n",
    "        (\"langgraph\", \"langgraph\"),\n",
    "        (\"chromadb\", \"chromadb\"),\n",
    "        (\"fastapi\", \"fastapi\"),\n",
    "        (\"pydantic_settings\", \"pydantic-settings\"),\n",
    "        (\"structlog\", \"structlog\"),\n",
    "        (\"psycopg\", \"psycopg\"),\n",
    "    ]\n",
    "    \n",
    "    all_good = True\n",
    "    for module_name, package_name in required:\n",
    "        try:\n",
    "            __import__(module_name)\n",
    "            print(f\"\u2705 {package_name}\")\n",
    "        except ImportError:\n",
    "            print(f\"\u274c {package_name} - run: pip install {package_name}\")\n",
    "            all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "\n",
    "def check_caspar_installed():\n",
    "    \"\"\"Verify the caspar package is installed in editable mode.\"\"\"\n",
    "    try:\n",
    "        import caspar\n",
    "        print(\"\u2705 caspar package is installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"\u274c caspar package not found\")\n",
    "        print(\"   Run: pip install -e .\")\n",
    "        print(\"   (Make sure you're in the project root where pyproject.toml is)\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_env_file():\n",
    "    \"\"\"Verify .env file exists and has required variables.\"\"\"\n",
    "    # Find .env relative to this script's location\n",
    "    env_path = Path(__file__).parent.parent / \".env\"\n",
    "    \n",
    "    if not env_path.exists():\n",
    "        print(\"\u274c .env file not found\")\n",
    "        print(\"   Create one with: OPENAI_API_KEY=sk-your-key-here\")\n",
    "        return False\n",
    "    \n",
    "    content = env_path.read_text()\n",
    "    \n",
    "    if \"OPENAI_API_KEY\" not in content:\n",
    "        print(\"\u274c OPENAI_API_KEY not found in .env\")\n",
    "        return False\n",
    "    \n",
    "    if \"sk-your\" in content or \"sk-xxx\" in content:\n",
    "        print(\"\u26a0\ufe0f  .env found but OPENAI_API_KEY appears to be a placeholder\")\n",
    "        print(\"   Replace it with your actual API key\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\u2705 .env file configured\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_configuration():\n",
    "    \"\"\"Verify configuration loads correctly.\"\"\"\n",
    "    try:\n",
    "        from caspar.config import settings\n",
    "        \n",
    "        # Check that we can access settings\n",
    "        _ = settings.openai_api_key\n",
    "        _ = settings.default_model\n",
    "        \n",
    "        print(f\"\u2705 Configuration loaded\")\n",
    "        print(f\"   Environment: {settings.environment}\")\n",
    "        print(f\"   Default model: {settings.default_model}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Configuration error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_openai_connection():\n",
    "    \"\"\"Verify OpenAI API connection works.\"\"\"\n",
    "    try:\n",
    "        from caspar.config import settings\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        \n",
    "        llm = ChatOpenAI(\n",
    "            model=settings.default_model,\n",
    "            api_key=settings.openai_api_key,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        # Make a minimal test call\n",
    "        response = llm.invoke(\"Say 'OK' and nothing else.\")\n",
    "        \n",
    "        print(f\"\u2705 OpenAI API connection successful\")\n",
    "        print(f\"   Model: {settings.default_model}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c OpenAI API error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_database_connection():\n",
    "    \"\"\"Verify PostgreSQL database connection works.\"\"\"\n",
    "    try:\n",
    "        import psycopg\n",
    "        from caspar.config import settings\n",
    "        \n",
    "        with psycopg.connect(settings.database_url) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT version();\")\n",
    "                version = cur.fetchone()[0]\n",
    "                print(f\"\u2705 PostgreSQL connection successful\")\n",
    "                print(f\"   {version[:50]}...\")\n",
    "                return True\n",
    "                \n",
    "    except ImportError:\n",
    "        print(f\"\u274c psycopg not installed - run: pip install psycopg\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Database connection failed: {e}\")\n",
    "        print(\"   Make sure PostgreSQL is running: docker compose up -d\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_directory_structure():\n",
    "    \"\"\"Verify project directory structure is correct.\"\"\"\n",
    "    base_path = Path(__file__).parent.parent\n",
    "    \n",
    "    required_dirs = [\n",
    "        \"src/caspar/agent\",\n",
    "        \"src/caspar/api\",\n",
    "        \"src/caspar/knowledge\",\n",
    "        \"src/caspar/tools\",\n",
    "        \"src/caspar/handoff\",\n",
    "        \"src/caspar/config\",\n",
    "        \"tests/unit\",\n",
    "        \"tests/integration\",\n",
    "        \"tests/evaluation\",\n",
    "        \"data/knowledge_base\",\n",
    "        \"data/sample_data\",\n",
    "    ]\n",
    "    \n",
    "    all_good = True\n",
    "    for dir_path in required_dirs:\n",
    "        full_path = base_path / dir_path\n",
    "        if full_path.exists():\n",
    "            print(f\"\u2705 {dir_path}/\")\n",
    "        else:\n",
    "            print(f\"\u274c {dir_path}/ - missing\")\n",
    "            all_good = False\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all verification checks.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\ud83d\udd0d CASPAR Setup Verification\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    checks = [\n",
    "        (\"Python Version\", check_python_version),\n",
    "        (\"Dependencies\", check_dependencies),\n",
    "        (\"CASPAR Package\", check_caspar_installed),\n",
    "        (\"Directory Structure\", check_directory_structure),\n",
    "        (\"Environment File\", check_env_file),\n",
    "        (\"Configuration\", check_configuration),\n",
    "        (\"Database Connection\", check_database_connection),\n",
    "        (\"OpenAI Connection\", check_openai_connection),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for name, check_func in checks:\n",
    "        print(f\"\\n\ud83d\udccb Checking {name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        results.append(check_func())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    if all(results):\n",
    "        print(\"\ud83c\udf89 All checks passed! You're ready to build CASPAR!\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Some checks failed. Please fix the issues above.\")\n",
    "        print(\"   Refer to the setup instructions in Section 20.1\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    sys.exit(0 if success else 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.2: Designing the agent architecture\n",
    "\n",
    "This section defines the LangGraph agent architecture.\n",
    "\n",
    "**Key files:**\n",
    "- `src/caspar/agent/state.py` - Agent state schema\n",
    "- `src/caspar/agent/nodes.py` - Graph nodes (intent classification, handlers)\n",
    "- `src/caspar/agent/graph.py` - Graph construction and routing\n",
    "- `src/caspar/agent/persistence.py` - PostgreSQL persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent/state.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.2\n",
    "# File: src/caspar/agent/state.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR Agent State Definition\n",
    "\n",
    "This module defines the state schema that flows through the LangGraph agent.\n",
    "Every node reads from and writes to this shared state.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "# Message handling - LangGraph's add_messages reducer handles conversation history\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    The state that flows through the CASPAR agent graph.\n",
    "    \n",
    "    This is a TypedDict because LangGraph requires it for state management.\n",
    "    Each field represents a piece of information that nodes can read or update.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conversation messages - uses add_messages reducer to append new messages\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "    # Customer identification\n",
    "    customer_id: str | None\n",
    "    conversation_id: str\n",
    "    \n",
    "    # Intent classification results\n",
    "    intent: str | None  # faq, order_inquiry, account, complaint, general, handoff_request\n",
    "    confidence: float | None\n",
    "    \n",
    "    # Sentiment tracking\n",
    "    sentiment_score: float | None  # -1.0 (very negative) to 1.0 (very positive)\n",
    "    frustration_level: Literal[\"low\", \"medium\", \"high\"] | None\n",
    "    \n",
    "    # Context from tools and knowledge base\n",
    "    retrieved_context: str | None  # RAG results\n",
    "    order_info: dict | None  # From order lookup tool\n",
    "    ticket_id: str | None  # If a support ticket was created\n",
    "    \n",
    "    # Routing and flow control\n",
    "    needs_escalation: bool\n",
    "    escalation_reason: str | None\n",
    "    \n",
    "    # Metadata\n",
    "    turn_count: int\n",
    "    created_at: str\n",
    "    last_updated: str\n",
    "\n",
    "\n",
    "class ConversationMetadata(BaseModel):\n",
    "    \"\"\"Metadata about a conversation for logging and analytics.\"\"\"\n",
    "    \n",
    "    conversation_id: str\n",
    "    customer_id: str | None = None\n",
    "    started_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n",
    "    ended_at: datetime | None = None\n",
    "    total_turns: int = 0\n",
    "    intents_detected: list[str] = Field(default_factory=list)\n",
    "    escalated: bool = False\n",
    "    escalation_reason: str | None = None\n",
    "    sentiment_trajectory: list[float] = Field(default_factory=list)\n",
    "    resolution_status: Literal[\"resolved\", \"escalated\", \"abandoned\", \"ongoing\"] = \"ongoing\"\n",
    "\n",
    "\n",
    "def create_initial_state(\n",
    "    conversation_id: str,\n",
    "    customer_id: str | None = None\n",
    ") -> AgentState:\n",
    "    \"\"\"\n",
    "    Create a fresh state for a new conversation.\n",
    "    \n",
    "    Args:\n",
    "        conversation_id: Unique identifier for this conversation\n",
    "        customer_id: Optional customer identifier if known\n",
    "        \n",
    "    Returns:\n",
    "        Initial AgentState with default values\n",
    "    \"\"\"\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    return AgentState(\n",
    "        messages=[],\n",
    "        customer_id=customer_id,\n",
    "        conversation_id=conversation_id,\n",
    "        intent=None,\n",
    "        confidence=None,\n",
    "        sentiment_score=None,\n",
    "        frustration_level=None,\n",
    "        retrieved_context=None,\n",
    "        order_info=None,\n",
    "        ticket_id=None,\n",
    "        needs_escalation=False,\n",
    "        escalation_reason=None,\n",
    "        turn_count=0,\n",
    "        created_at=now,\n",
    "        last_updated=now,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent/nodes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.3\n",
    "# File: src/caspar/agent/nodes.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR Agent Nodes - Processing functions for each step in the graph.\n",
    "\n",
    "Each node function:\n",
    "1. Takes the current state as input\n",
    "2. Performs some processing (often using an LLM)\n",
    "3. Returns updates to merge into the state\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from caspar.config import settings, get_logger\n",
    "from caspar.knowledge import get_retriever\n",
    "from caspar.tools import (\n",
    "    get_order_status,\n",
    "    get_account_info,\n",
    "    create_ticket,\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Intent Classification Node\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "async def classify_intent(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Classify the customer's intent from their message.\n",
    "    \n",
    "    This is the entry point - determines which handler to route to.\n",
    "    \"\"\"\n",
    "    logger.info(\"classify_intent_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        return {\"intent\": \"general\"}\n",
    "    \n",
    "    # Get the last customer message\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.default_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0  # Deterministic for classification\n",
    "    )\n",
    "    \n",
    "    classification_prompt = f\"\"\"Classify the customer's intent into ONE of these categories:\n",
    "\n",
    "- faq: General questions about policies, products, services, shipping times, return policies, how things work\n",
    "- order_inquiry: Questions about a SPECIFIC order (mentions order number, tracking number, \"my order\", \"my package\")\n",
    "- account: Account-related issues (login, profile, password, settings, \"my account\")\n",
    "- complaint: Expressing dissatisfaction, problems, wanting refunds, frustrated language\n",
    "- handoff_request: Explicitly asking for a human agent, representative, or real person\n",
    "- general: Anything else or unclear\n",
    "\n",
    "IMPORTANT: \n",
    "- \"How long does shipping take?\" = faq (general policy question)\n",
    "- \"Where is my order?\" or \"Track order #123\" = order_inquiry (specific order)\n",
    "\n",
    "Customer message: \"{last_message}\"\n",
    "\n",
    "Respond with just the category name, nothing else.\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=classification_prompt)])\n",
    "    intent = response.content.strip().lower()\n",
    "    \n",
    "    # Validate intent\n",
    "    valid_intents = [\"faq\", \"order_inquiry\", \"account\", \"complaint\", \"handoff_request\", \"general\"]\n",
    "    if intent not in valid_intents:\n",
    "        intent = \"general\"\n",
    "    \n",
    "    logger.info(\"classify_intent_complete\", intent=intent)\n",
    "    \n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Intent Handler Nodes\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "async def handle_faq(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle FAQ-type questions using the knowledge base.\n",
    "    \"\"\"\n",
    "    logger.info(\"handle_faq_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    # Retrieve relevant knowledge\n",
    "    retriever = get_retriever()\n",
    "    docs = retriever.retrieve(last_message)\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs]) if docs else \"\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"handler_used\": \"faq\",\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "async def handle_order_inquiry(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle order-related inquiries by looking up order information.\n",
    "    \"\"\"\n",
    "    logger.info(\"handle_order_inquiry_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    # Try to extract order ID from message\n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.default_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    extract_prompt = f\"\"\"Extract the order ID from this message if present.\n",
    "Order IDs look like: TF-XXXXX (e.g., TF-10001) or just the number (e.g., 10001)\n",
    "\n",
    "Message: \"{last_message}\"\n",
    "\n",
    "Respond with just the order ID (e.g., TF-10001 or 10001), or \"NONE\" if not found.\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=extract_prompt)])\n",
    "    order_id = response.content.strip()\n",
    "    \n",
    "    context = \"\"\n",
    "    order_info = None\n",
    "    \n",
    "    if order_id != \"NONE\":\n",
    "        # Normalize order ID - add TF- prefix if needed\n",
    "        if not order_id.startswith(\"TF-\"):\n",
    "            # Remove any non-numeric prefix and add TF-\n",
    "            numeric_part = ''.join(filter(str.isdigit, order_id))\n",
    "            if numeric_part:\n",
    "                order_id = f\"TF-{numeric_part}\"\n",
    "        \n",
    "        # Look up order\n",
    "        order_result = get_order_status(order_id)\n",
    "        if order_result[\"found\"]:\n",
    "            order = order_result[\"order\"]\n",
    "            # Extract item names from item dicts\n",
    "            item_names = [item[\"name\"] if isinstance(item, dict) else str(item) for item in order[\"items\"]]\n",
    "            order_info = {\n",
    "                \"order_id\": order[\"order_id\"],\n",
    "                \"status\": order[\"status\"],\n",
    "                \"items\": order[\"items\"],\n",
    "                \"shipping_address\": order.get(\"shipping_address\", \"N/A\"),\n",
    "                \"tracking_number\": order.get(\"tracking_number\", \"Not yet available\"),\n",
    "                \"estimated_delivery\": order.get(\"estimated_delivery\", \"TBD\"),\n",
    "            }\n",
    "            context = f\"\"\"Order Information:\n",
    "- Order ID: {order['order_id']}\n",
    "- Status: {order['status']}\n",
    "- Items: {', '.join(item_names)}\n",
    "- Shipping: {order.get('shipping_method', 'N/A')}\n",
    "- Tracking: {order.get('tracking_number') or 'Not yet available'}\n",
    "- Estimated Delivery: {order.get('estimated_delivery') or 'TBD'}\"\"\"\n",
    "        else:\n",
    "            order_info = {\"error\": \"Order not found\"}\n",
    "            context = f\"Order {order_id} not found in system.\"\n",
    "    else:\n",
    "        context = \"No order ID provided. Ask customer for their order number.\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"handler_used\": \"order_inquiry\",\n",
    "        \"order_id\": order_id if order_id != \"NONE\" else None,\n",
    "        \"order_info\": order_info,\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "async def handle_account(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle account-related inquiries.\n",
    "    \"\"\"\n",
    "    logger.info(\"handle_account_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    customer_id = state.get(\"customer_id\")\n",
    "    context = \"\"\n",
    "    \n",
    "    if customer_id:\n",
    "        account_result = get_account_info(customer_id)\n",
    "        if account_result[\"found\"]:\n",
    "            account = account_result[\"account\"]\n",
    "            context = f\"\"\"Customer Account Information:\n",
    "- Name: {account['name']}\n",
    "- Email: {account['email']}\n",
    "- Status: {account['status']}\n",
    "- Loyalty Tier: {account.get('loyalty_tier', 'Standard')}\n",
    "- Member Since: {account.get('member_since', 'N/A')}\"\"\"\n",
    "        else:\n",
    "            context = \"Customer account not found.\"\n",
    "    else:\n",
    "        context = \"No customer ID available. Ask customer to verify their identity.\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"handler_used\": \"account\",\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "async def handle_complaint(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle customer complaints with empathy and create a ticket.\n",
    "    \"\"\"\n",
    "    logger.info(\"handle_complaint_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    customer_id = state.get(\"customer_id\", \"UNKNOWN\")\n",
    "    \n",
    "    # Create a support ticket for the complaint\n",
    "    ticket_result = create_ticket(\n",
    "        customer_id=customer_id,\n",
    "        category=\"complaint\",\n",
    "        subject=\"Customer Complaint\",\n",
    "        description=last_message,\n",
    "        priority=\"high\",\n",
    "        conversation_id=state.get(\"conversation_id\")\n",
    "    )\n",
    "    \n",
    "    ticket_id = ticket_result[\"ticket\"][\"ticket_id\"]\n",
    "    \n",
    "    context = f\"\"\"Complaint ticket created:\n",
    "- Ticket ID: {ticket_id}\n",
    "- Priority: High\n",
    "- Status: Open\n",
    "\n",
    "Acknowledge the customer's frustration with empathy. Reference the ticket number.\n",
    "Assure them their concern is being taken seriously.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"handler_used\": \"complaint\",\n",
    "        \"ticket_id\": ticket_id,\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "async def handle_general(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle general inquiries that don't fit other categories.\n",
    "    \"\"\"\n",
    "    logger.info(\"handle_general_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    # Use knowledge base for general context\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    retriever = get_retriever()\n",
    "    docs = retriever.retrieve(last_message)\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs]) if docs else \"\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"handler_used\": \"general\",\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Response Generation Node\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "async def respond(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generate the final response to the customer.\n",
    "    \n",
    "    Uses all gathered context to craft a helpful, empathetic response.\n",
    "    \"\"\"\n",
    "    logger.info(\"respond_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    context = state.get(\"context\", \"\")\n",
    "    intent = state.get(\"intent\", \"general\")\n",
    "    handler_used = state.get(\"handler_used\", \"general\")\n",
    "    \n",
    "    # Build conversation history for context\n",
    "    conversation_history = \"\\n\".join([\n",
    "        f\"{'Customer' if isinstance(m, HumanMessage) else 'Agent'}: {m.content}\"\n",
    "        for m in messages[-5:]  # Last 5 messages for context\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.default_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0.7  # Slightly creative for natural responses\n",
    "    )\n",
    "    \n",
    "    system_prompt = \"\"\"You are CASPAR, a friendly and helpful customer service assistant for TechFlow Solutions.\n",
    "\n",
    "Your personality:\n",
    "- Warm, professional, and empathetic\n",
    "- Clear and concise in explanations\n",
    "- Always helpful and solution-oriented\n",
    "- Acknowledge customer feelings when appropriate\n",
    "\n",
    "Guidelines:\n",
    "- If you have specific information from the context, use it\n",
    "- If you don't have enough information, ask clarifying questions\n",
    "- Never make up information about orders, accounts, or policies\n",
    "- For complaints, acknowledge feelings first, then offer solutions\n",
    "- Keep responses conversational, not robotic\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Intent: {intent}\n",
    "Handler: {handler_used}\n",
    "\n",
    "Context/Information gathered:\n",
    "{context if context else \"No specific context available.\"}\n",
    "\n",
    "Recent conversation:\n",
    "{conversation_history}\n",
    "\n",
    "Generate a helpful response to the customer's last message. Be natural and conversational.\"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ])\n",
    "    \n",
    "    ai_response = response.content\n",
    "    \n",
    "    logger.info(\"respond_complete\", response_length=len(ai_response))\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=ai_response)],\n",
    "        \"pending_response\": ai_response,  # For approval workflow\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent/graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.3\n",
    "# File: src/caspar/agent/graph.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR Agent Graph - The workflow that connects all components.\n",
    "\n",
    "This module defines the StateGraph that orchestrates the agent's behavior,\n",
    "routing messages through classification, handling, and response generation.\n",
    "\"\"\"\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from caspar.config import get_logger\n",
    "from .state import AgentState\n",
    "from .nodes import (\n",
    "    classify_intent,\n",
    "    handle_faq,\n",
    "    handle_order_inquiry,\n",
    "    handle_account,\n",
    "    handle_complaint,\n",
    "    handle_general,\n",
    "    respond,\n",
    ")\n",
    "from .nodes_handoff_update import check_sentiment, human_handoff\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Routing Functions\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def route_by_intent(state: AgentState) -> str:\n",
    "    \"\"\"Route to the appropriate handler based on classified intent.\"\"\"\n",
    "    intent = state.get(\"intent\", \"general\")\n",
    "    \n",
    "    routes = {\n",
    "        \"faq\": \"handle_faq\",\n",
    "        \"order_inquiry\": \"handle_order_inquiry\",\n",
    "        \"account\": \"handle_account\",\n",
    "        \"complaint\": \"handle_complaint\",\n",
    "        \"handoff_request\": \"human_handoff\",\n",
    "        \"general\": \"handle_general\",\n",
    "    }\n",
    "    \n",
    "    return routes.get(intent, \"handle_general\")\n",
    "\n",
    "\n",
    "def route_after_sentiment(state: AgentState) -> str:\n",
    "    \"\"\"Route based on sentiment analysis - escalate if needed.\"\"\"\n",
    "    if state.get(\"needs_escalation\") and state.get(\"intent\") != \"handoff_request\":\n",
    "        return \"human_handoff\"\n",
    "    return \"respond\"\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# Graph Builder\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def build_graph() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build the CASPAR agent graph.\n",
    "    \n",
    "    The flow is:\n",
    "    1. classify_intent: Determine what the customer needs\n",
    "    2. handle_*: Process the specific type of request\n",
    "    3. check_sentiment: Analyze customer emotion\n",
    "    4. respond OR human_handoff: Generate response or escalate\n",
    "    \n",
    "    Returns:\n",
    "        StateGraph: The uncompiled graph (call .compile() to use)\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    graph.add_node(\"classify_intent\", classify_intent)\n",
    "    graph.add_node(\"handle_faq\", handle_faq)\n",
    "    graph.add_node(\"handle_order_inquiry\", handle_order_inquiry)\n",
    "    graph.add_node(\"handle_account\", handle_account)\n",
    "    graph.add_node(\"handle_complaint\", handle_complaint)\n",
    "    graph.add_node(\"handle_general\", handle_general)\n",
    "    graph.add_node(\"check_sentiment\", check_sentiment)\n",
    "    graph.add_node(\"respond\", respond)\n",
    "    graph.add_node(\"human_handoff\", human_handoff)\n",
    "    \n",
    "    # Set entry point\n",
    "    graph.set_entry_point(\"classify_intent\")\n",
    "    \n",
    "    # Route by intent after classification\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify_intent\",\n",
    "        route_by_intent,\n",
    "        {\n",
    "            \"handle_faq\": \"handle_faq\",\n",
    "            \"handle_order_inquiry\": \"handle_order_inquiry\",\n",
    "            \"handle_account\": \"handle_account\",\n",
    "            \"handle_complaint\": \"handle_complaint\",\n",
    "            \"handle_general\": \"handle_general\",\n",
    "            \"human_handoff\": \"human_handoff\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All handlers go to sentiment check\n",
    "    for handler in [\"handle_faq\", \"handle_order_inquiry\", \"handle_account\", \n",
    "                    \"handle_complaint\", \"handle_general\"]:\n",
    "        graph.add_edge(handler, \"check_sentiment\")\n",
    "    \n",
    "    # Sentiment check routes to respond or escalate\n",
    "    graph.add_conditional_edges(\n",
    "        \"check_sentiment\",\n",
    "        route_after_sentiment,\n",
    "        {\n",
    "            \"respond\": \"respond\",\n",
    "            \"human_handoff\": \"human_handoff\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # End nodes\n",
    "    graph.add_edge(\"respond\", END)\n",
    "    graph.add_edge(\"human_handoff\", END)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "async def create_agent(checkpointer=None):\n",
    "    \"\"\"\n",
    "    Create a compiled CASPAR agent ready for use.\n",
    "    \n",
    "    Args:\n",
    "        checkpointer: Optional checkpointer for persistence.\n",
    "                     If None, uses in-memory storage.\n",
    "    \n",
    "    Returns:\n",
    "        Compiled graph ready to process messages.\n",
    "    \"\"\"\n",
    "    graph = build_graph()\n",
    "    \n",
    "    if checkpointer is None:\n",
    "        checkpointer = MemorySaver()\n",
    "    \n",
    "    return graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# HITL (Human-in-the-Loop) Extensions\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# These are optional extensions for workflows requiring human approval\n",
    "\n",
    "from langgraph.types import interrupt, Command\n",
    "from caspar.handoff.approval import needs_approval, get_approval_reason\n",
    "\n",
    "\n",
    "async def check_approval_needed(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the pending response needs human approval.\n",
    "    \n",
    "    If approval is needed, interrupts the graph and waits for human decision.\n",
    "    \"\"\"\n",
    "    if not needs_approval(state):\n",
    "        return {\"approval_status\": \"not_required\"}\n",
    "    \n",
    "    pending_response = state.get(\"pending_response\", \"\")\n",
    "    reason = get_approval_reason(state)\n",
    "    \n",
    "    logger.info(\n",
    "        \"approval_required\",\n",
    "        conversation_id=state.get(\"conversation_id\"),\n",
    "        reason=reason\n",
    "    )\n",
    "    \n",
    "    # Interrupt and wait for human decision\n",
    "    human_decision = interrupt({\n",
    "        \"type\": \"approval_required\",\n",
    "        \"pending_response\": pending_response,\n",
    "        \"reason\": reason,\n",
    "        \"conversation_id\": state.get(\"conversation_id\"),\n",
    "        \"customer_id\": state.get(\"customer_id\"),\n",
    "    })\n",
    "    \n",
    "    if human_decision.get(\"approved\"):\n",
    "        final_response = human_decision.get(\"edited_response\") or pending_response\n",
    "        return {\n",
    "            \"approval_status\": \"approved\",\n",
    "            \"pending_response\": final_response,\n",
    "            \"reviewed_by\": human_decision.get(\"reviewer_id\"),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"approval_status\": \"rejected\",\n",
    "            \"reviewed_by\": human_decision.get(\"reviewer_id\"),\n",
    "            \"needs_escalation\": True,\n",
    "        }\n",
    "\n",
    "\n",
    "def route_after_approval(state: AgentState) -> str:\n",
    "    \"\"\"Route after approval check.\"\"\"\n",
    "    status = state.get(\"approval_status\", \"not_required\")\n",
    "    if status in [\"not_required\", \"approved\"]:\n",
    "        return \"send_response\"\n",
    "    return END\n",
    "\n",
    "\n",
    "async def send_response(state: AgentState) -> dict:\n",
    "    \"\"\"Send the final response to the customer.\"\"\"\n",
    "    logger.info(\"send_response\", conversation_id=state.get(\"conversation_id\"))\n",
    "    return {\"response_sent\": True}\n",
    "\n",
    "\n",
    "def build_graph_with_approval() -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build agent graph with human approval workflow.\n",
    "    \n",
    "    Extends the standard graph to add approval checks for high-stakes responses.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(AgentState)\n",
    "    \n",
    "    # Add all standard nodes\n",
    "    graph.add_node(\"classify_intent\", classify_intent)\n",
    "    graph.add_node(\"handle_faq\", handle_faq)\n",
    "    graph.add_node(\"handle_order_inquiry\", handle_order_inquiry)\n",
    "    graph.add_node(\"handle_account\", handle_account)\n",
    "    graph.add_node(\"handle_complaint\", handle_complaint)\n",
    "    graph.add_node(\"handle_general\", handle_general)\n",
    "    graph.add_node(\"check_sentiment\", check_sentiment)\n",
    "    graph.add_node(\"respond\", respond)\n",
    "    graph.add_node(\"human_handoff\", human_handoff)\n",
    "    \n",
    "    # Add approval nodes\n",
    "    graph.add_node(\"check_approval_needed\", check_approval_needed)\n",
    "    graph.add_node(\"send_response\", send_response)\n",
    "    \n",
    "    # Set entry point\n",
    "    graph.set_entry_point(\"classify_intent\")\n",
    "    \n",
    "    # Route by intent\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify_intent\",\n",
    "        route_by_intent,\n",
    "        {\n",
    "            \"handle_faq\": \"handle_faq\",\n",
    "            \"handle_order_inquiry\": \"handle_order_inquiry\",\n",
    "            \"handle_account\": \"handle_account\",\n",
    "            \"handle_complaint\": \"handle_complaint\",\n",
    "            \"handle_general\": \"handle_general\",\n",
    "            \"human_handoff\": \"human_handoff\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Handlers -> sentiment check\n",
    "    for handler in [\"handle_faq\", \"handle_order_inquiry\", \"handle_account\", \n",
    "                    \"handle_complaint\", \"handle_general\"]:\n",
    "        graph.add_edge(handler, \"check_sentiment\")\n",
    "    \n",
    "    # Sentiment -> respond or escalate\n",
    "    graph.add_conditional_edges(\n",
    "        \"check_sentiment\",\n",
    "        route_after_sentiment,\n",
    "        {\"respond\": \"respond\", \"human_handoff\": \"human_handoff\"}\n",
    "    )\n",
    "    \n",
    "    # Respond -> approval check\n",
    "    graph.add_edge(\"respond\", \"check_approval_needed\")\n",
    "    \n",
    "    # Approval -> send or end\n",
    "    graph.add_conditional_edges(\n",
    "        \"check_approval_needed\",\n",
    "        route_after_approval,\n",
    "        {\"send_response\": \"send_response\", END: END}\n",
    "    )\n",
    "    \n",
    "    # End nodes\n",
    "    graph.add_edge(\"send_response\", END)\n",
    "    graph.add_edge(\"human_handoff\", END)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "async def create_agent_with_approval(checkpointer=None):\n",
    "    \"\"\"\n",
    "    Create agent with human approval support.\n",
    "    \n",
    "    IMPORTANT: A checkpointer is REQUIRED for interrupts to work!\n",
    "    \"\"\"\n",
    "    graph = build_graph_with_approval()\n",
    "    \n",
    "    if checkpointer is None:\n",
    "        raise ValueError(\n",
    "            \"Checkpointer is required for interrupt support. \"\n",
    "            \"The graph must persist state to resume after approval.\"\n",
    "        )\n",
    "    \n",
    "    return graph.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent/persistence.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.2\n",
    "# File: src/caspar/agent/persistence.py\n",
    "\n",
    "\"\"\"\n",
    "Conversation persistence using PostgreSQL.\n",
    "\n",
    "This module provides checkpointing functionality that allows\n",
    "conversations to survive restarts and be resumed later.\n",
    "\n",
    "IMPORTANT: AsyncPostgresSaver must be used as an async context manager.\n",
    "The checkpointer should stay open for the lifetime of your application.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from contextlib import asynccontextmanager\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def create_checkpointer_context():\n",
    "    \"\"\"\n",
    "    Create a PostgreSQL checkpointer as an async context manager.\n",
    "    \n",
    "    This must be used with 'async with' and should wrap your entire\n",
    "    application lifecycle (e.g., in FastAPI's lifespan).\n",
    "    \n",
    "    Yields:\n",
    "        AsyncPostgresSaver if database is configured, None otherwise\n",
    "    \n",
    "    Environment Variables:\n",
    "        DATABASE_URL: PostgreSQL connection string\n",
    "                     Format: postgresql://user:pass@host:port/dbname\n",
    "    \n",
    "    Usage:\n",
    "        async with create_checkpointer_context() as checkpointer:\n",
    "            agent = await create_agent(checkpointer=checkpointer)\n",
    "            # ... run your application ...\n",
    "            # checkpointer stays open until you exit the 'async with'\n",
    "    \"\"\"\n",
    "    database_url = os.getenv(\"DATABASE_URL\")\n",
    "    \n",
    "    if not database_url:\n",
    "        logger.warning(\n",
    "            \"no_database_url\",\n",
    "            message=\"DATABASE_URL not set - conversations won't persist across restarts\"\n",
    "        )\n",
    "        yield None\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # AsyncPostgresSaver MUST be used as async context manager\n",
    "        async with AsyncPostgresSaver.from_conn_string(database_url) as checkpointer:\n",
    "            # Set up the required tables (safe to call multiple times)\n",
    "            await checkpointer.setup()\n",
    "            \n",
    "            logger.info(\"checkpointer_initialized\", database=\"postgresql\")\n",
    "            \n",
    "            # Yield the checkpointer - it stays open until we exit\n",
    "            yield checkpointer\n",
    "            \n",
    "            # Cleanup happens automatically when we exit the 'async with'\n",
    "            logger.info(\"checkpointer_closing\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"checkpointer_failed\",\n",
    "            error=str(e),\n",
    "            message=\"Falling back to in-memory state (no persistence)\"\n",
    "        )\n",
    "        yield None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.3: Implementing knowledge retrieval\n",
    "\n",
    "This section implements RAG (Retrieval-Augmented Generation) for answering customer questions.\n",
    "\n",
    "**Key files:**\n",
    "- `data/knowledge_base/*.md` - Knowledge base content (FAQs, policies, products)\n",
    "- `src/caspar/knowledge/loader.py` - Document loading\n",
    "- `src/caspar/knowledge/retriever.py` - Vector search and RAG\n",
    "- `scripts/build_knowledge_base.py` - KB indexing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data/knowledge_base/faq.md\n",
    "\n",
    "```markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of data/knowledge_base/faq.md\n",
    "# (Rendered below as reference)\n",
    "\n",
    "'''\n",
    "# Frequently Asked Questions\n",
    "\n",
    "## Orders & Shipping\n",
    "\n",
    "### How do I track my order?\n",
    "You can track your order by:\n",
    "1. Logging into your TechFlow account\n",
    "2. Going to \"Order History\"\n",
    "3. Clicking on your order number\n",
    "4. Clicking \"Track Package\"\n",
    "\n",
    "You'll also receive tracking emails when your order ships.\n",
    "\n",
    "### When will my order arrive?\n",
    "Delivery times depend on your shipping method:\n",
    "- Standard: 5-7 business days\n",
    "- Express: 2-3 business days\n",
    "- Overnight: Next business day\n",
    "\n",
    "Orders placed before 2 PM EST ship the same day.\n",
    "\n",
    "### Can I change my shipping address after ordering?\n",
    "You can change your shipping address only if the order hasn't shipped yet. Contact customer service immediately, and we'll do our best to update it. Once shipped, the address cannot be changed.\n",
    "\n",
    "### Do you ship internationally?\n",
    "Currently, we only ship within the continental United States. We do not ship to Alaska, Hawaii, or international destinations at this time.\n",
    "\n",
    "## Returns & Refunds\n",
    "\n",
    "### How do I return an item?\n",
    "To return an item:\n",
    "1. Log into your TechFlow account\n",
    "2. Go to \"Order History\"\n",
    "3. Find the order and click \"Return Item\"\n",
    "4. Select the items you want to return and your reason\n",
    "5. Print the prepaid shipping label\n",
    "6. Drop off at any UPS location\n",
    "\n",
    "### How long do refunds take?\n",
    "Once we receive your return:\n",
    "- Inspection takes 1-2 business days\n",
    "- Refund is processed within 5-7 business days\n",
    "- Depending on your bank, it may take an additional 3-5 days to appear\n",
    "\n",
    "### Can I return an opened product?\n",
    "Most opened products can be returned if they're in like-new condition. Exceptions include:\n",
    "- Downloaded software\n",
    "- Opened headphones/earbuds (hygiene reasons)\n",
    "- Personalized or customized items\n",
    "\n",
    "### What if my item arrived damaged?\n",
    "Contact us immediately! We'll either:\n",
    "- Send a replacement at no cost\n",
    "- Issue a full refund\n",
    "\n",
    "Keep the damaged packaging for carrier claims.\n",
    "\n",
    "## Products & Technical\n",
    "\n",
    "### How do I find the right laptop for me?\n",
    "Consider these factors:\n",
    "- **Students/Basic use**: TechFlow Air 13 ($899) - lightweight, great battery\n",
    "- **Professionals**: TechFlow Pro 15 ($1,299) - powerful, beautiful display\n",
    "- **Gamers**: TechFlow Gaming X17 ($1,799) - high performance, dedicated GPU\n",
    "\n",
    "### Are your refurbished products reliable?\n",
    "Yes! Our refurbished products:\n",
    "- Are thoroughly tested and certified\n",
    "- Look and function like new\n",
    "- Come with a 6-month TechFlow warranty\n",
    "- Include all original accessories\n",
    "- Are clearly marked and discounted 15-30%\n",
    "\n",
    "### How do I check warranty status?\n",
    "To check your warranty:\n",
    "1. Log into your TechFlow account\n",
    "2. Go to \"My Products\"\n",
    "3. Click on the product\n",
    "4. View warranty status and expiration date\n",
    "\n",
    "Or contact customer service with your order number.\n",
    "\n",
    "## Account & Payment\n",
    "\n",
    "### How do I reset my password?\n",
    "1. Go to techflow.com/login\n",
    "2. Click \"Forgot Password\"\n",
    "3. Enter your email address\n",
    "4. Check your email for a reset link\n",
    "5. Create a new password\n",
    "\n",
    "### What payment methods do you accept?\n",
    "We accept:\n",
    "- Credit cards (Visa, Mastercard, Amex, Discover)\n",
    "- PayPal\n",
    "- Apple Pay / Google Pay\n",
    "- TechFlow Gift Cards\n",
    "- Affirm financing (orders over $150)\n",
    "\n",
    "### Is my payment information secure?\n",
    "Absolutely. We use:\n",
    "- 256-bit SSL encryption\n",
    "- PCI DSS compliance\n",
    "- No storage of full credit card numbers\n",
    "- Fraud detection systems\n",
    "\n",
    "### How does Affirm financing work?\n",
    "Affirm lets you split purchases over $150 into monthly payments:\n",
    "1. Select Affirm at checkout\n",
    "2. Enter basic information\n",
    "3. Get approved in seconds\n",
    "4. Choose 3, 6, or 12 month terms\n",
    "5. Pay over time with no hidden fees\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data/knowledge_base/policies.md\n",
    "\n",
    "```markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of data/knowledge_base/policies.md\n",
    "# (Rendered below as reference)\n",
    "\n",
    "'''\n",
    "# TechFlow Company Policies\n",
    "\n",
    "## Return Policy\n",
    "\n",
    "TechFlow offers a 30-day return policy for most items. Here are the details:\n",
    "\n",
    "- **Timeframe**: Returns must be initiated within 30 days of delivery\n",
    "- **Condition**: Items must be in original packaging, unused, with all accessories\n",
    "- **Exceptions**: Software, opened headphones, and personalized items cannot be returned\n",
    "- **Process**: Start a return from your account page or contact customer service\n",
    "- **Refund timing**: Refunds are processed within 5-7 business days after we receive the item\n",
    "\n",
    "For defective items, we offer exchanges or full refunds regardless of the return window.\n",
    "\n",
    "## Shipping Policy\n",
    "\n",
    "We offer three shipping options for continental US orders:\n",
    "\n",
    "- **Standard Shipping**: 5-7 business days, FREE on orders over $50, otherwise $5.99\n",
    "- **Express Shipping**: 2-3 business days, $12.99\n",
    "- **Overnight Shipping**: Next business day, $24.99\n",
    "\n",
    "Orders placed before 2 PM EST ship the same day. We do not currently ship to Alaska, Hawaii, or international destinations.\n",
    "\n",
    "## Warranty Policy\n",
    "\n",
    "All TechFlow products come with manufacturer warranties:\n",
    "\n",
    "- **Laptops and Computers**: 1-year manufacturer warranty\n",
    "- **Phones and Tablets**: 1-year manufacturer warranty  \n",
    "- **Accessories**: 90-day warranty\n",
    "- **Refurbished Items**: 6-month TechFlow warranty\n",
    "\n",
    "Extended warranty plans are available for purchase within 30 days of your original order.\n",
    "\n",
    "## Price Match Policy\n",
    "\n",
    "We match prices from major retailers including Amazon, Best Buy, and Walmart:\n",
    "\n",
    "- Item must be identical (same model, color, condition)\n",
    "- Competitor must have item in stock\n",
    "- We don't match marketplace sellers, clearance, or doorbusters\n",
    "- Request must be made within 14 days of purchase\n",
    "\n",
    "## Payment Methods\n",
    "\n",
    "We accept:\n",
    "- All major credit cards (Visa, Mastercard, American Express, Discover)\n",
    "- PayPal\n",
    "- Apple Pay and Google Pay\n",
    "- TechFlow Gift Cards\n",
    "- Affirm financing (for orders over $150)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data/knowledge_base/products.md\n",
    "\n",
    "```markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of data/knowledge_base/products.md\n",
    "# (Rendered below as reference)\n",
    "\n",
    "'''\n",
    "# TechFlow Product Catalog\n",
    "\n",
    "## Laptops\n",
    "\n",
    "### TechFlow Pro 15\n",
    "- **Price**: $1,299\n",
    "- **Display**: 15.6\" 4K OLED\n",
    "- **Processor**: Intel Core i7-13700H\n",
    "- **RAM**: 16GB DDR5\n",
    "- **Storage**: 512GB NVMe SSD\n",
    "- **Battery**: Up to 10 hours\n",
    "- **Weight**: 4.2 lbs\n",
    "- **Best for**: Professional work, content creation, programming\n",
    "\n",
    "### TechFlow Air 13\n",
    "- **Price**: $899\n",
    "- **Display**: 13.3\" FHD IPS\n",
    "- **Processor**: Intel Core i5-1335U\n",
    "- **RAM**: 8GB DDR4\n",
    "- **Storage**: 256GB NVMe SSD\n",
    "- **Battery**: Up to 12 hours\n",
    "- **Weight**: 2.8 lbs\n",
    "- **Best for**: Students, everyday use, travel\n",
    "\n",
    "### TechFlow Gaming X17\n",
    "- **Price**: $1,799\n",
    "- **Display**: 17.3\" QHD 165Hz\n",
    "- **Processor**: Intel Core i9-13900HX\n",
    "- **RAM**: 32GB DDR5\n",
    "- **Graphics**: NVIDIA RTX 4070\n",
    "- **Storage**: 1TB NVMe SSD\n",
    "- **Best for**: Gaming, 3D rendering, video editing\n",
    "\n",
    "## Phones\n",
    "\n",
    "### TechFlow Phone 12\n",
    "- **Price**: $799\n",
    "- **Display**: 6.5\" AMOLED 120Hz\n",
    "- **Processor**: Snapdragon 8 Gen 2\n",
    "- **RAM**: 8GB\n",
    "- **Storage**: 128GB / 256GB options\n",
    "- **Camera**: 50MP main + 12MP ultrawide + 10MP telephoto\n",
    "- **Battery**: 4,500mAh with 65W fast charging\n",
    "- **Colors**: Midnight Black, Ocean Blue, Forest Green\n",
    "\n",
    "### TechFlow Phone 12 Pro\n",
    "- **Price**: $1,099\n",
    "- **Display**: 6.7\" AMOLED 120Hz\n",
    "- **Processor**: Snapdragon 8 Gen 2\n",
    "- **RAM**: 12GB\n",
    "- **Storage**: 256GB / 512GB options\n",
    "- **Camera**: 108MP main + 12MP ultrawide + 10MP telephoto + 2MP macro\n",
    "- **Battery**: 5,000mAh with 100W fast charging\n",
    "- **Colors**: Titanium Gray, Pearl White, Rose Gold\n",
    "\n",
    "## Tablets\n",
    "\n",
    "### TechFlow Tab 10\n",
    "- **Price**: $449\n",
    "- **Display**: 10.5\" LCD\n",
    "- **Processor**: Snapdragon 870\n",
    "- **RAM**: 6GB\n",
    "- **Storage**: 128GB (expandable via microSD)\n",
    "- **Battery**: Up to 14 hours\n",
    "- **Best for**: Entertainment, reading, light productivity\n",
    "\n",
    "### TechFlow Tab Pro 12\n",
    "- **Price**: $799\n",
    "- **Display**: 12.4\" AMOLED\n",
    "- **Processor**: Snapdragon 8 Gen 1\n",
    "- **RAM**: 8GB\n",
    "- **Storage**: 256GB\n",
    "- **Stylus**: TechFlow Pen included\n",
    "- **Best for**: Artists, professionals, note-taking\n",
    "\n",
    "## Accessories\n",
    "\n",
    "### TechFlow Wireless Earbuds\n",
    "- **Price**: $129\n",
    "- **Battery**: 6 hours (24 hours with case)\n",
    "- **Features**: Active noise cancellation, transparency mode, water resistant\n",
    "\n",
    "### TechFlow USB-C Hub\n",
    "- **Price**: $79\n",
    "- **Ports**: 2x USB-A, 1x USB-C, HDMI, SD card reader, Ethernet\n",
    "\n",
    "### TechFlow Laptop Stand\n",
    "- **Price**: $49\n",
    "- **Material**: Aluminum\n",
    "- **Features**: Adjustable height, foldable for travel\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data/knowledge_base/troubleshooting.md\n",
    "\n",
    "```markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content of data/knowledge_base/troubleshooting.md\n",
    "# (Rendered below as reference)\n",
    "\n",
    "'''\n",
    "# Troubleshooting Guide\n",
    "\n",
    "## Laptop Issues\n",
    "\n",
    "### My laptop won't turn on\n",
    "Try these steps in order:\n",
    "1. Make sure it's plugged in and the charging light is on\n",
    "2. Hold the power button for 15 seconds\n",
    "3. Disconnect all peripherals and try again\n",
    "4. Try a different power outlet\n",
    "5. If still not working, contact support for warranty service\n",
    "\n",
    "### My laptop is running slowly\n",
    "Common fixes:\n",
    "1. Restart your laptop (fixes most issues!)\n",
    "2. Check for Windows/macOS updates\n",
    "3. Close unused browser tabs and programs\n",
    "4. Run a disk cleanup to free up space\n",
    "5. Check Task Manager for resource-heavy programs\n",
    "6. Consider adding more RAM if consistently slow\n",
    "\n",
    "### My laptop battery drains quickly\n",
    "To improve battery life:\n",
    "1. Lower screen brightness\n",
    "2. Turn off Bluetooth and WiFi when not needed\n",
    "3. Close background applications\n",
    "4. Use \"Battery Saver\" mode\n",
    "5. Check for battery health in settings\n",
    "6. Normal capacity is 80%+ after 2 years - below that may need replacement\n",
    "\n",
    "## Phone Issues\n",
    "\n",
    "### My phone won't charge\n",
    "Try these solutions:\n",
    "1. Try a different cable and adapter\n",
    "2. Clean the charging port gently with a toothpick\n",
    "3. Check for debris or lint in the port\n",
    "4. Try wireless charging if available\n",
    "5. Restart the phone\n",
    "6. If still not charging, contact support\n",
    "\n",
    "### My phone screen is frozen\n",
    "To unfreeze:\n",
    "1. Try a force restart: hold Power + Volume Down for 10 seconds\n",
    "2. Wait for the phone to restart\n",
    "3. If it happens frequently, check for app updates\n",
    "4. Consider a factory reset as last resort (backup first!)\n",
    "\n",
    "### My phone's battery drains quickly\n",
    "To extend battery life:\n",
    "1. Check battery usage in Settings to find power-hungry apps\n",
    "2. Reduce screen brightness\n",
    "3. Turn off location services for apps that don't need it\n",
    "4. Disable background app refresh for non-essential apps\n",
    "5. Turn on battery saver mode\n",
    "\n",
    "## Audio/Earbuds Issues\n",
    "\n",
    "### My earbuds won't connect\n",
    "Reset and reconnect:\n",
    "1. Put earbuds in the case and close it\n",
    "2. Wait 30 seconds\n",
    "3. Open Settings > Bluetooth on your device\n",
    "4. \"Forget\" the TechFlow Earbuds\n",
    "5. Open the case, hold the button until light flashes\n",
    "6. Select TechFlow Earbuds in Bluetooth settings\n",
    "\n",
    "### One earbud is quieter than the other\n",
    "Try these fixes:\n",
    "1. Clean both earbuds with a dry cloth\n",
    "2. Check ear tips for wax buildup\n",
    "3. Reset the earbuds (see above)\n",
    "4. Check audio balance in phone settings\n",
    "5. If persists, may be a hardware issue - contact support\n",
    "\n",
    "## General\n",
    "\n",
    "### I forgot my account password\n",
    "1. Go to techflow.com/login\n",
    "2. Click \"Forgot Password\"\n",
    "3. Enter your email\n",
    "4. Check your inbox (and spam folder)\n",
    "5. Click the reset link\n",
    "6. Create a new password\n",
    "\n",
    "### How do I contact support?\n",
    "You can reach us through:\n",
    "- This chat (available 24/7)\n",
    "- Email: support@techflow.com (response within 24 hours)\n",
    "- Phone: 1-800-TECHFLOW (Mon-Fri, 9 AM - 6 PM EST)\n",
    "- Twitter/X: @TechFlowSupport\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knowledge/loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.3\n",
    "# File: src/caspar/knowledge/loader.py\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Base Loader\n",
    "\n",
    "Loads and processes knowledge base documents from markdown files,\n",
    "splits them into chunks, and prepares them for embedding.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class KnowledgeLoader:\n",
    "    \"\"\"\n",
    "    Loads knowledge base content from markdown files.\n",
    "    \n",
    "    The loader reads all .md files from the knowledge base directory,\n",
    "    splits them into manageable chunks, and prepares them for embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        knowledge_dir: str = \"data/knowledge_base\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge loader.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_dir: Path to directory containing .md files\n",
    "            chunk_size: Maximum size of each text chunk\n",
    "            chunk_overlap: Overlap between chunks to preserve context\n",
    "        \"\"\"\n",
    "        self.knowledge_dir = Path(knowledge_dir)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def load_documents(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Load all markdown files from the knowledge directory.\n",
    "        \n",
    "        Returns:\n",
    "            List of Document objects, each representing a chunk\n",
    "        \"\"\"\n",
    "        if not self.knowledge_dir.exists():\n",
    "            logger.warning(\n",
    "                \"knowledge_dir_not_found\",\n",
    "                path=str(self.knowledge_dir)\n",
    "            )\n",
    "            return []\n",
    "        \n",
    "        documents = []\n",
    "        md_files = list(self.knowledge_dir.glob(\"*.md\"))\n",
    "        \n",
    "        logger.info(\n",
    "            \"loading_knowledge_base\",\n",
    "            file_count=len(md_files),\n",
    "            directory=str(self.knowledge_dir)\n",
    "        )\n",
    "        \n",
    "        for file_path in md_files:\n",
    "            try:\n",
    "                content = file_path.read_text(encoding=\"utf-8\")\n",
    "                \n",
    "                # Create document with metadata\n",
    "                doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": file_path.name,\n",
    "                        \"category\": self._extract_category(file_path.name)\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                \n",
    "                logger.debug(\n",
    "                    \"loaded_file\",\n",
    "                    file=file_path.name,\n",
    "                    size=len(content)\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    \"file_load_error\",\n",
    "                    file=file_path.name,\n",
    "                    error=str(e)\n",
    "                )\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def load_and_split(self) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Load documents and split them into chunks.\n",
    "        \n",
    "        Returns:\n",
    "            List of chunked Document objects\n",
    "        \"\"\"\n",
    "        documents = self.load_documents()\n",
    "        \n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        logger.info(\n",
    "            \"documents_chunked\",\n",
    "            original_docs=len(documents),\n",
    "            chunks=len(chunks),\n",
    "            avg_chunk_size=sum(len(c.page_content) for c in chunks) // len(chunks)\n",
    "        )\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_category(self, filename: str) -> str:\n",
    "        \"\"\"Extract category from filename for filtering.\"\"\"\n",
    "        # Remove .md extension and use as category\n",
    "        name = filename.replace(\".md\", \"\").lower()\n",
    "        \n",
    "        category_map = {\n",
    "            \"policies\": \"policy\",\n",
    "            \"products\": \"product\",\n",
    "            \"faq\": \"faq\",\n",
    "            \"troubleshooting\": \"troubleshooting\"\n",
    "        }\n",
    "        \n",
    "        return category_map.get(name, \"general\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knowledge/retriever.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.3\n",
    "# File: src/caspar/knowledge/retriever.py\n",
    "\n",
    "\"\"\"\n",
    "Knowledge Base Retriever\n",
    "\n",
    "Handles embedding, storage, and retrieval of knowledge base content\n",
    "using ChromaDB for vector similarity search.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from caspar.config import settings, get_logger\n",
    "from .loader import KnowledgeLoader\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class KnowledgeRetriever:\n",
    "    \"\"\"\n",
    "    Retrieves relevant knowledge for customer queries.\n",
    "    \n",
    "    Uses ChromaDB for vector storage and OpenAI embeddings for\n",
    "    semantic similarity search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        persist_directory: str | None = None,\n",
    "        collection_name: str = \"techflow_knowledge\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge retriever.\n",
    "        \n",
    "        Args:\n",
    "            persist_directory: Where to store ChromaDB data (None for in-memory)\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory or settings.chroma_persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            api_key=settings.openai_api_key,\n",
    "            model=\"text-embedding-3-small\"  # Fast and cost-effective\n",
    "        )\n",
    "        \n",
    "        self.vectorstore: Chroma | None = None\n",
    "        self._initialized = False\n",
    "    \n",
    "    def initialize(self, force_reload: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the vector store, loading documents if needed.\n",
    "        \n",
    "        Args:\n",
    "            force_reload: If True, reload documents even if store exists\n",
    "        \"\"\"\n",
    "        persist_path = Path(self.persist_directory)\n",
    "        \n",
    "        # Check if we already have a persisted store\n",
    "        if persist_path.exists() and not force_reload:\n",
    "            logger.info(\n",
    "                \"loading_existing_vectorstore\",\n",
    "                path=str(persist_path)\n",
    "            )\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=str(persist_path),\n",
    "                collection_name=self.collection_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            self._initialized = True\n",
    "            \n",
    "            # Log collection stats\n",
    "            collection = self.vectorstore._collection\n",
    "            count = collection.count()\n",
    "            logger.info(\"vectorstore_loaded\", document_count=count)\n",
    "            return\n",
    "        \n",
    "        # Load and embed documents\n",
    "        logger.info(\"creating_new_vectorstore\")\n",
    "        \n",
    "        loader = KnowledgeLoader()\n",
    "        documents = loader.load_and_split()\n",
    "        \n",
    "        if not documents:\n",
    "            logger.warning(\"no_documents_to_embed\")\n",
    "            # Create empty store\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=str(persist_path),\n",
    "                collection_name=self.collection_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            self._initialized = True\n",
    "            return\n",
    "        \n",
    "        # Create vectorstore with documents\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(persist_path),\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        \n",
    "        self._initialized = True\n",
    "        logger.info(\n",
    "            \"vectorstore_created\",\n",
    "            document_count=len(documents),\n",
    "            path=str(persist_path)\n",
    "        )\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int | None = None,\n",
    "        category_filter: str | None = None\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of documents to retrieve (default from settings)\n",
    "            category_filter: Optional category to filter by\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant Document objects\n",
    "        \"\"\"\n",
    "        if not self._initialized:\n",
    "            self.initialize()\n",
    "        \n",
    "        if not self.vectorstore:\n",
    "            logger.warning(\"vectorstore_not_available\")\n",
    "            return []\n",
    "        \n",
    "        k = k or settings.retrieval_k\n",
    "        \n",
    "        # Build filter if category specified\n",
    "        where_filter = None\n",
    "        if category_filter:\n",
    "            where_filter = {\"category\": category_filter}\n",
    "        \n",
    "        logger.debug(\n",
    "            \"retrieving_documents\",\n",
    "            query=query[:50],\n",
    "            k=k,\n",
    "            filter=category_filter\n",
    "        )\n",
    "        \n",
    "        # Perform similarity search\n",
    "        if where_filter:\n",
    "            docs = self.vectorstore.similarity_search(\n",
    "                query=query,\n",
    "                k=k,\n",
    "                filter=where_filter\n",
    "            )\n",
    "        else:\n",
    "            docs = self.vectorstore.similarity_search(\n",
    "                query=query,\n",
    "                k=k\n",
    "            )\n",
    "        \n",
    "        logger.info(\n",
    "            \"documents_retrieved\",\n",
    "            query=query[:50],\n",
    "            count=len(docs)\n",
    "        )\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def retrieve_with_scores(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int | None = None\n",
    "    ) -> list[tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents with similarity scores.\n",
    "        \n",
    "        Useful for debugging and understanding retrieval quality.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of (Document, score) tuples, lower score = more similar\n",
    "        \"\"\"\n",
    "        if not self._initialized:\n",
    "            self.initialize()\n",
    "        \n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        \n",
    "        k = k or settings.retrieval_k\n",
    "        \n",
    "        results = self.vectorstore.similarity_search_with_score(\n",
    "            query=query,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def format_context(self, documents: list[Document]) -> str:\n",
    "        \"\"\"\n",
    "        Format retrieved documents into a context string for the LLM.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of retrieved documents\n",
    "            \n",
    "        Returns:\n",
    "            Formatted context string\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return \"No relevant information found in knowledge base.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            category = doc.metadata.get(\"category\", \"general\")\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"[Source {i}: {source} ({category})]\\n{doc.page_content}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Singleton instance for easy access\n",
    "_retriever_instance: KnowledgeRetriever | None = None\n",
    "\n",
    "\n",
    "def get_retriever() -> KnowledgeRetriever:\n",
    "    \"\"\"Get or create the global knowledge retriever instance.\"\"\"\n",
    "    global _retriever_instance\n",
    "    \n",
    "    if _retriever_instance is None:\n",
    "        _retriever_instance = KnowledgeRetriever()\n",
    "    \n",
    "    return _retriever_instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts/build_knowledge_base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.3\n",
    "# File: scripts/build_knowledge_base.py\n",
    "\n",
    "\"\"\"\n",
    "Build and test the CASPAR knowledge base.\n",
    "\n",
    "This script:\n",
    "1. Loads all knowledge documents\n",
    "2. Creates embeddings and stores in ChromaDB\n",
    "3. Tests retrieval with sample queries\n",
    "\n",
    "Note: Make sure you've run 'pip install -e .' from the project root first!\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from caspar.config import setup_logging, get_logger\n",
    "from caspar.knowledge import KnowledgeLoader, KnowledgeRetriever\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def build_knowledge_base():\n",
    "    \"\"\"Build the ChromaDB knowledge base from markdown files.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"\ud83d\udcda Building CASPAR Knowledge Base\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check that knowledge files exist\n",
    "    kb_path = Path(\"data/knowledge_base\")\n",
    "    if not kb_path.exists():\n",
    "        print(f\"\u274c Knowledge base directory not found: {kb_path}\")\n",
    "        print(\"   Create the directory and add your .md files\")\n",
    "        return False\n",
    "    \n",
    "    md_files = list(kb_path.glob(\"*.md\"))\n",
    "    print(f\"\\n\ud83d\udcc4 Found {len(md_files)} markdown files:\")\n",
    "    for f in md_files:\n",
    "        size = f.stat().st_size / 1024\n",
    "        print(f\"   \u2022 {f.name} ({size:.1f} KB)\")\n",
    "    \n",
    "    if not md_files:\n",
    "        print(\"\u274c No .md files found in knowledge base directory\")\n",
    "        return False\n",
    "    \n",
    "    # Load and preview documents\n",
    "    print(\"\\n\ud83d\udcd6 Loading documents...\")\n",
    "    loader = KnowledgeLoader()\n",
    "    chunks = loader.load_and_split()\n",
    "    \n",
    "    print(f\"\u2705 Created {len(chunks)} chunks\")\n",
    "    print(f\"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "    \n",
    "    # Build vector store\n",
    "    print(\"\\n\ud83d\udd28 Building vector store...\")\n",
    "    retriever = KnowledgeRetriever()\n",
    "    retriever.initialize(force_reload=True)\n",
    "    \n",
    "    print(\"\u2705 Vector store created and persisted\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def test_retrieval():\n",
    "    \"\"\"Test retrieval with sample queries.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Testing Knowledge Retrieval\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    retriever = KnowledgeRetriever()\n",
    "    retriever.initialize()\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is your return policy?\",\n",
    "        \"How do I track my order?\",\n",
    "        \"My laptop won't turn on, what should I do?\",\n",
    "        \"What laptops do you sell?\",\n",
    "        \"How long does shipping take?\",\n",
    "        \"Can I pay with PayPal?\",\n",
    "        \"My earbuds won't connect to my phone\",\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\ud83d\udcdd Query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = retriever.retrieve_with_scores(query, k=2)\n",
    "        \n",
    "        for doc, score in results:\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            preview = doc.page_content[:100].replace(\"\\n\", \" \")\n",
    "            print(f\"   \ud83d\udcc4 [{source}] (score: {score:.3f})\")\n",
    "            print(f\"      {preview}...\")\n",
    "    \n",
    "    print(\"\\n\u2705 Retrieval tests complete!\")\n",
    "\n",
    "\n",
    "def interactive_test():\n",
    "    \"\"\"Interactive mode for testing queries.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83d\udd0d Interactive Knowledge Search\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Type your questions to test retrieval. Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    retriever = KnowledgeRetriever()\n",
    "    retriever.initialize()\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Your question: \").strip()\n",
    "        \n",
    "        if query.lower() == \"quit\":\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        results = retriever.retrieve_with_scores(query, k=3)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcda Top {len(results)} results:\\n\")\n",
    "        \n",
    "        for i, (doc, score) in enumerate(results, 1):\n",
    "            source = doc.metadata.get(\"source\", \"unknown\")\n",
    "            category = doc.metadata.get(\"category\", \"general\")\n",
    "            print(f\"Result {i} [{source} - {category}] (score: {score:.3f}):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(doc.page_content[:300])\n",
    "            print(\"...\" if len(doc.page_content) > 300 else \"\")\n",
    "            print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all knowledge base operations.\"\"\"\n",
    "    \n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Build and test CASPAR knowledge base\")\n",
    "    parser.add_argument(\"--build\", action=\"store_true\", help=\"Build the vector store\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\", help=\"Run retrieval tests\")\n",
    "    parser.add_argument(\"--interactive\", action=\"store_true\", help=\"Interactive query mode\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Default to build + test if no args\n",
    "    if not any([args.build, args.test, args.interactive]):\n",
    "        args.build = True\n",
    "        args.test = True\n",
    "    \n",
    "    if args.build:\n",
    "        success = build_knowledge_base()\n",
    "        if not success:\n",
    "            sys.exit(1)\n",
    "    \n",
    "    if args.test:\n",
    "        test_retrieval()\n",
    "    \n",
    "    if args.interactive:\n",
    "        interactive_test()\n",
    "    \n",
    "    print(\"\\n\ud83c\udf89 Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.4: Building the conversation flow\n",
    "\n",
    "This section implements the tools that CASPAR uses to help customers.\n",
    "\n",
    "**Key files:**\n",
    "- `src/caspar/tools/orders.py` - Order lookup and tracking\n",
    "- `src/caspar/tools/tickets.py` - Support ticket creation\n",
    "- `src/caspar/tools/accounts.py` - Account information\n",
    "- `scripts/test_conversation_flow.py` - Conversation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools/orders.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.4\n",
    "# File: src/caspar/tools/orders.py\n",
    "\n",
    "\"\"\"\n",
    "Order Lookup Tool\n",
    "\n",
    "Provides order status and tracking information.\n",
    "In production, this would connect to your order management system.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class OrderInfo(BaseModel):\n",
    "    \"\"\"Information about a customer order.\"\"\"\n",
    "    \n",
    "    order_id: str\n",
    "    customer_id: str\n",
    "    status: Literal[\"processing\", \"shipped\", \"delivered\", \"cancelled\", \"returned\"]\n",
    "    items: list[dict]\n",
    "    total: float\n",
    "    order_date: str\n",
    "    shipping_method: str\n",
    "    tracking_number: str | None = None\n",
    "    estimated_delivery: str | None = None\n",
    "    delivery_date: str | None = None\n",
    "\n",
    "\n",
    "class OrderLookupTool:\n",
    "    \"\"\"\n",
    "    Tool for looking up order information.\n",
    "    \n",
    "    In production, this would query your order management system.\n",
    "    For demo purposes, we use mock data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._mock_orders = self._generate_mock_orders()\n",
    "    \n",
    "    def _generate_mock_orders(self) -> dict[str, OrderInfo]:\n",
    "        \"\"\"Generate mock order data for testing.\"\"\"\n",
    "        \n",
    "        products = [\n",
    "            {\"name\": \"TechFlow Pro 15 Laptop\", \"price\": 1299.00, \"quantity\": 1},\n",
    "            {\"name\": \"TechFlow Wireless Earbuds\", \"price\": 129.00, \"quantity\": 1},\n",
    "            {\"name\": \"TechFlow USB-C Hub\", \"price\": 79.00, \"quantity\": 2},\n",
    "            {\"name\": \"TechFlow Phone 12\", \"price\": 799.00, \"quantity\": 1},\n",
    "            {\"name\": \"TechFlow Tab 10\", \"price\": 449.00, \"quantity\": 1},\n",
    "        ]\n",
    "        \n",
    "        statuses = [\"processing\", \"shipped\", \"delivered\", \"shipped\", \"delivered\"]\n",
    "        shipping_methods = [\"standard\", \"express\", \"overnight\"]\n",
    "        \n",
    "        orders = {}\n",
    "        base_date = datetime.now()\n",
    "        \n",
    "        # Generate 20 mock orders\n",
    "        for i in range(20):\n",
    "            order_id = f\"TF-{10000 + i}\"\n",
    "            customer_id = f\"CUST-{1000 + (i % 5)}\"\n",
    "            \n",
    "            num_items = random.randint(1, 3)\n",
    "            order_items = random.sample(products, num_items)\n",
    "            total = sum(item[\"price\"] * item[\"quantity\"] for item in order_items)\n",
    "            \n",
    "            status = statuses[i % len(statuses)]\n",
    "            shipping = shipping_methods[i % len(shipping_methods)]\n",
    "            order_date = base_date - timedelta(days=random.randint(1, 30))\n",
    "            \n",
    "            # Add tracking for shipped/delivered orders\n",
    "            tracking = None\n",
    "            estimated_delivery = None\n",
    "            delivery_date = None\n",
    "            \n",
    "            if status in [\"shipped\", \"delivered\"]:\n",
    "                tracking = f\"1Z999AA{10000000 + i}\"\n",
    "                est_days = {\"standard\": 7, \"express\": 3, \"overnight\": 1}[shipping]\n",
    "                estimated_delivery = (order_date + timedelta(days=est_days)).strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "                if status == \"delivered\":\n",
    "                    delivery_date = estimated_delivery\n",
    "            \n",
    "            orders[order_id] = OrderInfo(\n",
    "                order_id=order_id,\n",
    "                customer_id=customer_id,\n",
    "                status=status,\n",
    "                items=order_items,\n",
    "                total=total,\n",
    "                order_date=order_date.strftime(\"%Y-%m-%d\"),\n",
    "                shipping_method=shipping,\n",
    "                tracking_number=tracking,\n",
    "                estimated_delivery=estimated_delivery,\n",
    "                delivery_date=delivery_date,\n",
    "            )\n",
    "        \n",
    "        return orders\n",
    "    \n",
    "    def lookup(self, order_id: str, customer_id: str | None = None) -> OrderInfo | None:\n",
    "        \"\"\"\n",
    "        Look up an order by ID.\n",
    "        \n",
    "        Args:\n",
    "            order_id: The order ID to look up\n",
    "            customer_id: Optional customer ID for verification\n",
    "            \n",
    "        Returns:\n",
    "            OrderInfo if found, None otherwise\n",
    "        \"\"\"\n",
    "        logger.info(\"order_lookup\", order_id=order_id, customer_id=customer_id)\n",
    "        \n",
    "        # Normalize order ID (accept \"10001\" or \"TF-10001\")\n",
    "        order_id = order_id.upper().strip()\n",
    "        if not order_id.startswith(\"TF-\"):\n",
    "            order_id = f\"TF-{order_id}\"\n",
    "        \n",
    "        order = self._mock_orders.get(order_id)\n",
    "        \n",
    "        if order is None:\n",
    "            logger.warning(\"order_not_found\", order_id=order_id)\n",
    "            return None\n",
    "        \n",
    "        # Security: verify customer owns this order\n",
    "        if customer_id and order.customer_id != customer_id:\n",
    "            logger.warning(\"order_customer_mismatch\", order_id=order_id)\n",
    "            return None\n",
    "        \n",
    "        logger.info(\"order_found\", order_id=order_id, status=order.status)\n",
    "        return order\n",
    "    \n",
    "    def get_tracking_url(self, tracking_number: str) -> str:\n",
    "        \"\"\"Generate a tracking URL for a shipment.\"\"\"\n",
    "        return f\"https://track.techflow.com/{tracking_number}\"\n",
    "    \n",
    "    def format_order_summary(self, order: OrderInfo) -> str:\n",
    "        \"\"\"Format order information for display to customer.\"\"\"\n",
    "        \n",
    "        lines = [\n",
    "            f\"**Order {order.order_id}**\",\n",
    "            f\"Status: {order.status.upper()}\",\n",
    "            f\"Order Date: {order.order_date}\",\n",
    "            f\"Shipping: {order.shipping_method.title()}\",\n",
    "            \"\",\n",
    "            \"Items:\",\n",
    "        ]\n",
    "        \n",
    "        for item in order.items:\n",
    "            lines.append(f\"  \u2022 {item['name']} (x{item['quantity']}) - ${item['price']:.2f}\")\n",
    "        \n",
    "        lines.append(f\"\\nTotal: ${order.total:.2f}\")\n",
    "        \n",
    "        if order.tracking_number:\n",
    "            lines.append(f\"\\nTracking: {order.tracking_number}\")\n",
    "            lines.append(f\"Track at: {self.get_tracking_url(order.tracking_number)}\")\n",
    "        \n",
    "        if order.status == \"shipped\" and order.estimated_delivery:\n",
    "            lines.append(f\"Expected Delivery: {order.estimated_delivery}\")\n",
    "        elif order.status == \"delivered\" and order.delivery_date:\n",
    "            lines.append(f\"Delivered: {order.delivery_date}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "_order_tool: OrderLookupTool | None = None\n",
    "\n",
    "\n",
    "def get_order_tool() -> OrderLookupTool:\n",
    "    \"\"\"Get or create the order lookup tool instance.\"\"\"\n",
    "    global _order_tool\n",
    "    if _order_tool is None:\n",
    "        _order_tool = OrderLookupTool()\n",
    "    return _order_tool\n",
    "\n",
    "\n",
    "def get_order_status(order_id: str, customer_id: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Convenience function to look up order status.\n",
    "    \n",
    "    Returns a dict with order info or error message.\n",
    "    \"\"\"\n",
    "    tool = get_order_tool()\n",
    "    order = tool.lookup(order_id, customer_id)\n",
    "    \n",
    "    if order is None:\n",
    "        return {\n",
    "            \"found\": False,\n",
    "            \"error\": f\"Order {order_id} not found. Please check the order number and try again.\"\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"found\": True,\n",
    "        \"order\": order.model_dump(),\n",
    "        \"summary\": tool.format_order_summary(order)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools/tickets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.4\n",
    "# File: src/caspar/tools/tickets.py\n",
    "\n",
    "\"\"\"\n",
    "Ticket Creation Tool\n",
    "\n",
    "Creates and manages customer support tickets.\n",
    "In production, this would integrate with your ticketing system.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class Ticket(BaseModel):\n",
    "    \"\"\"A customer support ticket.\"\"\"\n",
    "    \n",
    "    ticket_id: str\n",
    "    customer_id: str\n",
    "    conversation_id: str | None = None\n",
    "    category: Literal[\"return\", \"refund\", \"technical\", \"billing\", \"shipping\", \"general\"]\n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"]\n",
    "    subject: str\n",
    "    description: str\n",
    "    status: Literal[\"open\", \"in_progress\", \"waiting_customer\", \"resolved\", \"closed\"] = \"open\"\n",
    "    created_at: str\n",
    "    updated_at: str\n",
    "    assigned_to: str | None = None\n",
    "    resolution: str | None = None\n",
    "\n",
    "\n",
    "class TicketTool:\n",
    "    \"\"\"\n",
    "    Tool for creating and managing support tickets.\n",
    "    \n",
    "    In production, this would integrate with Zendesk, Freshdesk, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._tickets: dict[str, Ticket] = {}\n",
    "    \n",
    "    def create(\n",
    "        self,\n",
    "        customer_id: str,\n",
    "        category: str,\n",
    "        subject: str,\n",
    "        description: str,\n",
    "        priority: str = \"medium\",\n",
    "        conversation_id: str | None = None,\n",
    "    ) -> Ticket:\n",
    "        \"\"\"Create a new support ticket.\"\"\"\n",
    "        \n",
    "        ticket_id = f\"TKT-{uuid.uuid4().hex[:8].upper()}\"\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        \n",
    "        ticket = Ticket(\n",
    "            ticket_id=ticket_id,\n",
    "            customer_id=customer_id,\n",
    "            conversation_id=conversation_id,\n",
    "            category=category,\n",
    "            priority=priority,\n",
    "            subject=subject,\n",
    "            description=description,\n",
    "            created_at=now,\n",
    "            updated_at=now,\n",
    "        )\n",
    "        \n",
    "        self._tickets[ticket_id] = ticket\n",
    "        \n",
    "        logger.info(\n",
    "            \"ticket_created\",\n",
    "            ticket_id=ticket_id,\n",
    "            customer_id=customer_id,\n",
    "            category=category,\n",
    "            priority=priority\n",
    "        )\n",
    "        \n",
    "        return ticket\n",
    "    \n",
    "    def get(self, ticket_id: str) -> Ticket | None:\n",
    "        \"\"\"Retrieve a ticket by ID.\"\"\"\n",
    "        return self._tickets.get(ticket_id)\n",
    "    \n",
    "    def get_customer_tickets(self, customer_id: str) -> list[Ticket]:\n",
    "        \"\"\"Get all tickets for a customer.\"\"\"\n",
    "        return [t for t in self._tickets.values() if t.customer_id == customer_id]\n",
    "    \n",
    "    def format_ticket_confirmation(self, ticket: Ticket) -> str:\n",
    "        \"\"\"Format ticket info for customer confirmation.\"\"\"\n",
    "        \n",
    "        priority_emoji = {\"low\": \"\ud83d\udfe2\", \"medium\": \"\ud83d\udfe1\", \"high\": \"\ud83d\udfe0\", \"urgent\": \"\ud83d\udd34\"}\n",
    "        \n",
    "        return f\"\"\"**Support Ticket Created**\n",
    "\n",
    "Ticket ID: {ticket.ticket_id}\n",
    "Category: {ticket.category.title()}\n",
    "Priority: {priority_emoji.get(ticket.priority, \"\u26aa\")} {ticket.priority.title()}\n",
    "Subject: {ticket.subject}\n",
    "\n",
    "Our team will review your ticket and respond within:\n",
    "- Urgent: 2 hours\n",
    "- High: 4 hours  \n",
    "- Medium: 24 hours\n",
    "- Low: 48 hours\n",
    "\n",
    "You can reference ticket {ticket.ticket_id} in future conversations.\"\"\"\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "_ticket_tool: TicketTool | None = None\n",
    "\n",
    "\n",
    "def get_ticket_tool() -> TicketTool:\n",
    "    \"\"\"Get or create the ticket tool instance.\"\"\"\n",
    "    global _ticket_tool\n",
    "    if _ticket_tool is None:\n",
    "        _ticket_tool = TicketTool()\n",
    "    return _ticket_tool\n",
    "\n",
    "\n",
    "def create_ticket(\n",
    "    customer_id: str,\n",
    "    category: str,\n",
    "    subject: str,\n",
    "    description: str,\n",
    "    priority: str = \"medium\",\n",
    "    conversation_id: str | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Convenience function to create a ticket.\"\"\"\n",
    "    \n",
    "    tool = get_ticket_tool()\n",
    "    \n",
    "    # Validate inputs\n",
    "    valid_categories = [\"return\", \"refund\", \"technical\", \"billing\", \"shipping\", \"general\"]\n",
    "    if category.lower() not in valid_categories:\n",
    "        category = \"general\"\n",
    "    \n",
    "    valid_priorities = [\"low\", \"medium\", \"high\", \"urgent\"]\n",
    "    if priority.lower() not in valid_priorities:\n",
    "        priority = \"medium\"\n",
    "    \n",
    "    ticket = tool.create(\n",
    "        customer_id=customer_id,\n",
    "        category=category.lower(),\n",
    "        subject=subject,\n",
    "        description=description,\n",
    "        priority=priority.lower(),\n",
    "        conversation_id=conversation_id,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"ticket\": ticket.model_dump(),\n",
    "        \"confirmation\": tool.format_ticket_confirmation(ticket)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools/accounts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.4\n",
    "# File: src/caspar/tools/accounts.py\n",
    "\n",
    "\"\"\"\n",
    "Account Information Tool\n",
    "\n",
    "Retrieves customer account information.\n",
    "In production, this would connect to your CRM or user database.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pydantic import BaseModel\n",
    "import random\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class CustomerAccount(BaseModel):\n",
    "    \"\"\"Customer account information.\"\"\"\n",
    "    \n",
    "    customer_id: str\n",
    "    email: str\n",
    "    name: str\n",
    "    phone: str | None = None\n",
    "    member_since: str\n",
    "    loyalty_tier: str  # bronze, silver, gold, platinum\n",
    "    loyalty_points: int\n",
    "    total_orders: int\n",
    "    total_spent: float\n",
    "    default_shipping_address: dict | None = None\n",
    "    payment_methods_on_file: int\n",
    "    email_verified: bool = True\n",
    "    two_factor_enabled: bool = False\n",
    "\n",
    "\n",
    "class AccountTool:\n",
    "    \"\"\"Tool for retrieving customer account information.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._mock_accounts = self._generate_mock_accounts()\n",
    "    \n",
    "    def _generate_mock_accounts(self) -> dict[str, CustomerAccount]:\n",
    "        \"\"\"Generate mock customer data.\"\"\"\n",
    "        \n",
    "        tiers = [\"bronze\", \"silver\", \"gold\", \"platinum\"]\n",
    "        \n",
    "        mock_customers = [\n",
    "            (\"CUST-1000\", \"john.doe@email.com\", \"John Doe\"),\n",
    "            (\"CUST-1001\", \"jane.smith@email.com\", \"Jane Smith\"),\n",
    "            (\"CUST-1002\", \"bob.wilson@email.com\", \"Bob Wilson\"),\n",
    "            (\"CUST-1003\", \"alice.jones@email.com\", \"Alice Jones\"),\n",
    "            (\"CUST-1004\", \"charlie.brown@email.com\", \"Charlie Brown\"),\n",
    "        ]\n",
    "        \n",
    "        accounts = {}\n",
    "        for i, (cust_id, email, name) in enumerate(mock_customers):\n",
    "            tier_index = min(i, len(tiers) - 1)\n",
    "            orders = (i + 1) * 5\n",
    "            spent = orders * random.uniform(100, 500)\n",
    "            \n",
    "            accounts[cust_id] = CustomerAccount(\n",
    "                customer_id=cust_id,\n",
    "                email=email,\n",
    "                name=name,\n",
    "                phone=f\"+1-555-{1000 + i:04d}\" if i % 2 == 0 else None,\n",
    "                member_since=(datetime.now() - timedelta(days=365 * (i + 1))).strftime(\"%Y-%m-%d\"),\n",
    "                loyalty_tier=tiers[tier_index],\n",
    "                loyalty_points=int(spent * 10),\n",
    "                total_orders=orders,\n",
    "                total_spent=round(spent, 2),\n",
    "                default_shipping_address={\n",
    "                    \"street\": f\"{100 + i} Main Street\",\n",
    "                    \"city\": \"Anytown\",\n",
    "                    \"state\": \"CA\",\n",
    "                    \"zip\": f\"9{1000 + i}\",\n",
    "                } if i % 2 == 0 else None,\n",
    "                payment_methods_on_file=min(i + 1, 3),\n",
    "                two_factor_enabled=i > 2,\n",
    "            )\n",
    "        \n",
    "        return accounts\n",
    "    \n",
    "    def get_account(self, customer_id: str) -> CustomerAccount | None:\n",
    "        \"\"\"Retrieve account information by customer ID.\"\"\"\n",
    "        logger.info(\"account_lookup\", customer_id=customer_id)\n",
    "        \n",
    "        account = self._mock_accounts.get(customer_id)\n",
    "        \n",
    "        if account is None:\n",
    "            logger.warning(\"account_not_found\", customer_id=customer_id)\n",
    "            return None\n",
    "        \n",
    "        logger.info(\"account_found\", customer_id=customer_id, tier=account.loyalty_tier)\n",
    "        return account\n",
    "    \n",
    "    def format_account_summary(self, account: CustomerAccount) -> str:\n",
    "        \"\"\"Format account info for display to customer.\"\"\"\n",
    "        \n",
    "        tier_emoji = {\"bronze\": \"\ud83e\udd49\", \"silver\": \"\ud83e\udd48\", \"gold\": \"\ud83e\udd47\", \"platinum\": \"\ud83d\udc8e\"}\n",
    "        \n",
    "        lines = [\n",
    "            f\"**Account Summary for {account.name}**\",\n",
    "            \"\",\n",
    "            f\"Member Since: {account.member_since}\",\n",
    "            f\"Loyalty Status: {tier_emoji.get(account.loyalty_tier, '')} {account.loyalty_tier.title()}\",\n",
    "            f\"Loyalty Points: {account.loyalty_points:,}\",\n",
    "            \"\",\n",
    "            f\"Total Orders: {account.total_orders}\",\n",
    "            f\"Total Spent: ${account.total_spent:,.2f}\",\n",
    "            \"\",\n",
    "            f\"Email: {account.email} {'\u2713 Verified' if account.email_verified else '\u26a0 Not verified'}\",\n",
    "        ]\n",
    "        \n",
    "        if account.phone:\n",
    "            lines.append(f\"Phone: {account.phone}\")\n",
    "        \n",
    "        if account.default_shipping_address:\n",
    "            addr = account.default_shipping_address\n",
    "            lines.append(f\"\\nDefault Shipping Address:\")\n",
    "            lines.append(f\"  {addr['street']}\")\n",
    "            lines.append(f\"  {addr['city']}, {addr['state']} {addr['zip']}\")\n",
    "        \n",
    "        lines.append(f\"\\nPayment Methods: {account.payment_methods_on_file} on file\")\n",
    "        lines.append(f\"Two-Factor Auth: {'\u2713 Enabled' if account.two_factor_enabled else 'Not enabled'}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "_account_tool: AccountTool | None = None\n",
    "\n",
    "\n",
    "def get_account_tool() -> AccountTool:\n",
    "    \"\"\"Get or create the account tool instance.\"\"\"\n",
    "    global _account_tool\n",
    "    if _account_tool is None:\n",
    "        _account_tool = AccountTool()\n",
    "    return _account_tool\n",
    "\n",
    "\n",
    "def get_account_info(customer_id: str) -> dict:\n",
    "    \"\"\"Convenience function to get account information.\"\"\"\n",
    "    tool = get_account_tool()\n",
    "    account = tool.get_account(customer_id)\n",
    "    \n",
    "    if account is None:\n",
    "        return {\n",
    "            \"found\": False,\n",
    "            \"error\": f\"Account {customer_id} not found.\"\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"found\": True,\n",
    "        \"account\": account.model_dump(),\n",
    "        \"summary\": tool.format_account_summary(account)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts/test_conversation_flow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.4\n",
    "# File: scripts/test_conversation_flow.py\n",
    "\n",
    "\"\"\"Test complete conversation flows through CASPAR.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "from caspar.config import setup_logging, get_logger\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "async def test_flow(name: str, message: str, customer_id: str = \"CUST-1000\"):\n",
    "    \"\"\"Run a single test flow.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"\ud83e\uddea Test: {name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=f\"test-{name}\", customer_id=customer_id)\n",
    "    state[\"messages\"] = [HumanMessage(content=message)]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": f\"test-{name}\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    print(f\"Customer: {message}\")\n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Sentiment: {result.get('sentiment_score', 'N/A')}\")\n",
    "    if result.get('ticket_id'):\n",
    "        print(f\"Ticket: {result['ticket_id']}\")\n",
    "    print(f\"\\nCASPAR: {result['messages'][-1].content}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run all tests.\"\"\"\n",
    "    \n",
    "    # Test FAQ\n",
    "    await test_flow(\"FAQ\", \"What is your return policy?\")\n",
    "    \n",
    "    # Test Order Inquiry\n",
    "    await test_flow(\"Order\", \"Where is my order TF-10001?\")\n",
    "    \n",
    "    # Test Account\n",
    "    await test_flow(\"Account\", \"What's my loyalty status?\", \"CUST-1001\")\n",
    "    \n",
    "    # Test Complaint\n",
    "    await test_flow(\"Complaint\", \"My laptop arrived damaged! This is unacceptable!\")\n",
    "    \n",
    "    # Test Handoff\n",
    "    await test_flow(\"Handoff\", \"I want to speak to a human agent please\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"\u2705 All tests complete!\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.5: Adding human handoff capabilities\n",
    "\n",
    "This section implements the human escalation system for when the AI can't help.\n",
    "\n",
    "**Key files:**\n",
    "- `src/caspar/handoff/triggers.py` - Escalation detection\n",
    "- `src/caspar/handoff/queue.py` - Handoff queue management\n",
    "- `src/caspar/handoff/context.py` - Context packaging for handoff\n",
    "- `src/caspar/handoff/notifications.py` - Agent notifications\n",
    "- `src/caspar/handoff/approval.py` - Human approval workflows\n",
    "- `src/caspar/agent/nodes_handoff_update.py` - Sentiment and handoff nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoff/triggers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/handoff/triggers.py\n",
    "\n",
    "\"\"\"\n",
    "Escalation Trigger Detection\n",
    "\n",
    "Identifies situations that require human intervention.\n",
    "\"\"\"\n",
    "\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.config import settings, get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class EscalationTrigger(str, Enum):\n",
    "    \"\"\"Types of escalation triggers.\"\"\"\n",
    "    \n",
    "    EXPLICIT_REQUEST = \"explicit_request\"\n",
    "    HIGH_FRUSTRATION = \"high_frustration\"\n",
    "    REPEATED_FAILURES = \"repeated_failures\"\n",
    "    POLICY_EXCEPTION = \"policy_exception\"\n",
    "    VIP_CUSTOMER = \"vip_customer\"\n",
    "    SENSITIVE_TOPIC = \"sensitive_topic\"\n",
    "    COMPLEX_ISSUE = \"complex_issue\"\n",
    "    MAX_TURNS_REACHED = \"max_turns_reached\"\n",
    "\n",
    "\n",
    "class EscalationResult(BaseModel):\n",
    "    \"\"\"Result of escalation check.\"\"\"\n",
    "    \n",
    "    should_escalate: bool\n",
    "    triggers: list[EscalationTrigger]\n",
    "    priority: str  # \"low\", \"medium\", \"high\", \"urgent\"\n",
    "    reason: str\n",
    "\n",
    "\n",
    "def check_escalation_triggers(\n",
    "    state: dict,\n",
    "    customer_tier: str | None = None,\n",
    ") -> EscalationResult:\n",
    "    \"\"\"\n",
    "    Check all escalation triggers against current state.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        customer_tier: Customer's loyalty tier (if known)\n",
    "        \n",
    "    Returns:\n",
    "        EscalationResult with triggers found and recommended priority\n",
    "    \"\"\"\n",
    "    triggers = []\n",
    "    reasons = []\n",
    "    \n",
    "    # Check explicit request (already classified as handoff_request)\n",
    "    if state.get(\"intent\") == \"handoff_request\":\n",
    "        triggers.append(EscalationTrigger.EXPLICIT_REQUEST)\n",
    "        reasons.append(\"Customer requested human agent\")\n",
    "    \n",
    "    # Check frustration level (handle None values)\n",
    "    sentiment = state.get(\"sentiment_score\")\n",
    "    if sentiment is None:\n",
    "        sentiment = 0.0\n",
    "    frustration = state.get(\"frustration_level\") or \"low\"\n",
    "    \n",
    "    if sentiment < settings.sentiment_threshold or frustration == \"high\":\n",
    "        triggers.append(EscalationTrigger.HIGH_FRUSTRATION)\n",
    "        reasons.append(f\"High frustration detected (sentiment: {sentiment})\")\n",
    "    \n",
    "    # Check turn count\n",
    "    turn_count = state.get(\"turn_count\") or 0\n",
    "    if turn_count >= settings.max_conversation_turns:\n",
    "        triggers.append(EscalationTrigger.MAX_TURNS_REACHED)\n",
    "        reasons.append(f\"Conversation exceeded {settings.max_conversation_turns} turns\")\n",
    "    \n",
    "    # Check for VIP customer\n",
    "    if customer_tier in [\"gold\", \"platinum\"]:\n",
    "        # VIP customers get faster escalation on any issue\n",
    "        if state.get(\"intent\") == \"complaint\" or frustration in [\"medium\", \"high\"]:\n",
    "            triggers.append(EscalationTrigger.VIP_CUSTOMER)\n",
    "            reasons.append(f\"VIP customer ({customer_tier} tier) with issue\")\n",
    "    \n",
    "    # Check for policy exceptions (would need order info)\n",
    "    order_info = state.get(\"order_info\") or {}\n",
    "    if order_info.get(\"full_order\"):\n",
    "        order_total = order_info[\"full_order\"].get(\"total\", 0)\n",
    "        if order_total > 500 and state.get(\"intent\") == \"complaint\":\n",
    "            triggers.append(EscalationTrigger.POLICY_EXCEPTION)\n",
    "            reasons.append(f\"High-value order (${order_total}) with complaint\")\n",
    "    \n",
    "    # Determine priority based on triggers\n",
    "    priority = _calculate_priority(triggers)\n",
    "    \n",
    "    result = EscalationResult(\n",
    "        should_escalate=len(triggers) > 0,\n",
    "        triggers=triggers,\n",
    "        priority=priority,\n",
    "        reason=\"; \".join(reasons) if reasons else \"No escalation needed\"\n",
    "    )\n",
    "    \n",
    "    if result.should_escalate:\n",
    "        logger.info(\n",
    "            \"escalation_triggers_detected\",\n",
    "            triggers=[t.value for t in triggers],\n",
    "            priority=priority\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _calculate_priority(triggers: list[EscalationTrigger]) -> str:\n",
    "    \"\"\"Calculate escalation priority based on triggers.\"\"\"\n",
    "    \n",
    "    if not triggers:\n",
    "        return \"low\"\n",
    "    \n",
    "    # Urgent triggers\n",
    "    urgent_triggers = {\n",
    "        EscalationTrigger.EXPLICIT_REQUEST,\n",
    "        EscalationTrigger.HIGH_FRUSTRATION,\n",
    "        EscalationTrigger.SENSITIVE_TOPIC,\n",
    "    }\n",
    "    \n",
    "    # High priority triggers\n",
    "    high_triggers = {\n",
    "        EscalationTrigger.VIP_CUSTOMER,\n",
    "        EscalationTrigger.POLICY_EXCEPTION,\n",
    "        EscalationTrigger.REPEATED_FAILURES,\n",
    "    }\n",
    "    \n",
    "    if any(t in urgent_triggers for t in triggers):\n",
    "        return \"urgent\"\n",
    "    elif any(t in high_triggers for t in triggers):\n",
    "        return \"high\"\n",
    "    elif len(triggers) >= 2:\n",
    "        # Multiple medium triggers escalate to high\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"medium\"\n",
    "\n",
    "\n",
    "def check_sensitive_topics(message: str) -> bool:\n",
    "    \"\"\"Check if message contains sensitive topics requiring human handling.\"\"\"\n",
    "    \n",
    "    sensitive_keywords = [\n",
    "        \"lawyer\", \"lawsuit\", \"legal action\", \"sue\",\n",
    "        \"police\", \"fraud\", \"scam\", \"stolen\",\n",
    "        \"safety\", \"dangerous\", \"injury\", \"injured\", \"hurt\",\n",
    "        \"discrimination\", \"harassment\",\n",
    "        \"cancel account\", \"delete my data\", \"gdpr\",\n",
    "    ]\n",
    "    \n",
    "    message_lower = message.lower()\n",
    "    return any(keyword in message_lower for keyword in sensitive_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoff/queue.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/handoff/queue.py\n",
    "\n",
    "\"\"\"\n",
    "Handoff Queue Management\n",
    "\n",
    "Manages the queue of conversations waiting for human agents.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from enum import Enum\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import uuid\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class HandoffStatus(str, Enum):\n",
    "    \"\"\"Status of a handoff request.\"\"\"\n",
    "    \n",
    "    QUEUED = \"queued\"\n",
    "    ASSIGNED = \"assigned\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    RESOLVED = \"resolved\"\n",
    "    ABANDONED = \"abandoned\"\n",
    "\n",
    "\n",
    "class HandoffRequest(BaseModel):\n",
    "    \"\"\"A request for human agent assistance.\"\"\"\n",
    "    \n",
    "    request_id: str = Field(default_factory=lambda: f\"HO-{uuid.uuid4().hex[:8].upper()}\")\n",
    "    conversation_id: str\n",
    "    customer_id: str\n",
    "    ticket_id: str | None = None\n",
    "    \n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"]\n",
    "    triggers: list[str]  # EscalationTrigger values\n",
    "    reason: str\n",
    "    \n",
    "    status: HandoffStatus = HandoffStatus.QUEUED\n",
    "    assigned_agent: str | None = None\n",
    "    \n",
    "    created_at: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
    "    updated_at: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())\n",
    "    assigned_at: str | None = None\n",
    "    resolved_at: str | None = None\n",
    "    \n",
    "    # Estimated wait time in minutes (calculated based on queue position)\n",
    "    estimated_wait: int | None = None\n",
    "\n",
    "\n",
    "class HandoffQueue:\n",
    "    \"\"\"\n",
    "    Manages the queue of pending handoff requests.\n",
    "    \n",
    "    In production, this would be backed by Redis or a database.\n",
    "    For demo purposes, we use in-memory storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._queue: dict[str, HandoffRequest] = {}\n",
    "        self._by_conversation: dict[str, str] = {}  # conversation_id -> request_id\n",
    "    \n",
    "    def add(\n",
    "        self,\n",
    "        conversation_id: str,\n",
    "        customer_id: str,\n",
    "        priority: str,\n",
    "        triggers: list[str],\n",
    "        reason: str,\n",
    "        ticket_id: str | None = None,\n",
    "    ) -> HandoffRequest:\n",
    "        \"\"\"Add a new handoff request to the queue.\"\"\"\n",
    "        \n",
    "        # Check if conversation already has a pending request\n",
    "        if conversation_id in self._by_conversation:\n",
    "            existing_id = self._by_conversation[conversation_id]\n",
    "            existing = self._queue.get(existing_id)\n",
    "            if existing and existing.status == HandoffStatus.QUEUED:\n",
    "                logger.info(\"handoff_already_queued\", conversation_id=conversation_id)\n",
    "                return existing\n",
    "        \n",
    "        request = HandoffRequest(\n",
    "            conversation_id=conversation_id,\n",
    "            customer_id=customer_id,\n",
    "            ticket_id=ticket_id,\n",
    "            priority=priority,\n",
    "            triggers=triggers,\n",
    "            reason=reason,\n",
    "            estimated_wait=self._estimate_wait_time(priority),\n",
    "        )\n",
    "        \n",
    "        self._queue[request.request_id] = request\n",
    "        self._by_conversation[conversation_id] = request.request_id\n",
    "        \n",
    "        logger.info(\n",
    "            \"handoff_queued\",\n",
    "            request_id=request.request_id,\n",
    "            conversation_id=conversation_id,\n",
    "            priority=priority,\n",
    "            position=self.get_queue_position(request.request_id)\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def _estimate_wait_time(self, priority: str) -> int:\n",
    "        \"\"\"Estimate wait time based on queue and priority.\"\"\"\n",
    "        \n",
    "        # Count requests ahead in queue by priority\n",
    "        queued = [r for r in self._queue.values() if r.status == HandoffStatus.QUEUED]\n",
    "        \n",
    "        # Priority weights (urgent gets served first)\n",
    "        priority_order = {\"urgent\": 0, \"high\": 1, \"medium\": 2, \"low\": 3}\n",
    "        my_priority = priority_order.get(priority, 2)\n",
    "        \n",
    "        ahead_count = sum(\n",
    "            1 for r in queued \n",
    "            if priority_order.get(r.priority, 2) <= my_priority\n",
    "        )\n",
    "        \n",
    "        # Assume ~5 minutes per request ahead\n",
    "        base_wait = ahead_count * 5\n",
    "        \n",
    "        # Adjust by priority\n",
    "        if priority == \"urgent\":\n",
    "            return max(2, base_wait // 2)\n",
    "        elif priority == \"high\":\n",
    "            return max(5, base_wait)\n",
    "        else:\n",
    "            return base_wait + 5\n",
    "    \n",
    "    def get(self, request_id: str) -> HandoffRequest | None:\n",
    "        \"\"\"Get a handoff request by ID.\"\"\"\n",
    "        return self._queue.get(request_id)\n",
    "    \n",
    "    def get_by_conversation(self, conversation_id: str) -> HandoffRequest | None:\n",
    "        \"\"\"Get the handoff request for a conversation.\"\"\"\n",
    "        request_id = self._by_conversation.get(conversation_id)\n",
    "        if request_id:\n",
    "            return self._queue.get(request_id)\n",
    "        return None\n",
    "    \n",
    "    def get_queue_position(self, request_id: str) -> int:\n",
    "        \"\"\"Get position in queue (1-indexed).\"\"\"\n",
    "        request = self._queue.get(request_id)\n",
    "        if not request or request.status != HandoffStatus.QUEUED:\n",
    "            return 0\n",
    "        \n",
    "        # Sort by priority then by created_at\n",
    "        priority_order = {\"urgent\": 0, \"high\": 1, \"medium\": 2, \"low\": 3}\n",
    "        \n",
    "        queued = [\n",
    "            r for r in self._queue.values() \n",
    "            if r.status == HandoffStatus.QUEUED\n",
    "        ]\n",
    "        queued.sort(key=lambda r: (priority_order.get(r.priority, 2), r.created_at))\n",
    "        \n",
    "        for i, r in enumerate(queued, 1):\n",
    "            if r.request_id == request_id:\n",
    "                return i\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def assign(self, request_id: str, agent_id: str) -> HandoffRequest | None:\n",
    "        \"\"\"Assign a request to a human agent.\"\"\"\n",
    "        request = self._queue.get(request_id)\n",
    "        if not request:\n",
    "            return None\n",
    "        \n",
    "        request.status = HandoffStatus.ASSIGNED\n",
    "        request.assigned_agent = agent_id\n",
    "        request.assigned_at = datetime.now(timezone.utc).isoformat()\n",
    "        request.updated_at = datetime.now(timezone.utc).isoformat()\n",
    "        \n",
    "        logger.info(\n",
    "            \"handoff_assigned\",\n",
    "            request_id=request_id,\n",
    "            agent_id=agent_id\n",
    "        )\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def resolve(self, request_id: str, resolution: str = \"resolved\") -> HandoffRequest | None:\n",
    "        \"\"\"Mark a handoff request as resolved.\"\"\"\n",
    "        request = self._queue.get(request_id)\n",
    "        if not request:\n",
    "            return None\n",
    "        \n",
    "        request.status = HandoffStatus.RESOLVED\n",
    "        request.resolved_at = datetime.now(timezone.utc).isoformat()\n",
    "        request.updated_at = datetime.now(timezone.utc).isoformat()\n",
    "        \n",
    "        # Clean up conversation mapping\n",
    "        if request.conversation_id in self._by_conversation:\n",
    "            del self._by_conversation[request.conversation_id]\n",
    "        \n",
    "        logger.info(\"handoff_resolved\", request_id=request_id)\n",
    "        \n",
    "        return request\n",
    "    \n",
    "    def get_pending_count(self) -> dict[str, int]:\n",
    "        \"\"\"Get count of pending requests by priority.\"\"\"\n",
    "        counts = {\"urgent\": 0, \"high\": 0, \"medium\": 0, \"low\": 0}\n",
    "        \n",
    "        for request in self._queue.values():\n",
    "            if request.status == HandoffStatus.QUEUED:\n",
    "                counts[request.priority] = counts.get(request.priority, 0) + 1\n",
    "        \n",
    "        return counts\n",
    "\n",
    "\n",
    "# Singleton instance\n",
    "_handoff_queue: HandoffQueue | None = None\n",
    "\n",
    "\n",
    "def get_handoff_queue() -> HandoffQueue:\n",
    "    \"\"\"Get or create the global handoff queue.\"\"\"\n",
    "    global _handoff_queue\n",
    "    if _handoff_queue is None:\n",
    "        _handoff_queue = HandoffQueue()\n",
    "    return _handoff_queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoff/context.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/handoff/context.py\n",
    "\n",
    "\"\"\"\n",
    "Context Packaging for Human Agents\n",
    "\n",
    "Prepares comprehensive context to help human agents\n",
    "quickly understand and resolve customer issues.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class ConversationContext(BaseModel):\n",
    "    \"\"\"Complete context package for human agent.\"\"\"\n",
    "    \n",
    "    # Identification\n",
    "    conversation_id: str\n",
    "    customer_id: str\n",
    "    request_id: str\n",
    "    \n",
    "    # Customer Info\n",
    "    customer_name: str | None = None\n",
    "    customer_email: str | None = None\n",
    "    customer_tier: str | None = None\n",
    "    customer_history: str | None = None  # Brief history summary\n",
    "    \n",
    "    # Conversation Summary\n",
    "    conversation_summary: str\n",
    "    message_count: int\n",
    "    conversation_duration: str | None = None\n",
    "    \n",
    "    # Issue Details\n",
    "    detected_intent: str\n",
    "    escalation_triggers: list[str]\n",
    "    escalation_reason: str\n",
    "    sentiment_score: float\n",
    "    frustration_level: str\n",
    "    \n",
    "    # Relevant Data\n",
    "    order_info: dict | None = None\n",
    "    ticket_id: str | None = None\n",
    "    retrieved_knowledge: str | None = None\n",
    "    \n",
    "    # Full Transcript\n",
    "    transcript: list[dict]\n",
    "    \n",
    "    # Recommendations\n",
    "    suggested_actions: list[str]\n",
    "    \n",
    "    # Metadata\n",
    "    packaged_at: str\n",
    "\n",
    "\n",
    "def package_context_for_agent(\n",
    "    state: dict,\n",
    "    request_id: str,\n",
    "    customer_info: dict | None = None,\n",
    ") -> ConversationContext:\n",
    "    \"\"\"\n",
    "    Package all relevant context for a human agent.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        request_id: The handoff request ID\n",
    "        customer_info: Optional customer account info\n",
    "        \n",
    "    Returns:\n",
    "        ConversationContext with all relevant information\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\") or []\n",
    "    \n",
    "    # Build transcript\n",
    "    transcript = []\n",
    "    for msg in messages:\n",
    "        transcript.append({\n",
    "            \"role\": \"customer\" if isinstance(msg, HumanMessage) else \"caspar\",\n",
    "            \"content\": msg.content,\n",
    "        })\n",
    "    \n",
    "    # Generate conversation summary\n",
    "    summary = _generate_summary(messages)\n",
    "    \n",
    "    # Extract customer info if provided\n",
    "    customer_name = None\n",
    "    customer_email = None\n",
    "    customer_tier = None\n",
    "    customer_history = None\n",
    "    \n",
    "    if customer_info:\n",
    "        customer_name = customer_info.get(\"name\")\n",
    "        customer_email = customer_info.get(\"email\")\n",
    "        customer_tier = customer_info.get(\"loyalty_tier\")\n",
    "        customer_history = f\"{customer_info.get('total_orders', 0)} orders, ${customer_info.get('total_spent', 0):,.2f} total\"\n",
    "    \n",
    "    # Generate suggested actions based on intent and triggers\n",
    "    suggested_actions = _generate_suggestions(state)\n",
    "    \n",
    "    context = ConversationContext(\n",
    "        conversation_id=state.get(\"conversation_id\") or \"unknown\",\n",
    "        customer_id=state.get(\"customer_id\") or \"unknown\",\n",
    "        request_id=request_id,\n",
    "        customer_name=customer_name,\n",
    "        customer_email=customer_email,\n",
    "        customer_tier=customer_tier,\n",
    "        customer_history=customer_history,\n",
    "        conversation_summary=summary,\n",
    "        message_count=len(messages),\n",
    "        detected_intent=state.get(\"intent\") or \"unknown\",\n",
    "        escalation_triggers=state.get(\"escalation_triggers\") or [],\n",
    "        escalation_reason=state.get(\"escalation_reason\") or \"Unknown\",\n",
    "        sentiment_score=state.get(\"sentiment_score\") or 0.0,\n",
    "        frustration_level=state.get(\"frustration_level\") or \"unknown\",\n",
    "        order_info=state.get(\"order_info\"),\n",
    "        ticket_id=state.get(\"ticket_id\"),\n",
    "        retrieved_knowledge=state.get(\"retrieved_context\"),\n",
    "        transcript=transcript,\n",
    "        suggested_actions=suggested_actions,\n",
    "        packaged_at=datetime.now(timezone.utc).isoformat(),\n",
    "    )\n",
    "    \n",
    "    logger.info(\n",
    "        \"context_packaged\",\n",
    "        conversation_id=context.conversation_id,\n",
    "        message_count=context.message_count\n",
    "    )\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def _generate_summary(messages: list) -> str:\n",
    "    \"\"\"Generate a brief summary of the conversation.\"\"\"\n",
    "    \n",
    "    if not messages:\n",
    "        return \"No messages in conversation.\"\n",
    "    \n",
    "    # Get first customer message (the initial inquiry)\n",
    "    first_customer_msg = None\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            first_customer_msg = msg.content\n",
    "            break\n",
    "    \n",
    "    # Get last customer message (most recent concern)\n",
    "    last_customer_msg = None\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_customer_msg = msg.content\n",
    "            break\n",
    "    \n",
    "    summary_parts = []\n",
    "    \n",
    "    if first_customer_msg:\n",
    "        # Truncate if too long\n",
    "        initial = first_customer_msg[:150] + \"...\" if len(first_customer_msg) > 150 else first_customer_msg\n",
    "        summary_parts.append(f\"Initial inquiry: {initial}\")\n",
    "    \n",
    "    if last_customer_msg and last_customer_msg != first_customer_msg:\n",
    "        recent = last_customer_msg[:150] + \"...\" if len(last_customer_msg) > 150 else last_customer_msg\n",
    "        summary_parts.append(f\"Most recent message: {recent}\")\n",
    "    \n",
    "    summary_parts.append(f\"Total exchanges: {len(messages)} messages\")\n",
    "    \n",
    "    return \"\\n\".join(summary_parts)\n",
    "\n",
    "\n",
    "def _generate_suggestions(state: dict) -> list[str]:\n",
    "    \"\"\"Generate suggested actions for the human agent.\"\"\"\n",
    "    \n",
    "    suggestions = []\n",
    "    intent = state.get(\"intent\") or \"\"\n",
    "    triggers = state.get(\"escalation_triggers\") or []\n",
    "    \n",
    "    # Intent-based suggestions\n",
    "    if intent == \"complaint\":\n",
    "        suggestions.append(\"Acknowledge the customer's frustration and apologize for the inconvenience\")\n",
    "        suggestions.append(\"Review order history for context\")\n",
    "        \n",
    "    if intent == \"order_inquiry\":\n",
    "        suggestions.append(\"Verify order status in the system\")\n",
    "        suggestions.append(\"Check for any shipping delays or issues\")\n",
    "    \n",
    "    # Trigger-based suggestions\n",
    "    if \"high_frustration\" in triggers:\n",
    "        suggestions.append(\"\u26a0\ufe0f Customer is highly frustrated - prioritize empathy\")\n",
    "        suggestions.append(\"Consider offering a goodwill gesture (discount, expedited shipping)\")\n",
    "    \n",
    "    if \"vip_customer\" in triggers:\n",
    "        suggestions.append(\"\u2b50 VIP Customer - consider premium resolution options\")\n",
    "    \n",
    "    if \"policy_exception\" in triggers:\n",
    "        suggestions.append(\"This may require manager approval for policy exception\")\n",
    "    \n",
    "    # Order-specific suggestions\n",
    "    order_info = state.get(\"order_info\") or {}\n",
    "    if order_info.get(\"status\") == \"processing\":\n",
    "        suggestions.append(\"Order is still processing - can offer to expedite if needed\")\n",
    "    elif order_info.get(\"status\") == \"shipped\":\n",
    "        suggestions.append(\"Order is in transit - check tracking for delays\")\n",
    "    \n",
    "    # Default suggestions\n",
    "    if not suggestions:\n",
    "        suggestions.append(\"Review the conversation transcript for context\")\n",
    "        suggestions.append(\"Ask clarifying questions if needed\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "\n",
    "def format_context_for_display(context: ConversationContext) -> str:\n",
    "    \"\"\"Format context as readable text for agent interface.\"\"\"\n",
    "    \n",
    "    lines = [\n",
    "        \"=\" * 60,\n",
    "        \"\ud83c\udfab HANDOFF CONTEXT\",\n",
    "        \"=\" * 60,\n",
    "        \"\",\n",
    "        f\"Request ID: {context.request_id}\",\n",
    "        f\"Conversation: {context.conversation_id}\",\n",
    "        f\"Customer: {context.customer_id}\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    # Customer info if available\n",
    "    if context.customer_name:\n",
    "        lines.append(\"\ud83d\udc64 CUSTOMER INFO\")\n",
    "        lines.append(\"-\" * 40)\n",
    "        lines.append(f\"Name: {context.customer_name}\")\n",
    "        if context.customer_email:\n",
    "            lines.append(f\"Email: {context.customer_email}\")\n",
    "        if context.customer_tier:\n",
    "            lines.append(f\"Tier: {context.customer_tier.upper()}\")\n",
    "        if context.customer_history:\n",
    "            lines.append(f\"History: {context.customer_history}\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    # Issue summary\n",
    "    lines.append(\"\ud83d\udccb ISSUE SUMMARY\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    lines.append(f\"Intent: {context.detected_intent}\")\n",
    "    lines.append(f\"Sentiment: {context.sentiment_score:.2f} ({context.frustration_level} frustration)\")\n",
    "    lines.append(f\"Reason: {context.escalation_reason}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Suggested actions\n",
    "    lines.append(\"\ud83d\udca1 SUGGESTED ACTIONS\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    for action in context.suggested_actions:\n",
    "        lines.append(f\"  \u2022 {action}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Conversation summary\n",
    "    lines.append(\"\ud83d\udcdd CONVERSATION SUMMARY\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    lines.append(context.conversation_summary)\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Transcript\n",
    "    lines.append(\"\ud83d\udcac TRANSCRIPT\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    for msg in context.transcript:\n",
    "        role = \"Customer\" if msg[\"role\"] == \"customer\" else \"CASPAR\"\n",
    "        content = msg[\"content\"][:200] + \"...\" if len(msg[\"content\"]) > 200 else msg[\"content\"]\n",
    "        lines.append(f\"{role}: {content}\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    lines.append(\"=\" * 60)\n",
    "    \n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoff/notifications.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/handoff/notifications.py\n",
    "\n",
    "\"\"\"\n",
    "Agent Notification System\n",
    "\n",
    "Notifies available human agents about pending handoffs.\n",
    "In production, this would integrate with Slack, email, or a dashboard.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from caspar.config import get_logger\n",
    "from .queue import HandoffRequest\n",
    "from .context import ConversationContext\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class AgentNotification(BaseModel):\n",
    "    \"\"\"A notification sent to human agents.\"\"\"\n",
    "    \n",
    "    notification_id: str\n",
    "    request_id: str\n",
    "    priority: str\n",
    "    customer_id: str\n",
    "    brief_reason: str\n",
    "    estimated_wait: int | None\n",
    "    sent_at: str\n",
    "    channel: str  # \"dashboard\", \"slack\", \"email\"\n",
    "\n",
    "\n",
    "# Simulated agent pool\n",
    "AVAILABLE_AGENTS = [\n",
    "    {\"id\": \"AGENT-001\", \"name\": \"Sarah Johnson\", \"status\": \"available\", \"skills\": [\"technical\", \"billing\"]},\n",
    "    {\"id\": \"AGENT-002\", \"name\": \"Mike Chen\", \"status\": \"available\", \"skills\": [\"returns\", \"shipping\"]},\n",
    "    {\"id\": \"AGENT-003\", \"name\": \"Emily Davis\", \"status\": \"busy\", \"skills\": [\"vip\", \"complaints\"]},\n",
    "]\n",
    "\n",
    "\n",
    "def get_available_agents(required_skills: list[str] | None = None) -> list[dict]:\n",
    "    \"\"\"Get list of available agents, optionally filtered by skills.\"\"\"\n",
    "    \n",
    "    available = [a for a in AVAILABLE_AGENTS if a[\"status\"] == \"available\"]\n",
    "    \n",
    "    if required_skills:\n",
    "        available = [\n",
    "            a for a in available\n",
    "            if any(skill in a[\"skills\"] for skill in required_skills)\n",
    "        ]\n",
    "    \n",
    "    return available\n",
    "\n",
    "\n",
    "def notify_available_agents(\n",
    "    request: HandoffRequest,\n",
    "    context: ConversationContext | None = None,\n",
    ") -> list[AgentNotification]:\n",
    "    \"\"\"\n",
    "    Notify available agents about a new handoff request.\n",
    "    \n",
    "    In production, this would:\n",
    "    - Send Slack messages to a support channel\n",
    "    - Update a real-time dashboard\n",
    "    - Send push notifications to mobile apps\n",
    "    - Trigger phone alerts for urgent requests\n",
    "    \n",
    "    Args:\n",
    "        request: The handoff request\n",
    "        context: Optional conversation context\n",
    "        \n",
    "    Returns:\n",
    "        List of notifications sent\n",
    "    \"\"\"\n",
    "    notifications = []\n",
    "    \n",
    "    # Determine required skills based on triggers\n",
    "    required_skills = []\n",
    "    if \"vip_customer\" in request.triggers:\n",
    "        required_skills.append(\"vip\")\n",
    "    if \"complaint\" in str(request.reason).lower():\n",
    "        required_skills.append(\"complaints\")\n",
    "    \n",
    "    # Get available agents\n",
    "    agents = get_available_agents(required_skills)\n",
    "    \n",
    "    if not agents:\n",
    "        # Fall back to all available agents\n",
    "        agents = get_available_agents()\n",
    "    \n",
    "    # Create notification content\n",
    "    brief_reason = request.reason[:100] + \"...\" if len(request.reason) > 100 else request.reason\n",
    "    \n",
    "    for agent in agents:\n",
    "        notification = AgentNotification(\n",
    "            notification_id=f\"NOTIF-{request.request_id}-{agent['id']}\",\n",
    "            request_id=request.request_id,\n",
    "            priority=request.priority,\n",
    "            customer_id=request.customer_id,\n",
    "            brief_reason=brief_reason,\n",
    "            estimated_wait=request.estimated_wait,\n",
    "            sent_at=datetime.now(timezone.utc).isoformat(),\n",
    "            channel=\"dashboard\",\n",
    "        )\n",
    "        \n",
    "        notifications.append(notification)\n",
    "        \n",
    "        # Log the \"notification\" (in production, this would actually send)\n",
    "        logger.info(\n",
    "            \"agent_notified\",\n",
    "            agent_id=agent[\"id\"],\n",
    "            agent_name=agent[\"name\"],\n",
    "            request_id=request.request_id,\n",
    "            priority=request.priority\n",
    "        )\n",
    "        \n",
    "        # Simulate different notification channels based on priority\n",
    "        if request.priority == \"urgent\":\n",
    "            _send_urgent_notification(agent, request, brief_reason)\n",
    "        else:\n",
    "            _send_standard_notification(agent, request, brief_reason)\n",
    "    \n",
    "    return notifications\n",
    "\n",
    "\n",
    "def _send_urgent_notification(agent: dict, request: HandoffRequest, reason: str):\n",
    "    \"\"\"Simulate urgent notification (would trigger alerts).\"\"\"\n",
    "    print(f\"\\n\ud83d\udea8 URGENT HANDOFF ALERT for {agent['name']}!\")\n",
    "    print(f\"   Customer: {request.customer_id}\")\n",
    "    print(f\"   Reason: {reason}\")\n",
    "    print(f\"   \u2192 Immediate attention required\\n\")\n",
    "\n",
    "\n",
    "def _send_standard_notification(agent: dict, request: HandoffRequest, reason: str):\n",
    "    \"\"\"Simulate standard notification (would update dashboard).\"\"\"\n",
    "    print(f\"\\n\ud83d\udccb New handoff request for {agent['name']}\")\n",
    "    print(f\"   Priority: {request.priority.upper()}\")\n",
    "    print(f\"   Customer: {request.customer_id}\")\n",
    "    print(f\"   Reason: {reason}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoff/approval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/handoff/approval.py\n",
    "\n",
    "\"\"\"\n",
    "Human-in-the-Loop response approval.\n",
    "\n",
    "This module enables human review of AI responses before they're sent,\n",
    "useful for high-stakes or sensitive situations.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from enum import Enum\n",
    "\n",
    "from caspar.config import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "class ApprovalStatus(Enum):\n",
    "    \"\"\"Status of a pending approval.\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    APPROVED = \"approved\"\n",
    "    REJECTED = \"rejected\"\n",
    "    EDITED = \"edited\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PendingApproval:\n",
    "    \"\"\"A response waiting for human approval.\"\"\"\n",
    "    conversation_id: str\n",
    "    original_response: str\n",
    "    reason: str  # Why approval is needed\n",
    "    created_at: datetime\n",
    "    status: ApprovalStatus = ApprovalStatus.PENDING\n",
    "    reviewer_id: str | None = None\n",
    "    edited_response: str | None = None\n",
    "    reviewed_at: datetime | None = None\n",
    "\n",
    "\n",
    "def needs_approval(state: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a response needs human approval before sending.\n",
    "    \n",
    "    This is checked BEFORE the response is sent to the customer.\n",
    "    \"\"\"\n",
    "    # High-value actions need approval\n",
    "    if state.get(\"pending_refund_amount\", 0) > 100:\n",
    "        return True\n",
    "    \n",
    "    # Policy exceptions need approval\n",
    "    if state.get(\"policy_exception_requested\"):\n",
    "        return True\n",
    "    \n",
    "    # Very negative sentiment needs human review\n",
    "    sentiment = state.get(\"sentiment_score\", 0)\n",
    "    if sentiment < -0.7:\n",
    "        return True\n",
    "    \n",
    "    # New customers with complaints\n",
    "    if state.get(\"intent\") == \"complaint\" and state.get(\"customer_tenure_days\", 365) < 30:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def get_approval_reason(state: dict) -> str:\n",
    "    \"\"\"Get a human-readable reason for why approval is needed.\"\"\"\n",
    "    reasons = []\n",
    "    \n",
    "    if state.get(\"pending_refund_amount\", 0) > 100:\n",
    "        amount = state.get(\"pending_refund_amount\")\n",
    "        reasons.append(f\"High-value refund: ${amount}\")\n",
    "    \n",
    "    if state.get(\"policy_exception_requested\"):\n",
    "        reasons.append(\"Policy exception requested\")\n",
    "    \n",
    "    if state.get(\"sentiment_score\", 0) < -0.7:\n",
    "        reasons.append(\"Customer appears very upset\")\n",
    "    \n",
    "    if state.get(\"intent\") == \"complaint\" and state.get(\"customer_tenure_days\", 365) < 30:\n",
    "        reasons.append(\"New customer complaint - retention risk\")\n",
    "    \n",
    "    return \"; \".join(reasons) if reasons else \"Manual review requested\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent/nodes_handoff_update.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: src/caspar/agent/nodes_handoff_update.py\n",
    "\n",
    "\"\"\"\n",
    "Updated nodes for human handoff functionality.\n",
    "\n",
    "These functions extend the agent with handoff support:\n",
    "- check_sentiment: Analyze customer emotion and detect escalation needs\n",
    "- human_handoff: Handle the transition to a human agent\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from caspar.config import settings, get_logger\n",
    "from caspar.handoff import (\n",
    "    check_escalation_triggers,\n",
    "    get_handoff_queue,\n",
    "    package_context_for_agent,\n",
    "    notify_available_agents,\n",
    "    format_context_for_display,\n",
    "    check_sensitive_topics,\n",
    ")\n",
    "from caspar.tools import get_account_info, create_ticket\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "async def check_sentiment(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze customer sentiment and check all escalation triggers.\n",
    "    \n",
    "    This node runs after intent handlers to determine if:\n",
    "    1. The customer is frustrated (sentiment analysis)\n",
    "    2. Sensitive topics are detected\n",
    "    3. Escalation to a human is needed\n",
    "    \"\"\"\n",
    "    logger.info(\"check_sentiment_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        return {\n",
    "            \"sentiment_score\": 0.0,\n",
    "            \"frustration_level\": \"low\",\n",
    "            \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "    \n",
    "    # Get last few messages for context\n",
    "    recent_messages = messages[-3:] if len(messages) >= 3 else messages\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{'Customer' if isinstance(m, HumanMessage) else 'Agent'}: {m.content}\"\n",
    "        for m in recent_messages\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=settings.default_model,\n",
    "        api_key=settings.openai_api_key,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    sentiment_prompt = f\"\"\"Analyze the customer's emotional state in this conversation.\n",
    "\n",
    "Conversation:\n",
    "{conversation_text}\n",
    "\n",
    "Provide your analysis in this exact format:\n",
    "SENTIMENT: [number from -1.0 to 1.0, where -1 is very negative, 0 is neutral, 1 is very positive]\n",
    "FRUSTRATION: [low, medium, or high]\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=sentiment_prompt)])\n",
    "    \n",
    "    # Parse response\n",
    "    sentiment_score = 0.0\n",
    "    frustration_level = \"low\"\n",
    "    \n",
    "    for line in response.content.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"SENTIMENT:\"):\n",
    "            try:\n",
    "                sentiment_score = float(line.split(\":\")[1].strip())\n",
    "                sentiment_score = max(-1.0, min(1.0, sentiment_score))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif line.startswith(\"FRUSTRATION:\"):\n",
    "            level = line.split(\":\")[1].strip().lower()\n",
    "            if level in [\"low\", \"medium\", \"high\"]:\n",
    "                frustration_level = level\n",
    "    \n",
    "    result = {\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"frustration_level\": frustration_level,\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    \n",
    "    # Check for sensitive topics in the last message\n",
    "    last_message = messages[-1].content if messages else \"\"\n",
    "    if check_sensitive_topics(last_message):\n",
    "        result[\"needs_escalation\"] = True\n",
    "        result[\"escalation_reason\"] = \"Sensitive topic detected - requires human handling\"\n",
    "        logger.warning(\"sensitive_topic_detected\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    # Check if escalation needed based on sentiment\n",
    "    elif sentiment_score < settings.sentiment_threshold or frustration_level == \"high\":\n",
    "        result[\"needs_escalation\"] = True\n",
    "        result[\"escalation_reason\"] = f\"High frustration detected (sentiment: {sentiment_score}, frustration: {frustration_level})\"\n",
    "        logger.warning(\"escalation_triggered\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    logger.info(\n",
    "        \"check_sentiment_complete\",\n",
    "        sentiment_score=sentiment_score,\n",
    "        frustration_level=frustration_level\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "async def human_handoff(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Handle escalation to a human agent.\n",
    "    \n",
    "    This node:\n",
    "    1. Checks escalation triggers\n",
    "    2. Creates a handoff request\n",
    "    3. Packages context for the human agent\n",
    "    4. Notifies available agents\n",
    "    5. Informs the customer\n",
    "    \"\"\"\n",
    "    logger.info(\"human_handoff_start\", conversation_id=state.get(\"conversation_id\"))\n",
    "    \n",
    "    customer_id = state.get(\"customer_id\") or \"UNKNOWN\"\n",
    "    conversation_id = state.get(\"conversation_id\")\n",
    "    \n",
    "    # Get customer info for context\n",
    "    customer_info = None\n",
    "    if customer_id != \"UNKNOWN\":\n",
    "        account_result = get_account_info(customer_id)\n",
    "        if account_result[\"found\"]:\n",
    "            customer_info = account_result[\"account\"]\n",
    "    \n",
    "    # Check escalation triggers\n",
    "    customer_tier = customer_info.get(\"loyalty_tier\") if customer_info else None\n",
    "    escalation_result = check_escalation_triggers(state, customer_tier)\n",
    "    \n",
    "    # Create ticket for tracking\n",
    "    ticket_result = create_ticket(\n",
    "        customer_id=customer_id,\n",
    "        category=\"general\",\n",
    "        subject=\"Human Agent Requested\",\n",
    "        description=escalation_result.reason,\n",
    "        priority=escalation_result.priority,\n",
    "        conversation_id=conversation_id,\n",
    "    )\n",
    "    \n",
    "    # Add to handoff queue\n",
    "    queue = get_handoff_queue()\n",
    "    handoff_request = queue.add(\n",
    "        conversation_id=conversation_id,\n",
    "        customer_id=customer_id,\n",
    "        priority=escalation_result.priority,\n",
    "        triggers=[t.value for t in escalation_result.triggers],\n",
    "        reason=escalation_result.reason,\n",
    "        ticket_id=ticket_result[\"ticket\"][\"ticket_id\"],\n",
    "    )\n",
    "    \n",
    "    # Package context for human agent\n",
    "    state_with_triggers = {\n",
    "        **state,\n",
    "        \"escalation_triggers\": [t.value for t in escalation_result.triggers],\n",
    "    }\n",
    "    context = package_context_for_agent(\n",
    "        state=state_with_triggers,\n",
    "        request_id=handoff_request.request_id,\n",
    "        customer_info=customer_info,\n",
    "    )\n",
    "    \n",
    "    # Notify available agents\n",
    "    notifications = notify_available_agents(handoff_request, context)\n",
    "    \n",
    "    # Log the full context (in production, this would go to the agent dashboard)\n",
    "    context_display = format_context_for_display(context)\n",
    "    logger.info(\"handoff_context_prepared\", context_length=len(context_display))\n",
    "    \n",
    "    # Build customer-facing message\n",
    "    position = queue.get_queue_position(handoff_request.request_id)\n",
    "    wait_time = handoff_request.estimated_wait or 5\n",
    "    \n",
    "    handoff_message = _build_handoff_message(\n",
    "        ticket_id=ticket_result[\"ticket\"][\"ticket_id\"],\n",
    "        position=position,\n",
    "        wait_time=wait_time,\n",
    "        priority=escalation_result.priority,\n",
    "    )\n",
    "    \n",
    "    logger.info(\n",
    "        \"human_handoff_complete\",\n",
    "        request_id=handoff_request.request_id,\n",
    "        ticket_id=ticket_result[\"ticket\"][\"ticket_id\"],\n",
    "        agents_notified=len(notifications)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=handoff_message)],\n",
    "        \"needs_escalation\": True,\n",
    "        \"escalation_reason\": escalation_result.reason,\n",
    "        \"ticket_id\": ticket_result[\"ticket\"][\"ticket_id\"],\n",
    "        \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "def _build_handoff_message(\n",
    "    ticket_id: str,\n",
    "    position: int,\n",
    "    wait_time: int,\n",
    "    priority: str,\n",
    ") -> str:\n",
    "    \"\"\"Build the customer-facing handoff message.\"\"\"\n",
    "    \n",
    "    priority_messages = {\n",
    "        \"urgent\": \"I've flagged this as urgent, and a team member will be with you very shortly.\",\n",
    "        \"high\": \"I've marked this as high priority. A team member will be with you soon.\",\n",
    "        \"medium\": \"A team member will be with you as soon as possible.\",\n",
    "        \"low\": \"A team member will reach out to help you.\",\n",
    "    }\n",
    "    \n",
    "    message_parts = [\n",
    "        \"I understand you'd like to speak with a human agent, and I've arranged that for you.\",\n",
    "        \"\",\n",
    "        f\"**Your Reference Number: {ticket_id}**\",\n",
    "        \"\",\n",
    "        priority_messages.get(priority, priority_messages[\"medium\"]),\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    if position > 0:\n",
    "        message_parts.append(f\"You're currently #{position} in our queue.\")\n",
    "    \n",
    "    message_parts.extend([\n",
    "        f\"Estimated wait time: approximately {wait_time} minutes.\",\n",
    "        \"\",\n",
    "        \"While you wait:\",\n",
    "        \"\u2022 You don't need to stay on this chat - we'll reach out to you\",\n",
    "        \"\u2022 You can reference your ticket number in any follow-up\",\n",
    "        \"\u2022 Our team has the full context of our conversation\",\n",
    "        \"\",\n",
    "        \"Is there anything else I can help you with while you wait?\",\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(message_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts/test_handoff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: Zero to AI Agent, Chapter 20, Section 20.5\n",
    "# File: scripts/test_handoff.py\n",
    "\n",
    "\"\"\"Test the human handoff system.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "from caspar.handoff import get_handoff_queue, format_context_for_display\n",
    "from caspar.config import setup_logging, get_logger\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "async def test_explicit_handoff():\n",
    "    \"\"\"Test explicit request for human agent.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Test: Explicit Handoff Request\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(\n",
    "        conversation_id=\"test-handoff-explicit\",\n",
    "        customer_id=\"CUST-1000\"\n",
    "    )\n",
    "    state[\"messages\"] = [HumanMessage(content=\"I want to talk to a real person please\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-handoff-explicit\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Escalated: {result.get('needs_escalation')}\")\n",
    "    print(f\"Ticket: {result.get('ticket_id')}\")\n",
    "    print(f\"\\nCASPAR Response:\\n{result['messages'][-1].content}\")\n",
    "    \n",
    "    # Check queue\n",
    "    queue = get_handoff_queue()\n",
    "    request = queue.get_by_conversation(\"test-handoff-explicit\")\n",
    "    if request:\n",
    "        print(f\"\\n\ud83d\udccb Queue Position: {queue.get_queue_position(request.request_id)}\")\n",
    "        print(f\"   Priority: {request.priority}\")\n",
    "        print(f\"   Est. Wait: {request.estimated_wait} minutes\")\n",
    "\n",
    "\n",
    "async def test_frustration_escalation():\n",
    "    \"\"\"Test escalation triggered by frustration.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Test: Frustration-Triggered Escalation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(\n",
    "        conversation_id=\"test-handoff-frustration\",\n",
    "        customer_id=\"CUST-1001\"\n",
    "    )\n",
    "    \n",
    "    # Simulate a frustrated customer\n",
    "    state[\"messages\"] = [\n",
    "        HumanMessage(content=\"Where is my order?! I've been waiting for weeks!\"),\n",
    "    ]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-handoff-frustration\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Sentiment: {result.get('sentiment_score')}\")\n",
    "    print(f\"Frustration: {result.get('frustration_level')}\")\n",
    "    print(f\"Escalated: {result.get('needs_escalation')}\")\n",
    "    print(f\"\\nCASPAR Response:\\n{result['messages'][-1].content[:300]}...\")\n",
    "\n",
    "\n",
    "async def test_vip_customer():\n",
    "    \"\"\"Test VIP customer gets priority handling.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Test: VIP Customer Handling\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    agent = await create_agent()\n",
    "    \n",
    "    # CUST-1003 is a gold tier customer in our mock data\n",
    "    state = create_initial_state(\n",
    "        conversation_id=\"test-handoff-vip\",\n",
    "        customer_id=\"CUST-1003\"\n",
    "    )\n",
    "    state[\"messages\"] = [\n",
    "        HumanMessage(content=\"I have an issue with my recent order and I'm not happy about it.\"),\n",
    "    ]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-handoff-vip\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Escalated: {result.get('needs_escalation')}\")\n",
    "    \n",
    "    queue = get_handoff_queue()\n",
    "    request = queue.get_by_conversation(\"test-handoff-vip\")\n",
    "    if request:\n",
    "        print(f\"Priority: {request.priority}\")\n",
    "        print(f\"Triggers: {request.triggers}\")\n",
    "\n",
    "\n",
    "async def test_sensitive_topic():\n",
    "    \"\"\"Test sensitive topic detection.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Test: Sensitive Topic Detection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(\n",
    "        conversation_id=\"test-handoff-sensitive\",\n",
    "        customer_id=\"CUST-1000\"\n",
    "    )\n",
    "    state[\"messages\"] = [\n",
    "        HumanMessage(content=\"I think this might be fraud. Someone used my card without permission.\"),\n",
    "    ]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-handoff-sensitive\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    print(f\"Intent: {result['intent']}\")\n",
    "    print(f\"Escalated: {result.get('needs_escalation')}\")\n",
    "    print(f\"Reason: {result.get('escalation_reason', 'N/A')}\")\n",
    "    print(f\"\\nCASPAR Response:\\n{result['messages'][-1].content[:300]}...\")\n",
    "\n",
    "\n",
    "async def test_queue_management():\n",
    "    \"\"\"Test queue management with multiple requests.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83e\uddea Test: Queue Management\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    queue = get_handoff_queue()\n",
    "    \n",
    "    # Add several requests with different priorities\n",
    "    requests = [\n",
    "        (\"conv-1\", \"CUST-1000\", \"medium\", [\"general\"], \"General inquiry\"),\n",
    "        (\"conv-2\", \"CUST-1001\", \"urgent\", [\"explicit_request\"], \"Customer requested agent\"),\n",
    "        (\"conv-3\", \"CUST-1002\", \"high\", [\"vip_customer\"], \"VIP with issue\"),\n",
    "        (\"conv-4\", \"CUST-1003\", \"low\", [\"general\"], \"Simple question\"),\n",
    "    ]\n",
    "    \n",
    "    for conv_id, cust_id, priority, triggers, reason in requests:\n",
    "        queue.add(conv_id, cust_id, priority, triggers, reason)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Queue Status:\")\n",
    "    counts = queue.get_pending_count()\n",
    "    for priority, count in counts.items():\n",
    "        print(f\"   {priority.upper()}: {count}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udccb Queue Order (by priority):\")\n",
    "    for conv_id, _, _, _, _ in requests:\n",
    "        req = queue.get_by_conversation(conv_id)\n",
    "        if req:\n",
    "            pos = queue.get_queue_position(req.request_id)\n",
    "            print(f\"   #{pos}: {req.conversation_id} ({req.priority})\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run all handoff tests.\"\"\"\n",
    "    \n",
    "    await test_explicit_handoff()\n",
    "    await test_frustration_escalation()\n",
    "    await test_vip_customer()\n",
    "    await test_sensitive_topic()\n",
    "    await test_queue_management()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\u2705 All handoff tests complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.6: Testing and refinement\n",
    "\n",
    "This section covers testing the agent with unit tests, integration tests, and evaluation.\n",
    "\n",
    "**Key files:**\n",
    "- `tests/conftest.py` - Test fixtures\n",
    "- `tests/unit/` - Unit tests for individual components\n",
    "- `tests/integration/` - Integration tests for conversation flows\n",
    "- `tests/evaluation/` - Quality evaluation tests\n",
    "- `scripts/run_tests.py` - Test runner script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shared test fixtures and utilities.\n",
    "\n",
    "pytest automatically discovers this file and makes fixtures\n",
    "available to all tests.\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "import asyncio\n",
    "from unittest.mock import MagicMock, AsyncMock\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "from caspar.agent import create_initial_state\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_state():\n",
    "    \"\"\"Create a sample agent state for testing.\"\"\"\n",
    "    return create_initial_state(\n",
    "        conversation_id=\"test-conv-001\",\n",
    "        customer_id=\"CUST-1000\"\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_state_with_messages():\n",
    "    \"\"\"Create a state with some conversation history.\"\"\"\n",
    "    state = create_initial_state(\n",
    "        conversation_id=\"test-conv-002\",\n",
    "        customer_id=\"CUST-1000\"\n",
    "    )\n",
    "    state[\"messages\"] = [\n",
    "        HumanMessage(content=\"Hi, I have a question about my order\"),\n",
    "        AIMessage(content=\"Hello! I'd be happy to help with your order. Could you provide your order number?\"),\n",
    "        HumanMessage(content=\"It's TF-10001\"),\n",
    "    ]\n",
    "    return state\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_llm():\n",
    "    \"\"\"Create a mock LLM for testing without API calls.\"\"\"\n",
    "    mock = MagicMock()\n",
    "    mock.invoke = MagicMock(return_value=MagicMock(content=\"mocked response\"))\n",
    "    return mock\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def event_loop():\n",
    "    \"\"\"Create an event loop for async tests.\"\"\"\n",
    "    loop = asyncio.new_event_loop()\n",
    "    yield loop\n",
    "    loop.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/unit/test_tools_orders.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unit tests for the order lookup tool.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from caspar.tools.orders import (\n",
    "    OrderLookupTool,\n",
    "    get_order_status,\n",
    "    OrderInfo,\n",
    ")\n",
    "\n",
    "\n",
    "class TestOrderLookupTool:\n",
    "    \"\"\"Tests for OrderLookupTool class.\"\"\"\n",
    "    \n",
    "    def setup_method(self):\n",
    "        \"\"\"Set up a fresh tool instance for each test.\"\"\"\n",
    "        self.tool = OrderLookupTool()\n",
    "    \n",
    "    def test_lookup_existing_order(self):\n",
    "        \"\"\"Should find an order that exists.\"\"\"\n",
    "        order = self.tool.lookup(\"TF-10001\")\n",
    "        \n",
    "        assert order is not None\n",
    "        assert order.order_id == \"TF-10001\"\n",
    "        assert order.status in [\"processing\", \"shipped\", \"delivered\", \"cancelled\", \"returned\"]\n",
    "    \n",
    "    def test_lookup_nonexistent_order(self):\n",
    "        \"\"\"Should return None for orders that don't exist.\"\"\"\n",
    "        order = self.tool.lookup(\"TF-99999\")\n",
    "        \n",
    "        assert order is None\n",
    "    \n",
    "    def test_lookup_normalizes_order_id(self):\n",
    "        \"\"\"Should handle order IDs without the TF- prefix.\"\"\"\n",
    "        order = self.tool.lookup(\"10001\")\n",
    "        \n",
    "        assert order is not None\n",
    "        assert order.order_id == \"TF-10001\"\n",
    "    \n",
    "    def test_lookup_case_insensitive(self):\n",
    "        \"\"\"Should handle lowercase order IDs.\"\"\"\n",
    "        order = self.tool.lookup(\"tf-10001\")\n",
    "        \n",
    "        assert order is not None\n",
    "        assert order.order_id == \"TF-10001\"\n",
    "    \n",
    "    def test_lookup_with_customer_verification(self):\n",
    "        \"\"\"Should verify customer ownership when customer_id provided.\"\"\"\n",
    "        # TF-10001 belongs to CUST-1001 in our mock data\n",
    "        order = self.tool.lookup(\"TF-10001\", customer_id=\"CUST-1001\")\n",
    "        \n",
    "        assert order is not None\n",
    "    \n",
    "    def test_lookup_wrong_customer_returns_none(self):\n",
    "        \"\"\"Should return None if customer doesn't own the order.\"\"\"\n",
    "        # TF-10001 belongs to CUST-1001, not CUST-1002\n",
    "        order = self.tool.lookup(\"TF-10001\", customer_id=\"CUST-9999\")\n",
    "        \n",
    "        assert order is None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/unit/test_tools_tickets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unit tests for the ticket creation tool.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from caspar.tools.tickets import (\n",
    "    TicketTool,\n",
    "    create_ticket,\n",
    "    Ticket,\n",
    ")\n",
    "\n",
    "\n",
    "class TestTicketTool:\n",
    "    \"\"\"Tests for TicketTool class.\"\"\"\n",
    "    \n",
    "    def setup_method(self):\n",
    "        \"\"\"Set up a fresh tool instance for each test.\"\"\"\n",
    "        self.tool = TicketTool()\n",
    "    \n",
    "    def test_create_ticket_returns_ticket(self):\n",
    "        \"\"\"Should create and return a ticket.\"\"\"\n",
    "        ticket = self.tool.create(\n",
    "            customer_id=\"CUST-1000\",\n",
    "            category=\"technical\",\n",
    "            subject=\"Test ticket\",\n",
    "            description=\"This is a test\",\n",
    "        )\n",
    "        \n",
    "        assert ticket is not None\n",
    "        assert ticket.ticket_id.startswith(\"TKT-\")\n",
    "        assert ticket.customer_id == \"CUST-1000\"\n",
    "        assert ticket.category == \"technical\"\n",
    "        assert ticket.status == \"open\"\n",
    "    \n",
    "    def test_create_ticket_with_priority(self):\n",
    "        \"\"\"Should respect priority setting.\"\"\"\n",
    "        ticket = self.tool.create(\n",
    "            customer_id=\"CUST-1000\",\n",
    "            category=\"billing\",\n",
    "            subject=\"Urgent issue\",\n",
    "            description=\"Very urgent\",\n",
    "            priority=\"urgent\",\n",
    "        )\n",
    "        \n",
    "        assert ticket.priority == \"urgent\"\n",
    "    \n",
    "    def test_create_ticket_default_priority(self):\n",
    "        \"\"\"Should default to medium priority.\"\"\"\n",
    "        ticket = self.tool.create(\n",
    "            customer_id=\"CUST-1000\",\n",
    "            category=\"general\",\n",
    "            subject=\"General question\",\n",
    "            description=\"Just asking\",\n",
    "        )\n",
    "        \n",
    "        assert ticket.priority == \"medium\"\n",
    "    \n",
    "    def test_get_ticket_by_id(self):\n",
    "        \"\"\"Should retrieve ticket by ID.\"\"\"\n",
    "        created = self.tool.create(\n",
    "            customer_id=\"CUST-1000\",\n",
    "            category=\"return\",\n",
    "            subject=\"Return request\",\n",
    "            description=\"Want to return item\",\n",
    "        )\n",
    "        \n",
    "        retrieved = self.tool.get(created.ticket_id)\n",
    "        \n",
    "        assert retrieved is not None\n",
    "        assert retrieved.ticket_id == created.ticket_id\n",
    "    \n",
    "    def test_get_nonexistent_ticket(self):\n",
    "        \"\"\"Should return None for tickets that don't exist.\"\"\"\n",
    "        result = self.tool.get(\"TKT-NONEXISTENT\")\n",
    "        \n",
    "        assert result is None\n",
    "    \n",
    "    def test_get_customer_tickets(self):\n",
    "        \"\"\"Should retrieve all tickets for a customer.\"\"\"\n",
    "        # Create multiple tickets\n",
    "        self.tool.create(\n",
    "            customer_id=\"CUST-TEST\",\n",
    "            category=\"technical\",\n",
    "            subject=\"Issue 1\",\n",
    "            description=\"First issue\",\n",
    "        )\n",
    "        self.tool.create(\n",
    "            customer_id=\"CUST-TEST\",\n",
    "            category=\"billing\",\n",
    "            subject=\"Issue 2\",\n",
    "            description=\"Second issue\",\n",
    "        )\n",
    "        self.tool.create(\n",
    "            customer_id=\"CUST-OTHER\",\n",
    "            category=\"general\",\n",
    "            subject=\"Other customer\",\n",
    "            description=\"Different customer\",\n",
    "        )\n",
    "        \n",
    "        tickets = self.tool.get_customer_tickets(\"CUST-TEST\")\n",
    "        \n",
    "        assert len(tickets) == 2\n",
    "        assert all(t.customer_id == \"CUST-TEST\" for t in tickets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/unit/test_handoff_triggers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Unit tests for escalation trigger detection.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from caspar.handoff.triggers import (\n",
    "    check_escalation_triggers,\n",
    "    check_sensitive_topics,\n",
    "    EscalationTrigger,\n",
    ")\n",
    "\n",
    "\n",
    "class TestCheckEscalationTriggers:\n",
    "    \"\"\"Tests for check_escalation_triggers function.\"\"\"\n",
    "    \n",
    "    def test_explicit_request_triggers_escalation(self):\n",
    "        \"\"\"Should trigger on explicit handoff request.\"\"\"\n",
    "        state = {\"intent\": \"handoff_request\"}\n",
    "        \n",
    "        result = check_escalation_triggers(state)\n",
    "        \n",
    "        assert result.should_escalate is True\n",
    "        assert EscalationTrigger.EXPLICIT_REQUEST in result.triggers\n",
    "        assert result.priority == \"urgent\"\n",
    "    \n",
    "    def test_high_frustration_triggers_escalation(self):\n",
    "        \"\"\"Should trigger on high frustration.\"\"\"\n",
    "        state = {\n",
    "            \"intent\": \"complaint\",\n",
    "            \"sentiment_score\": -0.8,\n",
    "            \"frustration_level\": \"high\",\n",
    "        }\n",
    "        \n",
    "        result = check_escalation_triggers(state)\n",
    "        \n",
    "        assert result.should_escalate is True\n",
    "        assert EscalationTrigger.HIGH_FRUSTRATION in result.triggers\n",
    "    \n",
    "    def test_vip_customer_with_complaint_triggers(self):\n",
    "        \"\"\"Should trigger for VIP customers with complaints.\"\"\"\n",
    "        state = {\n",
    "            \"intent\": \"complaint\",\n",
    "            \"sentiment_score\": 0.0,\n",
    "            \"frustration_level\": \"medium\",\n",
    "        }\n",
    "        \n",
    "        result = check_escalation_triggers(state, customer_tier=\"gold\")\n",
    "        \n",
    "        assert result.should_escalate is True\n",
    "        assert EscalationTrigger.VIP_CUSTOMER in result.triggers\n",
    "    \n",
    "    def test_no_triggers_when_everything_ok(self):\n",
    "        \"\"\"Should not trigger when conversation is normal.\"\"\"\n",
    "        state = {\n",
    "            \"intent\": \"faq\",\n",
    "            \"sentiment_score\": 0.5,\n",
    "            \"frustration_level\": \"low\",\n",
    "            \"turn_count\": 2,\n",
    "        }\n",
    "        \n",
    "        result = check_escalation_triggers(state)\n",
    "        \n",
    "        assert result.should_escalate is False\n",
    "        assert len(result.triggers) == 0\n",
    "\n",
    "\n",
    "class TestCheckSensitiveTopics:\n",
    "    \"\"\"Tests for sensitive topic detection.\"\"\"\n",
    "    \n",
    "    def test_detects_legal_keywords(self):\n",
    "        \"\"\"Should detect legal-related keywords.\"\"\"\n",
    "        assert check_sensitive_topics(\"I'm going to sue you\") is True\n",
    "        assert check_sensitive_topics(\"I'll contact my lawyer\") is True\n",
    "        assert check_sensitive_topics(\"This is legal action\") is True\n",
    "    \n",
    "    def test_detects_fraud_keywords(self):\n",
    "        \"\"\"Should detect fraud-related keywords.\"\"\"\n",
    "        assert check_sensitive_topics(\"This is fraud!\") is True\n",
    "        assert check_sensitive_topics(\"Someone scammed me\") is True\n",
    "        assert check_sensitive_topics(\"My card was stolen\") is True\n",
    "    \n",
    "    def test_detects_safety_keywords(self):\n",
    "        \"\"\"Should detect safety-related keywords.\"\"\"\n",
    "        assert check_sensitive_topics(\"This product is dangerous\") is True\n",
    "        assert check_sensitive_topics(\"I was injured\") is True\n",
    "    \n",
    "    def test_ignores_normal_messages(self):\n",
    "        \"\"\"Should not trigger on normal messages.\"\"\"\n",
    "        assert check_sensitive_topics(\"Where is my order?\") is False\n",
    "        assert check_sensitive_topics(\"I want to return this\") is False\n",
    "        assert check_sensitive_topics(\"What's your return policy?\") is False\n",
    "    \n",
    "    def test_case_insensitive(self):\n",
    "        \"\"\"Should detect keywords regardless of case.\"\"\"\n",
    "        assert check_sensitive_topics(\"FRAUD\") is True\n",
    "        assert check_sensitive_topics(\"Lawyer\") is True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/integration/test_intent_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Integration tests for intent classification.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_faq_intent_classification():\n",
    "    \"\"\"Should classify FAQ questions correctly.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-faq\", customer_id=\"CUST-1000\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"What is your return policy?\",\n",
    "        \"How long does shipping take?\",\n",
    "        \"Do you offer warranties?\",\n",
    "        \"What payment methods do you accept?\",\n",
    "    ]\n",
    "    \n",
    "    for message in test_cases:\n",
    "        state[\"messages\"] = [HumanMessage(content=message)]\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test-faq-{hash(message)}\"}}\n",
    "        \n",
    "        result = await agent.ainvoke(state, config)\n",
    "        \n",
    "        assert result[\"intent\"] == \"faq\", f\"Expected 'faq' for: {message}, got: {result['intent']}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_order_inquiry_intent_classification():\n",
    "    \"\"\"Should classify order inquiries correctly.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    # CUST-1000 owns TF-10000, TF-10005, TF-10010, TF-10015\n",
    "    state = create_initial_state(conversation_id=\"test-order\", customer_id=\"CUST-1000\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"Where is my order TF-10000?\",\n",
    "        \"I want to track my order\",\n",
    "        \"What's the status of order 10005?\",\n",
    "        \"When will my package arrive?\",\n",
    "    ]\n",
    "    \n",
    "    for message in test_cases:\n",
    "        state[\"messages\"] = [HumanMessage(content=message)]\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test-order-{hash(message)}\"}}\n",
    "        \n",
    "        result = await agent.ainvoke(state, config)\n",
    "        \n",
    "        assert result[\"intent\"] == \"order_inquiry\", f\"Expected 'order_inquiry' for: {message}, got: {result['intent']}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_complaint_intent_classification():\n",
    "    \"\"\"Should classify complaints correctly.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-complaint\", customer_id=\"CUST-1000\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"This product is terrible!\",\n",
    "        \"I'm very disappointed with my purchase\",\n",
    "        \"Your service is awful\",\n",
    "        \"My item arrived damaged and I'm furious\",\n",
    "    ]\n",
    "    \n",
    "    for message in test_cases:\n",
    "        state[\"messages\"] = [HumanMessage(content=message)]\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test-complaint-{hash(message)}\"}}\n",
    "        \n",
    "        result = await agent.ainvoke(state, config)\n",
    "        \n",
    "        assert result[\"intent\"] == \"complaint\", f\"Expected 'complaint' for: {message}, got: {result['intent']}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_handoff_request_classification():\n",
    "    \"\"\"Should classify handoff requests correctly.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-handoff\", customer_id=\"CUST-1000\")\n",
    "    \n",
    "    test_cases = [\n",
    "        \"I want to speak to a human\",\n",
    "        \"Let me talk to a real person\",\n",
    "        \"Connect me with an agent\",\n",
    "        \"I need human support please\",\n",
    "    ]\n",
    "    \n",
    "    for message in test_cases:\n",
    "        state[\"messages\"] = [HumanMessage(content=message)]\n",
    "        config = {\"configurable\": {\"thread_id\": f\"test-handoff-{hash(message)}\"}}\n",
    "        \n",
    "        result = await agent.ainvoke(state, config)\n",
    "        \n",
    "        assert result[\"intent\"] == \"handoff_request\", f\"Expected 'handoff_request' for: {message}, got: {result['intent']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/integration/test_conversation_flows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Integration tests for complete conversation flows.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_faq_flow_returns_relevant_info():\n",
    "    \"\"\"FAQ flow should return relevant policy information.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-faq-flow\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"What is your return policy?\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-faq-flow\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # Should mention key return policy details\n",
    "    assert any(word in response for word in [\"return\", \"30\", \"day\", \"refund\"]), \\\n",
    "        f\"Response should mention return policy details: {response}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_order_inquiry_with_valid_order():\n",
    "    \"\"\"Order inquiry should return order details for valid orders.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    # Use CUST-1000 with TF-10000 (TF-10000 belongs to CUST-1000)\n",
    "    state = create_initial_state(conversation_id=\"test-order-flow\", customer_id=\"CUST-1000\")\n",
    "    # Use polite phrasing to reduce chance of sentiment escalation\n",
    "    state[\"messages\"] = [HumanMessage(content=\"Hi! Could you please check the status of order TF-10000? Thanks!\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-order-flow\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    # The order lookup should have succeeded - check the state\n",
    "    # Note: Even if sentiment triggers escalation, order_info should be populated\n",
    "    order_info = result.get(\"order_info\")\n",
    "    \n",
    "    # order_info should exist and have status (not error) when order is found\n",
    "    assert order_info is not None, \\\n",
    "        f\"Order info should be in state. Got: {order_info}\"\n",
    "    assert \"status\" in order_info, \\\n",
    "        f\"Order should be found (has status). Got: {order_info}\"\n",
    "    assert \"error\" not in order_info, \\\n",
    "        f\"Order lookup should not have error. Got: {order_info}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_order_inquiry_with_invalid_order():\n",
    "    \"\"\"Order inquiry should handle invalid orders gracefully.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-invalid-order\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"Where is my order TF-99999?\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-invalid-order\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # Should either indicate order not found OR escalate to human\n",
    "    # (escalation is acceptable when we can't find the order)\n",
    "    order_not_found = any(phrase in response for phrase in [\"not found\", \"couldn't find\", \"unable to locate\", \"check\", \"verify\"])\n",
    "    escalated = result.get(\"needs_escalation\", False) or \"human\" in response or \"agent\" in response\n",
    "    \n",
    "    assert order_not_found or escalated, \\\n",
    "        f\"Response should indicate order not found or escalate: {response}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_complaint_creates_ticket():\n",
    "    \"\"\"Complaints should create a support ticket.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-complaint-ticket\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"My laptop arrived completely broken! This is unacceptable!\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-complaint-ticket\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    # Should have created a ticket\n",
    "    assert result.get(\"ticket_id\") is not None, \"Complaint should create a ticket\"\n",
    "    assert result[\"ticket_id\"].startswith(\"TKT-\"), f\"Invalid ticket ID: {result.get('ticket_id')}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_handoff_request_triggers_escalation():\n",
    "    \"\"\"Explicit handoff requests should trigger escalation.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-explicit-handoff\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"I want to speak to a human agent please\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-explicit-handoff\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    # Should be escalated\n",
    "    assert result.get(\"needs_escalation\") is True, \"Handoff request should trigger escalation\"\n",
    "    assert result.get(\"ticket_id\") is not None, \"Handoff should create a ticket\"\n",
    "    \n",
    "    # Response should acknowledge the handoff\n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    assert any(word in response for word in [\"human\", \"agent\", \"team\", \"reach\"]), \\\n",
    "        f\"Response should mention human handoff: {response}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/integration/test_edge_cases.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Integration tests for edge cases and unusual inputs.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_empty_message():\n",
    "    \"\"\"Should handle empty messages gracefully.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-empty\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-empty\"}}\n",
    "    \n",
    "    # Should not crash\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    assert result is not None\n",
    "    assert len(result[\"messages\"]) > 0\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_very_long_message():\n",
    "    \"\"\"Should handle very long messages.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-long\", customer_id=\"CUST-1000\")\n",
    "    \n",
    "    # Create a long message\n",
    "    long_message = \"I have a question about my order. \" * 100\n",
    "    state[\"messages\"] = [HumanMessage(content=long_message)]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-long\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    assert result is not None\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_special_characters():\n",
    "    \"\"\"Should handle special characters in messages.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-special\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"What's the status of order #TF-10000? \ud83e\udd14 <test> & more\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-special\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    assert result is not None\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_multiple_questions_in_one_message():\n",
    "    \"\"\"Should handle multiple questions in a single message.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-multi-q\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(\n",
    "        content=\"Hi! Quick questions: What's your return policy? And do you offer warranties? Thanks!\"\n",
    "    )]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-multi-q\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # Should address at least some of the questions OR escalate for complex request\n",
    "    addressed_topics = any(word in response for word in [\"return\", \"warranty\", \"policy\", \"day\"])\n",
    "    escalated = result.get(\"needs_escalation\", False) or \"human\" in response or \"agent\" in response\n",
    "    \n",
    "    assert addressed_topics or escalated, \\\n",
    "        f\"Should address topics or escalate complex request: {response[:200]}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_all_caps_message():\n",
    "    \"\"\"Should handle ALL CAPS messages (often indicate frustration).\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-caps\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"WHERE IS MY ORDER THIS IS TAKING TOO LONG\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-caps\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    # Should recognize as order inquiry or complaint\n",
    "    assert result[\"intent\"] in [\"order_inquiry\", \"complaint\"], \\\n",
    "        f\"ALL CAPS should be recognized as order/complaint: {result['intent']}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_greeting_only():\n",
    "    \"\"\"Should handle simple greetings.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-greeting\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"Hello!\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-greeting\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # Should respond with a greeting\n",
    "    assert any(word in response for word in [\"hello\", \"hi\", \"help\", \"welcome\"]), \\\n",
    "        f\"Should greet the customer: {response}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_typos_and_misspellings():\n",
    "    \"\"\"Should handle messages with typos.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"test-typos\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"waht is ur retrun polcy?\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"test-typos\"}}\n",
    "    \n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    # Should still classify as FAQ about returns\n",
    "    assert result[\"intent\"] == \"faq\", f\"Should understand despite typos: {result['intent']}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/evaluation/evaluator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Response quality evaluation framework.\n",
    "\n",
    "Uses LLM-as-a-judge to assess agent responses.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.config import settings\n",
    "\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Result of evaluating a response.\"\"\"\n",
    "    \n",
    "    relevance_score: float  # 0-1: How relevant is the response?\n",
    "    accuracy_score: float   # 0-1: Is the information correct?\n",
    "    helpfulness_score: float  # 0-1: Does it help the customer?\n",
    "    tone_score: float       # 0-1: Is the tone appropriate?\n",
    "    overall_score: float    # 0-1: Overall quality\n",
    "    feedback: str           # Explanation of scores\n",
    "\n",
    "\n",
    "class ResponseEvaluator:\n",
    "    \"\"\"Evaluates agent responses using LLM-as-a-judge.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=settings.smart_model,  # Use better model for evaluation\n",
    "            api_key=settings.openai_api_key,\n",
    "            temperature=0,\n",
    "        )\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        customer_message: str,\n",
    "        agent_response: str,\n",
    "        expected_topics: list[str] | None = None,\n",
    "        context: str | None = None,\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate an agent response.\n",
    "        \n",
    "        Args:\n",
    "            customer_message: What the customer asked\n",
    "            agent_response: What the agent responded\n",
    "            expected_topics: Topics that should be covered\n",
    "            context: Additional context (e.g., order info, policies)\n",
    "        \"\"\"\n",
    "        eval_prompt = f\"\"\"You are evaluating a customer service AI agent's response.\n",
    "\n",
    "Customer Message: \"{customer_message}\"\n",
    "\n",
    "Agent Response: \"{agent_response}\"\n",
    "\n",
    "{f'Expected Topics: {\", \".join(expected_topics)}' if expected_topics else ''}\n",
    "{f'Context: {context}' if context else ''}\n",
    "\n",
    "Evaluate the response on these criteria (0.0 to 1.0):\n",
    "\n",
    "1. RELEVANCE: Does the response address what the customer asked?\n",
    "2. ACCURACY: Is the information provided correct and not hallucinated?\n",
    "3. HELPFULNESS: Does the response help solve the customer's problem?\n",
    "4. TONE: Is the tone professional, friendly, and appropriate?\n",
    "\n",
    "Respond in this exact format:\n",
    "RELEVANCE: [score]\n",
    "ACCURACY: [score]\n",
    "HELPFULNESS: [score]\n",
    "TONE: [score]\n",
    "FEEDBACK: [1-2 sentence explanation]\"\"\"\n",
    "\n",
    "        response = self.llm.invoke([HumanMessage(content=eval_prompt)])\n",
    "        \n",
    "        # Parse response\n",
    "        scores = {\"relevance\": 0.5, \"accuracy\": 0.5, \"helpfulness\": 0.5, \"tone\": 0.5}\n",
    "        feedback = \"Unable to parse evaluation\"\n",
    "        \n",
    "        for line in response.content.strip().split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"RELEVANCE:\"):\n",
    "                scores[\"relevance\"] = self._parse_score(line)\n",
    "            elif line.startswith(\"ACCURACY:\"):\n",
    "                scores[\"accuracy\"] = self._parse_score(line)\n",
    "            elif line.startswith(\"HELPFULNESS:\"):\n",
    "                scores[\"helpfulness\"] = self._parse_score(line)\n",
    "            elif line.startswith(\"TONE:\"):\n",
    "                scores[\"tone\"] = self._parse_score(line)\n",
    "            elif line.startswith(\"FEEDBACK:\"):\n",
    "                feedback = line.split(\":\", 1)[1].strip()\n",
    "        \n",
    "        overall = sum(scores.values()) / len(scores)\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            relevance_score=scores[\"relevance\"],\n",
    "            accuracy_score=scores[\"accuracy\"],\n",
    "            helpfulness_score=scores[\"helpfulness\"],\n",
    "            tone_score=scores[\"tone\"],\n",
    "            overall_score=overall,\n",
    "            feedback=feedback,\n",
    "        )\n",
    "    \n",
    "    def _parse_score(self, line: str) -> float:\n",
    "        \"\"\"Parse a score from an evaluation line.\"\"\"\n",
    "        try:\n",
    "            score_str = line.split(\":\")[1].strip()\n",
    "            score = float(score_str)\n",
    "            return max(0.0, min(1.0, score))\n",
    "        except (IndexError, ValueError):\n",
    "            return 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/evaluation/test_response_quality.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation tests for response quality.\"\"\"\n",
    "\n",
    "import pytest\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from caspar.agent import create_agent, create_initial_state\n",
    "from .evaluator import ResponseEvaluator\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def evaluator():\n",
    "    \"\"\"Create a response evaluator.\"\"\"\n",
    "    return ResponseEvaluator()\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_faq_response_quality(evaluator):\n",
    "    \"\"\"FAQ responses should be high quality.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"eval-faq\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"What is your return policy?\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"eval-faq\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    response = result[\"messages\"][-1].content\n",
    "    \n",
    "    evaluation = evaluator.evaluate(\n",
    "        customer_message=\"What is your return policy?\",\n",
    "        agent_response=response,\n",
    "        expected_topics=[\"return\", \"30 days\", \"refund\"],\n",
    "        context=\"TechFlow has a 30-day return policy for most items.\",\n",
    "    )\n",
    "    \n",
    "    assert evaluation.relevance_score >= 0.7, f\"Low relevance: {evaluation.feedback}\"\n",
    "    assert evaluation.overall_score >= 0.7, f\"Low overall score: {evaluation.feedback}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_complaint_response_quality(evaluator):\n",
    "    \"\"\"Complaint responses should be empathetic and helpful.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    state = create_initial_state(conversation_id=\"eval-complaint\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"My laptop arrived damaged! I'm very upset!\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"eval-complaint\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    response = result[\"messages\"][-1].content\n",
    "    \n",
    "    evaluation = evaluator.evaluate(\n",
    "        customer_message=\"My laptop arrived damaged! I'm very upset!\",\n",
    "        agent_response=response,\n",
    "        expected_topics=[\"apologize\", \"understand\", \"help\", \"ticket\"],\n",
    "    )\n",
    "    \n",
    "    assert evaluation.tone_score >= 0.7, f\"Tone not empathetic enough: {evaluation.feedback}\"\n",
    "    assert evaluation.helpfulness_score >= 0.6, f\"Not helpful enough: {evaluation.feedback}\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_order_status_response_accuracy(evaluator):\n",
    "    \"\"\"Order status responses should be accurate.\"\"\"\n",
    "    agent = await create_agent()\n",
    "    # Use CUST-1000 with TF-10000 (matching ownership)\n",
    "    state = create_initial_state(conversation_id=\"eval-order\", customer_id=\"CUST-1000\")\n",
    "    state[\"messages\"] = [HumanMessage(content=\"Where is my order TF-10000?\")]\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"eval-order\"}}\n",
    "    result = await agent.ainvoke(state, config)\n",
    "    \n",
    "    response = result[\"messages\"][-1].content\n",
    "    \n",
    "    evaluation = evaluator.evaluate(\n",
    "        customer_message=\"Where is my order TF-10000?\",\n",
    "        agent_response=response,\n",
    "        expected_topics=[\"order\", \"status\", \"TF-10000\"],\n",
    "    )\n",
    "    \n",
    "    assert evaluation.accuracy_score >= 0.7, f\"Inaccurate response: {evaluation.feedback}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests/evaluation/test_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test dataset for systematic evaluation.\n",
    "\n",
    "This dataset covers various scenarios the agent should handle.\n",
    "\"\"\"\n",
    "\n",
    "TEST_CASES = [\n",
    "    # FAQ Questions\n",
    "    {\n",
    "        \"category\": \"faq\",\n",
    "        \"input\": \"What is your return policy?\",\n",
    "        \"expected_intent\": \"faq\",\n",
    "        \"expected_topics\": [\"return\", \"30 days\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"faq\",\n",
    "        \"input\": \"How long does shipping take?\",\n",
    "        \"expected_intent\": \"faq\",\n",
    "        \"expected_topics\": [\"shipping\", \"days\", \"delivery\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"faq\",\n",
    "        \"input\": \"Do you offer warranties on laptops?\",\n",
    "        \"expected_intent\": \"faq\",\n",
    "        \"expected_topics\": [\"warranty\", \"year\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "    },\n",
    "    \n",
    "    # Order Inquiries\n",
    "    {\n",
    "        \"category\": \"order\",\n",
    "        \"input\": \"Where is my order TF-10000?\",\n",
    "        \"expected_intent\": \"order_inquiry\",\n",
    "        \"expected_topics\": [\"order\", \"status\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"order\",\n",
    "        \"input\": \"I want to track my package\",\n",
    "        \"expected_intent\": \"order_inquiry\",\n",
    "        \"expected_topics\": [\"track\", \"order\"],\n",
    "        \"min_quality_score\": 0.6,\n",
    "    },\n",
    "    \n",
    "    # Complaints\n",
    "    {\n",
    "        \"category\": \"complaint\",\n",
    "        \"input\": \"This product is defective and I want a refund!\",\n",
    "        \"expected_intent\": \"complaint\",\n",
    "        \"expected_topics\": [\"sorry\", \"help\", \"refund\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "        \"expect_ticket\": True,\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"complaint\",\n",
    "        \"input\": \"I've been waiting 3 weeks for my order. This is ridiculous!\",\n",
    "        \"expected_intent\": \"complaint\",\n",
    "        \"expected_topics\": [\"apologize\", \"order\", \"help\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "    },\n",
    "    \n",
    "    # Handoff Requests\n",
    "    {\n",
    "        \"category\": \"handoff\",\n",
    "        \"input\": \"I want to speak to a human agent\",\n",
    "        \"expected_intent\": \"handoff_request\",\n",
    "        \"expected_topics\": [\"human\", \"agent\", \"help\"],\n",
    "        \"min_quality_score\": 0.7,\n",
    "        \"expect_escalation\": True,\n",
    "    },\n",
    "    \n",
    "    # Edge Cases\n",
    "    {\n",
    "        \"category\": \"edge\",\n",
    "        \"input\": \"Hi\",\n",
    "        \"expected_intent\": \"general\",\n",
    "        \"expected_topics\": [\"hello\", \"help\"],\n",
    "        \"min_quality_score\": 0.6,\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"edge\",\n",
    "        \"input\": \"\",\n",
    "        \"expected_intent\": \"general\",\n",
    "        \"min_quality_score\": 0.0,  # Empty input, just shouldn't crash\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scripts/run_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified test runner for CASPAR.\n",
    "\n",
    "Run different test suites based on the situation.\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "def run_unit_tests():\n",
    "    \"\"\"Run fast unit tests.\"\"\"\n",
    "    print(\"\\n\ud83e\uddea Running Unit Tests...\")\n",
    "    result = subprocess.run(\n",
    "        [\"pytest\", \"tests/unit/\", \"-v\", \"--tb=short\"],\n",
    "        capture_output=False\n",
    "    )\n",
    "    return result.returncode == 0\n",
    "\n",
    "\n",
    "def run_integration_tests():\n",
    "    \"\"\"Run integration tests (requires API key).\"\"\"\n",
    "    print(\"\\n\ud83d\udd17 Running Integration Tests...\")\n",
    "    result = subprocess.run(\n",
    "        [\"pytest\", \"tests/integration/\", \"-v\", \"--tb=short\", \"--timeout=60\"],\n",
    "        capture_output=False\n",
    "    )\n",
    "    return result.returncode == 0\n",
    "\n",
    "\n",
    "def run_evaluation_tests():\n",
    "    \"\"\"Run evaluation tests (slowest, most thorough).\"\"\"\n",
    "    print(\"\\n\ud83d\udcca Running Evaluation Tests...\")\n",
    "    result = subprocess.run(\n",
    "        [\"pytest\", \"tests/evaluation/\", \"-v\", \"--tb=short\", \"--timeout=120\"],\n",
    "        capture_output=False\n",
    "    )\n",
    "    return result.returncode == 0\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all test suites.\"\"\"\n",
    "    results = {\n",
    "        \"unit\": run_unit_tests(),\n",
    "        \"integration\": run_integration_tests(),\n",
    "        \"evaluation\": run_evaluation_tests(),\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for suite, passed in results.items():\n",
    "        status = \"\u2705 PASSED\" if passed else \"\u274c FAILED\"\n",
    "        print(f\"  {suite.capitalize()}: {status}\")\n",
    "    \n",
    "    all_passed = all(results.values())\n",
    "    print(f\"\\nOverall: {'\u2705 ALL PASSED' if all_passed else '\u274c SOME FAILED'}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Run CASPAR tests\")\n",
    "    parser.add_argument(\n",
    "        \"--suite\",\n",
    "        choices=[\"unit\", \"integration\", \"evaluation\", \"all\"],\n",
    "        default=\"unit\",\n",
    "        help=\"Which test suite to run (default: unit)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--quick\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Run only unit tests (fastest)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.quick:\n",
    "        success = run_unit_tests()\n",
    "    elif args.suite == \"unit\":\n",
    "        success = run_unit_tests()\n",
    "    elif args.suite == \"integration\":\n",
    "        success = run_integration_tests()\n",
    "    elif args.suite == \"evaluation\":\n",
    "        success = run_evaluation_tests()\n",
    "    else:\n",
    "        success = run_all_tests()\n",
    "    \n",
    "    sys.exit(0 if success else 1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 20.7: Deployment and next steps\n",
    "\n",
    "This section covers deploying CASPAR with Docker and FastAPI.\n",
    "\n",
    "**Key files:**\n",
    "- `Dockerfile` - Container build configuration\n",
    "- `src/caspar/api/main.py` - FastAPI application\n",
    "- `src/caspar/api/metrics.py` - Monitoring metrics\n",
    "- `src/caspar/config/logging_config.py` - Production logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: Dockerfile\n",
    "\n",
    "# Use Python 3.13 slim image\n",
    "FROM python:3.13-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PORT=8000\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first (for caching)\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy package configuration and source code\n",
    "COPY pyproject.toml .\n",
    "COPY src/ ./src/\n",
    "COPY data/ ./data/\n",
    "COPY scripts/ ./scripts/\n",
    "\n",
    "# Install the caspar package\n",
    "# This makes 'from caspar.agent import ...' work properly\n",
    "RUN pip install --no-cache-dir -e .\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd --create-home appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Expose the API port (documentation only, Railway ignores this)\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check for local Docker (Railway uses its own)\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \\\n",
    "    CMD curl -f http://localhost:${PORT}/health || exit 1\n",
    "\n",
    "# Run the API server\n",
    "# IMPORTANT: Use $PORT for Railway compatibility (they set this env var)\n",
    "CMD uvicorn caspar.api.main:app --host 0.0.0.0 --port ${PORT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### api/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: src/caspar/api/main.py\n",
    "\n",
    "\"\"\"\n",
    "CASPAR API - Production-ready FastAPI application.\n",
    "\n",
    "Provides REST endpoints for the customer service agent.\n",
    "\"\"\"\n",
    "\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "import uuid\n",
    "\n",
    "from caspar.config import settings, get_logger\n",
    "from caspar.agent import create_checkpointer_context, create_agent, create_initial_state\n",
    "from caspar.knowledge import get_retriever\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Store active conversations in memory\n",
    "# Note: For horizontal scaling, use Redis instead\n",
    "conversations: dict = {}\n",
    "\n",
    "# The agent instance (initialized on startup)\n",
    "agent = None\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# LIFESPAN - STARTUP AND SHUTDOWN\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Initialize resources on startup, cleanup on shutdown.\n",
    "    \n",
    "    The checkpointer context manager MUST wrap the yield to keep\n",
    "    the database connection open during the server's lifetime.\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    logger.info(\"starting_caspar_api\", version=\"1.0.0\")\n",
    "    \n",
    "    # Step 1: Initialize knowledge base (validates it's ready)\n",
    "    retriever = get_retriever()\n",
    "    logger.info(\"knowledge_base_ready\")\n",
    "    \n",
    "    # Step 2: Create checkpointer context\n",
    "    # The 'async with' keeps the database connection open\n",
    "    async with create_checkpointer_context() as checkpointer:\n",
    "        \n",
    "        # Step 3: Create the agent with the checkpointer\n",
    "        agent = await create_agent(checkpointer=checkpointer)\n",
    "        logger.info(\n",
    "            \"agent_initialized\",\n",
    "            persistence_enabled=checkpointer is not None\n",
    "        )\n",
    "        \n",
    "        # Step 4: Server runs here (while inside the 'async with')\n",
    "        yield\n",
    "        \n",
    "        # Step 5: Cleanup on shutdown\n",
    "        logger.info(\"shutting_down_caspar_api\")\n",
    "        conversations.clear()\n",
    "    \n",
    "    # Database connection closes automatically when we exit 'async with'\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# FASTAPI APP SETUP\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"CASPAR API\",\n",
    "    description=\"Customer Service AI Agent powered by LangGraph\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan,  # Connect our startup/shutdown logic\n",
    ")\n",
    "\n",
    "# CORS middleware allows web browsers to call our API\n",
    "# Without this, browsers block requests from different domains\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, list specific domains\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# REQUEST MODELS (what clients send to us)\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "class StartConversationRequest(BaseModel):\n",
    "    \"\"\"Request to start a new conversation.\"\"\"\n",
    "    customer_id: str = Field(..., description=\"Customer identifier\")\n",
    "    initial_message: str | None = Field(None, description=\"Optional first message\")\n",
    "\n",
    "\n",
    "class SendMessageRequest(BaseModel):\n",
    "    \"\"\"Request to send a message in a conversation.\"\"\"\n",
    "    message: str = Field(..., min_length=1, max_length=10000)\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# RESPONSE MODELS (what we send back to clients)\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "class StartConversationResponse(BaseModel):\n",
    "    \"\"\"Response with new conversation details.\"\"\"\n",
    "    conversation_id: str\n",
    "    message: str\n",
    "\n",
    "\n",
    "class SendMessageResponse(BaseModel):\n",
    "    \"\"\"Response from the agent.\"\"\"\n",
    "    response: str\n",
    "    intent: str | None = None\n",
    "    needs_escalation: bool = False\n",
    "    ticket_id: str | None = None\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class ConversationStatus(BaseModel):\n",
    "    \"\"\"Current status of a conversation.\"\"\"\n",
    "    conversation_id: str\n",
    "    customer_id: str\n",
    "    message_count: int\n",
    "    intent: str | None\n",
    "    needs_escalation: bool\n",
    "    created_at: str\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response.\"\"\"\n",
    "    status: str\n",
    "    version: str\n",
    "    agent_ready: bool\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# ENDPOINTS\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"System\"])\n",
    "async def health_check():\n",
    "    \"\"\"\n",
    "    Check if the service is healthy.\n",
    "    \n",
    "    Used by:\n",
    "    - Load balancers to know if this instance can receive traffic\n",
    "    - Kubernetes/Docker health checks\n",
    "    - Monitoring systems\n",
    "    \"\"\"\n",
    "    return HealthResponse(\n",
    "        status=\"healthy\",\n",
    "        version=\"1.0.0\",\n",
    "        agent_ready=agent is not None,\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(\"/conversations\", response_model=StartConversationResponse, tags=[\"Conversations\"])\n",
    "async def start_conversation(request: StartConversationRequest):\n",
    "    \"\"\"\n",
    "    Start a new conversation with CASPAR.\n",
    "    \n",
    "    Returns a conversation ID to use for subsequent messages.\n",
    "    \"\"\"\n",
    "    # Generate a unique ID for this conversation\n",
    "    conversation_id = f\"conv-{uuid.uuid4().hex[:12]}\"\n",
    "    \n",
    "    # Initialize the agent's state for this conversation\n",
    "    state = create_initial_state(\n",
    "        conversation_id=conversation_id,\n",
    "        customer_id=request.customer_id,\n",
    "    )\n",
    "    \n",
    "    # Store in memory (keyed by conversation_id)\n",
    "    conversations[conversation_id] = {\n",
    "        \"state\": state,\n",
    "        \"customer_id\": request.customer_id,\n",
    "    }\n",
    "    \n",
    "    logger.info(\n",
    "        \"conversation_started\",\n",
    "        conversation_id=conversation_id,\n",
    "        customer_id=request.customer_id,\n",
    "    )\n",
    "    \n",
    "    # If the client sent an initial message, process it immediately\n",
    "    if request.initial_message:\n",
    "        response = await _process_message(conversation_id, request.initial_message)\n",
    "        return StartConversationResponse(\n",
    "            conversation_id=conversation_id,\n",
    "            message=response.response,\n",
    "        )\n",
    "    \n",
    "    # Otherwise, return a greeting\n",
    "    return StartConversationResponse(\n",
    "        conversation_id=conversation_id,\n",
    "        message=\"Hello! I'm CASPAR, your customer service assistant. How can I help you today?\",\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(\n",
    "    \"/conversations/{conversation_id}/messages\",\n",
    "    response_model=SendMessageResponse,\n",
    "    tags=[\"Conversations\"],\n",
    ")\n",
    "async def send_message(conversation_id: str, request: SendMessageRequest):\n",
    "    \"\"\"\n",
    "    Send a message in an existing conversation.\n",
    "    \n",
    "    The agent will process the message and return a response.\n",
    "    \"\"\"\n",
    "    # Check if conversation exists\n",
    "    if conversation_id not in conversations:\n",
    "        raise HTTPException(\n",
    "            status_code=404,\n",
    "            detail=f\"Conversation {conversation_id} not found\",\n",
    "        )\n",
    "    \n",
    "    # Process the message (see helper function below)\n",
    "    return await _process_message(conversation_id, request.message)\n",
    "\n",
    "\n",
    "@app.get(\n",
    "    \"/conversations/{conversation_id}\",\n",
    "    response_model=ConversationStatus,\n",
    "    tags=[\"Conversations\"],\n",
    ")\n",
    "async def get_conversation(conversation_id: str):\n",
    "    \"\"\"Get the current status of a conversation.\"\"\"\n",
    "    if conversation_id not in conversations:\n",
    "        raise HTTPException(\n",
    "            status_code=404,\n",
    "            detail=f\"Conversation {conversation_id} not found\",\n",
    "        )\n",
    "    \n",
    "    conv = conversations[conversation_id]\n",
    "    state = conv[\"state\"]\n",
    "    \n",
    "    return ConversationStatus(\n",
    "        conversation_id=conversation_id,\n",
    "        customer_id=conv[\"customer_id\"],\n",
    "        message_count=len(state.get(\"messages\", [])),\n",
    "        intent=state.get(\"intent\"),\n",
    "        needs_escalation=state.get(\"needs_escalation\", False),\n",
    "        created_at=state.get(\"started_at\", \"unknown\"),\n",
    "    )\n",
    "\n",
    "\n",
    "@app.delete(\"/conversations/{conversation_id}\", tags=[\"Conversations\"])\n",
    "async def end_conversation(conversation_id: str):\n",
    "    \"\"\"End a conversation and clean up resources.\"\"\"\n",
    "    if conversation_id not in conversations:\n",
    "        raise HTTPException(\n",
    "            status_code=404,\n",
    "            detail=f\"Conversation {conversation_id} not found\",\n",
    "        )\n",
    "    \n",
    "    del conversations[conversation_id]\n",
    "    \n",
    "    logger.info(\"conversation_ended\", conversation_id=conversation_id)\n",
    "    \n",
    "    return {\"status\": \"ended\", \"conversation_id\": conversation_id}\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# METRICS ENDPOINT\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "from caspar.api.metrics import metrics\n",
    "\n",
    "@app.get(\"/metrics\", tags=[\"System\"])\n",
    "async def get_metrics():\n",
    "    \"\"\"\n",
    "    Get current metrics.\n",
    "    \n",
    "    Returns counters, latencies, and uptime.\n",
    "    Useful for monitoring dashboards.\n",
    "    \"\"\"\n",
    "    return metrics.get_stats()\n",
    "\n",
    "\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# HELPER FUNCTIONS\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "async def _process_message(conversation_id: str, message: str) -> SendMessageResponse:\n",
    "    \"\"\"\n",
    "    Process a message through the agent.\n",
    "    \n",
    "    This is a helper function used by multiple endpoints.\n",
    "    The underscore prefix indicates it's private (not an endpoint).\n",
    "    \"\"\"\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    # Get the conversation from memory\n",
    "    conv = conversations[conversation_id]\n",
    "    state = conv[\"state\"]\n",
    "    \n",
    "    # Add the user's message to the conversation history\n",
    "    state[\"messages\"].append(HumanMessage(content=message))\n",
    "    \n",
    "    # Configure the agent with this conversation's thread_id\n",
    "    # This enables persistence (if checkpointer is available)\n",
    "    config = {\"configurable\": {\"thread_id\": conversation_id}}\n",
    "    \n",
    "    try:\n",
    "        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "        # THIS IS THE KEY LINE - Run the LangGraph agent!\n",
    "        # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "        result = await agent.ainvoke(state, config)\n",
    "        \n",
    "        # Update stored state with the result\n",
    "        conv[\"state\"] = result\n",
    "        \n",
    "        # Extract the AI's response (last message)\n",
    "        ai_response = result[\"messages\"][-1].content if result[\"messages\"] else \\\n",
    "            \"I apologize, but I couldn't process your request.\"\n",
    "        \n",
    "        logger.info(\n",
    "            \"message_processed\",\n",
    "            conversation_id=conversation_id,\n",
    "            intent=result.get(\"intent\"),\n",
    "            needs_escalation=result.get(\"needs_escalation\", False),\n",
    "        )\n",
    "        \n",
    "        # Build and return the response\n",
    "        return SendMessageResponse(\n",
    "            response=ai_response,\n",
    "            intent=result.get(\"intent\"),\n",
    "            needs_escalation=result.get(\"needs_escalation\", False),\n",
    "            ticket_id=result.get(\"ticket_id\"),\n",
    "            metadata={\n",
    "                \"sentiment_score\": result.get(\"sentiment_score\"),\n",
    "                \"frustration_level\": result.get(\"frustration_level\"),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"message_processing_error\", \n",
    "            error=str(e), \n",
    "            conversation_id=conversation_id\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"An error occurred processing your message. Please try again.\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### api/metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: src/caspar/api/metrics.py\n",
    "\n",
    "\"\"\"\n",
    "Simple metrics tracking for CASPAR.\n",
    "\n",
    "In production, you'd use Prometheus, DataDog, or similar.\n",
    "This simple implementation demonstrates the concepts.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "\n",
    "class SimpleMetrics:\n",
    "    \"\"\"\n",
    "    Thread-safe metrics collector.\n",
    "    \n",
    "    Why thread-safe? FastAPI handles multiple requests at once.\n",
    "    Without locks, concurrent updates could corrupt our data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Lock prevents race conditions when multiple requests update metrics\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Counters track \"how many times did X happen?\"\n",
    "        # defaultdict(int) means missing keys default to 0\n",
    "        self._counters = defaultdict(int)\n",
    "        \n",
    "        # Latencies track \"how long did X take?\"\n",
    "        # We store lists of measurements for each operation\n",
    "        self._latencies = defaultdict(list)\n",
    "        \n",
    "        # Track when we started (for uptime calculation)\n",
    "        self._started_at = datetime.now(timezone.utc)\n",
    "    \n",
    "    def increment(self, name: str, value: int = 1):\n",
    "        \"\"\"\n",
    "        Increment a counter.\n",
    "        \n",
    "        Usage:\n",
    "            metrics.increment(\"conversations_started\")\n",
    "            metrics.increment(\"messages_processed\")\n",
    "            metrics.increment(\"errors\", 1)\n",
    "        \"\"\"\n",
    "        with self._lock:  # Acquire lock before modifying\n",
    "            self._counters[name] += value\n",
    "        # Lock automatically released when we exit 'with' block\n",
    "    \n",
    "    def record_latency(self, name: str, seconds: float):\n",
    "        \"\"\"\n",
    "        Record how long an operation took.\n",
    "        \n",
    "        Usage:\n",
    "            start = time.time()\n",
    "            do_something()\n",
    "            metrics.record_latency(\"llm_call\", time.time() - start)\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            self._latencies[name].append(seconds)\n",
    "            \n",
    "            # Keep only last 1000 measurements to prevent memory bloat\n",
    "            # Older measurements \"fall off\" as new ones come in\n",
    "            if len(self._latencies[name]) > 1000:\n",
    "                self._latencies[name] = self._latencies[name][-1000:]\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get current statistics.\n",
    "        \n",
    "        Returns a dictionary that can be serialized to JSON.\n",
    "        \"\"\"\n",
    "        with self._lock:\n",
    "            stats = {\n",
    "                # How long has the server been running?\n",
    "                \"uptime_seconds\": (\n",
    "                    datetime.now(timezone.utc) - self._started_at\n",
    "                ).total_seconds(),\n",
    "                \n",
    "                # All counter values\n",
    "                \"counters\": dict(self._counters),\n",
    "                \n",
    "                # Latency statistics (calculated below)\n",
    "                \"latencies\": {},\n",
    "            }\n",
    "            \n",
    "            # Calculate latency statistics for each tracked operation\n",
    "            for name, values in self._latencies.items():\n",
    "                if values:\n",
    "                    stats[\"latencies\"][name] = {\n",
    "                        \"count\": len(values),\n",
    "                        \"avg_ms\": sum(values) * 1000 / len(values),\n",
    "                        \"max_ms\": max(values) * 1000,\n",
    "                        \"min_ms\": min(values) * 1000,\n",
    "                    }\n",
    "            \n",
    "            return stats\n",
    "\n",
    "\n",
    "# Create a single global instance\n",
    "# All parts of the app use this same instance\n",
    "metrics = SimpleMetrics()\n",
    "\n",
    "\n",
    "def track_latency(name: str):\n",
    "    \"\"\"\n",
    "    Decorator to automatically track function execution time.\n",
    "    \n",
    "    Usage:\n",
    "        @track_latency(\"llm_call\")\n",
    "        async def call_llm():\n",
    "            ...\n",
    "    \n",
    "    This will:\n",
    "    - Track how long the function takes\n",
    "    - Increment {name}_success on success\n",
    "    - Increment {name}_error on failure\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            start = datetime.now(timezone.utc)\n",
    "            try:\n",
    "                result = await func(*args, **kwargs)\n",
    "                metrics.increment(f\"{name}_success\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                metrics.increment(f\"{name}_error\")\n",
    "                raise  # Re-raise the exception\n",
    "            finally:\n",
    "                # 'finally' runs whether success or failure\n",
    "                elapsed = (datetime.now(timezone.utc) - start).total_seconds()\n",
    "                metrics.record_latency(name, elapsed)\n",
    "        return wrapper\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config/logging_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: src/caspar/config/logging_config.py\n",
    "\n",
    "\"\"\"\n",
    "Production logging configuration.\n",
    "\n",
    "Outputs JSON logs that can be shipped to any log aggregator.\n",
    "\"\"\"\n",
    "\n",
    "import structlog\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "def configure_production_logging():\n",
    "    \"\"\"\n",
    "    Configure logging for production.\n",
    "    \n",
    "    Call this once at application startup.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure structlog to output JSON\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            # Include context variables (set elsewhere in code)\n",
    "            structlog.contextvars.merge_contextvars,\n",
    "            \n",
    "            # Add log level (info, warning, error, etc.)\n",
    "            structlog.processors.add_log_level,\n",
    "            \n",
    "            # Include stack traces for errors\n",
    "            structlog.processors.StackInfoRenderer(),\n",
    "            \n",
    "            # Add ISO-format timestamp\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            \n",
    "            # Output as JSON (the key part!)\n",
    "            structlog.processors.JSONRenderer(),\n",
    "        ],\n",
    "        \n",
    "        # Only log INFO and above (not DEBUG)\n",
    "        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n",
    "        \n",
    "        # Use dict for context\n",
    "        context_class=dict,\n",
    "        \n",
    "        # Output to stdout (Docker captures this)\n",
    "        logger_factory=structlog.PrintLoggerFactory(),\n",
    "        \n",
    "        # Cache logger for performance\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "    \n",
    "    # Also configure Python's standard logging library\n",
    "    # (some libraries use this instead of structlog)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "\n",
    "def configure_development_logging():\n",
    "    \"\"\"\n",
    "    Configure logging for development.\n",
    "    \n",
    "    Uses human-readable output instead of JSON.\n",
    "    \"\"\"\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.contextvars.merge_contextvars,\n",
    "            structlog.processors.add_log_level,\n",
    "            structlog.processors.StackInfoRenderer(),\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            # Human-readable output with colors\n",
    "            structlog.dev.ConsoleRenderer(colors=True),\n",
    "        ],\n",
    "        wrapper_class=structlog.make_filtering_bound_logger(logging.DEBUG),\n",
    "        context_class=dict,\n",
    "        logger_factory=structlog.PrintLoggerFactory(),\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        stream=sys.stdout,\n",
    "        level=logging.DEBUG,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running CASPAR\n",
    "\n",
    "To run the complete CASPAR system:\n",
    "\n",
    "### 1. Setup\n",
    "```bash\n",
    "cd caspar\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### 2. Start PostgreSQL\n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "### 3. Configure Environment\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# Edit .env with your OpenAI API key\n",
    "```\n",
    "\n",
    "### 4. Build Knowledge Base\n",
    "```bash\n",
    "python scripts/build_knowledge_base.py\n",
    "```\n",
    "\n",
    "### 5. Run Tests\n",
    "```bash\n",
    "python scripts/run_tests.py\n",
    "```\n",
    "\n",
    "### 6. Start the API\n",
    "```bash\n",
    "uvicorn caspar.api.main:app --reload\n",
    "```\n",
    "\n",
    "### 7. Test Conversation\n",
    "```bash\n",
    "python scripts/test_conversation_flow.py\n",
    "```\n",
    "\n",
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Zero to AI Agent book! You now have the skills to build production-ready AI agents using:\n",
    "- Python fundamentals\n",
    "- LLM integration with OpenAI\n",
    "- Agent frameworks (LangChain, LangGraph)\n",
    "- RAG for knowledge retrieval\n",
    "- Human-in-the-loop workflows\n",
    "- Testing and evaluation\n",
    "- Production deployment\n",
    "\n",
    "**Next steps:**\n",
    "- Customize CASPAR for your own use case\n",
    "- Explore more advanced LangGraph patterns\n",
    "- Deploy to a cloud platform (Railway, AWS, GCP)\n",
    "- Build your own agents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}