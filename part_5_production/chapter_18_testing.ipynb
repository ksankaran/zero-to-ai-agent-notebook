{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Testing and Evaluation\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- Unit testing agent components\n",
    "- Integration testing strategies\n",
    "- Evaluating agent performance\n",
    "- Creating test datasets\n",
    "- Measuring accuracy and reliability\n",
    "- A/B testing agents\n",
    "- Continuous improvement workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.1: Unit testing agent components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_example.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_example.py\n",
    "# Description: Basic pytest examples to get started with testing\n",
    "\n",
    "def test_addition():\n",
    "    \"\"\"Our very first test!\"\"\"\n",
    "    result = 2 + 2\n",
    "    assert result == 4\n",
    "\n",
    "\n",
    "def test_string_contains():\n",
    "    \"\"\"Test that a string contains expected text.\"\"\"\n",
    "    message = \"Hello, World!\"\n",
    "    assert \"World\" in message\n",
    "\n",
    "\n",
    "def test_list_length():\n",
    "    \"\"\"Test the length of a list.\"\"\"\n",
    "    items = [1, 2, 3, 4, 5]\n",
    "    assert len(items) == 5\n",
    "\n",
    "\n",
    "def test_that_fails():\n",
    "    \"\"\"This test will fail - on purpose! \n",
    "    Uncomment to see what failure output looks like.\"\"\"\n",
    "    # result = 2 + 2\n",
    "    # assert result == 5, \"Math is broken!\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tools.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: tools.py\n",
    "# Description: Agent tool functions - shipping calculator and customer lookup\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "def calculate_shipping(weight_kg: float, distance_km: float, express: bool = False) -> dict:\n",
    "    \"\"\"Calculate shipping cost based on weight and distance.\"\"\"\n",
    "    base_rate = 5.00\n",
    "    weight_charge = weight_kg * 0.50\n",
    "    distance_charge = distance_km * 0.01\n",
    "    \n",
    "    subtotal = base_rate + weight_charge + distance_charge\n",
    "    \n",
    "    if express:\n",
    "        subtotal *= 1.5\n",
    "    \n",
    "    return {\n",
    "        \"cost\": round(subtotal, 2),\n",
    "        \"currency\": \"USD\",\n",
    "        \"express\": express,\n",
    "        \"estimated_days\": 1 if express else 5\n",
    "    }\n",
    "\n",
    "\n",
    "async def get_customer_info(customer_id: str) -> dict:\n",
    "    \"\"\"Fetch customer information from the CRM API.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(\n",
    "            f\"https://api.example.com/customers/{customer_id}\",\n",
    "            headers={\"Authorization\": \"Bearer secret-token\"}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_tools.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_tools.py\n",
    "# Description: Unit tests for agent tools - demonstrates mocking external services\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import AsyncMock, Mock, patch\n",
    "from tools import calculate_shipping, get_customer_info\n",
    "\n",
    "\n",
    "# Tests for calculate_shipping (deterministic - no mocking needed)\n",
    "\n",
    "def test_basic_shipping_calculation():\n",
    "    result = calculate_shipping(weight_kg=2.0, distance_km=100)\n",
    "    \n",
    "    assert result[\"cost\"] == 7.00  # 5 + (2 * 0.5) + (100 * 0.01)\n",
    "    assert result[\"currency\"] == \"USD\"\n",
    "    assert result[\"express\"] is False\n",
    "    assert result[\"estimated_days\"] == 5\n",
    "\n",
    "\n",
    "def test_express_shipping_multiplier():\n",
    "    standard = calculate_shipping(weight_kg=2.0, distance_km=100, express=False)\n",
    "    express = calculate_shipping(weight_kg=2.0, distance_km=100, express=True)\n",
    "    \n",
    "    assert express[\"cost\"] == standard[\"cost\"] * 1.5\n",
    "    assert express[\"estimated_days\"] == 1\n",
    "\n",
    "\n",
    "def test_zero_weight_and_distance():\n",
    "    result = calculate_shipping(weight_kg=0, distance_km=0)\n",
    "    \n",
    "    assert result[\"cost\"] == 5.00  # Just the base rate\n",
    "\n",
    "\n",
    "# Tests for get_customer_info (requires mocking the HTTP client)\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_get_customer_info_success():\n",
    "    mock_response = {\n",
    "        \"id\": \"cust_123\",\n",
    "        \"name\": \"Alice Smith\",\n",
    "        \"email\": \"alice@example.com\",\n",
    "        \"tier\": \"premium\"\n",
    "    }\n",
    "    \n",
    "    with patch(\"tools.httpx.AsyncClient\") as MockClient:\n",
    "        # Set up the mock to return our fake response\n",
    "        # Use regular Mock for response since json() is sync in httpx\n",
    "        mock_response_obj = Mock()\n",
    "        mock_response_obj.json.return_value = mock_response\n",
    "        mock_response_obj.raise_for_status = Mock()\n",
    "\n",
    "        mock_client_instance = AsyncMock()\n",
    "        mock_client_instance.get.return_value = mock_response_obj\n",
    "        MockClient.return_value.__aenter__.return_value = mock_client_instance\n",
    "        \n",
    "        result = await get_customer_info(\"cust_123\")\n",
    "        \n",
    "        assert result[\"name\"] == \"Alice Smith\"\n",
    "        assert result[\"tier\"] == \"premium\"\n",
    "        \n",
    "        # Verify the API was called correctly\n",
    "        mock_client_instance.get.assert_called_once()\n",
    "        call_args = mock_client_instance.get.call_args\n",
    "        assert \"cust_123\" in call_args[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: state.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: state.py\n",
    "# Description: Agent state definitions and helper functions for state transformations\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    current_tool: str | None\n",
    "    retry_count: int\n",
    "\n",
    "\n",
    "def should_retry(state: AgentState, max_retries: int = 3) -> bool:\n",
    "    \"\"\"Determine if we should retry the current operation.\"\"\"\n",
    "    return state[\"retry_count\"] < max_retries\n",
    "\n",
    "\n",
    "def increment_retry(state: AgentState) -> dict:\n",
    "    \"\"\"Return state update to increment retry count.\"\"\"\n",
    "    return {\"retry_count\": state[\"retry_count\"] + 1}\n",
    "\n",
    "\n",
    "def reset_retry(state: AgentState) -> dict:\n",
    "    \"\"\"Return state update to reset retry count.\"\"\"\n",
    "    return {\"retry_count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_state.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_state.py\n",
    "# Description: Unit tests for state transformation functions\n",
    "\n",
    "from state import should_retry, increment_retry, reset_retry\n",
    "\n",
    "\n",
    "def test_should_retry_under_limit():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 1}\n",
    "    assert should_retry(state, max_retries=3) is True\n",
    "\n",
    "\n",
    "def test_should_retry_at_limit():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 3}\n",
    "    assert should_retry(state, max_retries=3) is False\n",
    "\n",
    "\n",
    "def test_increment_retry():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 2}\n",
    "    update = increment_retry(state)\n",
    "    \n",
    "    assert update == {\"retry_count\": 3}\n",
    "\n",
    "\n",
    "def test_reset_retry():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 5}\n",
    "    update = reset_retry(state)\n",
    "    \n",
    "    assert update == {\"retry_count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: routing.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: routing.py\n",
    "# Description: Conditional edge routing logic for LangGraph agents\n",
    "\n",
    "\n",
    "def route_after_llm(state: dict) -> str:\n",
    "    \"\"\"Decide where to go after the LLM responds.\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    if not messages:\n",
    "        return \"error_handler\"\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if LLM wants to use a tool\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    \n",
    "    # Check if conversation should end\n",
    "    if state.get(\"should_end\", False):\n",
    "        return \"goodbye\"\n",
    "    \n",
    "    # Continue conversation\n",
    "    return \"respond_to_user\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_routing.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_routing.py\n",
    "# Description: Unit tests for conditional edge routing decisions\n",
    "\n",
    "from unittest.mock import MagicMock\n",
    "from routing import route_after_llm\n",
    "\n",
    "\n",
    "def test_route_to_tools_when_tool_calls_present():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = [{\"name\": \"search\", \"args\": {\"query\": \"test\"}}]\n",
    "    \n",
    "    state = {\"messages\": [mock_message]}\n",
    "    \n",
    "    assert route_after_llm(state) == \"execute_tools\"\n",
    "\n",
    "\n",
    "def test_route_to_goodbye_when_should_end():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = None\n",
    "    \n",
    "    state = {\"messages\": [mock_message], \"should_end\": True}\n",
    "    \n",
    "    assert route_after_llm(state) == \"goodbye\"\n",
    "\n",
    "\n",
    "def test_route_to_respond_by_default():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = None\n",
    "    \n",
    "    state = {\"messages\": [mock_message], \"should_end\": False}\n",
    "    \n",
    "    assert route_after_llm(state) == \"respond_to_user\"\n",
    "\n",
    "\n",
    "def test_route_to_error_when_no_messages():\n",
    "    state = {\"messages\": []}\n",
    "    \n",
    "    assert route_after_llm(state) == \"error_handler\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: nodes.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: nodes.py\n",
    "# Description: Agent node functions - demonstrates LLM integration pattern\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "async def analyze_sentiment(state: dict, llm: ChatOpenAI) -> dict:\n",
    "    \"\"\"Analyze the sentiment of the user's last message.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_user_message = None\n",
    "    \n",
    "    for msg in reversed(messages):\n",
    "        if msg.type == \"human\":\n",
    "            last_user_message = msg.content\n",
    "            break\n",
    "    \n",
    "    if not last_user_message:\n",
    "        return {\"sentiment\": \"unknown\", \"confidence\": 0.0}\n",
    "    \n",
    "    response = await llm.ainvoke([\n",
    "        {\"role\": \"system\", \"content\": \"Analyze sentiment. Respond with only: positive, negative, or neutral\"},\n",
    "        {\"role\": \"user\", \"content\": last_user_message}\n",
    "    ])\n",
    "    \n",
    "    sentiment = response.content.strip().lower()\n",
    "    \n",
    "    # Validate the response\n",
    "    if sentiment not in [\"positive\", \"negative\", \"neutral\"]:\n",
    "        sentiment = \"unknown\"\n",
    "    \n",
    "    return {\"sentiment\": sentiment, \"confidence\": 0.85}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_nodes.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_nodes.py\n",
    "# Description: Unit tests for node functions - demonstrates mocking LLM responses\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import AsyncMock, MagicMock\n",
    "from nodes import analyze_sentiment\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_analyze_sentiment_positive():\n",
    "    # Create mock LLM\n",
    "    mock_llm = AsyncMock()\n",
    "    mock_response = MagicMock()\n",
    "    mock_response.content = \"positive\"\n",
    "    mock_llm.ainvoke.return_value = mock_response\n",
    "    \n",
    "    # Create test state\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.type = \"human\"\n",
    "    mock_message.content = \"I love this product!\"\n",
    "    state = {\"messages\": [mock_message]}\n",
    "    \n",
    "    result = await analyze_sentiment(state, mock_llm)\n",
    "    \n",
    "    assert result[\"sentiment\"] == \"positive\"\n",
    "    assert result[\"confidence\"] == 0.85\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_analyze_sentiment_handles_invalid_response():\n",
    "    mock_llm = AsyncMock()\n",
    "    mock_response = MagicMock()\n",
    "    mock_response.content = \"I think it's somewhat positive but also...\"  # Invalid\n",
    "    mock_llm.ainvoke.return_value = mock_response\n",
    "    \n",
    "    mock_message = MagicMock()\n",
    "    mock_message.type = \"human\"\n",
    "    mock_message.content = \"It's okay I guess\"\n",
    "    state = {\"messages\": [mock_message]}\n",
    "    \n",
    "    result = await analyze_sentiment(state, mock_llm)\n",
    "    \n",
    "    assert result[\"sentiment\"] == \"unknown\"  # Falls back gracefully\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_analyze_sentiment_no_user_message():\n",
    "    mock_llm = AsyncMock()\n",
    "    \n",
    "    state = {\"messages\": []}  # No messages\n",
    "    \n",
    "    result = await analyze_sentiment(state, mock_llm)\n",
    "    \n",
    "    assert result[\"sentiment\"] == \"unknown\"\n",
    "    assert result[\"confidence\"] == 0.0\n",
    "    mock_llm.ainvoke.assert_not_called()  # LLM shouldn't be called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 18.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.1: Testing a Validation Tool\n",
    "\n",
    "Create a tool function called `validate_email` that checks if an email address is valid. It should return a dictionary with `valid` (boolean), `error` (string or None), and `normalized` (the email in lowercase). Write at least 5 unit tests covering valid emails, invalid emails, edge cases like empty strings, and emails with unusual but valid characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.2: Testing State Helpers\n",
    "\n",
    "Create a set of helper functions for managing a \"shopping cart\" state that includes `items` (list of dicts with name and quantity), `total` (float), and `coupon_code` (string or None). Write functions for `add_item`, `remove_item`, `apply_coupon`, and `calculate_total`. Write unit tests for each function, including edge cases like removing an item that doesn't exist or applying an invalid coupon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.1.3: Mocking an API Tool\n",
    "\n",
    "You have a tool that calls a weather API to get the current temperature for a city. Write the tool function and then write tests using mocks that verify:\n",
    "- The function correctly parses a successful API response\n",
    "- The function handles a 404 (city not found) gracefully\n",
    "- The function handles network timeouts gracefully\n",
    "- The correct city name is passed to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.2: Integration testing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: support_agent.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.2\n",
    "# File: support_agent.py\n",
    "# Description: Example support agent with classification and routing\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class SupportState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    issue_type: str | None\n",
    "    resolved: bool\n",
    "\n",
    "\n",
    "def classify_issue(state: SupportState) -> dict:\n",
    "    \"\"\"Classify the customer's issue based on their message.\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    if \"refund\" in last_message or \"money back\" in last_message:\n",
    "        return {\"issue_type\": \"refund\"}\n",
    "    elif \"broken\" in last_message or \"not working\" in last_message:\n",
    "        return {\"issue_type\": \"technical\"}\n",
    "    elif \"cancel\" in last_message:\n",
    "        return {\"issue_type\": \"cancellation\"}\n",
    "    else:\n",
    "        return {\"issue_type\": \"general\"}\n",
    "\n",
    "\n",
    "def route_by_issue(state: SupportState) -> str:\n",
    "    \"\"\"Route to appropriate handler based on issue type.\"\"\"\n",
    "    issue_type = state.get(\"issue_type\")\n",
    "    if issue_type == \"refund\":\n",
    "        return \"handle_refund\"\n",
    "    elif issue_type == \"technical\":\n",
    "        return \"handle_technical\"\n",
    "    elif issue_type == \"cancellation\":\n",
    "        return \"handle_cancellation\"\n",
    "    else:\n",
    "        return \"handle_general\"\n",
    "\n",
    "\n",
    "def handle_refund(state: SupportState, llm) -> dict:\n",
    "    \"\"\"Handle refund requests.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"] + [\n",
    "        HumanMessage(content=\"Generate a helpful response about our refund policy.\")\n",
    "    ])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"resolved\": True\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_technical(state: SupportState, llm) -> dict:\n",
    "    \"\"\"Handle technical issues.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"] + [\n",
    "        HumanMessage(content=\"Generate a helpful response for technical support.\")\n",
    "    ])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"resolved\": True\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_cancellation(state: SupportState, llm) -> dict:\n",
    "    \"\"\"Handle cancellation requests.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"] + [\n",
    "        HumanMessage(content=\"Generate a helpful response about cancellation.\")\n",
    "    ])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"resolved\": True\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_general(state: SupportState, llm) -> dict:\n",
    "    \"\"\"Handle general inquiries.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"] + [\n",
    "        HumanMessage(content=\"Generate a helpful general response.\")\n",
    "    ])\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "        \"resolved\": True\n",
    "    }\n",
    "\n",
    "\n",
    "def build_support_graph(llm):\n",
    "    \"\"\"Build the support agent graph.\"\"\"\n",
    "    graph = StateGraph(SupportState)\n",
    "    \n",
    "    graph.add_node(\"classify\", classify_issue)\n",
    "    graph.add_node(\"handle_refund\", lambda s: handle_refund(s, llm))\n",
    "    graph.add_node(\"handle_technical\", lambda s: handle_technical(s, llm))\n",
    "    graph.add_node(\"handle_cancellation\", lambda s: handle_cancellation(s, llm))\n",
    "    graph.add_node(\"handle_general\", lambda s: handle_general(s, llm))\n",
    "    \n",
    "    graph.add_edge(START, \"classify\")\n",
    "    graph.add_conditional_edges(\"classify\", route_by_issue)\n",
    "    graph.add_edge(\"handle_refund\", END)\n",
    "    graph.add_edge(\"handle_technical\", END)\n",
    "    graph.add_edge(\"handle_cancellation\", END)\n",
    "    graph.add_edge(\"handle_general\", END)\n",
    "    \n",
    "    return graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_support_agent.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.2\n",
    "# File: test_support_agent.py\n",
    "# Description: Integration tests demonstrating various testing strategies\n",
    "\n",
    "import pytest\n",
    "import os\n",
    "from unittest.mock import MagicMock\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from support_agent import build_support_graph, SupportState\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_llm():\n",
    "    \"\"\"Create a mock LLM that returns predictable responses.\"\"\"\n",
    "    llm = MagicMock()\n",
    "    llm.invoke.return_value = MagicMock(\n",
    "        content=\"I'd be happy to help you with that!\"\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "# --- Basic Routing Tests ---\n",
    "\n",
    "def test_refund_request_routes_correctly(mock_llm):\n",
    "    \"\"\"Test that refund requests go to the refund handler.\"\"\"\n",
    "    graph = build_support_graph(mock_llm)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I want a refund for my order\")],\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # Verify routing worked correctly\n",
    "    assert result[\"issue_type\"] == \"refund\"\n",
    "    assert result[\"resolved\"] is True\n",
    "    \n",
    "    # Verify LLM was called (handler executed)\n",
    "    assert mock_llm.invoke.called\n",
    "\n",
    "\n",
    "def test_technical_issue_routes_correctly(mock_llm):\n",
    "    \"\"\"Test that technical issues go to the technical handler.\"\"\"\n",
    "    graph = build_support_graph(mock_llm)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"My device is broken\")],\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    assert result[\"issue_type\"] == \"technical\"\n",
    "    assert result[\"resolved\"] is True\n",
    "\n",
    "\n",
    "# --- State Transition Tests ---\n",
    "\n",
    "def test_state_accumulates_through_workflow(mock_llm):\n",
    "    \"\"\"Verify state is correctly passed and accumulated.\"\"\"\n",
    "    graph = build_support_graph(mock_llm)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I need a refund please\")],\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # Original message should still be present\n",
    "    assert len(result[\"messages\"]) >= 1\n",
    "    assert result[\"messages\"][0].content == \"I need a refund please\"\n",
    "    \n",
    "    # AI response should have been added\n",
    "    ai_messages = [m for m in result[\"messages\"] if isinstance(m, AIMessage)]\n",
    "    assert len(ai_messages) >= 1\n",
    "    \n",
    "    # State fields should be populated\n",
    "    assert result[\"issue_type\"] is not None\n",
    "    assert result[\"resolved\"] is True\n",
    "\n",
    "\n",
    "# --- Property-Based Tests ---\n",
    "\n",
    "def test_agent_always_responds(mock_llm):\n",
    "    \"\"\"Verify the agent always produces a response.\"\"\"\n",
    "    graph = build_support_graph(mock_llm)\n",
    "    \n",
    "    test_messages = [\n",
    "        \"I want a refund\",\n",
    "        \"This is broken\",\n",
    "        \"Cancel my subscription\",\n",
    "        \"Hello, I have a question\",\n",
    "        \"asdfghjkl\",  # Gibberish input\n",
    "    ]\n",
    "    \n",
    "    for message in test_messages:\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=message)],\n",
    "            \"issue_type\": None,\n",
    "            \"resolved\": False\n",
    "        }\n",
    "        \n",
    "        result = graph.invoke(initial_state)\n",
    "        \n",
    "        # Property: Agent should always add a response\n",
    "        assert len(result[\"messages\"]) > 1, f\"No response for: {message}\"\n",
    "        \n",
    "        # Property: Workflow should complete\n",
    "        assert result[\"resolved\"] is True, f\"Not resolved for: {message}\"\n",
    "\n",
    "\n",
    "def test_agent_never_loses_messages(mock_llm):\n",
    "    \"\"\"Verify messages are never dropped during processing.\"\"\"\n",
    "    graph = build_support_graph(mock_llm)\n",
    "    \n",
    "    initial_messages = [\n",
    "        HumanMessage(content=\"First message\"),\n",
    "        AIMessage(content=\"First response\"),\n",
    "        HumanMessage(content=\"I want a refund now\"),\n",
    "    ]\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": initial_messages,\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "    \n",
    "    # All original messages should still be present\n",
    "    assert len(result[\"messages\"]) >= len(initial_messages)\n",
    "    \n",
    "    # Check that original content is preserved\n",
    "    result_contents = [m.content for m in result[\"messages\"]]\n",
    "    for original in initial_messages:\n",
    "        assert original.content in result_contents\n",
    "\n",
    "\n",
    "# --- Error Handling Tests ---\n",
    "\n",
    "def test_llm_error_is_handled_gracefully():\n",
    "    \"\"\"Test that LLM errors don't crash the entire workflow.\"\"\"\n",
    "    # Create an LLM that raises an error\n",
    "    failing_llm = MagicMock()\n",
    "    failing_llm.invoke.side_effect = Exception(\"API rate limit exceeded\")\n",
    "    \n",
    "    graph = build_support_graph(failing_llm)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I need help\")],\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    # Expect the exception to propagate (no built-in error handling)\n",
    "    with pytest.raises(Exception) as exc_info:\n",
    "        graph.invoke(initial_state)\n",
    "    \n",
    "    assert \"rate limit\" in str(exc_info.value).lower()\n",
    "\n",
    "\n",
    "# --- Integration Test with Real LLM (run sparingly) ---\n",
    "\n",
    "@pytest.mark.integration\n",
    "@pytest.mark.skipif(\n",
    "    not os.getenv(\"OPENAI_API_KEY\"),\n",
    "    reason=\"No API key available\"\n",
    ")\n",
    "def test_with_real_llm():\n",
    "    \"\"\"Integration test with actual LLM - run sparingly.\"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    graph = build_support_graph(llm)\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I want to return my order #12345\")],\n",
    "        \"issue_type\": None,\n",
    "        \"resolved\": False\n",
    "    }\n",
    "    \n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    # Test properties, not exact content\n",
    "    # Note: We accept multiple issue types because LLM classification is\n",
    "    # non-deterministic. Even with temperature=0, the model may classify\n",
    "    # \"return my order\" as \"refund\", \"general\", or \"order\" depending on\n",
    "    # subtle variations in model behavior across API calls.\n",
    "    assert result[\"issue_type\"] in [\"refund\", \"general\", \"order\"]\n",
    "    assert result[\"resolved\"] is True\n",
    "    \n",
    "    # Check that response mentions relevant information\n",
    "    response = result[\"messages\"][-1].content.lower()\n",
    "    assert any(word in response for word in [\"refund\", \"return\", \"order\", \"help\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: conftest.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.2\n",
    "# File: conftest.py\n",
    "# Description: Shared pytest fixtures for integration tests\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from unittest.mock import MagicMock\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file for tests that need API keys\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_llm():\n",
    "    \"\"\"Standard mock LLM for integration tests.\"\"\"\n",
    "    llm = MagicMock()\n",
    "    llm.invoke.return_value = MagicMock(content=\"Mock response\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def configured_mock_llm():\n",
    "    \"\"\"Mock LLM that can be configured with specific responses.\"\"\"\n",
    "    def _create_mock(responses: list[str]):\n",
    "        llm = MagicMock()\n",
    "        llm.invoke.side_effect = [\n",
    "            MagicMock(content=r) for r in responses\n",
    "        ]\n",
    "        return llm\n",
    "    return _create_mock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_tools.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_tools.py\n",
    "# Description: Unit tests for agent tools - demonstrates mocking external services\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import AsyncMock, Mock, patch\n",
    "from tools import calculate_shipping, get_customer_info\n",
    "\n",
    "\n",
    "# Tests for calculate_shipping (deterministic - no mocking needed)\n",
    "\n",
    "def test_basic_shipping_calculation():\n",
    "    result = calculate_shipping(weight_kg=2.0, distance_km=100)\n",
    "    \n",
    "    assert result[\"cost\"] == 7.00  # 5 + (2 * 0.5) + (100 * 0.01)\n",
    "    assert result[\"currency\"] == \"USD\"\n",
    "    assert result[\"express\"] is False\n",
    "    assert result[\"estimated_days\"] == 5\n",
    "\n",
    "\n",
    "def test_express_shipping_multiplier():\n",
    "    standard = calculate_shipping(weight_kg=2.0, distance_km=100, express=False)\n",
    "    express = calculate_shipping(weight_kg=2.0, distance_km=100, express=True)\n",
    "    \n",
    "    assert express[\"cost\"] == standard[\"cost\"] * 1.5\n",
    "    assert express[\"estimated_days\"] == 1\n",
    "\n",
    "\n",
    "def test_zero_weight_and_distance():\n",
    "    result = calculate_shipping(weight_kg=0, distance_km=0)\n",
    "    \n",
    "    assert result[\"cost\"] == 5.00  # Just the base rate\n",
    "\n",
    "\n",
    "# Tests for get_customer_info (requires mocking the HTTP client)\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_get_customer_info_success():\n",
    "    mock_response = {\n",
    "        \"id\": \"cust_123\",\n",
    "        \"name\": \"Alice Smith\",\n",
    "        \"email\": \"alice@example.com\",\n",
    "        \"tier\": \"premium\"\n",
    "    }\n",
    "    \n",
    "    with patch(\"tools.httpx.AsyncClient\") as MockClient:\n",
    "        # Set up the mock to return our fake response\n",
    "        # Use regular Mock for response since json() is sync in httpx\n",
    "        mock_response_obj = Mock()\n",
    "        mock_response_obj.json.return_value = mock_response\n",
    "        mock_response_obj.raise_for_status = Mock()\n",
    "\n",
    "        mock_client_instance = AsyncMock()\n",
    "        mock_client_instance.get.return_value = mock_response_obj\n",
    "        MockClient.return_value.__aenter__.return_value = mock_client_instance\n",
    "        \n",
    "        result = await get_customer_info(\"cust_123\")\n",
    "        \n",
    "        assert result[\"name\"] == \"Alice Smith\"\n",
    "        assert result[\"tier\"] == \"premium\"\n",
    "        \n",
    "        # Verify the API was called correctly\n",
    "        mock_client_instance.get.assert_called_once()\n",
    "        call_args = mock_client_instance.get.call_args\n",
    "        assert \"cust_123\" in call_args[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_state.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_state.py\n",
    "# Description: Unit tests for state transformation functions\n",
    "\n",
    "from state import should_retry, increment_retry, reset_retry\n",
    "\n",
    "\n",
    "def test_should_retry_under_limit():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 1}\n",
    "    assert should_retry(state, max_retries=3) is True\n",
    "\n",
    "\n",
    "def test_should_retry_at_limit():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 3}\n",
    "    assert should_retry(state, max_retries=3) is False\n",
    "\n",
    "\n",
    "def test_increment_retry():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 2}\n",
    "    update = increment_retry(state)\n",
    "    \n",
    "    assert update == {\"retry_count\": 3}\n",
    "\n",
    "\n",
    "def test_reset_retry():\n",
    "    state = {\"messages\": [], \"current_tool\": None, \"retry_count\": 5}\n",
    "    update = reset_retry(state)\n",
    "    \n",
    "    assert update == {\"retry_count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_routing.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.1\n",
    "# File: test_routing.py\n",
    "# Description: Unit tests for conditional edge routing decisions\n",
    "\n",
    "from unittest.mock import MagicMock\n",
    "from routing import route_after_llm\n",
    "\n",
    "\n",
    "def test_route_to_tools_when_tool_calls_present():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = [{\"name\": \"search\", \"args\": {\"query\": \"test\"}}]\n",
    "    \n",
    "    state = {\"messages\": [mock_message]}\n",
    "    \n",
    "    assert route_after_llm(state) == \"execute_tools\"\n",
    "\n",
    "\n",
    "def test_route_to_goodbye_when_should_end():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = None\n",
    "    \n",
    "    state = {\"messages\": [mock_message], \"should_end\": True}\n",
    "    \n",
    "    assert route_after_llm(state) == \"goodbye\"\n",
    "\n",
    "\n",
    "def test_route_to_respond_by_default():\n",
    "    mock_message = MagicMock()\n",
    "    mock_message.tool_calls = None\n",
    "    \n",
    "    state = {\"messages\": [mock_message], \"should_end\": False}\n",
    "    \n",
    "    assert route_after_llm(state) == \"respond_to_user\"\n",
    "\n",
    "\n",
    "def test_route_to_error_when_no_messages():\n",
    "    state = {\"messages\": []}\n",
    "    \n",
    "    assert route_after_llm(state) == \"error_handler\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 18.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.1: Testing a Multi-Node Workflow\n",
    "\n",
    "Create a simple order processing graph with three nodes: `validate_order` (checks if order data is valid), `calculate_total` (computes total with tax), and `confirm_order` (generates confirmation). Write integration tests that verify: (1) valid orders flow through all three nodes, (2) invalid orders stop at validation, and (3) state accumulates correctly through the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.2: Testing Branching Logic\n",
    "\n",
    "Build a graph that routes customer messages to different handlers based on sentiment (positive, negative, neutral). The routing should happen after a classification node. Write integration tests that verify each branch is taken for appropriate inputs, and that the correct handler is invoked. Use mocks to make tests deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.2.3: Testing Conversation Context\n",
    "\n",
    "Create a simple FAQ agent that answers questions differently based on conversation history. For example, if the user already asked about pricing, a follow-up question like \"what about discounts?\" should be understood in context. Write integration tests that verify context is maintained across turns and influences responses appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.3: Evaluating agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: evaluators.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: evaluators.py\n",
    "# Description: Simple evaluation functions for exact match and keyword checking\n",
    "\n",
    "\n",
    "def evaluate_exact_match(response: str, expected: str) -> dict:\n",
    "    \"\"\"Check if response exactly matches expected output.\"\"\"\n",
    "    match = response.strip().lower() == expected.strip().lower()\n",
    "    return {\n",
    "        \"metric\": \"exact_match\",\n",
    "        \"score\": 1.0 if match else 0.0,\n",
    "        \"passed\": match\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_contains_keywords(response: str, required_keywords: list[str]) -> dict:\n",
    "    \"\"\"Check if response contains all required keywords.\"\"\"\n",
    "    response_lower = response.lower()\n",
    "    found = [kw for kw in required_keywords if kw.lower() in response_lower]\n",
    "    missing = [kw for kw in required_keywords if kw.lower() not in response_lower]\n",
    "    \n",
    "    score = len(found) / len(required_keywords) if required_keywords else 1.0\n",
    "    \n",
    "    return {\n",
    "        \"metric\": \"keyword_coverage\",\n",
    "        \"score\": score,\n",
    "        \"found\": found,\n",
    "        \"missing\": missing,\n",
    "        \"passed\": len(missing) == 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test exact match\n",
    "    result = evaluate_exact_match(\"Paris\", \"paris\")\n",
    "    print(f\"Exact match: {result}\")\n",
    "    \n",
    "    # Test keyword coverage\n",
    "    response = \"The capital of France is Paris, a beautiful city.\"\n",
    "    result = evaluate_contains_keywords(response, [\"Paris\", \"capital\", \"France\"])\n",
    "    print(f\"Keyword coverage: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: semantic_evaluator.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: semantic_evaluator.py\n",
    "# Description: Evaluate responses using semantic similarity with embeddings\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "\n",
    "class SemanticEvaluator:\n",
    "    \"\"\"Evaluate responses using semantic similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    def evaluate(self, response: str, reference: str, threshold: float = 0.85) -> dict:\n",
    "        \"\"\"Compare response to reference using embeddings.\"\"\"\n",
    "        response_embedding = self.embeddings.embed_query(response)\n",
    "        reference_embedding = self.embeddings.embed_query(reference)\n",
    "        \n",
    "        similarity = cosine_similarity(response_embedding, reference_embedding)\n",
    "        \n",
    "        return {\n",
    "            \"metric\": \"semantic_similarity\",\n",
    "            \"score\": similarity,\n",
    "            \"threshold\": threshold,\n",
    "            \"passed\": similarity >= threshold\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = SemanticEvaluator()\n",
    "    \n",
    "    # These have different words but same meaning\n",
    "    response = \"Paris is the capital of France.\"\n",
    "    reference = \"The capital city of France is Paris.\"\n",
    "    \n",
    "    result = evaluator.evaluate(response, reference)\n",
    "    print(f\"Semantic similarity: {result['score']:.3f}\")\n",
    "    print(f\"Passed: {result['passed']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: llm_judge.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: llm_judge.py\n",
    "# Description: Use an LLM to evaluate agent responses (LLM-as-Judge pattern)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    \"\"\"Use an LLM to evaluate agent responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        # Use a strong model for evaluation\n",
    "        self.llm = ChatOpenAI(model=model, temperature=0)\n",
    "    \n",
    "    def evaluate(\n",
    "        self, \n",
    "        question: str, \n",
    "        response: str, \n",
    "        criteria: str,\n",
    "        reference: str | None = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate a response using LLM judgment.\n",
    "        \n",
    "        Args:\n",
    "            question: The original question/prompt\n",
    "            response: The agent's response to evaluate\n",
    "            criteria: What to evaluate (e.g., \"accuracy\", \"helpfulness\")\n",
    "            reference: Optional reference answer for comparison\n",
    "        \"\"\"\n",
    "        system_prompt = \"\"\"You are an expert evaluator. Your job is to evaluate \n",
    "AI assistant responses based on specific criteria.\n",
    "\n",
    "Be objective and consistent. Provide a score from 1-5 where:\n",
    "1 = Very poor, fails to meet the criteria\n",
    "2 = Poor, partially meets criteria with significant issues  \n",
    "3 = Acceptable, meets basic criteria but has room for improvement\n",
    "4 = Good, meets criteria well with minor issues\n",
    "5 = Excellent, fully meets or exceeds criteria\n",
    "\n",
    "Always respond in this exact format:\n",
    "SCORE: [1-5]\n",
    "REASONING: [Your explanation]\"\"\"\n",
    "\n",
    "        evaluation_prompt = f\"\"\"Evaluate the following response for {criteria}.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RESPONSE: {response}\n",
    "\"\"\"\n",
    "        if reference:\n",
    "            evaluation_prompt += f\"\\nREFERENCE ANSWER: {reference}\\n\"\n",
    "        \n",
    "        evaluation_prompt += f\"\\nEvaluate for: {criteria}\"\n",
    "        \n",
    "        result = self.llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=evaluation_prompt)\n",
    "        ])\n",
    "        \n",
    "        # Parse the response\n",
    "        content = result.content\n",
    "        score = self._extract_score(content)\n",
    "        reasoning = self._extract_reasoning(content)\n",
    "        \n",
    "        return {\n",
    "            \"metric\": f\"llm_judge_{criteria}\",\n",
    "            \"score\": score,\n",
    "            \"normalized_score\": score / 5.0,  # Normalize to 0-1\n",
    "            \"reasoning\": reasoning,\n",
    "            \"passed\": score >= 3\n",
    "        }\n",
    "    \n",
    "    def _extract_score(self, content: str) -> int:\n",
    "        \"\"\"Extract score from LLM response.\"\"\"\n",
    "        for line in content.split('\\n'):\n",
    "            if line.startswith('SCORE:'):\n",
    "                try:\n",
    "                    return int(line.split(':')[1].strip())\n",
    "                except (ValueError, IndexError):\n",
    "                    pass\n",
    "        return 3  # Default to middle score if parsing fails\n",
    "    \n",
    "    def _extract_reasoning(self, content: str) -> str:\n",
    "        \"\"\"Extract reasoning from LLM response.\"\"\"\n",
    "        for i, line in enumerate(content.split('\\n')):\n",
    "            if line.startswith('REASONING:'):\n",
    "                return '\\n'.join(content.split('\\n')[i:]).replace('REASONING:', '').strip()\n",
    "        return content\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    judge = LLMJudge()\n",
    "    \n",
    "    result = judge.evaluate(\n",
    "        question=\"What is the capital of France?\",\n",
    "        response=\"Paris is the capital of France, located on the Seine River.\",\n",
    "        criteria=\"accuracy\",\n",
    "        reference=\"Paris\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Score: {result['score']}/5\")\n",
    "    print(f\"Normalized: {result['normalized_score']:.2f}\")\n",
    "    print(f\"Passed: {result['passed']}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: multi_criteria_evaluator.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: multi_criteria_evaluator.py\n",
    "# Description: Evaluate responses across multiple criteria\n",
    "\n",
    "from llm_judge import LLMJudge\n",
    "from semantic_evaluator import SemanticEvaluator\n",
    "\n",
    "\n",
    "class MultiCriteriaEvaluator:\n",
    "    \"\"\"Evaluate responses across multiple criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.judge = LLMJudge()\n",
    "        self.semantic = SemanticEvaluator()\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        question: str,\n",
    "        response: str,\n",
    "        reference: str | None = None,\n",
    "        criteria: list[str] | None = None\n",
    "    ) -> dict:\n",
    "        \"\"\"Run comprehensive evaluation.\"\"\"\n",
    "        \n",
    "        criteria = criteria or [\"accuracy\", \"helpfulness\", \"clarity\"]\n",
    "        \n",
    "        results = {\n",
    "            \"question\": question,\n",
    "            \"response\": response[:200] + \"...\" if len(response) > 200 else response,\n",
    "            \"evaluations\": {},\n",
    "            \"summary\": {}\n",
    "        }\n",
    "        \n",
    "        # Run LLM judge for each criterion\n",
    "        for criterion in criteria:\n",
    "            eval_result = self.judge.evaluate(\n",
    "                question=question,\n",
    "                response=response,\n",
    "                criteria=criterion,\n",
    "                reference=reference\n",
    "            )\n",
    "            results[\"evaluations\"][criterion] = eval_result\n",
    "        \n",
    "        # Add semantic similarity if reference provided\n",
    "        if reference:\n",
    "            semantic_result = self.semantic.evaluate(response, reference)\n",
    "            results[\"evaluations\"][\"semantic_similarity\"] = semantic_result\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        scores = [\n",
    "            e[\"normalized_score\"] \n",
    "            for e in results[\"evaluations\"].values() \n",
    "            if \"normalized_score\" in e\n",
    "        ]\n",
    "        \n",
    "        results[\"summary\"] = {\n",
    "            \"average_score\": sum(scores) / len(scores) if scores else 0,\n",
    "            \"min_score\": min(scores) if scores else 0,\n",
    "            \"max_score\": max(scores) if scores else 0,\n",
    "            \"all_passed\": all(\n",
    "                e.get(\"passed\", True) \n",
    "                for e in results[\"evaluations\"].values()\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MultiCriteriaEvaluator()\n",
    "    \n",
    "    result = evaluator.evaluate(\n",
    "        question=\"How do I reset my password?\",\n",
    "        response=\"To reset your password, go to Settings > Security > Reset Password. You'll receive an email with a reset link.\",\n",
    "        reference=\"Navigate to Settings, then Security, and click Reset Password.\",\n",
    "        criteria=[\"accuracy\", \"helpfulness\", \"clarity\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Average Score: {result['summary']['average_score']:.2f}\")\n",
    "    print(f\"All Passed: {result['summary']['all_passed']}\")\n",
    "    \n",
    "    for criterion, eval_data in result[\"evaluations\"].items():\n",
    "        print(f\"  {criterion}: {eval_data.get('normalized_score', eval_data.get('score', 'N/A'))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: tool_usage_evaluator.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: tool_usage_evaluator.py\n",
    "# Description: Evaluate how well an agent uses its tools\n",
    "\n",
    "\n",
    "class ToolUsageEvaluator:\n",
    "    \"\"\"Evaluate how well an agent uses its tools.\"\"\"\n",
    "    \n",
    "    def evaluate_tool_selection(\n",
    "        self,\n",
    "        query: str,\n",
    "        tools_used: list[str],\n",
    "        expected_tools: list[str]\n",
    "    ) -> dict:\n",
    "        \"\"\"Evaluate if the agent selected appropriate tools.\"\"\"\n",
    "        used_set = set(tools_used)\n",
    "        expected_set = set(expected_tools)\n",
    "        \n",
    "        correct = used_set & expected_set  # Tools correctly used\n",
    "        missed = expected_set - used_set    # Tools that should have been used\n",
    "        extra = used_set - expected_set     # Tools used unnecessarily\n",
    "        \n",
    "        # Precision: of tools used, how many were correct?\n",
    "        precision = len(correct) / len(used_set) if used_set else 1.0\n",
    "        \n",
    "        # Recall: of tools needed, how many were used?\n",
    "        recall = len(correct) / len(expected_set) if expected_set else 1.0\n",
    "        \n",
    "        # F1 score balances precision and recall\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"metric\": \"tool_selection\",\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"correct_tools\": list(correct),\n",
    "            \"missed_tools\": list(missed),\n",
    "            \"extra_tools\": list(extra),\n",
    "            \"passed\": recall >= 0.8 and precision >= 0.5\n",
    "        }\n",
    "    \n",
    "    def evaluate_tool_arguments(\n",
    "        self,\n",
    "        tool_calls: list[dict],\n",
    "        expected_calls: list[dict]\n",
    "    ) -> dict:\n",
    "        \"\"\"Evaluate if tools were called with correct arguments.\"\"\"\n",
    "        if not expected_calls:\n",
    "            return {\"metric\": \"tool_arguments\", \"score\": 1.0, \"passed\": True}\n",
    "        \n",
    "        correct_count = 0\n",
    "        details = []\n",
    "        \n",
    "        for expected in expected_calls:\n",
    "            # Find matching tool call\n",
    "            matching = [\n",
    "                tc for tc in tool_calls \n",
    "                if tc.get(\"tool\") == expected.get(\"tool\")\n",
    "            ]\n",
    "            \n",
    "            if not matching:\n",
    "                details.append({\n",
    "                    \"tool\": expected[\"tool\"],\n",
    "                    \"status\": \"not_called\",\n",
    "                    \"correct\": False\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Check arguments\n",
    "            actual = matching[0]\n",
    "            args_match = self._compare_arguments(\n",
    "                actual.get(\"arguments\", {}),\n",
    "                expected.get(\"arguments\", {})\n",
    "            )\n",
    "            \n",
    "            if args_match:\n",
    "                correct_count += 1\n",
    "            \n",
    "            details.append({\n",
    "                \"tool\": expected[\"tool\"],\n",
    "                \"status\": \"correct\" if args_match else \"wrong_arguments\",\n",
    "                \"expected_args\": expected.get(\"arguments\"),\n",
    "                \"actual_args\": actual.get(\"arguments\"),\n",
    "                \"correct\": args_match\n",
    "            })\n",
    "        \n",
    "        score = correct_count / len(expected_calls)\n",
    "        \n",
    "        return {\n",
    "            \"metric\": \"tool_arguments\",\n",
    "            \"score\": score,\n",
    "            \"details\": details,\n",
    "            \"passed\": score >= 0.8\n",
    "        }\n",
    "    \n",
    "    def _compare_arguments(self, actual: dict, expected: dict) -> bool:\n",
    "        \"\"\"Compare tool arguments, allowing for minor variations.\"\"\"\n",
    "        for key, expected_value in expected.items():\n",
    "            if key not in actual:\n",
    "                return False\n",
    "            \n",
    "            actual_value = actual[key]\n",
    "            \n",
    "            # String comparison (case-insensitive, stripped)\n",
    "            if isinstance(expected_value, str) and isinstance(actual_value, str):\n",
    "                if expected_value.strip().lower() != actual_value.strip().lower():\n",
    "                    return False\n",
    "            # Numeric comparison (allow small differences)\n",
    "            elif isinstance(expected_value, (int, float)) and isinstance(actual_value, (int, float)):\n",
    "                if abs(expected_value - actual_value) > 0.01:\n",
    "                    return False\n",
    "            # Exact comparison for other types\n",
    "            elif expected_value != actual_value:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = ToolUsageEvaluator()\n",
    "    \n",
    "    # Test tool selection\n",
    "    result = evaluator.evaluate_tool_selection(\n",
    "        query=\"What's the weather in Paris?\",\n",
    "        tools_used=[\"weather_api\", \"location_lookup\"],\n",
    "        expected_tools=[\"weather_api\"]\n",
    "    )\n",
    "    print(f\"Tool Selection: F1={result['f1_score']:.2f}, Passed={result['passed']}\")\n",
    "    \n",
    "    # Test tool arguments\n",
    "    result = evaluator.evaluate_tool_arguments(\n",
    "        tool_calls=[\n",
    "            {\"tool\": \"weather_api\", \"arguments\": {\"city\": \"Paris\", \"units\": \"celsius\"}}\n",
    "        ],\n",
    "        expected_calls=[\n",
    "            {\"tool\": \"weather_api\", \"arguments\": {\"city\": \"Paris\"}}\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Tool Arguments: Score={result['score']:.2f}, Passed={result['passed']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: evaluation_pipeline.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: evaluation_pipeline.py\n",
    "# Description: Run systematic evaluations across test cases\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from multi_criteria_evaluator import MultiCriteriaEvaluator\n",
    "from tool_usage_evaluator import ToolUsageEvaluator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationCase:\n",
    "    \"\"\"A single evaluation test case.\"\"\"\n",
    "    id: str\n",
    "    question: str\n",
    "    expected_response: str | None = None\n",
    "    expected_tools: list[str] | None = None\n",
    "    criteria: list[str] | None = None\n",
    "    metadata: dict | None = None\n",
    "\n",
    "\n",
    "class EvaluationPipeline:\n",
    "    \"\"\"Run systematic evaluations across test cases.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_fn):\n",
    "        \"\"\"\n",
    "        Initialize with a function that takes a question and returns a response.\n",
    "        \n",
    "        agent_fn should return a dict with at least:\n",
    "        - \"response\": str\n",
    "        - \"tools_used\": list[str] (optional)\n",
    "        \"\"\"\n",
    "        self.agent_fn = agent_fn\n",
    "        self.multi_evaluator = MultiCriteriaEvaluator()\n",
    "        self.tool_evaluator = ToolUsageEvaluator()\n",
    "    \n",
    "    def run(self, test_cases: list[EvaluationCase]) -> dict:\n",
    "        \"\"\"Run evaluation on all test cases.\"\"\"\n",
    "        results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_cases\": len(test_cases),\n",
    "            \"cases\": [],\n",
    "            \"aggregate\": {}\n",
    "        }\n",
    "        \n",
    "        all_scores = []\n",
    "        passed_count = 0\n",
    "        \n",
    "        for case in test_cases:\n",
    "            case_result = self._evaluate_case(case)\n",
    "            results[\"cases\"].append(case_result)\n",
    "            \n",
    "            if case_result[\"summary\"][\"all_passed\"]:\n",
    "                passed_count += 1\n",
    "            \n",
    "            all_scores.append(case_result[\"summary\"][\"average_score\"])\n",
    "        \n",
    "        # Aggregate statistics\n",
    "        results[\"aggregate\"] = {\n",
    "            \"pass_rate\": passed_count / len(test_cases) if test_cases else 0,\n",
    "            \"average_score\": sum(all_scores) / len(all_scores) if all_scores else 0,\n",
    "            \"passed_count\": passed_count,\n",
    "            \"failed_count\": len(test_cases) - passed_count\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_case(self, case: EvaluationCase) -> dict:\n",
    "        \"\"\"Evaluate a single test case.\"\"\"\n",
    "        # Get agent response\n",
    "        agent_result = self.agent_fn(case.question)\n",
    "        response = agent_result.get(\"response\", \"\")\n",
    "        tools_used = agent_result.get(\"tools_used\", [])\n",
    "        \n",
    "        # Run multi-criteria evaluation\n",
    "        eval_result = self.multi_evaluator.evaluate(\n",
    "            question=case.question,\n",
    "            response=response,\n",
    "            reference=case.expected_response,\n",
    "            criteria=case.criteria\n",
    "        )\n",
    "        \n",
    "        # Add tool evaluation if expected tools specified\n",
    "        if case.expected_tools:\n",
    "            tool_result = self.tool_evaluator.evaluate_tool_selection(\n",
    "                query=case.question,\n",
    "                tools_used=tools_used,\n",
    "                expected_tools=case.expected_tools\n",
    "            )\n",
    "            eval_result[\"evaluations\"][\"tool_selection\"] = tool_result\n",
    "        \n",
    "        eval_result[\"case_id\"] = case.id\n",
    "        return eval_result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Mock agent function for demonstration\n",
    "    def mock_agent(question: str) -> dict:\n",
    "        return {\n",
    "            \"response\": f\"This is a mock response to: {question}\",\n",
    "            \"tools_used\": [\"search\", \"calculator\"]\n",
    "        }\n",
    "    \n",
    "    # Create test cases\n",
    "    test_cases = [\n",
    "        EvaluationCase(\n",
    "            id=\"test_001\",\n",
    "            question=\"What is 2 + 2?\",\n",
    "            expected_response=\"4\",\n",
    "            expected_tools=[\"calculator\"],\n",
    "            criteria=[\"accuracy\", \"clarity\"]\n",
    "        ),\n",
    "        EvaluationCase(\n",
    "            id=\"test_002\",\n",
    "            question=\"What is the capital of France?\",\n",
    "            expected_response=\"Paris\",\n",
    "            criteria=[\"accuracy\", \"helpfulness\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline = EvaluationPipeline(mock_agent)\n",
    "    results = pipeline.run(test_cases)\n",
    "    \n",
    "    print(f\"Total Cases: {results['total_cases']}\")\n",
    "    print(f\"Pass Rate: {results['aggregate']['pass_rate']:.1%}\")\n",
    "    print(f\"Average Score: {results['aggregate']['average_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: evaluation_tracker.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.3\n",
    "# File: evaluation_tracker.py\n",
    "# Description: Track evaluation results over time\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class EvaluationTracker:\n",
    "    \"\"\"Track evaluation results over time.\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"evaluations.json\"):\n",
    "        self.storage_path = storage_path\n",
    "        self.history = self._load_history()\n",
    "    \n",
    "    def _load_history(self) -> list:\n",
    "        \"\"\"Load evaluation history from storage.\"\"\"\n",
    "        try:\n",
    "            with open(self.storage_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "    \n",
    "    def _save_history(self):\n",
    "        \"\"\"Save evaluation history to storage.\"\"\"\n",
    "        with open(self.storage_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "    \n",
    "    def record(self, evaluation_result: dict, version: str = \"unknown\"):\n",
    "        \"\"\"Record an evaluation result.\"\"\"\n",
    "        record = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"version\": version,\n",
    "            \"aggregate\": evaluation_result.get(\"aggregate\", {}),\n",
    "            \"total_cases\": evaluation_result.get(\"total_cases\", 0)\n",
    "        }\n",
    "        self.history.append(record)\n",
    "        self._save_history()\n",
    "    \n",
    "    def get_trend(self, metric: str = \"average_score\", last_n: int = 10) -> list:\n",
    "        \"\"\"Get trend for a specific metric.\"\"\"\n",
    "        recent = self.history[-last_n:]\n",
    "        return [\n",
    "            {\n",
    "                \"timestamp\": r[\"timestamp\"],\n",
    "                \"version\": r[\"version\"],\n",
    "                \"value\": r[\"aggregate\"].get(metric, 0)\n",
    "            }\n",
    "            for r in recent\n",
    "        ]\n",
    "    \n",
    "    def compare_versions(self, version_a: str, version_b: str) -> dict:\n",
    "        \"\"\"Compare metrics between two versions.\"\"\"\n",
    "        results_a = [r for r in self.history if r[\"version\"] == version_a]\n",
    "        results_b = [r for r in self.history if r[\"version\"] == version_b]\n",
    "        \n",
    "        if not results_a or not results_b:\n",
    "            return {\"error\": \"One or both versions not found\"}\n",
    "        \n",
    "        # Use most recent result for each version\n",
    "        latest_a = results_a[-1][\"aggregate\"]\n",
    "        latest_b = results_b[-1][\"aggregate\"]\n",
    "        \n",
    "        comparison = {}\n",
    "        for metric in latest_a.keys():\n",
    "            if metric in latest_b:\n",
    "                val_a = latest_a[metric]\n",
    "                val_b = latest_b[metric]\n",
    "                if isinstance(val_a, (int, float)) and isinstance(val_b, (int, float)):\n",
    "                    comparison[metric] = {\n",
    "                        \"version_a\": val_a,\n",
    "                        \"version_b\": val_b,\n",
    "                        \"difference\": val_b - val_a,\n",
    "                        \"percent_change\": ((val_b - val_a) / val_a * 100) if val_a != 0 else 0\n",
    "                    }\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tracker = EvaluationTracker(\"test_evaluations.json\")\n",
    "    \n",
    "    # Record some sample evaluations\n",
    "    tracker.record({\n",
    "        \"total_cases\": 10,\n",
    "        \"aggregate\": {\"pass_rate\": 0.7, \"average_score\": 0.65}\n",
    "    }, version=\"v1.0\")\n",
    "    \n",
    "    tracker.record({\n",
    "        \"total_cases\": 10,\n",
    "        \"aggregate\": {\"pass_rate\": 0.8, \"average_score\": 0.75}\n",
    "    }, version=\"v1.1\")\n",
    "    \n",
    "    # Get trend\n",
    "    trend = tracker.get_trend(\"average_score\")\n",
    "    print(\"Score Trend:\")\n",
    "    for point in trend:\n",
    "        print(f\"  {point['version']}: {point['value']:.2f}\")\n",
    "    \n",
    "    # Compare versions\n",
    "    comparison = tracker.compare_versions(\"v1.0\", \"v1.1\")\n",
    "    print(\"\\nVersion Comparison (v1.0 -> v1.1):\")\n",
    "    for metric, data in comparison.items():\n",
    "        print(f\"  {metric}: {data['percent_change']:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 18.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.1: Custom Evaluation Rubric\n",
    "\n",
    "Create an evaluation rubric for a customer service agent that rates responses on four criteria: accuracy (factual correctness), empathy (emotional tone), actionability (does it help solve the problem?), and policy compliance (stays within company guidelines). Build an LLMJudge-based evaluator that scores each criterion and returns detailed feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.2: Comparative Evaluation\n",
    "\n",
    "Build an evaluator that compares two different agent responses to the same question and decides which is better. It should explain its reasoning and output a clear winner (or tie). This is useful for A/B testing different agent versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18.3.3: Evaluation Dashboard\n",
    "\n",
    "Create a simple evaluation reporting function that takes a list of evaluation results and generates a summary report. The report should include: overall pass rate, breakdown by criterion, the worst-performing cases (for debugging), and trend information if historical data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.4: Creating test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: dataset_generators.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.4\n",
    "# File: dataset_generators.py\n",
    "# Description: Tools for generating synthetic test datasets\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def generate_variations(original_query: str, llm, count: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate phrasing variations of a query.\n",
    "    \n",
    "    Takes a single query and generates multiple phrasings that mean the same thing.\n",
    "    Useful for testing that your agent handles different phrasings consistently.\n",
    "    \n",
    "    Args:\n",
    "        original_query: The original user query\n",
    "        llm: A language model to generate variations\n",
    "        count: Number of variations to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of query variations\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate {count} different ways a user might ask this same question.\n",
    "Vary the formality, length, and phrasing while keeping the meaning identical.\n",
    "\n",
    "Original: {original_query}\n",
    "\n",
    "Variations (one per line, numbered):\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse the numbered list from response\n",
    "    variations = []\n",
    "    for line in response.content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 2:\n",
    "            # Remove numbering like \"1.\", \"1)\", \"1:\"\n",
    "            if line[0].isdigit():\n",
    "                # Find where the actual text starts\n",
    "                for i, char in enumerate(line):\n",
    "                    if char in '.):' and i < 3:\n",
    "                        line = line[i+1:].strip()\n",
    "                        break\n",
    "            if line:\n",
    "                variations.append(line)\n",
    "    \n",
    "    return variations[:count]\n",
    "\n",
    "\n",
    "def generate_scenario_cases(scenario_description: str, count: int, llm) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate test cases for a specific scenario.\n",
    "    \n",
    "    Args:\n",
    "        scenario_description: Description of the scenario to generate cases for\n",
    "        count: Number of test cases to generate\n",
    "        llm: A language model to generate cases\n",
    "    \n",
    "    Returns:\n",
    "        List of test case dictionaries with 'query' and 'should_address' keys\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate {count} realistic user queries for this scenario:\n",
    "{scenario_description}\n",
    "\n",
    "For each query, also provide what a correct response should address.\n",
    "\n",
    "Format each as:\n",
    "QUERY: [the user's question]\n",
    "SHOULD_ADDRESS: [key points the response must cover]\n",
    "\n",
    "Generate {count} different cases:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse into structured cases\n",
    "    cases = []\n",
    "    current_query = None\n",
    "    current_should_address = None\n",
    "    \n",
    "    for line in response.content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('QUERY:'):\n",
    "            if current_query and current_should_address:\n",
    "                cases.append({\n",
    "                    'query': current_query,\n",
    "                    'should_address': current_should_address\n",
    "                })\n",
    "            current_query = line[6:].strip()\n",
    "            current_should_address = None\n",
    "        elif line.startswith('SHOULD_ADDRESS:'):\n",
    "            current_should_address = line[15:].strip()\n",
    "    \n",
    "    # Don't forget the last case\n",
    "    if current_query and current_should_address:\n",
    "        cases.append({\n",
    "            'query': current_query,\n",
    "            'should_address': current_should_address\n",
    "        })\n",
    "    \n",
    "    return cases[:count]\n",
    "\n",
    "\n",
    "def generate_edge_cases(llm, edge_type: str = \"ambiguous\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate edge case queries for testing robustness.\n",
    "    \n",
    "    Args:\n",
    "        llm: A language model to generate cases\n",
    "        edge_type: Type of edge case to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of edge case queries\n",
    "    \"\"\"\n",
    "    edge_case_prompts = {\n",
    "        \"ambiguous\": \"Generate 5 queries where the user's intent is ambiguous and could be interpreted multiple ways\",\n",
    "        \"multi_request\": \"Generate 5 queries that combine multiple unrelated requests in a single message\",\n",
    "        \"typos\": \"Generate 5 queries with realistic typos and grammatical errors\",\n",
    "        \"out_of_scope\": \"Generate 5 queries that are just barely outside a typical customer service agent's scope\",\n",
    "        \"boundary\": \"Generate 5 queries that test the boundaries between different categories (e.g., is this a billing question or a technical question?)\",\n",
    "    }\n",
    "    \n",
    "    prompt = edge_case_prompts.get(edge_type, edge_case_prompts[\"ambiguous\"])\n",
    "    prompt += \"\\n\\nList each query on its own line:\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse into list\n",
    "    cases = []\n",
    "    for line in response.content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and len(line) > 10:  # Filter out empty/short lines\n",
    "            # Remove numbering\n",
    "            if line[0].isdigit() and len(line) > 3:\n",
    "                for i, char in enumerate(line):\n",
    "                    if char in '.):' and i < 3:\n",
    "                        line = line[i+1:].strip()\n",
    "                        break\n",
    "            if line:\n",
    "                cases.append(line)\n",
    "    \n",
    "    return cases[:5]\n",
    "\n",
    "\n",
    "def generate_adversarial_cases(llm) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Generate a suite of adversarial test cases.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping adversarial category to list of test queries\n",
    "    \"\"\"\n",
    "    categories = {\n",
    "        \"ambiguity\": \"Generate queries where the correct answer is 'I need more information' because key details are missing\",\n",
    "        \"boundary\": \"Generate queries at the edge of a customer service agent's capabilities - almost but not quite in scope\",\n",
    "        \"consistency\": \"Generate 5 different phrasings of the same factual question to test for consistent answers\",\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for category, prompt in categories.items():\n",
    "        full_prompt = f\"{prompt}\\n\\nList 5 examples, one per line:\"\n",
    "        response = llm.invoke(full_prompt)\n",
    "        \n",
    "        cases = []\n",
    "        for line in response.content.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line and len(line) > 10:\n",
    "                if line[0].isdigit():\n",
    "                    for i, char in enumerate(line):\n",
    "                        if char in '.):' and i < 3:\n",
    "                            line = line[i+1:].strip()\n",
    "                            break\n",
    "                if line:\n",
    "                    cases.append(line)\n",
    "        \n",
    "        results[category] = cases[:5]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "    \n",
    "    # Generate variations\n",
    "    print(\"=== Query Variations ===\")\n",
    "    original = \"How do I reset my password?\"\n",
    "    variations = generate_variations(original, llm)\n",
    "    print(f\"Original: {original}\")\n",
    "    for i, var in enumerate(variations, 1):\n",
    "        print(f\"  {i}. {var}\")\n",
    "    \n",
    "    # Generate scenario cases\n",
    "    print(\"\\n=== Scenario Cases ===\")\n",
    "    scenario = \"A user is frustrated because their order hasn't arrived after 2 weeks\"\n",
    "    cases = generate_scenario_cases(scenario, 3, llm)\n",
    "    for case in cases:\n",
    "        print(f\"Query: {case['query']}\")\n",
    "        print(f\"Should address: {case['should_address']}\\n\")\n",
    "    \n",
    "    # Generate edge cases\n",
    "    print(\"=== Edge Cases (Ambiguous) ===\")\n",
    "    edge_cases = generate_edge_cases(llm, \"ambiguous\")\n",
    "    for case in edge_cases:\n",
    "        print(f\"  - {case}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: dataset_manager.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.4\n",
    "# File: dataset_manager.py\n",
    "# Description: Manage test datasets - load, validate, filter, and analyze\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TestDataset:\n",
    "    \"\"\"Manage a test dataset for agent evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, path: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize dataset, optionally loading from a file.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to JSON dataset file\n",
    "        \"\"\"\n",
    "        self.cases = []\n",
    "        self.metadata = {\n",
    "            \"dataset_version\": \"1.0\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"description\": \"\"\n",
    "        }\n",
    "        \n",
    "        if path:\n",
    "            self.load(path)\n",
    "    \n",
    "    def load(self, path: str) -> None:\n",
    "        \"\"\"Load dataset from JSON file.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.metadata = {k: v for k, v in data.items() if k != 'cases'}\n",
    "        self.cases = data.get('cases', [])\n",
    "        \n",
    "        print(f\"Loaded {len(self.cases)} cases from {path}\")\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save dataset to JSON file.\"\"\"\n",
    "        data = {**self.metadata, \"cases\": self.cases}\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved {len(self.cases)} cases to {path}\")\n",
    "    \n",
    "    def add_case(self, case: dict) -> None:\n",
    "        \"\"\"Add a single test case.\"\"\"\n",
    "        # Ensure required fields\n",
    "        required = ['id', 'query']\n",
    "        for field in required:\n",
    "            if field not in case:\n",
    "                raise ValueError(f\"Case missing required field: {field}\")\n",
    "        \n",
    "        # Check for duplicate IDs\n",
    "        existing_ids = {c['id'] for c in self.cases}\n",
    "        if case['id'] in existing_ids:\n",
    "            raise ValueError(f\"Duplicate case ID: {case['id']}\")\n",
    "        \n",
    "        self.cases.append(case)\n",
    "    \n",
    "    def get_case(self, case_id: str) -> dict | None:\n",
    "        \"\"\"Get a case by ID.\"\"\"\n",
    "        for case in self.cases:\n",
    "            if case['id'] == case_id:\n",
    "                return case\n",
    "        return None\n",
    "    \n",
    "    def filter(\n",
    "        self,\n",
    "        category: str | None = None,\n",
    "        difficulty: str | None = None,\n",
    "        source: str | None = None,\n",
    "        tags: list[str] | None = None\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Filter cases by criteria.\n",
    "        \n",
    "        Args:\n",
    "            category: Filter by category\n",
    "            difficulty: Filter by difficulty (easy, medium, hard)\n",
    "            source: Filter by source (golden, real_user, synthetic, adversarial)\n",
    "            tags: Filter by tags (case must have ALL specified tags)\n",
    "        \n",
    "        Returns:\n",
    "            List of matching cases\n",
    "        \"\"\"\n",
    "        results = self.cases\n",
    "        \n",
    "        if category:\n",
    "            results = [c for c in results if c.get('category') == category]\n",
    "        \n",
    "        if difficulty:\n",
    "            results = [c for c in results if c.get('difficulty') == difficulty]\n",
    "        \n",
    "        if source:\n",
    "            results = [c for c in results if c.get('source') == source]\n",
    "        \n",
    "        if tags:\n",
    "            results = [c for c in results \n",
    "                      if all(t in c.get('tags', []) for t in tags)]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self) -> dict:\n",
    "        \"\"\"Get dataset statistics.\"\"\"\n",
    "        stats = {\n",
    "            \"total_cases\": len(self.cases),\n",
    "            \"by_category\": Counter(c.get('category', 'unknown') for c in self.cases),\n",
    "            \"by_difficulty\": Counter(c.get('difficulty', 'unknown') for c in self.cases),\n",
    "            \"by_source\": Counter(c.get('source', 'unknown') for c in self.cases),\n",
    "            \"all_tags\": Counter(tag for c in self.cases for tag in c.get('tags', []))\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"Print a summary of the dataset.\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Dataset Summary\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Version: {self.metadata.get('dataset_version', 'unknown')}\")\n",
    "        print(f\"Total Cases: {stats['total_cases']}\")\n",
    "        \n",
    "        print(f\"\\nBy Category:\")\n",
    "        for cat, count in stats['by_category'].most_common():\n",
    "            print(f\"  {cat}: {count}\")\n",
    "        \n",
    "        print(f\"\\nBy Difficulty:\")\n",
    "        for diff, count in stats['by_difficulty'].most_common():\n",
    "            print(f\"  {diff}: {count}\")\n",
    "        \n",
    "        print(f\"\\nBy Source:\")\n",
    "        for src, count in stats['by_source'].most_common():\n",
    "            print(f\"  {src}: {count}\")\n",
    "        \n",
    "        print(f\"\\nTop Tags:\")\n",
    "        for tag, count in stats['all_tags'].most_common(5):\n",
    "            print(f\"  {tag}: {count}\")\n",
    "    \n",
    "    def validate(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Validate dataset integrity.\n",
    "        \n",
    "        Returns:\n",
    "            List of validation errors (empty if valid)\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        ids_seen = set()\n",
    "        \n",
    "        for i, case in enumerate(self.cases):\n",
    "            # Check required fields\n",
    "            if 'id' not in case:\n",
    "                errors.append(f\"Case {i}: missing 'id' field\")\n",
    "            elif case['id'] in ids_seen:\n",
    "                errors.append(f\"Case {i}: duplicate id '{case['id']}'\")\n",
    "            else:\n",
    "                ids_seen.add(case['id'])\n",
    "            \n",
    "            if 'query' not in case:\n",
    "                errors.append(f\"Case {case.get('id', i)}: missing 'query' field\")\n",
    "            \n",
    "            # Check expected_response structure\n",
    "            if 'expected_response' in case:\n",
    "                exp = case['expected_response']\n",
    "                if not isinstance(exp, dict):\n",
    "                    errors.append(f\"Case {case.get('id', i)}: 'expected_response' should be a dict\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"Validation found {len(errors)} errors\")\n",
    "        else:\n",
    "            print(\"Dataset validation passed!\")\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def to_evaluation_cases(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Convert to format suitable for evaluation pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            List of cases formatted for EvaluationPipeline\n",
    "        \"\"\"\n",
    "        eval_cases = []\n",
    "        for case in self.cases:\n",
    "            eval_case = {\n",
    "                \"id\": case[\"id\"],\n",
    "                \"question\": case[\"query\"],\n",
    "                \"expected_response\": case.get(\"expected_response\", {}).get(\"reference_answer\"),\n",
    "                \"criteria\": self._infer_criteria(case),\n",
    "                \"metadata\": {\n",
    "                    \"category\": case.get(\"category\"),\n",
    "                    \"difficulty\": case.get(\"difficulty\"),\n",
    "                    \"source\": case.get(\"source\"),\n",
    "                    \"tags\": case.get(\"tags\", [])\n",
    "                }\n",
    "            }\n",
    "            eval_cases.append(eval_case)\n",
    "        return eval_cases\n",
    "    \n",
    "    def _infer_criteria(self, case: dict) -> list[str]:\n",
    "        \"\"\"Infer evaluation criteria from case properties.\"\"\"\n",
    "        criteria = [\"accuracy\", \"helpfulness\"]\n",
    "        \n",
    "        tags = case.get(\"tags\", [])\n",
    "        if \"complaint\" in tags:\n",
    "            criteria.append(\"empathy\")\n",
    "        if \"needs_clarification\" in tags or \"ambiguous\" in tags:\n",
    "            criteria.append(\"appropriateness\")\n",
    "        \n",
    "        return criteria\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the sample dataset\n",
    "    dataset = TestDataset(\"sample_dataset.json\")\n",
    "    \n",
    "    # Print summary\n",
    "    dataset.print_summary()\n",
    "    \n",
    "    # Validate\n",
    "    errors = dataset.validate()\n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    # Filter examples\n",
    "    print(\"\\n=== Adversarial Cases ===\")\n",
    "    adversarial = dataset.filter(source=\"adversarial\")\n",
    "    for case in adversarial:\n",
    "        print(f\"  {case['id']}: {case['query'][:50]}...\")\n",
    "    \n",
    "    print(\"\\n=== Easy Cases ===\")\n",
    "    easy = dataset.filter(difficulty=\"easy\")\n",
    "    for case in easy:\n",
    "        print(f\"  {case['id']}: {case['query'][:50]}...\")\n",
    "    \n",
    "    # Convert to evaluation format\n",
    "    print(\"\\n=== First 3 as Evaluation Cases ===\")\n",
    "    eval_cases = dataset.to_evaluation_cases()[:3]\n",
    "    for ec in eval_cases:\n",
    "        print(f\"  {ec['id']}: criteria={ec['criteria']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.5: Measuring accuracy and reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: accuracy_calculator.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.5\n",
    "# File: accuracy_calculator.py\n",
    "# Description: Calculate accuracy metrics with severity weighting and category breakdown\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Result of a single evaluation.\"\"\"\n",
    "    case_id: str\n",
    "    passed: bool\n",
    "    category: str\n",
    "    severity: str = \"minor\"  # critical, major, minor, trivial\n",
    "    score: float = 0.0  # 0.0 to 1.0\n",
    "\n",
    "\n",
    "class AccuracyCalculator:\n",
    "    \"\"\"\n",
    "    Calculate accuracy metrics with severity weighting and category breakdown.\n",
    "    \n",
    "    Implements concepts from Section 18.5:\n",
    "    - Weighted accuracy by error severity\n",
    "    - Category-specific breakdowns\n",
    "    - Confidence intervals\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default severity weights (higher = more impactful)\n",
    "    DEFAULT_WEIGHTS = {\n",
    "        \"critical\": 10.0,\n",
    "        \"major\": 5.0,\n",
    "        \"minor\": 2.0,\n",
    "        \"trivial\": 1.0\n",
    "    }\n",
    "    \n",
    "    def __init__(self, severity_weights: dict | None = None):\n",
    "        \"\"\"\n",
    "        Initialize calculator.\n",
    "        \n",
    "        Args:\n",
    "            severity_weights: Custom weights for severity levels\n",
    "        \"\"\"\n",
    "        self.weights = severity_weights or self.DEFAULT_WEIGHTS\n",
    "        self.results: list[EvaluationResult] = []\n",
    "    \n",
    "    def add_result(self, result: EvaluationResult) -> None:\n",
    "        \"\"\"Add an evaluation result.\"\"\"\n",
    "        self.results.append(result)\n",
    "    \n",
    "    def add_results(self, results: list[EvaluationResult]) -> None:\n",
    "        \"\"\"Add multiple evaluation results.\"\"\"\n",
    "        self.results.extend(results)\n",
    "    \n",
    "    def overall_accuracy(self) -> float:\n",
    "        \"\"\"Calculate simple overall accuracy (pass rate).\"\"\"\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        passed = sum(1 for r in self.results if r.passed)\n",
    "        return passed / len(self.results)\n",
    "    \n",
    "    def weighted_accuracy(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate weighted accuracy considering error severity.\n",
    "        \n",
    "        Errors with higher severity have more impact on the score.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return 0.0\n",
    "        \n",
    "        total_weight = 0.0\n",
    "        weighted_pass = 0.0\n",
    "        \n",
    "        for r in self.results:\n",
    "            weight = self.weights.get(r.severity, 1.0)\n",
    "            total_weight += weight\n",
    "            if r.passed:\n",
    "                weighted_pass += weight\n",
    "        \n",
    "        return weighted_pass / total_weight if total_weight > 0 else 0.0\n",
    "    \n",
    "    def accuracy_by_category(self) -> dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Break down accuracy by category.\n",
    "        \n",
    "        Returns dict with category -> {accuracy, count, passed, failed}\n",
    "        \"\"\"\n",
    "        categories = defaultdict(lambda: {\"passed\": 0, \"failed\": 0})\n",
    "        \n",
    "        for r in self.results:\n",
    "            if r.passed:\n",
    "                categories[r.category][\"passed\"] += 1\n",
    "            else:\n",
    "                categories[r.category][\"failed\"] += 1\n",
    "        \n",
    "        breakdown = {}\n",
    "        for cat, counts in categories.items():\n",
    "            total = counts[\"passed\"] + counts[\"failed\"]\n",
    "            breakdown[cat] = {\n",
    "                \"accuracy\": counts[\"passed\"] / total if total > 0 else 0.0,\n",
    "                \"count\": total,\n",
    "                \"passed\": counts[\"passed\"],\n",
    "                \"failed\": counts[\"failed\"]\n",
    "            }\n",
    "        \n",
    "        return breakdown\n",
    "    \n",
    "    def accuracy_by_severity(self) -> dict[str, dict]:\n",
    "        \"\"\"Break down accuracy by error severity level.\"\"\"\n",
    "        severities = defaultdict(lambda: {\"passed\": 0, \"failed\": 0})\n",
    "        \n",
    "        for r in self.results:\n",
    "            if r.passed:\n",
    "                severities[r.severity][\"passed\"] += 1\n",
    "            else:\n",
    "                severities[r.severity][\"failed\"] += 1\n",
    "        \n",
    "        breakdown = {}\n",
    "        for sev, counts in severities.items():\n",
    "            total = counts[\"passed\"] + counts[\"failed\"]\n",
    "            breakdown[sev] = {\n",
    "                \"accuracy\": counts[\"passed\"] / total if total > 0 else 0.0,\n",
    "                \"count\": total,\n",
    "                \"error_rate\": counts[\"failed\"] / total if total > 0 else 0.0\n",
    "            }\n",
    "        \n",
    "        return breakdown\n",
    "    \n",
    "    def critical_error_rate(self) -> float:\n",
    "        \"\"\"Calculate the rate of critical errors specifically.\"\"\"\n",
    "        critical_results = [r for r in self.results if r.severity == \"critical\"]\n",
    "        if not critical_results:\n",
    "            return 0.0\n",
    "        failed = sum(1 for r in critical_results if not r.passed)\n",
    "        return failed / len(critical_results)\n",
    "    \n",
    "    def confidence_interval(self, confidence: float = 0.95) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate confidence interval for overall accuracy.\n",
    "        \n",
    "        Uses normal approximation to binomial.\n",
    "        \n",
    "        Args:\n",
    "            confidence: Confidence level (default 0.95 for 95% CI)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (lower_bound, upper_bound)\n",
    "        \"\"\"\n",
    "        n = len(self.results)\n",
    "        if n == 0:\n",
    "            return (0.0, 0.0)\n",
    "        \n",
    "        p = self.overall_accuracy()\n",
    "        \n",
    "        # Z-score for confidence level\n",
    "        z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "        z = z_scores.get(confidence, 1.96)\n",
    "        \n",
    "        # Standard error\n",
    "        se = math.sqrt(p * (1 - p) / n)\n",
    "        \n",
    "        # Confidence interval\n",
    "        margin = z * se\n",
    "        lower = max(0.0, p - margin)\n",
    "        upper = min(1.0, p + margin)\n",
    "        \n",
    "        return (lower, upper)\n",
    "    \n",
    "    def summary(self) -> dict:\n",
    "        \"\"\"Generate a complete summary of accuracy metrics.\"\"\"\n",
    "        ci_lower, ci_upper = self.confidence_interval()\n",
    "        \n",
    "        return {\n",
    "            \"total_cases\": len(self.results),\n",
    "            \"overall_accuracy\": self.overall_accuracy(),\n",
    "            \"weighted_accuracy\": self.weighted_accuracy(),\n",
    "            \"confidence_interval_95\": {\n",
    "                \"lower\": ci_lower,\n",
    "                \"upper\": ci_upper\n",
    "            },\n",
    "            \"critical_error_rate\": self.critical_error_rate(),\n",
    "            \"by_category\": self.accuracy_by_category(),\n",
    "            \"by_severity\": self.accuracy_by_severity()\n",
    "        }\n",
    "    \n",
    "    def print_report(self) -> None:\n",
    "        \"\"\"Print a formatted accuracy report.\"\"\"\n",
    "        summary = self.summary()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ACCURACY REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nTotal Cases: {summary['total_cases']}\")\n",
    "        print(f\"Overall Accuracy: {summary['overall_accuracy']:.1%}\")\n",
    "        print(f\"Weighted Accuracy: {summary['weighted_accuracy']:.1%}\")\n",
    "        \n",
    "        ci = summary['confidence_interval_95']\n",
    "        print(f\"95% Confidence Interval: [{ci['lower']:.1%}, {ci['upper']:.1%}]\")\n",
    "        \n",
    "        print(f\"\\nCritical Error Rate: {summary['critical_error_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\n--- By Category ---\")\n",
    "        for cat, stats in summary['by_category'].items():\n",
    "            print(f\"  {cat}: {stats['accuracy']:.1%} \"\n",
    "                  f\"({stats['passed']}/{stats['count']})\")\n",
    "        \n",
    "        print(\"\\n--- By Severity ---\")\n",
    "        for sev in [\"critical\", \"major\", \"minor\", \"trivial\"]:\n",
    "            if sev in summary['by_severity']:\n",
    "                stats = summary['by_severity'][sev]\n",
    "                print(f\"  {sev}: {stats['accuracy']:.1%} accuracy, \"\n",
    "                      f\"{stats['error_rate']:.1%} error rate \"\n",
    "                      f\"(n={stats['count']})\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    calc = AccuracyCalculator()\n",
    "    \n",
    "    # Simulate evaluation results\n",
    "    results = [\n",
    "        # Password reset - mostly passes\n",
    "        EvaluationResult(\"pr_001\", True, \"password_reset\", \"minor\"),\n",
    "        EvaluationResult(\"pr_002\", True, \"password_reset\", \"minor\"),\n",
    "        EvaluationResult(\"pr_003\", True, \"password_reset\", \"minor\"),\n",
    "        EvaluationResult(\"pr_004\", False, \"password_reset\", \"minor\"),\n",
    "        \n",
    "        # Order status - good performance\n",
    "        EvaluationResult(\"os_001\", True, \"order_status\", \"major\"),\n",
    "        EvaluationResult(\"os_002\", True, \"order_status\", \"major\"),\n",
    "        EvaluationResult(\"os_003\", True, \"order_status\", \"major\"),\n",
    "        EvaluationResult(\"os_004\", True, \"order_status\", \"major\"),\n",
    "        EvaluationResult(\"os_005\", False, \"order_status\", \"major\"),\n",
    "        \n",
    "        # Refunds - problematic area\n",
    "        EvaluationResult(\"rf_001\", True, \"refunds\", \"critical\"),\n",
    "        EvaluationResult(\"rf_002\", False, \"refunds\", \"critical\"),\n",
    "        EvaluationResult(\"rf_003\", False, \"refunds\", \"critical\"),\n",
    "        EvaluationResult(\"rf_004\", True, \"refunds\", \"critical\"),\n",
    "        \n",
    "        # Technical - decent\n",
    "        EvaluationResult(\"tech_001\", True, \"technical\", \"major\"),\n",
    "        EvaluationResult(\"tech_002\", True, \"technical\", \"major\"),\n",
    "        EvaluationResult(\"tech_003\", False, \"technical\", \"major\"),\n",
    "    ]\n",
    "    \n",
    "    calc.add_results(results)\n",
    "    calc.print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: reliability_metrics.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.5\n",
    "# File: reliability_metrics.py\n",
    "# Description: Measure reliability through consistency, multiple runs, and statistical comparison\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class RunResult:\n",
    "    \"\"\"Result of a single run of a test case.\"\"\"\n",
    "    case_id: str\n",
    "    run_number: int\n",
    "    passed: bool\n",
    "    score: float = 0.0\n",
    "\n",
    "\n",
    "class ReliabilityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze reliability through consistency and multiple runs.\n",
    "    \n",
    "    Implements concepts from Section 18.5:\n",
    "    - Multiple runs per test case\n",
    "    - Consistency measurement\n",
    "    - Statistical significance testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.runs: dict[str, list[RunResult]] = defaultdict(list)\n",
    "    \n",
    "    def add_run(self, result: RunResult) -> None:\n",
    "        \"\"\"Add a run result for a test case.\"\"\"\n",
    "        self.runs[result.case_id].append(result)\n",
    "    \n",
    "    def add_runs(self, results: list[RunResult]) -> None:\n",
    "        \"\"\"Add multiple run results.\"\"\"\n",
    "        for result in results:\n",
    "            self.add_run(result)\n",
    "    \n",
    "    def consistency_score(self, case_id: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate consistency for a specific test case.\n",
    "        \n",
    "        Returns 1.0 if all runs have same outcome, 0.0 if split 50/50.\n",
    "        \"\"\"\n",
    "        runs = self.runs.get(case_id, [])\n",
    "        if len(runs) < 2:\n",
    "            return 1.0  # Can't measure consistency with < 2 runs\n",
    "        \n",
    "        pass_count = sum(1 for r in runs if r.passed)\n",
    "        pass_rate = pass_count / len(runs)\n",
    "        \n",
    "        # Consistency is distance from 50/50\n",
    "        return abs(pass_rate - 0.5) * 2\n",
    "    \n",
    "    def overall_consistency(self) -> float:\n",
    "        \"\"\"Calculate average consistency across all test cases.\"\"\"\n",
    "        if not self.runs:\n",
    "            return 0.0\n",
    "        \n",
    "        consistencies = [self.consistency_score(case_id) \n",
    "                        for case_id in self.runs.keys()]\n",
    "        return sum(consistencies) / len(consistencies)\n",
    "    \n",
    "    def pass_rate_per_case(self) -> dict[str, float]:\n",
    "        \"\"\"Calculate pass rate for each test case across runs.\"\"\"\n",
    "        rates = {}\n",
    "        for case_id, runs in self.runs.items():\n",
    "            if runs:\n",
    "                rates[case_id] = sum(1 for r in runs if r.passed) / len(runs)\n",
    "        return rates\n",
    "    \n",
    "    def aggregate_pass_rate(self) -> float:\n",
    "        \"\"\"Calculate overall pass rate across all runs.\"\"\"\n",
    "        all_runs = [r for runs in self.runs.values() for r in runs]\n",
    "        if not all_runs:\n",
    "            return 0.0\n",
    "        return sum(1 for r in all_runs if r.passed) / len(all_runs)\n",
    "    \n",
    "    def worst_case_accuracy(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate accuracy using worst run for each case.\n",
    "        \n",
    "        Conservative estimate - if any run fails, case counts as failed.\n",
    "        \"\"\"\n",
    "        if not self.runs:\n",
    "            return 0.0\n",
    "        \n",
    "        worst_pass = 0\n",
    "        for case_id, runs in self.runs.items():\n",
    "            # Case passes only if ALL runs pass\n",
    "            if all(r.passed for r in runs):\n",
    "                worst_pass += 1\n",
    "        \n",
    "        return worst_pass / len(self.runs)\n",
    "    \n",
    "    def best_case_accuracy(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate accuracy using best run for each case.\n",
    "        \n",
    "        Optimistic estimate - if any run passes, case counts as passed.\n",
    "        \"\"\"\n",
    "        if not self.runs:\n",
    "            return 0.0\n",
    "        \n",
    "        best_pass = 0\n",
    "        for case_id, runs in self.runs.items():\n",
    "            # Case passes if ANY run passes\n",
    "            if any(r.passed for r in runs):\n",
    "                best_pass += 1\n",
    "        \n",
    "        return best_pass / len(self.runs)\n",
    "    \n",
    "    def flaky_cases(self, threshold: float = 0.8) -> list[str]:\n",
    "        \"\"\"\n",
    "        Identify flaky test cases (inconsistent results).\n",
    "        \n",
    "        Args:\n",
    "            threshold: Consistency below this is considered flaky\n",
    "        \n",
    "        Returns:\n",
    "            List of case IDs that are flaky\n",
    "        \"\"\"\n",
    "        flaky = []\n",
    "        for case_id in self.runs.keys():\n",
    "            if self.consistency_score(case_id) < threshold:\n",
    "                flaky.append(case_id)\n",
    "        return flaky\n",
    "    \n",
    "    def summary(self) -> dict:\n",
    "        \"\"\"Generate reliability summary.\"\"\"\n",
    "        return {\n",
    "            \"total_cases\": len(self.runs),\n",
    "            \"total_runs\": sum(len(runs) for runs in self.runs.values()),\n",
    "            \"aggregate_pass_rate\": self.aggregate_pass_rate(),\n",
    "            \"worst_case_accuracy\": self.worst_case_accuracy(),\n",
    "            \"best_case_accuracy\": self.best_case_accuracy(),\n",
    "            \"overall_consistency\": self.overall_consistency(),\n",
    "            \"flaky_cases\": self.flaky_cases()\n",
    "        }\n",
    "    \n",
    "    def print_report(self) -> None:\n",
    "        \"\"\"Print a formatted reliability report.\"\"\"\n",
    "        summary = self.summary()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RELIABILITY REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nTest Cases: {summary['total_cases']}\")\n",
    "        print(f\"Total Runs: {summary['total_runs']}\")\n",
    "        avg_runs = summary['total_runs'] / summary['total_cases'] if summary['total_cases'] > 0 else 0\n",
    "        print(f\"Average Runs per Case: {avg_runs:.1f}\")\n",
    "        \n",
    "        print(f\"\\nAggregate Pass Rate: {summary['aggregate_pass_rate']:.1%}\")\n",
    "        print(f\"Worst-Case Accuracy: {summary['worst_case_accuracy']:.1%}\")\n",
    "        print(f\"Best-Case Accuracy: {summary['best_case_accuracy']:.1%}\")\n",
    "        print(f\"Overall Consistency: {summary['overall_consistency']:.1%}\")\n",
    "        \n",
    "        if summary['flaky_cases']:\n",
    "            print(f\"\\n  Flaky Cases ({len(summary['flaky_cases'])}):\")\n",
    "            for case_id in summary['flaky_cases'][:5]:  # Show first 5\n",
    "                rates = self.pass_rate_per_case()\n",
    "                print(f\"    {case_id}: {rates[case_id]:.0%} pass rate\")\n",
    "            if len(summary['flaky_cases']) > 5:\n",
    "                print(f\"    ... and {len(summary['flaky_cases']) - 5} more\")\n",
    "        else:\n",
    "            print(\"\\n No flaky cases detected\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def compare_versions(\n",
    "    version_a_results: list[tuple[str, bool]],  # (case_id, passed)\n",
    "    version_b_results: list[tuple[str, bool]],\n",
    "    alpha: float = 0.05\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare two versions using statistical significance testing.\n",
    "    \n",
    "    Uses chi-squared test for comparing pass rates.\n",
    "    \n",
    "    Args:\n",
    "        version_a_results: List of (case_id, passed) for version A\n",
    "        version_b_results: List of (case_id, passed) for version B\n",
    "        alpha: Significance level (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "        Comparison statistics and significance assessment\n",
    "    \"\"\"\n",
    "    a_passed = sum(1 for _, p in version_a_results if p)\n",
    "    a_total = len(version_a_results)\n",
    "    b_passed = sum(1 for _, p in version_b_results if p)\n",
    "    b_total = len(version_b_results)\n",
    "    \n",
    "    a_rate = a_passed / a_total if a_total > 0 else 0\n",
    "    b_rate = b_passed / b_total if b_total > 0 else 0\n",
    "    \n",
    "    # Pooled proportion for chi-squared test\n",
    "    pooled = (a_passed + b_passed) / (a_total + b_total) if (a_total + b_total) > 0 else 0\n",
    "    \n",
    "    # Standard error\n",
    "    se = math.sqrt(pooled * (1 - pooled) * (1/a_total + 1/b_total)) if a_total > 0 and b_total > 0 else 0\n",
    "    \n",
    "    # Z-score\n",
    "    z_score = (b_rate - a_rate) / se if se > 0 else 0\n",
    "    \n",
    "    # Two-tailed p-value approximation (using standard normal)\n",
    "    # For simplicity, we'll just check against critical values\n",
    "    z_critical = {0.10: 1.645, 0.05: 1.96, 0.01: 2.576}\n",
    "    \n",
    "    is_significant = abs(z_score) > z_critical.get(alpha, 1.96)\n",
    "    \n",
    "    return {\n",
    "        \"version_a\": {\n",
    "            \"pass_rate\": a_rate,\n",
    "            \"passed\": a_passed,\n",
    "            \"total\": a_total\n",
    "        },\n",
    "        \"version_b\": {\n",
    "            \"pass_rate\": b_rate,\n",
    "            \"passed\": b_passed,\n",
    "            \"total\": b_total\n",
    "        },\n",
    "        \"difference\": b_rate - a_rate,\n",
    "        \"percent_change\": ((b_rate - a_rate) / a_rate * 100) if a_rate > 0 else 0,\n",
    "        \"z_score\": z_score,\n",
    "        \"is_significant\": is_significant,\n",
    "        \"alpha\": alpha,\n",
    "        \"conclusion\": (\n",
    "            f\"Version B is {'significantly ' if is_significant else ''}{'better' if b_rate > a_rate else 'worse'} \"\n",
    "            f\"({'p < {}'.format(alpha) if is_significant else 'not statistically significant'})\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Reliability analysis with multiple runs\n",
    "    print(\"=== Reliability Analysis ===\")\n",
    "    analyzer = ReliabilityAnalyzer()\n",
    "    \n",
    "    # Simulate multiple runs for several test cases\n",
    "    test_runs = [\n",
    "        # Consistent case - always passes\n",
    "        RunResult(\"case_001\", 1, True),\n",
    "        RunResult(\"case_001\", 2, True),\n",
    "        RunResult(\"case_001\", 3, True),\n",
    "        \n",
    "        # Consistent case - always fails  \n",
    "        RunResult(\"case_002\", 1, False),\n",
    "        RunResult(\"case_002\", 2, False),\n",
    "        RunResult(\"case_002\", 3, False),\n",
    "        \n",
    "        # Flaky case - inconsistent\n",
    "        RunResult(\"case_003\", 1, True),\n",
    "        RunResult(\"case_003\", 2, False),\n",
    "        RunResult(\"case_003\", 3, True),\n",
    "        \n",
    "        # Mostly passes\n",
    "        RunResult(\"case_004\", 1, True),\n",
    "        RunResult(\"case_004\", 2, True),\n",
    "        RunResult(\"case_004\", 3, False),\n",
    "        \n",
    "        # Very flaky\n",
    "        RunResult(\"case_005\", 1, True),\n",
    "        RunResult(\"case_005\", 2, False),\n",
    "        RunResult(\"case_005\", 3, False),\n",
    "    ]\n",
    "    \n",
    "    analyzer.add_runs(test_runs)\n",
    "    analyzer.print_report()\n",
    "    \n",
    "    # Example 2: Version comparison\n",
    "    print(\"\\n=== Version Comparison ===\")\n",
    "    \n",
    "    # Simulated results from two versions\n",
    "    version_a = [\n",
    "        (\"test_1\", True), (\"test_2\", True), (\"test_3\", False),\n",
    "        (\"test_4\", True), (\"test_5\", False), (\"test_6\", True),\n",
    "        (\"test_7\", False), (\"test_8\", True), (\"test_9\", True),\n",
    "        (\"test_10\", False),\n",
    "    ] * 10  # 100 cases total\n",
    "    \n",
    "    version_b = [\n",
    "        (\"test_1\", True), (\"test_2\", True), (\"test_3\", True),\n",
    "        (\"test_4\", True), (\"test_5\", False), (\"test_6\", True),\n",
    "        (\"test_7\", True), (\"test_8\", True), (\"test_9\", True),\n",
    "        (\"test_10\", False),\n",
    "    ] * 10  # 100 cases total\n",
    "    \n",
    "    comparison = compare_versions(version_a, version_b)\n",
    "    \n",
    "    print(f\"Version A: {comparison['version_a']['pass_rate']:.1%}\")\n",
    "    print(f\"Version B: {comparison['version_b']['pass_rate']:.1%}\")\n",
    "    print(f\"Difference: {comparison['difference']:+.1%}\")\n",
    "    print(f\"Z-score: {comparison['z_score']:.2f}\")\n",
    "    print(f\"Conclusion: {comparison['conclusion']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.6: A/B testing agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ab_testing.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.6\n",
    "# File: ab_testing.py\n",
    "# Description: A/B testing framework for comparing agent variants\n",
    "\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Result from a single experiment trial.\"\"\"\n",
    "    variant: str\n",
    "    success: bool\n",
    "    user_id: str | None = None\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for an A/B experiment.\"\"\"\n",
    "    name: str\n",
    "    hypothesis: str\n",
    "    primary_metric: str\n",
    "    variants: list[str] = field(default_factory=lambda: [\"A\", \"B\"])\n",
    "    target_sample_size: int = 500  # Per variant\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"hypothesis\": self.hypothesis,\n",
    "            \"primary_metric\": self.primary_metric,\n",
    "            \"variants\": self.variants,\n",
    "            \"target_sample_size\": self.target_sample_size\n",
    "        }\n",
    "\n",
    "\n",
    "def get_variant(user_id: str, experiment_name: str, variants: list[str] = [\"A\", \"B\"]) -> str:\n",
    "    \"\"\"\n",
    "    Consistently assign a user to a variant.\n",
    "    Same user always gets the same variant for a given experiment.\n",
    "    \n",
    "    Args:\n",
    "        user_id: Unique identifier for the user\n",
    "        experiment_name: Name of the experiment\n",
    "        variants: List of variant names to distribute across\n",
    "    \n",
    "    Returns:\n",
    "        The assigned variant name\n",
    "    \"\"\"\n",
    "    # Create a hash of user_id + experiment_name\n",
    "    hash_input = f\"{user_id}:{experiment_name}\"\n",
    "    hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)\n",
    "    \n",
    "    # Map to variant\n",
    "    variant_index = hash_value % len(variants)\n",
    "    return variants[variant_index]\n",
    "\n",
    "\n",
    "def compare_variants_simple(\n",
    "    a_successes: int, \n",
    "    a_total: int, \n",
    "    b_successes: int, \n",
    "    b_total: int,\n",
    "    alpha: float = 0.05\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare two variants using a simple proportion test.\n",
    "    \n",
    "    This is a simplified version that doesn't require scipy.\n",
    "    Uses normal approximation to the binomial.\n",
    "    \n",
    "    Args:\n",
    "        a_successes: Number of successes in variant A\n",
    "        a_total: Total trials in variant A\n",
    "        b_successes: Number of successes in variant B\n",
    "        b_total: Total trials in variant B\n",
    "        alpha: Significance level (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    a_rate = a_successes / a_total if a_total > 0 else 0\n",
    "    b_rate = b_successes / b_total if b_total > 0 else 0\n",
    "    \n",
    "    # Pooled proportion\n",
    "    pooled = (a_successes + b_successes) / (a_total + b_total) if (a_total + b_total) > 0 else 0\n",
    "    \n",
    "    # Standard error\n",
    "    se = math.sqrt(pooled * (1 - pooled) * (1/a_total + 1/b_total)) if a_total > 0 and b_total > 0 and pooled > 0 and pooled < 1 else 0\n",
    "    \n",
    "    # Z-score\n",
    "    z_score = (b_rate - a_rate) / se if se > 0 else 0\n",
    "    \n",
    "    # Critical value for two-tailed test\n",
    "    z_critical = {0.10: 1.645, 0.05: 1.96, 0.01: 2.576}.get(alpha, 1.96)\n",
    "    \n",
    "    significant = abs(z_score) > z_critical\n",
    "    \n",
    "    # Determine recommendation\n",
    "    if significant:\n",
    "        if b_rate > a_rate:\n",
    "            recommendation = \"B is better - consider rolling out\"\n",
    "        else:\n",
    "            recommendation = \"A is better - keep current version\"\n",
    "    else:\n",
    "        recommendation = \"No significant difference - need more data or keep A\"\n",
    "    \n",
    "    return {\n",
    "        \"a_rate\": a_rate,\n",
    "        \"a_rate_formatted\": f\"{a_rate:.1%}\",\n",
    "        \"b_rate\": b_rate,\n",
    "        \"b_rate_formatted\": f\"{b_rate:.1%}\",\n",
    "        \"difference\": b_rate - a_rate,\n",
    "        \"difference_formatted\": f\"{(b_rate - a_rate):+.1%}\",\n",
    "        \"relative_improvement\": ((b_rate - a_rate) / a_rate * 100) if a_rate > 0 else 0,\n",
    "        \"z_score\": z_score,\n",
    "        \"significant\": significant,\n",
    "        \"alpha\": alpha,\n",
    "        \"recommendation\": recommendation\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_variants_scipy(\n",
    "    a_successes: int, \n",
    "    a_total: int, \n",
    "    b_successes: int, \n",
    "    b_total: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare two variants using chi-squared test (requires scipy).\n",
    "    \n",
    "    Returns whether the difference is statistically significant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from scipy import stats\n",
    "    except ImportError:\n",
    "        print(\"scipy not available, using simple comparison\")\n",
    "        return compare_variants_simple(a_successes, a_total, b_successes, b_total)\n",
    "    \n",
    "    # Build contingency table\n",
    "    table = [\n",
    "        [a_successes, a_total - a_successes],\n",
    "        [b_successes, b_total - b_successes]\n",
    "    ]\n",
    "    \n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(table)\n",
    "    \n",
    "    a_rate = a_successes / a_total\n",
    "    b_rate = b_successes / b_total\n",
    "    \n",
    "    return {\n",
    "        \"a_rate\": f\"{a_rate:.1%}\",\n",
    "        \"b_rate\": f\"{b_rate:.1%}\",\n",
    "        \"difference\": f\"{(b_rate - a_rate):+.1%}\",\n",
    "        \"chi2\": chi2,\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < 0.05,\n",
    "        \"recommendation\": (\n",
    "            \"B is better\" if (p_value < 0.05 and b_rate > a_rate) \n",
    "            else \"A is better\" if (p_value < 0.05 and a_rate > b_rate)\n",
    "            else \"No significant difference\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "class ABExperiment:\n",
    "    \"\"\"\n",
    "    Manage an A/B testing experiment.\n",
    "    \n",
    "    Handles:\n",
    "    - Variant assignment\n",
    "    - Result collection\n",
    "    - Statistical analysis\n",
    "    - Result persistence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.results: list[ExperimentResult] = []\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def assign_variant(self, user_id: str) -> str:\n",
    "        \"\"\"Assign a user to a variant.\"\"\"\n",
    "        return get_variant(user_id, self.config.name, self.config.variants)\n",
    "    \n",
    "    def record_result(self, result: ExperimentResult) -> None:\n",
    "        \"\"\"Record a result from the experiment.\"\"\"\n",
    "        self.results.append(result)\n",
    "    \n",
    "    def record(self, variant: str, success: bool, user_id: str | None = None, **metadata) -> None:\n",
    "        \"\"\"Convenience method to record a result.\"\"\"\n",
    "        self.record_result(ExperimentResult(\n",
    "            variant=variant,\n",
    "            success=success,\n",
    "            user_id=user_id,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    def get_counts(self) -> dict[str, dict]:\n",
    "        \"\"\"Get success/failure counts per variant.\"\"\"\n",
    "        counts = {v: {\"successes\": 0, \"failures\": 0, \"total\": 0} \n",
    "                  for v in self.config.variants}\n",
    "        \n",
    "        for result in self.results:\n",
    "            if result.variant in counts:\n",
    "                counts[result.variant][\"total\"] += 1\n",
    "                if result.success:\n",
    "                    counts[result.variant][\"successes\"] += 1\n",
    "                else:\n",
    "                    counts[result.variant][\"failures\"] += 1\n",
    "        \n",
    "        return counts\n",
    "    \n",
    "    def progress(self) -> dict:\n",
    "        \"\"\"Check experiment progress toward target sample size.\"\"\"\n",
    "        counts = self.get_counts()\n",
    "        \n",
    "        progress = {}\n",
    "        for variant, data in counts.items():\n",
    "            progress[variant] = {\n",
    "                \"current\": data[\"total\"],\n",
    "                \"target\": self.config.target_sample_size,\n",
    "                \"percent_complete\": data[\"total\"] / self.config.target_sample_size * 100\n",
    "            }\n",
    "        \n",
    "        all_complete = all(\n",
    "            data[\"total\"] >= self.config.target_sample_size \n",
    "            for data in counts.values()\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"by_variant\": progress,\n",
    "            \"complete\": all_complete,\n",
    "            \"total_results\": len(self.results)\n",
    "        }\n",
    "    \n",
    "    def analyze(self) -> dict:\n",
    "        \"\"\"Analyze experiment results.\"\"\"\n",
    "        counts = self.get_counts()\n",
    "        \n",
    "        # For two variants, do direct comparison\n",
    "        if len(self.config.variants) == 2:\n",
    "            v_a, v_b = self.config.variants\n",
    "            comparison = compare_variants_simple(\n",
    "                counts[v_a][\"successes\"], counts[v_a][\"total\"],\n",
    "                counts[v_b][\"successes\"], counts[v_b][\"total\"]\n",
    "            )\n",
    "        else:\n",
    "            comparison = None\n",
    "        \n",
    "        # Calculate rates for each variant\n",
    "        rates = {}\n",
    "        for variant, data in counts.items():\n",
    "            rate = data[\"successes\"] / data[\"total\"] if data[\"total\"] > 0 else 0\n",
    "            rates[variant] = {\n",
    "                \"rate\": rate,\n",
    "                \"rate_formatted\": f\"{rate:.1%}\",\n",
    "                \"successes\": data[\"successes\"],\n",
    "                \"total\": data[\"total\"]\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"experiment\": self.config.name,\n",
    "            \"hypothesis\": self.config.hypothesis,\n",
    "            \"primary_metric\": self.config.primary_metric,\n",
    "            \"rates\": rates,\n",
    "            \"comparison\": comparison,\n",
    "            \"progress\": self.progress()\n",
    "        }\n",
    "    \n",
    "    def print_report(self) -> None:\n",
    "        \"\"\"Print a formatted experiment report.\"\"\"\n",
    "        analysis = self.analyze()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"A/B EXPERIMENT: {analysis['experiment']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\nHypothesis: {analysis['hypothesis']}\")\n",
    "        print(f\"Primary Metric: {analysis['primary_metric']}\")\n",
    "        \n",
    "        print(\"\\n--- Results by Variant ---\")\n",
    "        for variant, data in analysis['rates'].items():\n",
    "            bar_len = int(data['rate'] * 30)\n",
    "            bar = \"\" * bar_len + \"\" * (30 - bar_len)\n",
    "            print(f\"  {variant}: {data['rate_formatted']} [{bar}]\")\n",
    "            print(f\"      ({data['successes']}/{data['total']} successes)\")\n",
    "        \n",
    "        if analysis['comparison']:\n",
    "            comp = analysis['comparison']\n",
    "            print(\"\\n--- Statistical Analysis ---\")\n",
    "            print(f\"  Difference: {comp['difference_formatted']}\")\n",
    "            print(f\"  Relative improvement: {comp['relative_improvement']:+.1f}%\")\n",
    "            print(f\"  Z-score: {comp['z_score']:.2f}\")\n",
    "            print(f\"  Significant (={comp['alpha']}): {'Yes ' if comp['significant'] else 'No'}\")\n",
    "            print(f\"\\n   Recommendation: {comp['recommendation']}\")\n",
    "        \n",
    "        prog = analysis['progress']\n",
    "        print(\"\\n--- Progress ---\")\n",
    "        for variant, data in prog['by_variant'].items():\n",
    "            print(f\"  {variant}: {data['current']}/{data['target']} ({data['percent_complete']:.0f}%)\")\n",
    "        \n",
    "        status = \" Complete\" if prog['complete'] else \" In Progress\"\n",
    "        print(f\"\\n  Status: {status}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save experiment state to JSON.\"\"\"\n",
    "        data = {\n",
    "            \"config\": self.config.to_dict(),\n",
    "            \"start_time\": self.start_time.isoformat(),\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"variant\": r.variant,\n",
    "                    \"success\": r.success,\n",
    "                    \"user_id\": r.user_id,\n",
    "                    \"timestamp\": r.timestamp,\n",
    "                    \"metadata\": r.metadata\n",
    "                }\n",
    "                for r in self.results\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"Experiment saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"ABExperiment\":\n",
    "        \"\"\"Load experiment from JSON.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        config = ExperimentConfig(**data[\"config\"])\n",
    "        experiment = cls(config)\n",
    "        experiment.start_time = datetime.fromisoformat(data[\"start_time\"])\n",
    "        \n",
    "        for r in data[\"results\"]:\n",
    "            experiment.results.append(ExperimentResult(\n",
    "                variant=r[\"variant\"],\n",
    "                success=r[\"success\"],\n",
    "                user_id=r[\"user_id\"],\n",
    "                timestamp=r[\"timestamp\"],\n",
    "                metadata=r[\"metadata\"]\n",
    "            ))\n",
    "        \n",
    "        return experiment\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    \n",
    "    # Create an experiment\n",
    "    config = ExperimentConfig(\n",
    "        name=\"prompt_v2_test\",\n",
    "        hypothesis=\"A more detailed system prompt will increase task completion from 75% to 85%\",\n",
    "        primary_metric=\"task_completion_rate\",\n",
    "        target_sample_size=100  # Small for demo\n",
    "    )\n",
    "    \n",
    "    experiment = ABExperiment(config)\n",
    "    \n",
    "    # Simulate running the experiment\n",
    "    print(\"Simulating experiment...\")\n",
    "    \n",
    "    # Simulate 200 users\n",
    "    for i in range(200):\n",
    "        user_id = f\"user_{i}\"\n",
    "        variant = experiment.assign_variant(user_id)\n",
    "        \n",
    "        # Simulate different success rates\n",
    "        # A has 75% success, B has 85% success\n",
    "        if variant == \"A\":\n",
    "            success = random.random() < 0.75\n",
    "        else:\n",
    "            success = random.random() < 0.85\n",
    "        \n",
    "        experiment.record(variant=variant, success=success, user_id=user_id)\n",
    "    \n",
    "    # Print results\n",
    "    experiment.print_report()\n",
    "    \n",
    "    # Demonstrate variant assignment consistency\n",
    "    print(\"\\n--- Variant Assignment Demo ---\")\n",
    "    test_user = \"user_12345\"\n",
    "    for _ in range(5):\n",
    "        v = get_variant(test_user, \"prompt_v2_test\")\n",
    "        print(f\"  {test_user} -> {v}\")\n",
    "    print(\"  (Same user always gets same variant)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 18.7: Continuous improvement workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: continuous_improvement.py\n",
    "\n",
    "# From: AI Agents Book, Chapter 18, Section 18.7\n",
    "# File: continuous_improvement.py\n",
    "# Description: Tools for continuous improvement workflows - scheduled evaluation, monitoring, reporting\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationRun:\n",
    "    \"\"\"Record of a single evaluation run.\"\"\"\n",
    "    timestamp: str\n",
    "    version: str\n",
    "    overall_score: float\n",
    "    pass_rate: float\n",
    "    total_cases: int\n",
    "    by_category: dict = field(default_factory=dict)\n",
    "    failures: list = field(default_factory=list)\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class QualityTracker:\n",
    "    \"\"\"\n",
    "    Track quality metrics over time for continuous improvement.\n",
    "    \n",
    "    Implements the measurement and analysis phases of the improvement cycle.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"quality_history.json\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.history: list[EvaluationRun] = []\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self) -> None:\n",
    "        \"\"\"Load history from storage.\"\"\"\n",
    "        if self.storage_path.exists():\n",
    "            with open(self.storage_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.history = [EvaluationRun(**run) for run in data]\n",
    "    \n",
    "    def _save(self) -> None:\n",
    "        \"\"\"Save history to storage.\"\"\"\n",
    "        data = [\n",
    "            {\n",
    "                \"timestamp\": run.timestamp,\n",
    "                \"version\": run.version,\n",
    "                \"overall_score\": run.overall_score,\n",
    "                \"pass_rate\": run.pass_rate,\n",
    "                \"total_cases\": run.total_cases,\n",
    "                \"by_category\": run.by_category,\n",
    "                \"failures\": run.failures,\n",
    "                \"metadata\": run.metadata\n",
    "            }\n",
    "            for run in self.history\n",
    "        ]\n",
    "        with open(self.storage_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def record_run(self, run: EvaluationRun) -> None:\n",
    "        \"\"\"Record an evaluation run.\"\"\"\n",
    "        self.history.append(run)\n",
    "        self._save()\n",
    "    \n",
    "    def record(\n",
    "        self,\n",
    "        version: str,\n",
    "        overall_score: float,\n",
    "        pass_rate: float,\n",
    "        total_cases: int,\n",
    "        by_category: dict | None = None,\n",
    "        failures: list | None = None,\n",
    "        **metadata\n",
    "    ) -> EvaluationRun:\n",
    "        \"\"\"Convenience method to record a run.\"\"\"\n",
    "        run = EvaluationRun(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            version=version,\n",
    "            overall_score=overall_score,\n",
    "            pass_rate=pass_rate,\n",
    "            total_cases=total_cases,\n",
    "            by_category=by_category or {},\n",
    "            failures=failures or [],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        self.record_run(run)\n",
    "        return run\n",
    "    \n",
    "    def get_latest(self) -> EvaluationRun | None:\n",
    "        \"\"\"Get the most recent evaluation run.\"\"\"\n",
    "        return self.history[-1] if self.history else None\n",
    "    \n",
    "    def get_trend(self, metric: str = \"pass_rate\", days: int = 30) -> list[tuple[str, float]]:\n",
    "        \"\"\"Get trend data for a metric over time.\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        trend = []\n",
    "        for run in self.history:\n",
    "            run_time = datetime.fromisoformat(run.timestamp)\n",
    "            if run_time >= cutoff:\n",
    "                value = getattr(run, metric, None)\n",
    "                if value is not None:\n",
    "                    trend.append((run.timestamp[:10], value))\n",
    "        \n",
    "        return trend\n",
    "    \n",
    "    def compare_to_baseline(self, baseline_version: str) -> dict | None:\n",
    "        \"\"\"Compare latest results to a baseline version.\"\"\"\n",
    "        # Find baseline\n",
    "        baseline = None\n",
    "        for run in self.history:\n",
    "            if run.version == baseline_version:\n",
    "                baseline = run\n",
    "                break\n",
    "        \n",
    "        if not baseline:\n",
    "            return None\n",
    "        \n",
    "        latest = self.get_latest()\n",
    "        if not latest:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"baseline\": {\n",
    "                \"version\": baseline.version,\n",
    "                \"pass_rate\": baseline.pass_rate,\n",
    "                \"overall_score\": baseline.overall_score\n",
    "            },\n",
    "            \"current\": {\n",
    "                \"version\": latest.version,\n",
    "                \"pass_rate\": latest.pass_rate,\n",
    "                \"overall_score\": latest.overall_score\n",
    "            },\n",
    "            \"change\": {\n",
    "                \"pass_rate\": latest.pass_rate - baseline.pass_rate,\n",
    "                \"overall_score\": latest.overall_score - baseline.overall_score\n",
    "            },\n",
    "            \"improved\": latest.pass_rate > baseline.pass_rate\n",
    "        }\n",
    "    \n",
    "    def detect_regression(self, threshold: float = 0.05) -> dict | None:\n",
    "        \"\"\"\n",
    "        Detect if recent performance has regressed.\n",
    "        \n",
    "        Args:\n",
    "            threshold: Minimum drop to consider a regression\n",
    "        \n",
    "        Returns:\n",
    "            Regression info if detected, None otherwise\n",
    "        \"\"\"\n",
    "        if len(self.history) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Compare last run to average of previous 5\n",
    "        recent = self.history[-1]\n",
    "        previous = self.history[-6:-1] if len(self.history) >= 6 else self.history[:-1]\n",
    "        \n",
    "        avg_pass_rate = sum(r.pass_rate for r in previous) / len(previous)\n",
    "        \n",
    "        drop = avg_pass_rate - recent.pass_rate\n",
    "        \n",
    "        if drop > threshold:\n",
    "            return {\n",
    "                \"detected\": True,\n",
    "                \"current_pass_rate\": recent.pass_rate,\n",
    "                \"average_pass_rate\": avg_pass_rate,\n",
    "                \"drop\": drop,\n",
    "                \"version\": recent.version,\n",
    "                \"message\": f\" Regression detected: {drop:.1%} drop from recent average\"\n",
    "            }\n",
    "        \n",
    "        return {\"detected\": False}\n",
    "\n",
    "\n",
    "class QualityReporter:\n",
    "    \"\"\"Generate quality reports from tracking data.\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker: QualityTracker):\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def weekly_report(self) -> str:\n",
    "        \"\"\"Generate a weekly quality report.\"\"\"\n",
    "        # Get runs from last 7 days\n",
    "        cutoff = datetime.now() - timedelta(days=7)\n",
    "        recent_runs = [\n",
    "            run for run in self.tracker.history\n",
    "            if datetime.fromisoformat(run.timestamp) >= cutoff\n",
    "        ]\n",
    "        \n",
    "        if not recent_runs:\n",
    "            return \"No evaluation runs in the past week.\"\n",
    "        \n",
    "        # Calculate statistics\n",
    "        pass_rates = [r.pass_rate for r in recent_runs]\n",
    "        scores = [r.overall_score for r in recent_runs]\n",
    "        \n",
    "        avg_pass_rate = sum(pass_rates) / len(pass_rates)\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        latest = recent_runs[-1]\n",
    "        \n",
    "        # Collect all failures\n",
    "        all_failures = []\n",
    "        for run in recent_runs:\n",
    "            all_failures.extend(run.failures)\n",
    "        \n",
    "        # Count failure frequency\n",
    "        failure_counts = defaultdict(int)\n",
    "        for failure in all_failures:\n",
    "            failure_id = failure.get('case_id', str(failure))\n",
    "            failure_counts[failure_id] += 1\n",
    "        \n",
    "        # Build report\n",
    "        lines = [\n",
    "            \"=\" * 60,\n",
    "            \"WEEKLY QUALITY REPORT\",\n",
    "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "            \"=\" * 60,\n",
    "            \"\",\n",
    "            \" SUMMARY\",\n",
    "            f\"  Evaluation runs: {len(recent_runs)}\",\n",
    "            f\"  Average pass rate: {avg_pass_rate:.1%}\",\n",
    "            f\"  Average score: {avg_score:.2f}\",\n",
    "            \"\",\n",
    "            \" LATEST RUN\",\n",
    "            f\"  Version: {latest.version}\",\n",
    "            f\"  Pass rate: {latest.pass_rate:.1%}\",\n",
    "            f\"  Score: {latest.overall_score:.2f}\",\n",
    "            f\"  Cases: {latest.total_cases}\",\n",
    "        ]\n",
    "        \n",
    "        # Category breakdown\n",
    "        if latest.by_category:\n",
    "            lines.append(\"\")\n",
    "            lines.append(\" BY CATEGORY\")\n",
    "            for cat, stats in latest.by_category.items():\n",
    "                rate = stats.get('pass_rate', stats.get('accuracy', 0))\n",
    "                lines.append(f\"  {cat}: {rate:.1%}\")\n",
    "        \n",
    "        # Top failures\n",
    "        if failure_counts:\n",
    "            lines.append(\"\")\n",
    "            lines.append(\" FREQUENT FAILURES\")\n",
    "            top_failures = sorted(failure_counts.items(), key=lambda x: -x[1])[:5]\n",
    "            for failure_id, count in top_failures:\n",
    "                lines.append(f\"  {failure_id}: {count} times\")\n",
    "        \n",
    "        # Trend\n",
    "        trend = self.tracker.get_trend(\"pass_rate\", days=7)\n",
    "        if len(trend) >= 2:\n",
    "            first_rate = trend[0][1]\n",
    "            last_rate = trend[-1][1]\n",
    "            change = last_rate - first_rate\n",
    "            \n",
    "            lines.append(\"\")\n",
    "            lines.append(\" TREND\")\n",
    "            if change > 0.01:\n",
    "                lines.append(f\"   Improving: +{change:.1%} over the week\")\n",
    "            elif change < -0.01:\n",
    "                lines.append(f\"   Declining: {change:.1%} over the week\")\n",
    "            else:\n",
    "                lines.append(f\"   Stable: {change:+.1%} over the week\")\n",
    "        \n",
    "        # Regression check\n",
    "        regression = self.tracker.detect_regression()\n",
    "        if regression and regression.get(\"detected\"):\n",
    "            lines.append(\"\")\n",
    "            lines.append(\" ALERT\")\n",
    "            lines.append(f\"  {regression['message']}\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def improvement_priorities(self) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Identify improvement priorities based on recent data.\n",
    "        \n",
    "        Returns list sorted by estimated impact.\n",
    "        \"\"\"\n",
    "        latest = self.tracker.get_latest()\n",
    "        if not latest:\n",
    "            return []\n",
    "        \n",
    "        priorities = []\n",
    "        \n",
    "        # Analyze category performance\n",
    "        for cat, stats in latest.by_category.items():\n",
    "            rate = stats.get('pass_rate', stats.get('accuracy', 0))\n",
    "            count = stats.get('count', stats.get('total', 0))\n",
    "            \n",
    "            # Impact = volume * failure rate\n",
    "            failure_rate = 1 - rate\n",
    "            impact = count * failure_rate\n",
    "            \n",
    "            if failure_rate > 0.1:  # Only flag categories with >10% failure\n",
    "                priorities.append({\n",
    "                    \"category\": cat,\n",
    "                    \"pass_rate\": rate,\n",
    "                    \"failure_rate\": failure_rate,\n",
    "                    \"volume\": count,\n",
    "                    \"estimated_impact\": impact,\n",
    "                    \"recommendation\": f\"Improve {cat} handling (currently {rate:.0%} pass rate)\"\n",
    "                })\n",
    "        \n",
    "        # Sort by impact\n",
    "        priorities.sort(key=lambda x: -x[\"estimated_impact\"])\n",
    "        \n",
    "        return priorities\n",
    "\n",
    "\n",
    "class ScheduledEvaluator:\n",
    "    \"\"\"\n",
    "    Run evaluations on a schedule.\n",
    "    \n",
    "    In production, this would be triggered by cron or a task scheduler.\n",
    "    This class provides the logic for what to run.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tracker: QualityTracker,\n",
    "        evaluate_fn: Callable[[], dict],\n",
    "        version: str = \"current\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tracker: QualityTracker to record results\n",
    "            evaluate_fn: Function that runs evaluation and returns results dict\n",
    "                        Expected format: {\"pass_rate\": float, \"overall_score\": float, \n",
    "                                         \"total_cases\": int, \"by_category\": dict, \"failures\": list}\n",
    "            version: Version identifier for this agent\n",
    "        \"\"\"\n",
    "        self.tracker = tracker\n",
    "        self.evaluate_fn = evaluate_fn\n",
    "        self.version = version\n",
    "    \n",
    "    def run_evaluation(self) -> EvaluationRun:\n",
    "        \"\"\"Run evaluation and record results.\"\"\"\n",
    "        print(f\" Running evaluation for version {self.version}...\")\n",
    "        \n",
    "        results = self.evaluate_fn()\n",
    "        \n",
    "        run = self.tracker.record(\n",
    "            version=self.version,\n",
    "            overall_score=results.get(\"overall_score\", 0),\n",
    "            pass_rate=results.get(\"pass_rate\", 0),\n",
    "            total_cases=results.get(\"total_cases\", 0),\n",
    "            by_category=results.get(\"by_category\", {}),\n",
    "            failures=results.get(\"failures\", [])\n",
    "        )\n",
    "        \n",
    "        print(f\" Evaluation complete: {run.pass_rate:.1%} pass rate\")\n",
    "        \n",
    "        # Check for regression\n",
    "        regression = self.tracker.detect_regression()\n",
    "        if regression and regression.get(\"detected\"):\n",
    "            print(f\" {regression['message']}\")\n",
    "        \n",
    "        return run\n",
    "    \n",
    "    def should_run(self, schedule: str = \"daily\") -> bool:\n",
    "        \"\"\"\n",
    "        Check if evaluation should run based on schedule.\n",
    "        \n",
    "        Args:\n",
    "            schedule: \"daily\", \"weekly\", or \"hourly\"\n",
    "        \"\"\"\n",
    "        latest = self.tracker.get_latest()\n",
    "        if not latest:\n",
    "            return True  # Never run before\n",
    "        \n",
    "        last_run = datetime.fromisoformat(latest.timestamp)\n",
    "        now = datetime.now()\n",
    "        \n",
    "        if schedule == \"hourly\":\n",
    "            return (now - last_run) >= timedelta(hours=1)\n",
    "        elif schedule == \"daily\":\n",
    "            return (now - last_run) >= timedelta(days=1)\n",
    "        elif schedule == \"weekly\":\n",
    "            return (now - last_run) >= timedelta(weeks=1)\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = QualityTracker(\"demo_quality_history.json\")\n",
    "    \n",
    "    # Simulate some historical evaluation runs\n",
    "    print(\"Simulating evaluation history...\")\n",
    "    \n",
    "    categories = [\"password_reset\", \"order_status\", \"refunds\", \"technical\"]\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Simulate gradually improving performance\n",
    "        base_rate = 0.75 + (i * 0.015) + random.uniform(-0.03, 0.03)\n",
    "        \n",
    "        by_category = {}\n",
    "        for cat in categories:\n",
    "            cat_rate = base_rate + random.uniform(-0.1, 0.1)\n",
    "            cat_rate = max(0.5, min(1.0, cat_rate))\n",
    "            by_category[cat] = {\n",
    "                \"pass_rate\": cat_rate,\n",
    "                \"count\": random.randint(20, 50)\n",
    "            }\n",
    "        \n",
    "        failures = []\n",
    "        if random.random() > base_rate:\n",
    "            failures.append({\"case_id\": f\"case_{random.randint(1, 100)}\"})\n",
    "        \n",
    "        tracker.record(\n",
    "            version=f\"v1.{i}\",\n",
    "            overall_score=base_rate * 5,  # Score out of 5\n",
    "            pass_rate=base_rate,\n",
    "            total_cases=100,\n",
    "            by_category=by_category,\n",
    "            failures=failures\n",
    "        )\n",
    "    \n",
    "    # Generate reports\n",
    "    reporter = QualityReporter(tracker)\n",
    "    \n",
    "    print(\"\\n\" + reporter.weekly_report())\n",
    "    \n",
    "    # Show improvement priorities\n",
    "    print(\"\\n IMPROVEMENT PRIORITIES\")\n",
    "    print(\"-\" * 40)\n",
    "    priorities = reporter.improvement_priorities()\n",
    "    for i, p in enumerate(priorities[:3], 1):\n",
    "        print(f\"{i}. {p['recommendation']}\")\n",
    "        print(f\"   Impact score: {p['estimated_impact']:.1f}\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    print(\"\\n COMPARISON TO BASELINE\")\n",
    "    print(\"-\" * 40)\n",
    "    comparison = tracker.compare_to_baseline(\"v1.0\")\n",
    "    if comparison:\n",
    "        print(f\"Baseline (v1.0): {comparison['baseline']['pass_rate']:.1%}\")\n",
    "        print(f\"Current: {comparison['current']['pass_rate']:.1%}\")\n",
    "        print(f\"Change: {comparison['change']['pass_rate']:+.1%}\")\n",
    "        status = \" Improved!\" if comparison['improved'] else \" Declined\"\n",
    "        print(f\"Status: {status}\")\n",
    "    \n",
    "    # Clean up demo file\n",
    "    Path(\"demo_quality_history.json\").unlink(missing_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_18_testing_solutions.ipynb**\n",
    "- Proceed to **Chapter 19**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}