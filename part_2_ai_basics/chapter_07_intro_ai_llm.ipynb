{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Introduction to AI and Large Language Models\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- What is artificial intelligence?\n",
    "- Understanding Large Language Models (LLMs)\n",
    "- How LLMs work (conceptual overview)\n",
    "- Introduction to prompts and completions\n",
    "- Common LLM providers (OpenAI, Anthropic, Google, open-source)\n",
    "- API keys and authentication\n",
    "- Costs and rate limiting considerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.1: What is artificial intelligence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ai_vs_traditional.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.1\n",
    "# File: ai_vs_traditional.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates the difference between traditional programming and AI approaches\n",
    "to spam detection.\n",
    "\"\"\"\n",
    "\n",
    "# Traditional Programming: You define the rules\n",
    "def is_spam_traditional(email):\n",
    "    \"\"\"\n",
    "    Traditional approach to spam detection using predefined rules.\n",
    "    You explicitly program what words indicate spam.\n",
    "    \"\"\"\n",
    "    spam_words = ['winner', 'free', 'click here', 'urgent']\n",
    "    for word in spam_words:\n",
    "        if word.lower() in email.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# AI Approach: The system learns the patterns\n",
    "# (This is a conceptual example - we'll build real AI models in later chapters!)\n",
    "def is_spam_ai(email, ai_model=None):\n",
    "    \"\"\"\n",
    "    AI approach to spam detection using a trained model.\n",
    "    The model learns patterns from thousands of examples.\n",
    "    \n",
    "    Note: This is a simplified example. In real applications,\n",
    "    the model would be loaded from a trained neural network or\n",
    "    machine learning model.\n",
    "    \"\"\"\n",
    "    if ai_model is None:\n",
    "        # Placeholder for when we don't have a real model yet\n",
    "        # In practice, you'd load a trained model here\n",
    "        return 0.5  # Return neutral probability\n",
    "    \n",
    "    # An AI model would have learned from thousands of examples\n",
    "    # what makes an email spam, finding patterns we might miss\n",
    "    probability = ai_model.predict(email)  # Returns 0.0 to 1.0\n",
    "    return probability > 0.5\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_emails = [\n",
    "        \"Congratulations! You're a WINNER! Click here for your FREE prize!\",\n",
    "        \"Meeting rescheduled to 3 PM tomorrow\",\n",
    "        \"URGENT: Your account needs verification\",\n",
    "        \"Can you review the attached proposal?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Traditional Programming Approach:\")\n",
    "    print(\"-\" * 40)\n",
    "    for email in test_emails:\n",
    "        result = is_spam_traditional(email)\n",
    "        print(f\"Email: {email[:50]}...\")\n",
    "        print(f\"Spam: {result}\\n\")\n",
    "    \n",
    "    print(\"\\nAI Approach (conceptual):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"With a trained AI model, the system would analyze:\")\n",
    "    print(\"- Writing style patterns\")\n",
    "    print(\"- Sender reputation\")\n",
    "    print(\"- Context and semantics\")\n",
    "    print(\"- Thousands of subtle features humans might miss\")\n",
    "    print(\"\\nResult: Much more accurate spam detection!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: narrow_ai_example.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.1  \n",
    "# File: narrow_ai_example.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates Narrow AI - systems that excel at specific tasks\n",
    "but can't generalize to other domains.\n",
    "\"\"\"\n",
    "\n",
    "class ChessAI:\n",
    "    \"\"\"\n",
    "    Example of Narrow AI - Amazing at chess, useless at everything else.\n",
    "    This illustrates how current AI systems are specialized tools,\n",
    "    not general-purpose intelligence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"DeepChess\"\n",
    "        self.specialty = \"Chess\"\n",
    "    \n",
    "    def find_best_move(self, board):\n",
    "        \"\"\"\n",
    "        In a real chess AI, this would analyze millions of positions.\n",
    "        Can beat world champions at chess!\n",
    "        \"\"\"\n",
    "        # Simplified for demonstration\n",
    "        # Real chess AI would use minimax, alpha-beta pruning, \n",
    "        # neural networks, etc.\n",
    "        return \"e2-e4\"  # Classic opening move\n",
    "    \n",
    "    def write_poetry(self):\n",
    "        \"\"\"\n",
    "        This chess AI can't write poetry - it only knows chess.\n",
    "        This is the limitation of narrow AI.\n",
    "        \"\"\"\n",
    "        return \"Error: I only know chess. Poetry is outside my domain.\"\n",
    "    \n",
    "    def translate_language(self, text):\n",
    "        \"\"\"Another task this narrow AI can't do.\"\"\"\n",
    "        return \"Error: I only know chess. Translation is outside my domain.\"\n",
    "    \n",
    "    def diagnose_illness(self, symptoms):\n",
    "        \"\"\"Yet another task beyond this narrow AI.\"\"\"\n",
    "        return \"Error: I only know chess. Medical diagnosis is outside my domain.\"\n",
    "    \n",
    "    def demonstrate_narrow_ai(self):\n",
    "        \"\"\"Shows both the strength and limitation of narrow AI.\"\"\"\n",
    "        print(f\"Hi, I'm {self.name}, a Narrow AI specialized in {self.specialty}\")\n",
    "        print(\"\\nWhat I CAN do:\")\n",
    "        print(f\"\u2713 Chess move: {self.find_best_move('starting_position')}\")\n",
    "        print(\"\u2713 Analyze millions of chess positions per second\")\n",
    "        print(\"\u2713 Beat world chess champions\")\n",
    "        \n",
    "        print(\"\\nWhat I CAN'T do:\")\n",
    "        print(f\"\u2717 Write poetry: {self.write_poetry()}\")\n",
    "        print(f\"\u2717 Translate: {self.translate_language('Hello')}\")\n",
    "        print(f\"\u2717 Medical diagnosis: {self.diagnose_illness('headache')}\")\n",
    "        \n",
    "        print(\"\\nThis is Narrow AI: Superhuman at one thing, helpless at everything else!\")\n",
    "\n",
    "\n",
    "# Example of how different narrow AIs specialize\n",
    "class TranslationAI:\n",
    "    \"\"\"Another narrow AI, but for translation.\"\"\"\n",
    "    \n",
    "    def translate(self, text, target_language):\n",
    "        # Simplified - real translation AI uses complex neural networks\n",
    "        translations = {\n",
    "            'Hello': {'spanish': 'Hola', 'french': 'Bonjour'},\n",
    "            'Thank you': {'spanish': 'Gracias', 'french': 'Merci'}\n",
    "        }\n",
    "        return translations.get(text, {}).get(target_language, \"Unknown\")\n",
    "    \n",
    "    def play_chess(self):\n",
    "        return \"Error: I only know translation. Chess is outside my domain.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate narrow AI limitations\n",
    "    chess_ai = ChessAI()\n",
    "    chess_ai.demonstrate_narrow_ai()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Key Concept: Every AI today is Narrow AI\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\u2022 Excel at specific tasks\")\n",
    "    print(\"\u2022 Can't generalize to other domains\")\n",
    "    print(\"\u2022 Multiple narrow AIs needed for different tasks\")\n",
    "    print(\"\u2022 AGI (Artificial General Intelligence) doesn't exist yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: rule_based_vs_ml.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.1\n",
    "# File: rule_based_vs_ml.py\n",
    "\n",
    "\"\"\"\n",
    "Compares rule-based systems (traditional AI) with machine learning approaches\n",
    "using a plant problem diagnosis example.\n",
    "\"\"\"\n",
    "\n",
    "# Rule-Based System (Old School AI)\n",
    "def diagnose_plant_problem_rules(symptoms):\n",
    "    \"\"\"\n",
    "    Traditional rule-based approach to plant diagnosis.\n",
    "    Every decision is explicitly programmed.\n",
    "    \n",
    "    Problems with this approach:\n",
    "    - Requires extensive manual rule creation\n",
    "    - Can't handle cases not explicitly programmed\n",
    "    - Doesn't improve with experience\n",
    "    - Misses subtle pattern combinations\n",
    "    \"\"\"\n",
    "    # Convert symptoms to lowercase for comparison\n",
    "    symptoms_lower = [s.lower() for s in symptoms]\n",
    "    \n",
    "    # Explicitly programmed decision tree\n",
    "    if \"yellow_leaves\" in symptoms_lower:\n",
    "        if \"brown_tips\" in symptoms_lower:\n",
    "            return \"Overwatering - reduce watering frequency\"\n",
    "        elif \"pale_green\" in symptoms_lower:\n",
    "            return \"Iron deficiency - add iron supplement\"\n",
    "        else:\n",
    "            return \"Nitrogen deficiency - add fertilizer\"\n",
    "    \n",
    "    elif \"brown_spots\" in symptoms_lower:\n",
    "        if \"fuzzy_growth\" in symptoms_lower:\n",
    "            return \"Fungal infection - apply fungicide\"\n",
    "        else:\n",
    "            return \"Bacterial infection - remove affected leaves\"\n",
    "    \n",
    "    elif \"wilting\" in symptoms_lower:\n",
    "        if \"dry_soil\" in symptoms_lower:\n",
    "            return \"Underwatering - increase water\"\n",
    "        else:\n",
    "            return \"Root rot - check drainage\"\n",
    "    \n",
    "    elif \"holes_in_leaves\" in symptoms_lower:\n",
    "        return \"Pest damage - inspect for insects\"\n",
    "    \n",
    "    else:\n",
    "        return \"Unknown problem - consult expert\"\n",
    "\n",
    "\n",
    "# Machine Learning Approach (Modern AI) - Conceptual\n",
    "class PlantDiagnosisML:\n",
    "    \"\"\"\n",
    "    Machine Learning approach to plant diagnosis.\n",
    "    The system learns patterns from thousands of examples.\n",
    "    \n",
    "    In reality, this would use:\n",
    "    - Computer vision to analyze plant images\n",
    "    - Neural networks trained on plant disease databases\n",
    "    - Pattern recognition across multiple features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In a real system, we'd load a trained model here\n",
    "        self.model_trained = False\n",
    "        self.training_examples = 0\n",
    "    \n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Simulates training on plant examples.\n",
    "        Real ML would use algorithms like:\n",
    "        - Convolutional Neural Networks for image analysis\n",
    "        - Random Forests for symptom classification\n",
    "        - Deep Learning for complex pattern recognition\n",
    "        \"\"\"\n",
    "        self.training_examples += len(examples)\n",
    "        self.model_trained = True\n",
    "        print(f\"Model trained on {self.training_examples} plant examples\")\n",
    "        \n",
    "        # A real model would learn patterns like:\n",
    "        # - Yellow + drooping often means overwatering\n",
    "        # - Spots with rings usually indicate fungal issues\n",
    "        # - Certain patterns appear together in specific diseases\n",
    "    \n",
    "    def diagnose(self, plant_image_or_symptoms):\n",
    "        \"\"\"\n",
    "        ML diagnosis based on learned patterns.\n",
    "        \n",
    "        Real advantages over rules:\n",
    "        - Handles cases never explicitly programmed\n",
    "        - Identifies subtle patterns humans miss\n",
    "        - Improves with more data\n",
    "        - Can consider hundreds of features simultaneously\n",
    "        \"\"\"\n",
    "        if not self.model_trained:\n",
    "            return \"Model needs training first\"\n",
    "        \n",
    "        # Simplified demonstration\n",
    "        # Real ML would process image pixels, extract features,\n",
    "        # and run through neural network layers\n",
    "        \n",
    "        return {\n",
    "            'diagnosis': 'Likely fungal infection (confidence: 87%)',\n",
    "            'alternative': 'Possible overwatering (confidence: 12%)',\n",
    "            'recommendation': 'Reduce humidity, improve air circulation',\n",
    "            'learned_from': f'{self.training_examples} similar cases'\n",
    "        }\n",
    "\n",
    "\n",
    "def demonstrate_approaches():\n",
    "    \"\"\"Shows the difference between rule-based and ML approaches.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RULE-BASED vs MACHINE LEARNING PLANT DIAGNOSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test case\n",
    "    symptoms = [\"yellow_leaves\", \"brown_tips\", \"drooping\"]\n",
    "    \n",
    "    # Rule-based approach\n",
    "    print(\"\\n1. RULE-BASED APPROACH:\")\n",
    "    print(\"-\" * 30)\n",
    "    diagnosis_rules = diagnose_plant_problem_rules(symptoms)\n",
    "    print(f\"Symptoms: {symptoms}\")\n",
    "    print(f\"Diagnosis: {diagnosis_rules}\")\n",
    "    print(\"\\nLimitations:\")\n",
    "    print(\"\u2022 Only handles pre-programmed combinations\")\n",
    "    print(\"\u2022 Can't improve with experience\")\n",
    "    print(\"\u2022 Might miss subtle patterns\")\n",
    "    \n",
    "    # ML approach\n",
    "    print(\"\\n2. MACHINE LEARNING APPROACH:\")\n",
    "    print(\"-\" * 30)\n",
    "    ml_system = PlantDiagnosisML()\n",
    "    \n",
    "    # Simulate training\n",
    "    training_data = [\n",
    "        {'image': 'plant1.jpg', 'diagnosis': 'overwatering'},\n",
    "        {'image': 'plant2.jpg', 'diagnosis': 'fungal'},\n",
    "        # ... thousands more examples\n",
    "    ]\n",
    "    ml_system.train(training_data)\n",
    "    \n",
    "    # Get diagnosis\n",
    "    ml_diagnosis = ml_system.diagnose(symptoms)\n",
    "    print(f\"Symptoms: {symptoms}\")\n",
    "    print(f\"ML Diagnosis: {ml_diagnosis}\")\n",
    "    \n",
    "    print(\"\\nAdvantages:\")\n",
    "    print(\"\u2022 Learns from experience\")\n",
    "    print(\"\u2022 Finds patterns humans might miss\")\n",
    "    print(\"\u2022 Handles novel combinations\")\n",
    "    print(\"\u2022 Improves with more data\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_approaches()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY TAKEAWAY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Rule-Based: You program every decision\")\n",
    "    print(\"Machine Learning: System learns patterns from data\")\n",
    "    print(\"\\nModern AI uses ML because the world is too complex for rules!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: recommendation_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.1\n",
    "# File: recommendation_system.py\n",
    "\n",
    "\"\"\"\n",
    "A simplified recommendation system demonstrating how services like\n",
    "Netflix, YouTube, and Spotify recommend content using AI patterns.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleRecommender:\n",
    "    \"\"\"\n",
    "    Simulates how streaming services recommend content.\n",
    "    This is a simplified version - real systems use:\n",
    "    - Collaborative filtering (users who liked X also liked Y)\n",
    "    - Content-based filtering (similar genres, actors, themes)\n",
    "    - Deep learning for complex pattern recognition\n",
    "    - Matrix factorization for finding hidden preferences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_history = []\n",
    "        self.all_movies = {\n",
    "            'action': ['Die Hard', 'Mad Max', 'John Wick', 'The Matrix', 'Mission Impossible'],\n",
    "            'comedy': ['Airplane', 'Ghostbusters', 'The Hangover', 'Bridesmaids', 'Superbad'],\n",
    "            'sci-fi': ['Interstellar', 'Arrival', 'Blade Runner', 'Dune', 'Ex Machina'],\n",
    "            'drama': ['The Shawshank Redemption', 'Forrest Gump', 'The Godfather', 'Moonlight'],\n",
    "            'horror': ['The Shining', 'Get Out', 'Hereditary', 'A Quiet Place', 'The Witch']\n",
    "        }\n",
    "        \n",
    "        # In real systems, this would be learned from millions of users\n",
    "        self.genre_relationships = {\n",
    "            'action': ['sci-fi', 'thriller'],\n",
    "            'sci-fi': ['action', 'thriller'],\n",
    "            'comedy': ['romance', 'drama'],\n",
    "            'drama': ['romance', 'thriller'],\n",
    "            'horror': ['thriller', 'sci-fi']\n",
    "        }\n",
    "    \n",
    "    def watch_movie(self, movie, genre, rating=None):\n",
    "        \"\"\"\n",
    "        Records a movie watch event.\n",
    "        Real systems track much more:\n",
    "        - Time of day watched\n",
    "        - How much was watched (did they finish?)\n",
    "        - Device used\n",
    "        - Whether they searched for it or clicked a recommendation\n",
    "        \"\"\"\n",
    "        watch_data = {\n",
    "            'movie': movie,\n",
    "            'genre': genre,\n",
    "            'rating': rating if rating else random.randint(3, 5)\n",
    "        }\n",
    "        self.user_history.append(watch_data)\n",
    "        print(f\"\u2705 You watched: {movie} ({genre}) - Rating: {watch_data['rating']}/5\")\n",
    "    \n",
    "    def get_user_preferences(self):\n",
    "        \"\"\"\n",
    "        Analyzes viewing history to understand preferences.\n",
    "        Real systems use sophisticated algorithms to find patterns.\n",
    "        \"\"\"\n",
    "        if not self.user_history:\n",
    "            return None\n",
    "        \n",
    "        # Find favorite genres based on frequency and ratings\n",
    "        genre_scores = {}\n",
    "        for watch in self.user_history:\n",
    "            genre = watch['genre']\n",
    "            rating = watch['rating']\n",
    "            \n",
    "            if genre not in genre_scores:\n",
    "                genre_scores[genre] = {'count': 0, 'total_rating': 0}\n",
    "            \n",
    "            genre_scores[genre]['count'] += 1\n",
    "            genre_scores[genre]['total_rating'] += rating\n",
    "        \n",
    "        # Calculate weighted preferences\n",
    "        preferences = {}\n",
    "        for genre, scores in genre_scores.items():\n",
    "            avg_rating = scores['total_rating'] / scores['count']\n",
    "            # Weight by both frequency and rating\n",
    "            preferences[genre] = scores['count'] * avg_rating\n",
    "        \n",
    "        return preferences\n",
    "    \n",
    "    def get_recommendations(self, num_recommendations=5):\n",
    "        \"\"\"\n",
    "        Generates personalized recommendations.\n",
    "        \n",
    "        Real recommendation systems use:\n",
    "        - Collaborative filtering: \"Users like you also watched...\"\n",
    "        - Content similarity: \"Because you liked The Matrix, try Inception\"\n",
    "        - Trending adjustments: Boost popular/new content\n",
    "        - Diversity injection: Don't recommend all from same genre\n",
    "        - Business rules: Promote originals, new releases\n",
    "        \"\"\"\n",
    "        if not self.user_history:\n",
    "            # Cold start problem - new users with no history\n",
    "            print(\"\ud83c\udfac New user detected! Here are popular picks:\")\n",
    "            popular_picks = []\n",
    "            for genre, movies in self.all_movies.items():\n",
    "                popular_picks.append(random.choice(movies))\n",
    "            return popular_picks[:num_recommendations]\n",
    "        \n",
    "        # Get user preferences\n",
    "        preferences = self.get_user_preferences()\n",
    "        \n",
    "        # Find top genres\n",
    "        top_genres = sorted(preferences.items(), key=lambda x: x[1], reverse=True)\n",
    "        if not top_genres:\n",
    "            return []\n",
    "        \n",
    "        favorite_genre = top_genres[0][0]\n",
    "        \n",
    "        # Get unwatched movies from favorite genre\n",
    "        watched = [watch['movie'] for watch in self.user_history]\n",
    "        recommendations = []\n",
    "        \n",
    "        # Primary recommendations from favorite genre\n",
    "        for movie in self.all_movies.get(favorite_genre, []):\n",
    "            if movie not in watched:\n",
    "                recommendations.append({\n",
    "                    'movie': movie,\n",
    "                    'reason': f\"Because you love {favorite_genre} movies\",\n",
    "                    'confidence': 0.9\n",
    "                })\n",
    "        \n",
    "        # Add related genre recommendations for diversity\n",
    "        related_genres = self.genre_relationships.get(favorite_genre, [])\n",
    "        for related_genre in related_genres:\n",
    "            if related_genre in self.all_movies:\n",
    "                for movie in self.all_movies[related_genre]:\n",
    "                    if movie not in watched and len(recommendations) < num_recommendations * 2:\n",
    "                        recommendations.append({\n",
    "                            'movie': movie,\n",
    "                            'reason': f\"You might like {related_genre} (similar to {favorite_genre})\",\n",
    "                            'confidence': 0.7\n",
    "                        })\n",
    "        \n",
    "        # Sort by confidence and return top N\n",
    "        recommendations.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        return recommendations[:num_recommendations]\n",
    "    \n",
    "    def explain_recommendations(self):\n",
    "        \"\"\"\n",
    "        Shows how recommendations are generated.\n",
    "        Transparency in AI systems helps build trust.\n",
    "        \"\"\"\n",
    "        preferences = self.get_user_preferences()\n",
    "        \n",
    "        if not preferences:\n",
    "            print(\"No viewing history yet!\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca YOUR VIEWING PROFILE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        total_watched = len(self.user_history)\n",
    "        print(f\"Movies watched: {total_watched}\")\n",
    "        \n",
    "        # Show genre breakdown\n",
    "        genre_counts = Counter(watch['genre'] for watch in self.user_history)\n",
    "        print(\"\\nGenre preferences:\")\n",
    "        for genre, count in genre_counts.most_common():\n",
    "            percentage = (count / total_watched) * 100\n",
    "            avg_rating = sum(w['rating'] for w in self.user_history if w['genre'] == genre) / count\n",
    "            print(f\"  \u2022 {genre}: {count} movies ({percentage:.0f}%) - Avg rating: {avg_rating:.1f}\")\n",
    "        \n",
    "        print(\"\\n\ud83e\udd16 HOW WE RECOMMEND:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"1. Analyze your viewing history\")\n",
    "        print(\"2. Find your favorite genres\")\n",
    "        print(\"3. Consider your ratings\")\n",
    "        print(\"4. Look at related genres you might enjoy\")\n",
    "        print(\"5. Filter out what you've already seen\")\n",
    "        print(\"6. Rank by predicted enjoyment\")\n",
    "\n",
    "\n",
    "def demonstrate_recommendation_system():\n",
    "    \"\"\"Interactive demonstration of the recommendation system.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"AI RECOMMENDATION SYSTEM DEMO\")\n",
    "    print(\"Like Netflix, YouTube, or Spotify\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create recommender\n",
    "    netflix_ai = SimpleRecommender()\n",
    "    \n",
    "    # Simulate viewing history\n",
    "    print(\"\\n\ud83d\udcfa SIMULATING YOUR VIEWING HISTORY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # User likes action movies\n",
    "    netflix_ai.watch_movie('Die Hard', 'action', 5)\n",
    "    netflix_ai.watch_movie('Mad Max', 'action', 4)\n",
    "    netflix_ai.watch_movie('John Wick', 'action', 5)\n",
    "    \n",
    "    # Tried one comedy\n",
    "    netflix_ai.watch_movie('Airplane', 'comedy', 3)\n",
    "    \n",
    "    # Loved a sci-fi movie\n",
    "    netflix_ai.watch_movie('Interstellar', 'sci-fi', 5)\n",
    "    \n",
    "    # Explain the AI's understanding\n",
    "    netflix_ai.explain_recommendations()\n",
    "    \n",
    "    # Get recommendations\n",
    "    print(\"\\n\ud83c\udfac PERSONALIZED RECOMMENDATIONS FOR YOU:\")\n",
    "    print(\"-\" * 40)\n",
    "    recommendations = netflix_ai.get_recommendations()\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec['movie']}\")\n",
    "        print(f\"   Why: {rec['reason']}\")\n",
    "        print(f\"   Confidence: {rec['confidence']*100:.0f}%\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 REAL SYSTEMS ARE MORE COMPLEX:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\u2022 Track millions of users' behaviors\")\n",
    "    print(\"\u2022 Use deep neural networks\")\n",
    "    print(\"\u2022 Consider time of day, device, mood\")\n",
    "    print(\"\u2022 A/B test different algorithms\")\n",
    "    print(\"\u2022 Balance personalization with discovery\")\n",
    "    print(\"\u2022 Update in real-time as you watch\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_recommendation_system()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"This is AI in your daily life!\")\n",
    "    print(\"Every 'For You' page uses similar patterns.\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ai_api_preview.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.1\n",
    "# File: ai_api_preview.py\n",
    "\n",
    "\"\"\"\n",
    "Preview of what you'll build soon - integrating with AI APIs.\n",
    "This demonstrates how your Python skills directly apply to AI development.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, Optional\n",
    "\n",
    "def ask_ai(question: str, api_key: str = \"YOUR_KEY\", max_tokens: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Example of calling an AI API - you already know every part of this code!\n",
    "    \n",
    "    Your existing Python skills:\n",
    "    - JSON for data format \u2713 (Chapter 6)\n",
    "    - Requests for API calls \u2713 (Chapter 6)\n",
    "    - Error handling \u2713 (Chapter 6)\n",
    "    - Functions for organization \u2713 (Chapter 5)\n",
    "    - Type hints \u2713 (Throughout)\n",
    "    \n",
    "    Args:\n",
    "        question: The prompt to send to the AI\n",
    "        api_key: Your API key (you'll get this in Chapter 8)\n",
    "        max_tokens: Maximum length of response\n",
    "    \n",
    "    Returns:\n",
    "        The AI's response text\n",
    "    \n",
    "    Note: This is a template. In Chapter 8, you'll make this work\n",
    "    with real AI services like OpenAI, Anthropic, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the API request (Chapter 6 skills!)\n",
    "        api_data = {\n",
    "            \"prompt\": question,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,  # Creativity level (0=focused, 1=creative)\n",
    "            \"model\": \"gpt-3.5-turbo\"  # Which AI model to use\n",
    "        }\n",
    "        \n",
    "        # Make the API call (You did this in Chapter 6!)\n",
    "        response = requests.post(\n",
    "            \"https://api.ai-service.com/chat\",  # You'll use real endpoints soon\n",
    "            json=api_data,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            timeout=30  # Good practice: always set timeouts\n",
    "        )\n",
    "        \n",
    "        # Check if request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the JSON response (Chapter 6 again!)\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract the AI's response\n",
    "        # Different APIs structure this differently\n",
    "        # OpenAI: result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Anthropic: result[\"completion\"]\n",
    "        # You'll learn the specifics for each provider\n",
    "        \n",
    "        return result.get(\"response\", \"No response received\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Network errors\n",
    "        return f\"Network error: {str(e)}\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Invalid JSON response\n",
    "        return f\"Invalid response format: {str(e)}\"\n",
    "    except KeyError as e:\n",
    "        # Missing expected fields\n",
    "        return f\"Unexpected response structure: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        # Catch-all for other errors\n",
    "        return f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "\n",
    "def demonstrate_conversation_flow():\n",
    "    \"\"\"\n",
    "    Shows how you'll manage conversations with AI.\n",
    "    This pattern works with any AI provider.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conversation history (like a chat app)\n",
    "    conversation = []\n",
    "    \n",
    "    def add_message(role: str, content: str):\n",
    "        \"\"\"Add a message to the conversation.\"\"\"\n",
    "        conversation.append({\n",
    "            \"role\": role,  # \"user\", \"assistant\", or \"system\"\n",
    "            \"content\": content,\n",
    "            \"timestamp\": \"2024-01-01 12:00:00\"  # You might track time\n",
    "        })\n",
    "    \n",
    "    def get_ai_response(user_input: str) -> str:\n",
    "        \"\"\"Get AI response while maintaining context.\"\"\"\n",
    "        # Add user message\n",
    "        add_message(\"user\", user_input)\n",
    "        \n",
    "        # In real implementation, you'd send the entire\n",
    "        # conversation history for context\n",
    "        # response = ask_ai_with_context(conversation)\n",
    "        \n",
    "        # For now, just echo\n",
    "        response = f\"AI would respond to: '{user_input}'\"\n",
    "        \n",
    "        # Add AI response\n",
    "        add_message(\"assistant\", response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # Simulate a conversation\n",
    "    print(\"=\"*60)\n",
    "    print(\"CONVERSATION FLOW EXAMPLE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # System message sets the AI's behavior\n",
    "    add_message(\"system\", \"You are a helpful Python tutor.\")\n",
    "    \n",
    "    # User interactions\n",
    "    questions = [\n",
    "        \"What is a list in Python?\",\n",
    "        \"Can you show me an example?\",\n",
    "        \"How is it different from a tuple?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n\ud83d\udc64 User: {question}\")\n",
    "        response = get_ai_response(question)\n",
    "        print(f\"\ud83e\udd16 AI: {response}\")\n",
    "    \n",
    "    # Show conversation history\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSATION HISTORY (What we send to AI):\")\n",
    "    print(\"=\"*60)\n",
    "    for msg in conversation:\n",
    "        print(f\"{msg['role'].upper()}: {msg['content'][:50]}...\")\n",
    "\n",
    "\n",
    "def show_different_ai_tasks():\n",
    "    \"\"\"\n",
    "    Examples of different tasks you'll accomplish with AI APIs.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"WHAT YOU'LL BUILD WITH AI APIs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    tasks = {\n",
    "        \"Translation\": {\n",
    "            \"prompt\": \"Translate 'Hello, how are you?' to Spanish\",\n",
    "            \"expected\": \"Hola, \u00bfc\u00f3mo est\u00e1s?\"\n",
    "        },\n",
    "        \"Summarization\": {\n",
    "            \"prompt\": \"Summarize this text in one sentence: [long article]\",\n",
    "            \"expected\": \"One sentence summary of the article\"\n",
    "        },\n",
    "        \"Code Generation\": {\n",
    "            \"prompt\": \"Write a Python function to reverse a string\",\n",
    "            \"expected\": \"def reverse_string(s): return s[::-1]\"\n",
    "        },\n",
    "        \"Question Answering\": {\n",
    "            \"prompt\": \"What is the capital of France?\",\n",
    "            \"expected\": \"Paris\"\n",
    "        },\n",
    "        \"Creative Writing\": {\n",
    "            \"prompt\": \"Write a haiku about programming\",\n",
    "            \"expected\": \"Code flows like water / Bugs hide in the silent depths / Debug brings the light\"\n",
    "        },\n",
    "        \"Data Extraction\": {\n",
    "            \"prompt\": \"Extract the date from: 'Meeting on January 15th at 3pm'\",\n",
    "            \"expected\": \"January 15th\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for task_name, task_info in tasks.items():\n",
    "        print(f\"\\n\ud83d\udccc {task_name}:\")\n",
    "        print(f\"   Prompt: {task_info['prompt']}\")\n",
    "        print(f\"   AI Output: {task_info['expected']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83c\udfaf Each task uses the same simple pattern:\")\n",
    "    print(\"1. Prepare your prompt\")\n",
    "    print(\"2. Call the AI API\")\n",
    "    print(\"3. Process the response\")\n",
    "    print(\"That's it! You already know how to do all three!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Show what's coming\n",
    "    print(\"=\"*60)\n",
    "    print(\"\ud83d\ude80 PREVIEW: AI API INTEGRATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou already have ALL the Python skills needed!\")\n",
    "    print(\"\\nWhat you know:\")\n",
    "    print(\"\u2713 Making API calls (requests library)\")\n",
    "    print(\"\u2713 Working with JSON data\")\n",
    "    print(\"\u2713 Error handling\")\n",
    "    print(\"\u2713 Functions and organization\")\n",
    "    print(\"\\nWhat you'll learn:\")\n",
    "    print(\"\u2022 How to get API keys\")\n",
    "    print(\"\u2022 Specific endpoints for each AI service\")\n",
    "    print(\"\u2022 How to structure prompts effectively\")\n",
    "    print(\"\u2022 Managing conversation context\")\n",
    "    print(\"\u2022 Cost optimization strategies\")\n",
    "    \n",
    "    # Demonstrate the patterns\n",
    "    print(\"\\n\")\n",
    "    demonstrate_conversation_flow()\n",
    "    print(\"\\n\")\n",
    "    show_different_ai_tasks()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udca1 Remember: AI APIs are just web APIs!\")\n",
    "    print(\"You've already done this in Chapter 6!\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.1: AI or Not AI?\n",
    "\n",
    "Consider each system below. Is it using AI or traditional programming? Think about whether the system follows fixed rules or learns from patterns.\n",
    "\n",
    "**Systems to evaluate:**\n",
    "1. A calculator app that adds numbers\n",
    "2. Google Photos finding all pictures of your dog\n",
    "3. A website login that checks if password matches\n",
    "4. Spotify creating your 'Discover Weekly' playlist\n",
    "5. An alarm clock that rings at 7 AM\n",
    "6. Your phone's face unlock\n",
    "7. A thermostat that turns on at 70\u00b0F\n",
    "8. Gmail's spam filter\n",
    "9. A video game where enemies always patrol the same path\n",
    "10. YouTube's recommendation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.2: Categorizing AI Types\n",
    "\n",
    "Match each AI application to its learning type. Remember:\n",
    "- **Supervised Learning**: Learns from labeled examples (like a teacher showing correct answers)\n",
    "- **Unsupervised Learning**: Finds patterns without being told what to look for\n",
    "- **Reinforcement Learning**: Learns through trial and error with rewards/penalties\n",
    "\n",
    "**Applications to categorize:**\n",
    "\n",
    "A. An email filter trained on examples of spam and not-spam emails\n",
    "B. A system that groups customers by shopping behavior without predefined categories\n",
    "C. A robot learning to walk by trying different movements and getting points for distance traveled\n",
    "D. A model that predicts house prices from past sales data with known prices\n",
    "E. An AI finding hidden patterns in genetic data without knowing what diseases to look for\n",
    "F. A game-playing AI that improves by winning/losing thousands of games\n",
    "G. A photo app that learned to identify faces after seeing millions of labeled face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.3: Design Your Own AI Application\n",
    "\n",
    "Think of a problem in your daily life that AI could solve. This is a thought exercise \u2013 no coding required!\n",
    "\n",
    "Fill in these details for your AI idea:\n",
    "\n",
    "\ud83d\udccc **Problem it solves:** \n",
    "(What daily annoyance or challenge does it address?)\n",
    "\n",
    "**Type of AI:** \n",
    "(Supervised / Unsupervised / Reinforcement \u2013 and why?)\n",
    "\n",
    "\ud83d\udcc1 **Data it would need:** \n",
    "(What information would it need to learn from?)\n",
    "\n",
    "\u27a1\ufe0f **Inputs:** \n",
    "(What information does the user provide?)\n",
    "\n",
    "\u2b05\ufe0f **Outputs:** \n",
    "(What does the AI produce or recommend?)\n",
    "\n",
    "- **Why AI instead of traditional programming?** \n",
    "(What makes this problem suitable for learning rather than rules?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.2: Understanding Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.2 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.1: Token Estimation\n",
    "\n",
    "Estimate how many tokens each text would use (remember: ~4 characters or \u00be word per token):\n",
    "\n",
    "1. \"Hello, world!\"\n",
    "2. \"The quick brown fox jumps over the lazy dog.\"\n",
    "3. A typical email (200 words)\n",
    "4. This entire section you're reading\n",
    "5. \"def calculate_sum(a, b): return a + b\"\n",
    "\n",
    "*Estimates below \u2013 try it yourself first!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.2: Choosing the Right Parameters\n",
    "\n",
    "For each scenario, what temperature would you choose and why?\n",
    "\n",
    "A. Writing legal contract language\n",
    "B. Generating creative story ideas\n",
    "C. Translating technical documentation\n",
    "D. Writing varied product descriptions\n",
    "E. Solving coding problems\n",
    "F. Brainstorming business names\n",
    "\n",
    "*Think about the tradeoff between consistency and creativity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.3: Identifying Good vs Bad LLM Tasks\n",
    "\n",
    "Categorize each task as \"Great for LLMs,\" \"Okay with Caveats,\" or \"Bad Idea\":\n",
    "\n",
    "1. Writing a first draft of a blog post\n",
    "2. Calculating compound interest over 30 years\n",
    "3. Checking if an email sounds professional\n",
    "4. Getting today's stock prices\n",
    "5. Explaining a complex concept simply\n",
    "6. Generating test data for your application\n",
    "7. Making medical diagnoses\n",
    "8. Summarizing a long document\n",
    "9. Checking if a password is secure\n",
    "10. Writing poetry in Shakespeare's style\n",
    "\n",
    "*Consider: Does it need real-time data? Precise calculations? Creative language?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.3: How LLMs work (conceptual overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.3 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.1: Trace the Flow\n",
    "\n",
    "Walk through what happens with this prompt: \"The weather today is\"\n",
    "\n",
    "Consider:\n",
    "1. How does it become tokens?\n",
    "2. What patterns might activate?\n",
    "3. What completions are likely?\n",
    "4. What information is missing?\n",
    "\n",
    "*Think through each step before checking the solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.2: Context Window Planning\n",
    "\n",
    "You have a 4,000 token context window. Design an approach for:\n",
    "\n",
    "A. Having a 10,000 token conversation\n",
    "B. Analyzing a 50,000 token document\n",
    "C. Maintaining chat history over multiple sessions\n",
    "\n",
    "*Consider: What to keep, what to summarize, what to drop?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.3: Understanding Failures\n",
    "\n",
    "For each scenario, explain why the LLM fails using what you learned:\n",
    "\n",
    "1. Can't do exact arithmetic on large numbers\n",
    "2. Makes up fake citations\n",
    "3. Contradicts itself in long conversations\n",
    "4. Can't learn your preferences permanently\n",
    "5. Sometimes says factually wrong things confidently\n",
    "\n",
    "*Hint: Think about the mechanism \u2013 pattern matching, no memory, statistical training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.4: Introduction to prompts and completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.4 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.1: Prompt Improvement Challenge\n",
    "\n",
    "Take these weak prompts and improve them using the techniques you've learned:\n",
    "\n",
    "1. \"Write about space\"\n",
    "2. \"Fix this: def func(x): return x/0\"\n",
    "3. \"Translate: Hello\"\n",
    "4. \"Make a list\"\n",
    "5. \"Explain AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.2: Few-Shot Template Creation\n",
    "\n",
    "Create a few-shot prompt template for:\n",
    "- Extracting dates from text\n",
    "- Classifying customer support tickets\n",
    "- Converting informal text to formal business language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.3: Role-Based Prompting\n",
    "\n",
    "Write system prompts for these AI assistants:\n",
    "- A Socratic tutor who guides through questions\n",
    "- A code reviewer focusing on security\n",
    "- A creative writing partner for brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.4: Completion Control Experiment\n",
    "\n",
    "Using the same base prompt, experiment with:\n",
    "- Different temperatures (0, 0.5, 1.0, 1.5)\n",
    "- Different max_tokens (50, 200, 500)\n",
    "- Different stop sequences\n",
    "\n",
    "Document how each parameter changes the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.5: Common LLM providers (OpenAI, Anthropic, Google, open-source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7.5 content\n",
    "# No source files found for this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.1: Cost Calculator\n",
    "\n",
    "Create a function that calculates monthly costs for different providers based on usage patterns. Consider a chatbot that handles varying message volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.2: Provider Comparison Matrix\n",
    "\n",
    "Build a comparison matrix for your specific use case. Include factors like cost, performance, features, and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.3: Migration Planning\n",
    "\n",
    "Design a migration plan from OpenAI to an open-source model. What challenges would you face? How would you handle them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.4: API Abstraction\n",
    "\n",
    "Write a simple wrapper class that can work with both OpenAI and Anthropic APIs, allowing easy switching between providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.6: API keys and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: api_key_storage.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "# File: api_key_storage.py\n",
    "\n",
    "\"\"\"\n",
    "Demonstrates the right and wrong ways to store API keys in your code.\n",
    "Critical for security and preventing expensive mistakes.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# \u274c NEVER DO THIS - EXAMPLES OF WHAT NOT TO DO\n",
    "# ============================================================================\n",
    "\n",
    "def bad_example_hardcoded():\n",
    "    \"\"\"\n",
    "    NEVER hardcode API keys directly in your code!\n",
    "    This is the #1 security mistake beginners make.\n",
    "    \"\"\"\n",
    "    # \u274c NEVER DO THIS\n",
    "    # api_key = \"sk-proj-KJ5hLvP9Ym8NwZ3bT7BlbkFJ4Xq2RnD8sFgH6kLpMc1A\"\n",
    "    \n",
    "    # Even if the repo is private, don't do this!\n",
    "    # Keys can be exposed through:\n",
    "    # - Accidentally making repo public\n",
    "    # - Sharing code snippets\n",
    "    # - Screenshots or demos\n",
    "    # - Git history (even after deletion)\n",
    "    \n",
    "    print(\"\u274c Example of what NOT to do - hardcoding keys\")\n",
    "    print(\"This would expose your key to anyone who sees the code\")\n",
    "\n",
    "\n",
    "def bad_example_in_comments():\n",
    "    \"\"\"Even in comments, never include real API keys!\"\"\"\n",
    "    \n",
    "    # \u274c NEVER DO THIS EITHER\n",
    "    # My API key: sk-proj-KJ5hLvP9Ym8NwZ3bT7BlbkFJ4Xq2RnD8sFgH6kLpMc1A\n",
    "    # TODO: Remove this before committing\n",
    "    \n",
    "    print(\"\u274c Don't put keys in comments either - they often get forgotten\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# \u2705 ALWAYS DO THIS - SECURE API KEY HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def good_example_environment_variable():\n",
    "    \"\"\"\n",
    "    \u2705 CORRECT: Load API keys from environment variables\n",
    "    This keeps your keys separate from your code.\n",
    "    \"\"\"\n",
    "    # Load from environment variable\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"No API key found!\")\n",
    "        print(\"Please set it using:\")\n",
    "        print(\"  Mac/Linux: export OPENAI_API_KEY='your-key-here'\")\n",
    "        print(\"  Windows: set OPENAI_API_KEY=your-key-here\")\n",
    "        return None\n",
    "    \n",
    "    # Never print the actual key!\n",
    "    print(f\"\u2705 API key loaded successfully (starts with {api_key[:7]}...)\")\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def good_example_dotenv():\n",
    "    \"\"\"\n",
    "    \u2705 CORRECT: Load from .env file using python-dotenv\n",
    "    This is convenient for development.\n",
    "    \"\"\"\n",
    "    # Load .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Now get the key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"No API key found in .env file!\")\n",
    "        print(\"Create a .env file with:\")\n",
    "        print(\"  OPENAI_API_KEY=your-key-here\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\u2705 API key loaded from .env file\")\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def good_example_with_validation():\n",
    "    \"\"\"\n",
    "    \u2705 BEST PRACTICE: Load with validation and error handling\n",
    "    \"\"\"\n",
    "    # Try multiple sources\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        # Try loading from .env as fallback\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\n",
    "            \"API key not found!\\n\"\n",
    "            \"Please set OPENAI_API_KEY environment variable or add to .env file\"\n",
    "        )\n",
    "    \n",
    "    # Validate format (basic check)\n",
    "    if not api_key.startswith(\"sk-\"):\n",
    "        print(\"\u26a0\ufe0f Warning: API key format might be incorrect\")\n",
    "    \n",
    "    # Check for common mistakes\n",
    "    if \" \" in api_key:\n",
    "        raise ValueError(\"API key contains spaces - probably a copy/paste error\")\n",
    "    \n",
    "    if len(api_key) < 20:\n",
    "        raise ValueError(\"API key seems too short - might be truncated\")\n",
    "    \n",
    "    print(\"\u2705 API key validated and ready to use\")\n",
    "    return api_key\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SETTING UP YOUR PROJECT CORRECTLY\n",
    "# ============================================================================\n",
    "\n",
    "def setup_project_security():\n",
    "    \"\"\"\n",
    "    Set up a new project with proper security from the start\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"Setting up secure API key management...\")\n",
    "    \n",
    "    # 1. Create .env file if it doesn't exist\n",
    "    if not Path(\".env\").exists():\n",
    "        with open(\".env\", \"w\") as f:\n",
    "            f.write(\"# API Keys - NEVER commit this file!\\n\")\n",
    "            f.write(\"OPENAI_API_KEY=your-key-here\\n\")\n",
    "            f.write(\"ANTHROPIC_API_KEY=your-key-here\\n\")\n",
    "            f.write(\"GOOGLE_API_KEY=your-key-here\\n\")\n",
    "        print(\"\u2705 Created .env file\")\n",
    "    \n",
    "    # 2. Create .env.example for others\n",
    "    with open(\".env.example\", \"w\") as f:\n",
    "        f.write(\"# Copy this to .env and add your actual keys\\n\")\n",
    "        f.write(\"OPENAI_API_KEY=ADD_YOUR_KEY_HERE\\n\")\n",
    "        f.write(\"ANTHROPIC_API_KEY=ADD_YOUR_KEY_HERE\\n\")\n",
    "        f.write(\"GOOGLE_API_KEY=ADD_YOUR_KEY_HERE\\n\")\n",
    "    print(\"\u2705 Created .env.example (safe to commit)\")\n",
    "    \n",
    "    # 3. Create/update .gitignore\n",
    "    gitignore_lines = [\n",
    "        \"# Environment variables\",\n",
    "        \".env\",\n",
    "        \".env.local\",\n",
    "        \".env.*.local\",\n",
    "        \"\",\n",
    "        \"# API keys and secrets\",\n",
    "        \"config.json\",\n",
    "        \"secrets.json\",\n",
    "        \"*.key\",\n",
    "        \"\",\n",
    "        \"# Python\",\n",
    "        \"__pycache__/\",\n",
    "        \"*.py[cod]\",\n",
    "        \"venv/\",\n",
    "        \"env/\",\n",
    "    ]\n",
    "    \n",
    "    with open(\".gitignore\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(gitignore_lines))\n",
    "    print(\"\u2705 Created .gitignore to protect secrets\")\n",
    "    \n",
    "    print(\"\\n\u2705 Project security setup complete!\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Edit .env and add your actual API keys\")\n",
    "    print(\"2. Run: pip install python-dotenv\")\n",
    "    print(\"3. Use load_dotenv() in your code\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"API KEY SECURITY DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show what NOT to do\n",
    "    print(\"\\n\u26a0\ufe0f EXAMPLES OF BAD PRACTICES:\")\n",
    "    print(\"-\" * 40)\n",
    "    bad_example_hardcoded()\n",
    "    \n",
    "    print(\"\\n\u2705 EXAMPLES OF GOOD PRACTICES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Try to load API key the right way\n",
    "    try:\n",
    "        key = good_example_with_validation()\n",
    "        if key:\n",
    "            print(f\"Success! Key starts with: {key[:10]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Setup needed: {e}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 TIP: Run setup_project_security() to set up your project correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: api_config_manager.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "# File: api_config_manager.py\n",
    "\n",
    "\"\"\"\n",
    "Centralized API configuration manager for working with multiple AI providers.\n",
    "Handles loading, validation, and organization of API keys.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class APIConfig:\n",
    "    \"\"\"Centralized API configuration for multiple providers\"\"\"\n",
    "    openai_key: Optional[str] = None\n",
    "    anthropic_key: Optional[str] = None\n",
    "    google_key: Optional[str] = None\n",
    "    replicate_key: Optional[str] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls):\n",
    "        \"\"\"Load all API keys from environment variables\"\"\"\n",
    "        # Try to load .env file first\n",
    "        load_dotenv()\n",
    "        \n",
    "        return cls(\n",
    "            openai_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            anthropic_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            google_key=os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"),\n",
    "            replicate_key=os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, filepath: str = \"config.json\"):\n",
    "        \"\"\"Load API keys from a JSON configuration file\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(\n",
    "                f\"{filepath} not found! Copy config.example.json and add your keys\"\n",
    "            )\n",
    "        \n",
    "        with open(filepath) as f:\n",
    "            config_data = json.load(f)\n",
    "        \n",
    "        return cls(\n",
    "            openai_key=config_data.get(\"openai_key\"),\n",
    "            anthropic_key=config_data.get(\"anthropic_key\"),\n",
    "            google_key=config_data.get(\"google_key\"),\n",
    "            replicate_key=config_data.get(\"replicate_key\")\n",
    "        )\n",
    "    \n",
    "    def get_available_providers(self) -> List[str]:\n",
    "        \"\"\"Return list of providers with valid keys\"\"\"\n",
    "        providers = []\n",
    "        \n",
    "        if self.openai_key and self.openai_key != \"ADD_YOUR_KEY_HERE\":\n",
    "            providers.append(\"openai\")\n",
    "        if self.anthropic_key and self.anthropic_key != \"ADD_YOUR_KEY_HERE\":\n",
    "            providers.append(\"anthropic\")\n",
    "        if self.google_key and self.google_key != \"ADD_YOUR_KEY_HERE\":\n",
    "            providers.append(\"google\")\n",
    "        if self.replicate_key and self.replicate_key != \"ADD_YOUR_KEY_HERE\":\n",
    "            providers.append(\"replicate\")\n",
    "        \n",
    "        return providers\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Check if at least one key is configured properly\"\"\"\n",
    "        available = self.get_available_providers()\n",
    "        \n",
    "        if not available:\n",
    "            print(\"\u274c No valid API keys found!\")\n",
    "            print(\"\\nPlease configure at least one:\")\n",
    "            print(\"  - OPENAI_API_KEY\")\n",
    "            print(\"  - ANTHROPIC_API_KEY\")\n",
    "            print(\"  - GOOGLE_API_KEY\")\n",
    "            print(\"  - REPLICATE_API_TOKEN\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\u2705 API keys loaded for: {', '.join(available)}\")\n",
    "        return True\n",
    "    \n",
    "    def get_primary_provider(self) -> Optional[str]:\n",
    "        \"\"\"Get the first available provider (useful for fallback)\"\"\"\n",
    "        providers = self.get_available_providers()\n",
    "        return providers[0] if providers else None\n",
    "    \n",
    "    def mask_key(self, key: str) -> str:\n",
    "        \"\"\"Safely display a masked version of an API key\"\"\"\n",
    "        if not key:\n",
    "            return \"Not configured\"\n",
    "        if len(key) < 10:\n",
    "            return \"Invalid key\"\n",
    "        return f\"{key[:7]}...{key[-4:]}\"\n",
    "    \n",
    "    def display_status(self):\n",
    "        \"\"\"Display the status of all API keys\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"API KEY CONFIGURATION STATUS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        providers = {\n",
    "            \"OpenAI\": self.openai_key,\n",
    "            \"Anthropic\": self.anthropic_key,\n",
    "            \"Google\": self.google_key,\n",
    "            \"Replicate\": self.replicate_key\n",
    "        }\n",
    "        \n",
    "        for name, key in providers.items():\n",
    "            if key and key != \"ADD_YOUR_KEY_HERE\":\n",
    "                print(f\"\u2705 {name:12} : {self.mask_key(key)}\")\n",
    "            else:\n",
    "                print(f\"\u274c {name:12} : Not configured\")\n",
    "        \n",
    "        available = self.get_available_providers()\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total configured: {len(available)}/{len(providers)}\")\n",
    "        \n",
    "        if available:\n",
    "            print(f\"Primary provider: {self.get_primary_provider()}\")\n",
    "\n",
    "\n",
    "class MultiProviderClient:\n",
    "    \"\"\"\n",
    "    Manage multiple AI provider clients with automatic fallback\n",
    "    \"\"\"\n",
    "    def __init__(self, config: APIConfig):\n",
    "        self.config = config\n",
    "        self.clients = {}\n",
    "        self._initialize_clients()\n",
    "    \n",
    "    def _initialize_clients(self):\n",
    "        \"\"\"Initialize available clients based on configured keys\"\"\"\n",
    "        \n",
    "        # OpenAI\n",
    "        if self.config.openai_key:\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                self.clients[\"openai\"] = OpenAI(api_key=self.config.openai_key)\n",
    "                print(\"\u2705 OpenAI client initialized\")\n",
    "            except ImportError:\n",
    "                print(\"\u26a0\ufe0f OpenAI package not installed. Run: pip install openai\")\n",
    "        \n",
    "        # Anthropic\n",
    "        if self.config.anthropic_key:\n",
    "            try:\n",
    "                from anthropic import Anthropic\n",
    "                self.clients[\"anthropic\"] = Anthropic(api_key=self.config.anthropic_key)\n",
    "                print(\"\u2705 Anthropic client initialized\")\n",
    "            except ImportError:\n",
    "                print(\"\u26a0\ufe0f Anthropic package not installed. Run: pip install anthropic\")\n",
    "        \n",
    "        # Google\n",
    "        if self.config.google_key:\n",
    "            try:\n",
    "                import google.generativeai as genai\n",
    "                genai.configure(api_key=self.config.google_key)\n",
    "                self.clients[\"google\"] = genai\n",
    "                print(\"\u2705 Google Gemini client initialized\")\n",
    "            except ImportError:\n",
    "                print(\"\u26a0\ufe0f Google package not installed. Run: pip install google-generativeai\")\n",
    "    \n",
    "    def get_client(self, provider: str = None):\n",
    "        \"\"\"Get a specific client or the first available one\"\"\"\n",
    "        if provider:\n",
    "            return self.clients.get(provider)\n",
    "        \n",
    "        # Return first available client\n",
    "        for provider, client in self.clients.items():\n",
    "            return client\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def list_available_models(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"List available models for each configured provider\"\"\"\n",
    "        models = {}\n",
    "        \n",
    "        if \"openai\" in self.clients:\n",
    "            models[\"openai\"] = [\n",
    "                \"gpt-3.5-turbo\",\n",
    "                \"gpt-4\",\n",
    "                \"gpt-4-turbo-preview\"\n",
    "            ]\n",
    "        \n",
    "        if \"anthropic\" in self.clients:\n",
    "            models[\"anthropic\"] = [\n",
    "                \"claude-3-opus-20240229\",\n",
    "                \"claude-3-sonnet-20240229\",\n",
    "                \"claude-3-haiku-20240307\"\n",
    "            ]\n",
    "        \n",
    "        if \"google\" in self.clients:\n",
    "            models[\"google\"] = [\n",
    "                \"gemini-pro\",\n",
    "                \"gemini-pro-vision\"\n",
    "            ]\n",
    "        \n",
    "        return models\n",
    "\n",
    "\n",
    "def create_example_config_files():\n",
    "    \"\"\"Create example configuration files for users to customize\"\"\"\n",
    "    \n",
    "    # Create config.example.json\n",
    "    example_config = {\n",
    "        \"openai_key\": \"ADD_YOUR_KEY_HERE\",\n",
    "        \"anthropic_key\": \"ADD_YOUR_KEY_HERE\",\n",
    "        \"google_key\": \"ADD_YOUR_KEY_HERE\",\n",
    "        \"replicate_key\": \"ADD_YOUR_KEY_HERE\",\n",
    "        \"default_temperature\": 0.7,\n",
    "        \"max_tokens\": 500,\n",
    "        \"timeout\": 30\n",
    "    }\n",
    "    \n",
    "    with open(\"config.example.json\", \"w\") as f:\n",
    "        json.dump(example_config, f, indent=2)\n",
    "    \n",
    "    print(\"\u2705 Created config.example.json\")\n",
    "    \n",
    "    # Create .env.example\n",
    "    env_example = \"\"\"# API Keys Configuration\n",
    "# Copy this file to .env and add your actual keys\n",
    "\n",
    "# OpenAI API Key (https://platform.openai.com/api-keys)\n",
    "OPENAI_API_KEY=ADD_YOUR_KEY_HERE\n",
    "\n",
    "# Anthropic API Key (https://console.anthropic.com/)\n",
    "ANTHROPIC_API_KEY=ADD_YOUR_KEY_HERE\n",
    "\n",
    "# Google AI Studio Key (https://aistudio.google.com/)\n",
    "GOOGLE_API_KEY=ADD_YOUR_KEY_HERE\n",
    "\n",
    "# Replicate API Token (https://replicate.com/account/api-tokens)\n",
    "REPLICATE_API_TOKEN=ADD_YOUR_KEY_HERE\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\".env.example\", \"w\") as f:\n",
    "        f.write(env_example)\n",
    "    \n",
    "    print(\"\u2705 Created .env.example\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate the configuration manager\n",
    "    print(\"API Configuration Manager Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load configuration from environment\n",
    "    config = APIConfig.from_env()\n",
    "    \n",
    "    # Display status\n",
    "    config.display_status()\n",
    "    \n",
    "    # Validate configuration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if config.validate():\n",
    "        print(\"\\n\ud83c\udf89 Ready to use AI APIs!\")\n",
    "        \n",
    "        # Initialize multi-provider client\n",
    "        client_manager = MultiProviderClient(config)\n",
    "        \n",
    "        # Show available models\n",
    "        models = client_manager.list_available_models()\n",
    "        if models:\n",
    "            print(\"\\nAvailable models:\")\n",
    "            for provider, model_list in models.items():\n",
    "                print(f\"\\n{provider}:\")\n",
    "                for model in model_list:\n",
    "                    print(f\"  - {model}\")\n",
    "    else:\n",
    "        print(\"\\n\ud83d\udcdd To get started:\")\n",
    "        print(\"1. Copy .env.example to .env\")\n",
    "        print(\"2. Add your API keys\")\n",
    "        print(\"3. Run this script again\")\n",
    "        \n",
    "        # Create example files if they don't exist\n",
    "        if not os.path.exists(\".env.example\"):\n",
    "            print(\"\\nCreating example configuration files...\")\n",
    "            create_example_config_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: rate_limit_handler.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "# File: rate_limit_handler.py\n",
    "\n",
    "\"\"\"\n",
    "Robust rate limit handling with exponential backoff for AI API calls.\n",
    "Prevents hitting rate limits and handles them gracefully when they occur.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "from typing import Callable, Any, Optional, Dict\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import functools\n",
    "\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    func: Callable = None,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 5,\n",
    "    max_delay: float = 60\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator to retry a function with exponential backoff.\n",
    "    Perfect for handling rate limits and transient failures.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to retry (used when called as decorator)\n",
    "        initial_delay: Starting delay in seconds\n",
    "        exponential_base: Multiplier for each retry\n",
    "        jitter: Add randomness to prevent thundering herd\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        max_delay: Maximum delay between retries\n",
    "    \n",
    "    Usage:\n",
    "        @retry_with_exponential_backoff(max_retries=3)\n",
    "        def make_api_call():\n",
    "            # Your API call here\n",
    "            pass\n",
    "    \"\"\"\n",
    "    def decorator(f):\n",
    "        @functools.wraps(f)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            num_retries = 0\n",
    "            delay = initial_delay\n",
    "            \n",
    "            while num_retries < max_retries:\n",
    "                try:\n",
    "                    # Try to execute the function\n",
    "                    return f(*args, **kwargs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_str = str(e).lower()\n",
    "                    \n",
    "                    # Check if it's a rate limit error\n",
    "                    if any(indicator in error_str for indicator in \n",
    "                           [\"rate_limit\", \"rate limit\", \"429\", \"too many requests\"]):\n",
    "                        \n",
    "                        if num_retries < max_retries - 1:\n",
    "                            # Calculate delay with exponential backoff\n",
    "                            actual_delay = min(delay, max_delay)\n",
    "                            \n",
    "                            # Add jitter if requested\n",
    "                            if jitter:\n",
    "                                actual_delay += random.uniform(0, 1)\n",
    "                            \n",
    "                            print(f\"\u23f3 Rate limited. Waiting {actual_delay:.1f} seconds...\")\n",
    "                            print(f\"   Retry {num_retries + 1}/{max_retries}\")\n",
    "                            \n",
    "                            time.sleep(actual_delay)\n",
    "                            \n",
    "                            # Increase delay for next retry\n",
    "                            delay *= exponential_base\n",
    "                            num_retries += 1\n",
    "                        else:\n",
    "                            print(f\"\u274c Max retries ({max_retries}) exceeded\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        # Not a rate limit error, re-raise immediately\n",
    "                        raise e\n",
    "            \n",
    "            # Should never reach here, but just in case\n",
    "            raise Exception(f\"Maximum retries ({max_retries}) exceeded\")\n",
    "        \n",
    "        return wrapper\n",
    "    \n",
    "    # Handle both @retry_with_exponential_backoff and @retry_with_exponential_backoff()\n",
    "    if func is None:\n",
    "        return decorator\n",
    "    else:\n",
    "        return decorator(func)\n",
    "\n",
    "\n",
    "class RateLimitTracker:\n",
    "    \"\"\"\n",
    "    Track API calls and implement client-side rate limiting\n",
    "    to prevent hitting server rate limits.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests_per_minute: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize rate limit tracker\n",
    "        \n",
    "        Args:\n",
    "            max_requests_per_minute: Maximum requests allowed per minute\n",
    "        \"\"\"\n",
    "        self.max_rpm = max_requests_per_minute\n",
    "        self.request_times = deque()\n",
    "        self.total_requests = 0\n",
    "        self.total_throttled = 0\n",
    "    \n",
    "    def can_make_request(self) -> bool:\n",
    "        \"\"\"Check if we can make a request without exceeding limits\"\"\"\n",
    "        self._clean_old_requests()\n",
    "        return len(self.request_times) < self.max_rpm\n",
    "    \n",
    "    def wait_if_needed(self) -> float:\n",
    "        \"\"\"\n",
    "        Wait if necessary to avoid rate limits.\n",
    "        Returns the number of seconds waited.\n",
    "        \"\"\"\n",
    "        self._clean_old_requests()\n",
    "        \n",
    "        if len(self.request_times) >= self.max_rpm:\n",
    "            # Calculate how long to wait\n",
    "            oldest_request = self.request_times[0]\n",
    "            wait_until = oldest_request + timedelta(minutes=1)\n",
    "            wait_seconds = (wait_until - datetime.now()).total_seconds()\n",
    "            \n",
    "            if wait_seconds > 0:\n",
    "                print(f\"\u23f3 Approaching rate limit ({self.max_rpm} req/min)\")\n",
    "                print(f\"   Waiting {wait_seconds:.1f} seconds...\")\n",
    "                time.sleep(wait_seconds + 0.1)  # Add small buffer\n",
    "                self.total_throttled += 1\n",
    "                return wait_seconds\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def record_request(self):\n",
    "        \"\"\"Record that a request was made\"\"\"\n",
    "        self.request_times.append(datetime.now())\n",
    "        self.total_requests += 1\n",
    "    \n",
    "    def _clean_old_requests(self):\n",
    "        \"\"\"Remove requests older than 1 minute from tracking\"\"\"\n",
    "        one_minute_ago = datetime.now() - timedelta(minutes=1)\n",
    "        \n",
    "        while self.request_times and self.request_times[0] < one_minute_ago:\n",
    "            self.request_times.popleft()\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current rate limit statistics\"\"\"\n",
    "        self._clean_old_requests()\n",
    "        \n",
    "        return {\n",
    "            \"requests_in_last_minute\": len(self.request_times),\n",
    "            \"max_requests_per_minute\": self.max_rpm,\n",
    "            \"available_requests\": self.max_rpm - len(self.request_times),\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"times_throttled\": self.total_throttled,\n",
    "            \"current_usage_percent\": (len(self.request_times) / self.max_rpm) * 100\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all tracking\"\"\"\n",
    "        self.request_times.clear()\n",
    "        self.total_requests = 0\n",
    "        self.total_throttled = 0\n",
    "\n",
    "\n",
    "class APIRateLimiter:\n",
    "    \"\"\"\n",
    "    Complete rate limiting solution for API calls with\n",
    "    provider-specific limits and automatic throttling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default rate limits for different providers (requests per minute)\n",
    "    DEFAULT_LIMITS = {\n",
    "        \"openai\": {\n",
    "            \"gpt-3.5-turbo\": 60,\n",
    "            \"gpt-4\": 20,\n",
    "            \"default\": 50\n",
    "        },\n",
    "        \"anthropic\": {\n",
    "            \"default\": 50\n",
    "        },\n",
    "        \"google\": {\n",
    "            \"free\": 60,\n",
    "            \"paid\": 360,\n",
    "            \"default\": 60\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize rate limiters for different providers\"\"\"\n",
    "        self.trackers = {}\n",
    "        self.last_errors = {}\n",
    "    \n",
    "    def get_tracker(self, provider: str, model: str = None) -> RateLimitTracker:\n",
    "        \"\"\"Get or create a rate limit tracker for a specific provider/model\"\"\"\n",
    "        key = f\"{provider}:{model}\" if model else provider\n",
    "        \n",
    "        if key not in self.trackers:\n",
    "            # Get appropriate limit\n",
    "            provider_limits = self.DEFAULT_LIMITS.get(provider, {\"default\": 50})\n",
    "            \n",
    "            if model and model in provider_limits:\n",
    "                limit = provider_limits[model]\n",
    "            else:\n",
    "                limit = provider_limits.get(\"default\", 50)\n",
    "            \n",
    "            self.trackers[key] = RateLimitTracker(limit)\n",
    "        \n",
    "        return self.trackers[key]\n",
    "    \n",
    "    @retry_with_exponential_backoff(max_retries=3)\n",
    "    def make_api_call(self, provider: str, api_function: Callable, \n",
    "                      model: str = None, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Make an API call with rate limiting and retry logic\n",
    "        \n",
    "        Args:\n",
    "            provider: Name of the API provider\n",
    "            api_function: The actual API function to call\n",
    "            model: Optional model name for provider-specific limits\n",
    "            **kwargs: Arguments to pass to the API function\n",
    "        \n",
    "        Returns:\n",
    "            The result of the API call\n",
    "        \"\"\"\n",
    "        tracker = self.get_tracker(provider, model)\n",
    "        \n",
    "        # Wait if approaching rate limit\n",
    "        wait_time = tracker.wait_if_needed()\n",
    "        \n",
    "        # Record the request\n",
    "        tracker.record_request()\n",
    "        \n",
    "        try:\n",
    "            # Make the actual API call\n",
    "            result = api_function(**kwargs)\n",
    "            \n",
    "            # Clear any previous errors for this provider\n",
    "            if provider in self.last_errors:\n",
    "                del self.last_errors[provider]\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record the error\n",
    "            self.last_errors[provider] = {\n",
    "                \"error\": str(e),\n",
    "                \"time\": datetime.now(),\n",
    "                \"model\": model\n",
    "            }\n",
    "            raise\n",
    "    \n",
    "    def get_all_stats(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Get statistics for all tracked providers\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for key, tracker in self.trackers.items():\n",
    "            stats[key] = tracker.get_stats()\n",
    "            \n",
    "            # Add error info if available\n",
    "            provider = key.split(\":\")[0]\n",
    "            if provider in self.last_errors:\n",
    "                error_info = self.last_errors[provider]\n",
    "                time_since_error = (datetime.now() - error_info[\"time\"]).total_seconds()\n",
    "                stats[key][\"last_error\"] = {\n",
    "                    \"message\": error_info[\"error\"][:100],  # Truncate long errors\n",
    "                    \"seconds_ago\": round(time_since_error, 1)\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def display_status(self):\n",
    "        \"\"\"Display current rate limit status for all providers\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"RATE LIMIT STATUS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        stats = self.get_all_stats()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No API calls tracked yet\")\n",
    "            return\n",
    "        \n",
    "        for key, stat in stats.items():\n",
    "            provider_model = key.replace(\":\", \" - \")\n",
    "            usage_bar = self._create_usage_bar(stat[\"current_usage_percent\"])\n",
    "            \n",
    "            print(f\"\\n{provider_model}:\")\n",
    "            print(f\"  Usage: {usage_bar} {stat['current_usage_percent']:.0f}%\")\n",
    "            print(f\"  Requests: {stat['requests_in_last_minute']}/{stat['max_requests_per_minute']}\")\n",
    "            print(f\"  Available: {stat['available_requests']}\")\n",
    "            \n",
    "            if \"last_error\" in stat:\n",
    "                print(f\"  \u26a0\ufe0f Last error: {stat['last_error']['seconds_ago']}s ago\")\n",
    "    \n",
    "    def _create_usage_bar(self, percent: float, width: int = 20) -> str:\n",
    "        \"\"\"Create a visual progress bar for usage\"\"\"\n",
    "        filled = int((percent / 100) * width)\n",
    "        bar = \"\u2588\" * filled + \"\u2591\" * (width - filled)\n",
    "        \n",
    "        # Color coding (would need terminal colors in real implementation)\n",
    "        if percent < 50:\n",
    "            return f\"[{bar}]\"  # Green\n",
    "        elif percent < 80:\n",
    "            return f\"[{bar}]\"  # Yellow\n",
    "        else:\n",
    "            return f\"[{bar}]\"  # Red\n",
    "\n",
    "\n",
    "# Example usage functions\n",
    "def demo_rate_limiting():\n",
    "    \"\"\"Demonstrate rate limiting in action\"\"\"\n",
    "    \n",
    "    print(\"Rate Limiting Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create a rate limiter\n",
    "    limiter = APIRateLimiter()\n",
    "    \n",
    "    # Simulate API calls\n",
    "    def mock_api_call(message: str):\n",
    "        \"\"\"Simulate an API call\"\"\"\n",
    "        # Random chance of rate limit error for demo\n",
    "        if random.random() < 0.1:\n",
    "            raise Exception(\"Rate limit exceeded (429)\")\n",
    "        return f\"Response to: {message}\"\n",
    "    \n",
    "    # Make several calls\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            result = limiter.make_api_call(\n",
    "                provider=\"openai\",\n",
    "                api_function=mock_api_call,\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                message=f\"Test message {i+1}\"\n",
    "            )\n",
    "            print(f\"\u2705 Call {i+1}: Success\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Call {i+1}: {e}\")\n",
    "        \n",
    "        time.sleep(0.5)  # Small delay between calls\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"\\n\")\n",
    "    limiter.display_status()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the demonstration\n",
    "    demo_rate_limiting()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\ud83d\udca1 Rate Limiting Best Practices:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Always implement client-side rate limiting\")\n",
    "    print(\"2. Use exponential backoff for retries\")\n",
    "    print(\"3. Add jitter to prevent thundering herd\")\n",
    "    print(\"4. Track statistics to optimize usage\")\n",
    "    print(\"5. Set conservative limits to avoid surprises\")\n",
    "    print(\"6. Monitor and alert on repeated failures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: security_audit.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "# File: security_audit.py\n",
    "\n",
    "\"\"\"\n",
    "Security audit tool to scan projects for exposed API keys and security issues.\n",
    "Essential for preventing expensive mistakes and security breaches.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class APIKeyAuditor:\n",
    "    \"\"\"\n",
    "    Comprehensive security auditor for API keys in your project\n",
    "    \"\"\"\n",
    "    \n",
    "    # Patterns that might indicate API keys\n",
    "    KEY_PATTERNS = {\n",
    "        \"OpenAI\": r'sk-[a-zA-Z0-9]{48}',\n",
    "        \"Anthropic\": r'sk-ant-[a-zA-Z0-9-]+',\n",
    "        \"Google\": r'AIza[a-zA-Z0-9_-]{35}',\n",
    "        \"Replicate\": r'[a-f0-9]{40}',  # Less specific, more false positives\n",
    "        \"Generic Secret\": r'(api[_-]?key|secret|token|password)[\\s]*=[\\s]*[\"\\'][^\"\\']{20,}[\"\\']'\n",
    "    }\n",
    "    \n",
    "    # Files/folders to skip\n",
    "    SKIP_PATHS = {\n",
    "        \".git\", \".env\", \"venv\", \"env\", \"__pycache__\", \n",
    "        \"node_modules\", \".pytest_cache\", \"dist\", \"build\"\n",
    "    }\n",
    "    \n",
    "    # File extensions to check\n",
    "    CHECK_EXTENSIONS = {\n",
    "        \".py\", \".js\", \".jsx\", \".ts\", \".tsx\", \".json\", \".yaml\", \n",
    "        \".yml\", \".md\", \".txt\", \".sh\", \".bash\", \".config\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, project_dir: str = \".\"):\n",
    "        \"\"\"Initialize the auditor with a project directory\"\"\"\n",
    "        self.project_dir = Path(project_dir)\n",
    "        self.violations = []\n",
    "        self.warnings = []\n",
    "        self.safe_files = []\n",
    "    \n",
    "    def audit_file(self, filepath: Path) -> List[Dict]:\n",
    "        \"\"\"Audit a single file for exposed keys\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                # Check each line\n",
    "                for line_num, line in enumerate(content.splitlines(), 1):\n",
    "                    # Skip comments and docstrings for false positives\n",
    "                    if line.strip().startswith(('#', '//', '\"\"\"', \"'''\")):\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for each pattern\n",
    "                    for key_type, pattern in self.KEY_PATTERNS.items():\n",
    "                        matches = re.finditer(pattern, line)\n",
    "                        for match in matches:\n",
    "                            # Check if it's likely a real key\n",
    "                            if self._is_likely_real_key(match.group(), key_type):\n",
    "                                violations.append({\n",
    "                                    \"file\": str(filepath.relative_to(self.project_dir)),\n",
    "                                    \"line\": line_num,\n",
    "                                    \"type\": key_type,\n",
    "                                    \"preview\": self._mask_sensitive_data(line.strip()),\n",
    "                                    \"severity\": \"HIGH\"\n",
    "                                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.warnings.append(f\"Could not scan {filepath}: {e}\")\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def _is_likely_real_key(self, text: str, key_type: str) -> bool:\n",
    "        \"\"\"Check if a match is likely a real API key\"\"\"\n",
    "        # Common false positive indicators\n",
    "        false_positive_indicators = [\n",
    "            \"example\", \"your-key\", \"add_your\", \"placeholder\",\n",
    "            \"xxx\", \"...\", \"abc\", \"123\", \"test\", \"demo\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for obvious placeholders\n",
    "        for indicator in false_positive_indicators:\n",
    "            if indicator in text_lower:\n",
    "                return False\n",
    "        \n",
    "        # Check for repeated characters (likely fake)\n",
    "        if len(set(text)) < len(text) / 3:  # Too many repeated chars\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _mask_sensitive_data(self, text: str) -> str:\n",
    "        \"\"\"Mask potential sensitive data in preview\"\"\"\n",
    "        # Replace potential keys with masked versions\n",
    "        for key_type, pattern in self.KEY_PATTERNS.items():\n",
    "            text = re.sub(pattern, lambda m: m.group()[:10] + \"***MASKED***\", text)\n",
    "        return text[:100] + \"...\" if len(text) > 100 else text\n",
    "    \n",
    "    def audit_project(self) -> Dict:\n",
    "        \"\"\"Audit the entire project for security issues\"\"\"\n",
    "        print(f\"\ud83d\udd0d Auditing project: {self.project_dir.absolute()}\")\n",
    "        \n",
    "        files_scanned = 0\n",
    "        \n",
    "        # Walk through project files\n",
    "        for file_path in self._get_files_to_scan():\n",
    "            violations = self.audit_file(file_path)\n",
    "            \n",
    "            if violations:\n",
    "                self.violations.extend(violations)\n",
    "            else:\n",
    "                self.safe_files.append(str(file_path.relative_to(self.project_dir)))\n",
    "            \n",
    "            files_scanned += 1\n",
    "        \n",
    "        # Check additional security issues\n",
    "        self._check_gitignore()\n",
    "        self._check_git_history()\n",
    "        self._check_environment_files()\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            \"scan_time\": datetime.now().isoformat(),\n",
    "            \"project_dir\": str(self.project_dir.absolute()),\n",
    "            \"files_scanned\": files_scanned,\n",
    "            \"violations\": self.violations,\n",
    "            \"warnings\": self.warnings,\n",
    "            \"safe_files_count\": len(self.safe_files),\n",
    "            \"summary\": self._generate_summary()\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_files_to_scan(self) -> List[Path]:\n",
    "        \"\"\"Get list of files to scan, respecting skip patterns\"\"\"\n",
    "        files_to_scan = []\n",
    "        \n",
    "        for file_path in self.project_dir.rglob(\"*\"):\n",
    "            # Skip directories\n",
    "            if file_path.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Skip excluded paths\n",
    "            if any(skip in file_path.parts for skip in self.SKIP_PATHS):\n",
    "                continue\n",
    "            \n",
    "            # Only check relevant extensions\n",
    "            if file_path.suffix not in self.CHECK_EXTENSIONS:\n",
    "                continue\n",
    "            \n",
    "            files_to_scan.append(file_path)\n",
    "        \n",
    "        return files_to_scan\n",
    "    \n",
    "    def _check_gitignore(self):\n",
    "        \"\"\"Check if .gitignore properly excludes sensitive files\"\"\"\n",
    "        gitignore_path = self.project_dir / \".gitignore\"\n",
    "        \n",
    "        if not gitignore_path.exists():\n",
    "            self.warnings.append(\"\u26a0\ufe0f No .gitignore file found!\")\n",
    "            return\n",
    "        \n",
    "        with open(gitignore_path) as f:\n",
    "            gitignore_content = f.read()\n",
    "        \n",
    "        # Check for important exclusions\n",
    "        important_exclusions = [\".env\", \"config.json\", \"secrets\", \"*.key\"]\n",
    "        missing_exclusions = []\n",
    "        \n",
    "        for exclusion in important_exclusions:\n",
    "            if exclusion not in gitignore_content:\n",
    "                missing_exclusions.append(exclusion)\n",
    "        \n",
    "        if missing_exclusions:\n",
    "            self.warnings.append(\n",
    "                f\"\u26a0\ufe0f .gitignore missing important exclusions: {', '.join(missing_exclusions)}\"\n",
    "            )\n",
    "    \n",
    "    def _check_git_history(self):\n",
    "        \"\"\"Check git history for accidentally committed keys\"\"\"\n",
    "        try:\n",
    "            # Only run if in a git repository\n",
    "            if not (self.project_dir / \".git\").exists():\n",
    "                return\n",
    "            \n",
    "            # Search git history for key patterns (last 50 commits)\n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"log\", \"-50\", \"--grep\", \"sk-\", \"--oneline\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                cwd=self.project_dir\n",
    "            )\n",
    "            \n",
    "            if result.stdout:\n",
    "                self.warnings.append(\n",
    "                    \"\u26a0\ufe0f Git history might contain API keys. Review commit history!\"\n",
    "                )\n",
    "        except Exception:\n",
    "            # Git might not be available\n",
    "            pass\n",
    "    \n",
    "    def _check_environment_files(self):\n",
    "        \"\"\"Check for improperly secured environment files\"\"\"\n",
    "        env_files = [\".env\", \".env.local\", \"config.json\", \"secrets.json\"]\n",
    "        \n",
    "        for env_file in env_files:\n",
    "            file_path = self.project_dir / env_file\n",
    "            if file_path.exists():\n",
    "                # Check permissions (Unix-like systems)\n",
    "                try:\n",
    "                    stats = file_path.stat()\n",
    "                    mode = oct(stats.st_mode)[-3:]\n",
    "                    if mode != \"600\":  # Should be readable only by owner\n",
    "                        self.warnings.append(\n",
    "                            f\"\u26a0\ufe0f {env_file} has loose permissions: {mode}\"\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    def _generate_summary(self) -> Dict:\n",
    "        \"\"\"Generate a summary of the audit results\"\"\"\n",
    "        severity_counts = {\"HIGH\": 0, \"MEDIUM\": 0, \"LOW\": 0}\n",
    "        \n",
    "        for violation in self.violations:\n",
    "            severity = violation.get(\"severity\", \"MEDIUM\")\n",
    "            severity_counts[severity] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_violations\": len(self.violations),\n",
    "            \"total_warnings\": len(self.warnings),\n",
    "            \"severity_breakdown\": severity_counts,\n",
    "            \"status\": self._get_status()\n",
    "        }\n",
    "    \n",
    "    def _get_status(self) -> str:\n",
    "        \"\"\"Determine overall security status\"\"\"\n",
    "        if len(self.violations) == 0 and len(self.warnings) == 0:\n",
    "            return \"\u2705 SECURE\"\n",
    "        elif len(self.violations) == 0:\n",
    "            return \"\u26a0\ufe0f WARNINGS\"\n",
    "        else:\n",
    "            return \"\u274c VULNERABLE\"\n",
    "    \n",
    "    def generate_report(self, results: Dict, output_format: str = \"console\"):\n",
    "        \"\"\"Generate a security report in various formats\"\"\"\n",
    "        \n",
    "        if output_format == \"console\":\n",
    "            self._print_console_report(results)\n",
    "        elif output_format == \"json\":\n",
    "            return json.dumps(results, indent=2)\n",
    "        elif output_format == \"markdown\":\n",
    "            return self._generate_markdown_report(results)\n",
    "    \n",
    "    def _print_console_report(self, results: Dict):\n",
    "        \"\"\"Print a formatted console report\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"\ud83d\udd12 SECURITY AUDIT REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Status\n",
    "        print(f\"\\nStatus: {results['summary']['status']}\")\n",
    "        print(f\"Files Scanned: {results['files_scanned']}\")\n",
    "        \n",
    "        # Violations\n",
    "        if results['violations']:\n",
    "            print(f\"\\n\u274c VIOLATIONS FOUND: {len(results['violations'])}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for violation in results['violations'][:5]:  # Show first 5\n",
    "                print(f\"\\n\ud83d\udcc1 {violation['file']} (line {violation['line']})\")\n",
    "                print(f\"   Type: {violation['type']}\")\n",
    "                print(f\"   Preview: {violation['preview']}\")\n",
    "            \n",
    "            if len(results['violations']) > 5:\n",
    "                print(f\"\\n... and {len(results['violations']) - 5} more violations\")\n",
    "        \n",
    "        # Warnings\n",
    "        if results['warnings']:\n",
    "            print(f\"\\n\u26a0\ufe0f WARNINGS: {len(results['warnings'])}\")\n",
    "            print(\"-\" * 40)\n",
    "            for warning in results['warnings']:\n",
    "                print(f\"  \u2022 {warning}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"\ud83d\udccb RECOMMENDATIONS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if results['violations']:\n",
    "            print(\"1. \u26a0\ufe0f IMMEDIATELY remove exposed keys from code\")\n",
    "            print(\"2. \ud83d\udd04 Rotate any exposed API keys\")\n",
    "            print(\"3. \ud83d\udcdd Move keys to environment variables or .env file\")\n",
    "            print(\"4. \ud83d\udeab Add .env to .gitignore\")\n",
    "            print(\"5. \ud83e\uddf9 Clean git history if keys were committed\")\n",
    "        else:\n",
    "            print(\"\u2705 No critical issues found!\")\n",
    "            print(\"Continue following security best practices:\")\n",
    "            print(\"  \u2022 Never hardcode API keys\")\n",
    "            print(\"  \u2022 Use environment variables\")\n",
    "            print(\"  \u2022 Keep .gitignore updated\")\n",
    "            print(\"  \u2022 Rotate keys periodically\")\n",
    "    \n",
    "    def _generate_markdown_report(self, results: Dict) -> str:\n",
    "        \"\"\"Generate a markdown-formatted report\"\"\"\n",
    "        md = f\"\"\"# Security Audit Report\n",
    "\n",
    "**Date:** {results['scan_time']}\n",
    "**Project:** {results['project_dir']}\n",
    "**Status:** {results['summary']['status']}\n",
    "\n",
    "## Summary\n",
    "- Files Scanned: {results['files_scanned']}\n",
    "- Violations: {results['summary']['total_violations']}\n",
    "- Warnings: {results['summary']['total_warnings']}\n",
    "\n",
    "## Violations\n",
    "\"\"\"\n",
    "        \n",
    "        if results['violations']:\n",
    "            for v in results['violations']:\n",
    "                md += f\"\\n### {v['file']} (line {v['line']})\\n\"\n",
    "                md += f\"- **Type:** {v['type']}\\n\"\n",
    "                md += f\"- **Severity:** {v['severity']}\\n\"\n",
    "                md += f\"- **Preview:** `{v['preview']}`\\n\"\n",
    "        else:\n",
    "            md += \"\\nNo violations found \u2705\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "\n",
    "def quick_security_check():\n",
    "    \"\"\"Run a quick security check on the current directory\"\"\"\n",
    "    auditor = APIKeyAuditor()\n",
    "    results = auditor.audit_project()\n",
    "    auditor.generate_report(results)\n",
    "    \n",
    "    return results['summary']['status'] == \"\u2705 SECURE\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Security Audit...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Run audit on current directory\n",
    "    auditor = APIKeyAuditor(\".\")\n",
    "    results = auditor.audit_project()\n",
    "    \n",
    "    # Generate report\n",
    "    auditor.generate_report(results, \"console\")\n",
    "    \n",
    "    # Save detailed report\n",
    "    with open(\"security_audit_report.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc4 Detailed report saved to: security_audit_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: first_api_call.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "# File: first_api_call.py\n",
    "\n",
    "\"\"\"\n",
    "Your first authenticated API calls to real AI services!\n",
    "Tests connections to OpenAI, Anthropic, and Google Gemini.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def test_openai() -> bool:\n",
    "    \"\"\"\n",
    "    Test OpenAI API connection with GPT-3.5-Turbo\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        # Get API key\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            print(\"\u274c OpenAI: No valid API key found\")\n",
    "            print(\"   Set OPENAI_API_KEY in your .env file\")\n",
    "            return False\n",
    "        \n",
    "        # Initialize client\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        # Make a simple test call\n",
    "        print(\"\ud83d\udd04 Testing OpenAI connection...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Respond with exactly: 'Hello, API world!'\"}\n",
    "            ],\n",
    "            max_tokens=20,\n",
    "            temperature=0  # Make response deterministic\n",
    "        )\n",
    "        \n",
    "        # Extract response\n",
    "        message = response.choices[0].message.content\n",
    "        print(f\"\ud83c\udf89 OpenAI Response: {message}\")\n",
    "        \n",
    "        # Show token usage\n",
    "        if hasattr(response, 'usage'):\n",
    "            print(f\"   Tokens used: {response.usage.total_tokens}\")\n",
    "            print(f\"   Model: {response.model}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u274c OpenAI: Package not installed\")\n",
    "        print(\"   Run: pip install openai\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c OpenAI Error: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_anthropic() -> bool:\n",
    "    \"\"\"\n",
    "    Test Anthropic API connection with Claude\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from anthropic import Anthropic\n",
    "        \n",
    "        # Get API key\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            print(\"\u274c Anthropic: No valid API key found\")\n",
    "            print(\"   Set ANTHROPIC_API_KEY in your .env file\")\n",
    "            return False\n",
    "        \n",
    "        # Initialize client\n",
    "        client = Anthropic(api_key=api_key)\n",
    "        \n",
    "        # Make a simple test call\n",
    "        print(\"\ud83d\udd04 Testing Anthropic connection...\")\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # Cheapest Claude model\n",
    "            max_tokens=20,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Respond with exactly: 'Hello, API world!'\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract response\n",
    "        response_text = message.content[0].text\n",
    "        print(f\"\ud83c\udf89 Anthropic Response: {response_text}\")\n",
    "        \n",
    "        # Show usage info\n",
    "        if hasattr(message, 'usage'):\n",
    "            print(f\"   Tokens used: {message.usage.input_tokens + message.usage.output_tokens}\")\n",
    "        print(f\"   Model: {message.model}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u274c Anthropic: Package not installed\")\n",
    "        print(\"   Run: pip install anthropic\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Anthropic Error: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_google() -> bool:\n",
    "    \"\"\"\n",
    "    Test Google Gemini API connection\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        \n",
    "        # Get API key\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            print(\"\u274c Google: No valid API key found\")\n",
    "            print(\"   Set GOOGLE_API_KEY in your .env file\")\n",
    "            return False\n",
    "        \n",
    "        # Configure API\n",
    "        genai.configure(api_key=api_key)\n",
    "        \n",
    "        # Create model\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        # Make a simple test call\n",
    "        print(\"\ud83d\udd04 Testing Google Gemini connection...\")\n",
    "        response = model.generate_content(\n",
    "            \"Respond with exactly: 'Hello, API world!'\",\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0,\n",
    "                max_output_tokens=20,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract response\n",
    "        print(f\"\ud83c\udf89 Google Response: {response.text}\")\n",
    "        print(f\"   Model: gemini-pro\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\u274c Google: Package not installed\")\n",
    "        print(\"   Run: pip install google-generativeai\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Google Error: {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_all_providers() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Test all configured AI providers\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test results for each provider\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"\ud83d\ude80 Testing AI API Connections\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check which providers have keys configured\n",
    "    providers = []\n",
    "    \n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        providers.append((\"OpenAI\", test_openai))\n",
    "    \n",
    "    if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "        providers.append((\"Anthropic\", test_anthropic))\n",
    "    \n",
    "    if os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"):\n",
    "        providers.append((\"Google\", test_google))\n",
    "    \n",
    "    if not providers:\n",
    "        print(\"\\n\u26a0\ufe0f No API keys configured!\")\n",
    "        print(\"\\nTo get started:\")\n",
    "        print(\"1. Create a .env file in your project directory\")\n",
    "        print(\"2. Add your API keys:\")\n",
    "        print(\"   OPENAI_API_KEY=your-key-here\")\n",
    "        print(\"   ANTHROPIC_API_KEY=your-key-here\")\n",
    "        print(\"   GOOGLE_API_KEY=your-key-here\")\n",
    "        return results\n",
    "    \n",
    "    # Test each provider\n",
    "    for name, test_func in providers:\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        results[name] = test_func()\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_summary(results: Dict[str, bool]):\n",
    "    \"\"\"Display a summary of test results\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"\ud83d\udcca Test Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No providers tested (no API keys configured)\")\n",
    "        return\n",
    "    \n",
    "    working = sum(1 for success in results.values() if success)\n",
    "    total = len(results)\n",
    "    \n",
    "    print(f\"\\nProviders tested: {total}\")\n",
    "    print(f\"Working: {working}\")\n",
    "    print(f\"Failed: {total - working}\")\n",
    "    \n",
    "    print(\"\\nDetails:\")\n",
    "    for provider, success in results.items():\n",
    "        status = \"\u2705 Working\" if success else \"\u274c Failed\"\n",
    "        print(f\"  {provider}: {status}\")\n",
    "    \n",
    "    if working == 0:\n",
    "        print(\"\\n\u26a0\ufe0f No working API connections!\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check that API keys are correctly set in .env\")\n",
    "        print(\"2. Verify you have credits/billing set up\")\n",
    "        print(\"3. Install required packages:\")\n",
    "        print(\"   pip install openai anthropic google-generativeai\")\n",
    "        print(\"4. Check your internet connection\")\n",
    "    elif working == total:\n",
    "        print(\"\\n\ud83c\udf89 All API connections working perfectly!\")\n",
    "        print(\"You're ready to build AI applications!\")\n",
    "    else:\n",
    "        print(f\"\\n\u2705 {working} provider(s) working - enough to get started!\")\n",
    "\n",
    "\n",
    "def check_prerequisites():\n",
    "    \"\"\"Check if all prerequisites are met\"\"\"\n",
    "    \n",
    "    print(\"Checking prerequisites...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check for .env file\n",
    "    if not os.path.exists(\".env\"):\n",
    "        print(\"\ud83d\udcdd No .env file found\")\n",
    "        print(\"Creating template .env file...\")\n",
    "        \n",
    "        template = \"\"\"# AI API Keys\n",
    "# Get your keys from:\n",
    "# OpenAI: https://platform.openai.com/api-keys\n",
    "# Anthropic: https://console.anthropic.com/\n",
    "# Google: https://aistudio.google.com/\n",
    "\n",
    "OPENAI_API_KEY=ADD_YOUR_KEY_HERE\n",
    "ANTHROPIC_API_KEY=ADD_YOUR_KEY_HERE\n",
    "GOOGLE_API_KEY=ADD_YOUR_KEY_HERE\n",
    "\"\"\"\n",
    "        \n",
    "        with open(\".env\", \"w\") as f:\n",
    "            f.write(template)\n",
    "        \n",
    "        print(\"\u2705 Created .env file - please add your API keys\")\n",
    "        return False\n",
    "    \n",
    "    # Check for python-dotenv\n",
    "    try:\n",
    "        import dotenv\n",
    "        print(\"\u2705 python-dotenv installed\")\n",
    "    except ImportError:\n",
    "        print(\"\u274c python-dotenv not installed\")\n",
    "        print(\"   Run: pip install python-dotenv\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\ud83c\udfaf First API Call - Testing Your AI Connections\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check prerequisites\n",
    "    if not check_prerequisites():\n",
    "        print(\"\\n\u26a0\ufe0f Please complete setup before testing APIs\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Test all providers\n",
    "    results = test_all_providers()\n",
    "    \n",
    "    # Display summary\n",
    "    display_summary(results)\n",
    "    \n",
    "    # Exit code based on results\n",
    "    if results and any(results.values()):\n",
    "        sys.exit(0)  # At least one working\n",
    "    else:\n",
    "        sys.exit(1)  # None working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 7.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.1: Secure Key Storage\n",
    "\n",
    "Create a complete key management system that:\n",
    "- Loads keys from multiple sources (.env, environment, config file)\n",
    "- Validates key format\n",
    "- Provides fallback options\n",
    "- Never exposes keys in logs or errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.2: Multi-Provider Authentication\n",
    "\n",
    "Build a class that can authenticate with multiple providers and automatically failover if one is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.3: Rate Limit Handler\n",
    "\n",
    "Implement a robust rate limit handler that:\n",
    "- Tracks requests per minute\n",
    "- Automatically throttles when approaching limits\n",
    "- Provides helpful feedback about wait times\n",
    "- Works with any API provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.4: API Key Audit Tool\n",
    "\n",
    "Create a tool that:\n",
    "- Scans a project for exposed API keys\n",
    "- Checks Git history for accidentally committed keys\n",
    "- Validates that all keys in use are properly secured\n",
    "- Generates a security report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.7: Costs and rate limiting considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: token_cost_calculator.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: token_cost_calculator.py\n",
    "\n",
    "\"\"\"\n",
    "Understanding token costs and calculating API expenses.\n",
    "Essential for building cost-effective AI applications.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def understand_token_pricing():\n",
    "    \"\"\"\n",
    "    Tokens are the currency of LLMs - understand how they work\n",
    "    \"\"\"\n",
    "    \n",
    "    # Approximate token counts for common text\n",
    "    examples = {\n",
    "        \"Hello\": 1,  # 1 token\n",
    "        \"Hello, world!\": 4,  # 4 tokens (Hello | , | world | !)\n",
    "        \"The quick brown fox\": 4,  # Common words = 1 token each\n",
    "        \"Anthropomorphization\": 3,  # Uncommon words split up\n",
    "        \"\ud83d\udc4d\": 1,  # Emojis usually 1-2 tokens\n",
    "        \"import numpy as np\": 5,  # Code tokens\n",
    "        \"def calculate_cost():\": 6,  # Function definitions\n",
    "        \"https://example.com\": 4,  # URLs split into parts\n",
    "    }\n",
    "    \n",
    "    # Token estimation rules of thumb\n",
    "    print(\"Token Estimation Rules:\")\n",
    "    print(\"\u2022 1 token \u2248 4 characters in English\")\n",
    "    print(\"\u2022 1 token \u2248 \u00be words\")\n",
    "    print(\"\u2022 100 tokens \u2248 75 words\")\n",
    "    print(\"\u2022 1 page of text \u2248 500 tokens\")\n",
    "    print(\"\u2022 1 conversation turn \u2248 50-200 tokens\")\n",
    "    print(\"\\nExamples:\")\n",
    "    for text, tokens in examples.items():\n",
    "        print(f\"  '{text}' = ~{tokens} tokens\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "def calculate_cost(input_tokens: int, output_tokens: int, model: str = \"gpt-3.5-turbo\") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate actual costs for different models\n",
    "    \n",
    "    Args:\n",
    "        input_tokens: Number of input/prompt tokens\n",
    "        output_tokens: Number of output/completion tokens\n",
    "        model: Model name\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with cost breakdown or None if model not found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prices per 1K tokens (as of 2024 - check provider docs for current prices)\n",
    "    pricing = {\n",
    "        # OpenAI Models\n",
    "        \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "        \"gpt-3.5-turbo-16k\": {\"input\": 0.003, \"output\": 0.004},\n",
    "        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "        \"gpt-4-32k\": {\"input\": 0.06, \"output\": 0.12},\n",
    "        \n",
    "        # Anthropic Models\n",
    "        \"claude-3-haiku\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "        \"claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n",
    "        \"claude-3-opus\": {\"input\": 0.015, \"output\": 0.075},\n",
    "        \"claude-2.1\": {\"input\": 0.008, \"output\": 0.024},\n",
    "        \n",
    "        # Google Models\n",
    "        \"gemini-pro\": {\"input\": 0.000125, \"output\": 0.000375},\n",
    "        \"gemini-pro-vision\": {\"input\": 0.000125, \"output\": 0.000375},\n",
    "        \n",
    "        # Other Models\n",
    "        \"llama-2-70b\": {\"input\": 0.001, \"output\": 0.001},  # Via Replicate\n",
    "        \"mixtral-8x7b\": {\"input\": 0.0005, \"output\": 0.0005},  # Via Replicate\n",
    "    }\n",
    "    \n",
    "    if model not in pricing:\n",
    "        print(f\"Warning: Model '{model}' not in pricing database\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate costs\n",
    "    input_cost = (input_tokens / 1000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Cost per 1K tokens (weighted average)\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    cost_per_1k = total_cost / (total_tokens / 1000) if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"cost_per_1k_tokens\": cost_per_1k,\n",
    "        \"breakdown\": f\"${input_cost:.6f} (input) + ${output_cost:.6f} (output)\"\n",
    "    }\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Estimate token count from text\n",
    "    \n",
    "    Args:\n",
    "        text: Text to estimate\n",
    "    \n",
    "    Returns:\n",
    "        Estimated token count\n",
    "    \"\"\"\n",
    "    # Rule of thumb: 1 token \u2248 4 characters or \u00be words\n",
    "    char_estimate = len(text) / 4\n",
    "    word_estimate = len(text.split()) * 4 / 3\n",
    "    \n",
    "    # Use average of both methods\n",
    "    return int((char_estimate + word_estimate) / 2)\n",
    "\n",
    "\n",
    "def calculate_real_world_costs():\n",
    "    \"\"\"\n",
    "    Calculate costs for real-world scenarios\n",
    "    \"\"\"\n",
    "    \n",
    "    scenarios = {\n",
    "        \"Customer Service Bot\": {\n",
    "            \"daily_conversations\": 100,\n",
    "            \"messages_per_conversation\": 10,\n",
    "            \"avg_input_tokens\": 50,\n",
    "            \"avg_output_tokens\": 100,\n",
    "            \"model\": \"gpt-3.5-turbo\"\n",
    "        },\n",
    "        \"Code Assistant\": {\n",
    "            \"daily_conversations\": 50,\n",
    "            \"messages_per_conversation\": 1,\n",
    "            \"avg_input_tokens\": 200,  # Code context\n",
    "            \"avg_output_tokens\": 300,  # Generated code\n",
    "            \"model\": \"gpt-4\"\n",
    "        },\n",
    "        \"Content Generator\": {\n",
    "            \"daily_conversations\": 10,\n",
    "            \"messages_per_conversation\": 1,\n",
    "            \"avg_input_tokens\": 100,  # Prompt\n",
    "            \"avg_output_tokens\": 800,  # Article\n",
    "            \"model\": \"claude-3-sonnet\"\n",
    "        },\n",
    "        \"Study Assistant\": {\n",
    "            \"daily_conversations\": 20,\n",
    "            \"messages_per_conversation\": 15,\n",
    "            \"avg_input_tokens\": 75,\n",
    "            \"avg_output_tokens\": 150,\n",
    "            \"model\": \"gemini-pro\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"REAL-WORLD COST SCENARIOS (Monthly Estimates)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, scenario in scenarios.items():\n",
    "        # Calculate daily usage\n",
    "        daily_messages = scenario[\"daily_conversations\"] * scenario[\"messages_per_conversation\"]\n",
    "        daily_input_tokens = daily_messages * scenario[\"avg_input_tokens\"]\n",
    "        daily_output_tokens = daily_messages * scenario[\"avg_output_tokens\"]\n",
    "        \n",
    "        # Calculate costs\n",
    "        cost_data = calculate_cost(\n",
    "            daily_input_tokens, \n",
    "            daily_output_tokens, \n",
    "            scenario[\"model\"]\n",
    "        )\n",
    "        \n",
    "        if cost_data:\n",
    "            daily_cost = cost_data[\"total_cost\"]\n",
    "            monthly_cost = daily_cost * 30\n",
    "            yearly_cost = monthly_cost * 12\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcca {name}:\")\n",
    "            print(f\"  Model: {scenario['model']}\")\n",
    "            print(f\"  Usage: {daily_messages} messages/day\")\n",
    "            print(f\"  Tokens: {daily_input_tokens + daily_output_tokens:,} tokens/day\")\n",
    "            print(f\"  Daily: ${daily_cost:.2f}\")\n",
    "            print(f\"  Monthly: ${monthly_cost:.2f}\")\n",
    "            print(f\"  Yearly: ${yearly_cost:.2f}\")\n",
    "            \n",
    "            # Cost breakdown\n",
    "            if monthly_cost > 100:\n",
    "                print(f\"  \u26a0\ufe0f High cost! Consider optimization strategies\")\n",
    "            elif monthly_cost < 10:\n",
    "                print(f\"  \u2705 Very affordable for this use case\")\n",
    "\n",
    "\n",
    "def compare_model_costs(prompt: str, expected_response_length: int = 200):\n",
    "    \"\"\"\n",
    "    Compare costs across different models for the same task\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt\n",
    "        expected_response_length: Expected response in tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    input_tokens = estimate_tokens(prompt)\n",
    "    output_tokens = expected_response_length\n",
    "    \n",
    "    models = [\n",
    "        \"gemini-pro\",      # Cheapest\n",
    "        \"claude-3-haiku\",  # Fast & cheap\n",
    "        \"gpt-3.5-turbo\",   # Popular choice\n",
    "        \"claude-3-sonnet\", # Good balance\n",
    "        \"gpt-4\",          # High quality\n",
    "        \"claude-3-opus\",   # Top tier\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COST COMPARISON\")\n",
    "    print(f\"Prompt: {len(prompt)} chars (~{input_tokens} tokens)\")\n",
    "    print(f\"Expected response: ~{output_tokens} tokens\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model in models:\n",
    "        cost_data = calculate_cost(input_tokens, output_tokens, model)\n",
    "        if cost_data:\n",
    "            results.append((model, cost_data[\"total_cost\"]))\n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  Cost per call: ${cost_data['total_cost']:.6f}\")\n",
    "            print(f\"  1,000 calls: ${cost_data['total_cost'] * 1000:.2f}\")\n",
    "            print(f\"  10,000 calls: ${cost_data['total_cost'] * 10000:.2f}\")\n",
    "    \n",
    "    # Show cheapest vs most expensive\n",
    "    if results:\n",
    "        results.sort(key=lambda x: x[1])\n",
    "        cheapest = results[0]\n",
    "        most_expensive = results[-1]\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"\ud83d\udcb0 Cheapest: {cheapest[0]} (${cheapest[1]:.6f}/call)\")\n",
    "        print(f\"\ud83d\udc8e Most expensive: {most_expensive[0]} (${most_expensive[1]:.6f}/call)\")\n",
    "        print(f\"\ud83d\udcca Price difference: {most_expensive[1]/cheapest[1]:.1f}x more expensive\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate token understanding\n",
    "    print(\"Understanding Tokens\")\n",
    "    print(\"-\" * 60)\n",
    "    understand_token_pricing()\n",
    "    \n",
    "    # Calculate real-world costs\n",
    "    print(\"\\n\")\n",
    "    calculate_real_world_costs()\n",
    "    \n",
    "    # Compare models\n",
    "    sample_prompt = \"Explain the concept of recursion in programming with an example\"\n",
    "    compare_model_costs(sample_prompt, expected_response_length=300)\n",
    "    \n",
    "    # Example: Calculate specific cost\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Specific Cost Example\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = calculate_cost(500, 1500, \"gpt-3.5-turbo\")\n",
    "    if result:\n",
    "        print(f\"Model: {result['model']}\")\n",
    "        print(f\"Total tokens: {result['total_tokens']}\")\n",
    "        print(f\"Total cost: ${result['total_cost']:.4f}\")\n",
    "        print(f\"Breakdown: {result['breakdown']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: context_management.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: context_management.py\n",
    "\n",
    "\"\"\"\n",
    "Managing conversation context to avoid the hidden cost trap.\n",
    "Shows the difference between bad and good context management.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "import time\n",
    "\n",
    "\n",
    "def bad_context_accumulation():\n",
    "    \"\"\"\n",
    "    \u274c BAD: Context grows with each message, costs grow quadratically!\n",
    "    This is a common mistake that can make costs explode.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"\u274c BAD EXAMPLE: Uncontrolled Context Growth\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    messages = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Simulate 100 message exchanges\n",
    "    for i in range(20):  # Using 20 for demo, imagine 100+\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Message {i}\"})\n",
    "        \n",
    "        # Each API call includes ALL previous messages!\n",
    "        tokens_in_request = sum(len(m[\"content\"]) for m in messages) * 5  # Rough token estimate\n",
    "        total_tokens += tokens_in_request\n",
    "        \n",
    "        print(f\"Message {i+1}: Sending {len(messages)} messages ({tokens_in_request} tokens)\")\n",
    "        \n",
    "        # Add assistant response (simulated)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Response {i}\"})\n",
    "    \n",
    "    print(f\"\\nTotal tokens sent: {total_tokens}\")\n",
    "    print(f\"Average tokens per request: {total_tokens/20:.0f}\")\n",
    "    print(\"\u26a0\ufe0f Notice how token count grows with each message!\")\n",
    "    \n",
    "    # Calculate approximate cost\n",
    "    cost_per_1k = 0.002  # GPT-3.5-turbo average\n",
    "    total_cost = (total_tokens / 1000) * cost_per_1k\n",
    "    print(f\"Estimated cost: ${total_cost:.4f}\")\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def good_context_management():\n",
    "    \"\"\"\n",
    "    \u2705 GOOD: Manage context size to keep costs under control\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\u2705 GOOD EXAMPLE: Managed Context\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    messages = []\n",
    "    total_tokens = 0\n",
    "    max_context_tokens = 2000  # Set a limit\n",
    "    \n",
    "    for i in range(20):\n",
    "        # Add user message\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Message {i}\"})\n",
    "        \n",
    "        # Manage context size BEFORE making API call\n",
    "        messages = manage_context(messages, max_tokens=max_context_tokens)\n",
    "        \n",
    "        tokens_in_request = sum(len(m[\"content\"]) for m in messages) * 5\n",
    "        total_tokens += tokens_in_request\n",
    "        \n",
    "        print(f\"Message {i+1}: Sending {len(messages)} messages ({tokens_in_request} tokens)\")\n",
    "        \n",
    "        # Add assistant response\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Response {i}\"})\n",
    "    \n",
    "    print(f\"\\nTotal tokens sent: {total_tokens}\")\n",
    "    print(f\"Average tokens per request: {total_tokens/20:.0f}\")\n",
    "    print(\"\u2705 Token count stays controlled!\")\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_per_1k = 0.002\n",
    "    total_cost = (total_tokens / 1000) * cost_per_1k\n",
    "    print(f\"Estimated cost: ${total_cost:.4f}\")\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def manage_context(messages: List[Dict], max_tokens: int = 2000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Keep context under control by removing old messages\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries\n",
    "        max_tokens: Maximum tokens to keep in context\n",
    "    \n",
    "    Returns:\n",
    "        Trimmed message list\n",
    "    \"\"\"\n",
    "    # Always keep system message if present\n",
    "    has_system = messages and messages[0].get(\"role\") == \"system\"\n",
    "    start_index = 1 if has_system else 0\n",
    "    \n",
    "    # Estimate total tokens (rough: 1 token \u2248 4 characters)\n",
    "    def estimate_tokens(msgs):\n",
    "        return sum(len(m.get(\"content\", \"\")) / 4 for m in msgs)\n",
    "    \n",
    "    total_tokens = estimate_tokens(messages)\n",
    "    \n",
    "    # Remove oldest messages if over limit\n",
    "    while total_tokens > max_tokens and len(messages) > start_index + 2:\n",
    "        # Remove oldest user-assistant pair (keep system message)\n",
    "        messages.pop(start_index)  # Remove oldest user message\n",
    "        if start_index < len(messages) and messages[start_index].get(\"role\") == \"assistant\":\n",
    "            messages.pop(start_index)  # Remove corresponding assistant message\n",
    "        \n",
    "        total_tokens = estimate_tokens(messages)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def smart_context_strategies():\n",
    "    \"\"\"\n",
    "    Advanced context management strategies\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SMART CONTEXT MANAGEMENT STRATEGIES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    strategies = {\n",
    "        \"Sliding Window\": {\n",
    "            \"description\": \"Keep only last N messages\",\n",
    "            \"pros\": \"Simple, predictable cost\",\n",
    "            \"cons\": \"Loses older context\",\n",
    "            \"code\": \"\"\"\n",
    "messages = messages[-10:]  # Keep last 10 messages\n",
    "            \"\"\"\n",
    "        },\n",
    "        \"Summarization\": {\n",
    "            \"description\": \"Periodically summarize old messages\",\n",
    "            \"pros\": \"Preserves key information\",\n",
    "            \"cons\": \"Requires extra API call for summary\",\n",
    "            \"code\": \"\"\"\n",
    "if len(messages) > 20:\n",
    "    summary = summarize_messages(messages[:-10])\n",
    "    messages = [{\"role\": \"system\", \"content\": summary}] + messages[-10:]\n",
    "            \"\"\"\n",
    "        },\n",
    "        \"Importance Scoring\": {\n",
    "            \"description\": \"Keep only important messages\",\n",
    "            \"pros\": \"Retains critical context\",\n",
    "            \"cons\": \"Complex to implement\",\n",
    "            \"code\": \"\"\"\n",
    "messages = [m for m in messages if m.get('importance', 0) > threshold]\n",
    "            \"\"\"\n",
    "        },\n",
    "        \"Token Budget\": {\n",
    "            \"description\": \"Allocate token budget per conversation turn\",\n",
    "            \"pros\": \"Precise cost control\",\n",
    "            \"cons\": \"May cut off mid-conversation\",\n",
    "            \"code\": \"\"\"\n",
    "while calculate_tokens(messages) > budget:\n",
    "    messages.pop(1)  # Remove oldest after system\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, strategy in strategies.items():\n",
    "        print(f\"\\n\ud83d\udccb {name}:\")\n",
    "        print(f\"  Description: {strategy['description']}\")\n",
    "        print(f\"  \u2705 Pros: {strategy['pros']}\")\n",
    "        print(f\"  \u274c Cons: {strategy['cons']}\")\n",
    "        print(f\"  Code snippet:{strategy['code']}\")\n",
    "\n",
    "\n",
    "def safe_retry_pattern():\n",
    "    \"\"\"\n",
    "    Safe retry pattern that avoids infinite cost loops\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAFE RETRY PATTERNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # \u274c BAD: Infinite retries\n",
    "    print(\"\\n\u274c BAD: Infinite retry loop\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\"\"\n",
    "while True:\n",
    "    try:\n",
    "        response = expensive_api_call()\n",
    "        break\n",
    "    except:\n",
    "        continue  # This could run forever!\n",
    "\"\"\")\n",
    "    \n",
    "    # \u2705 GOOD: Limited retries with backoff\n",
    "    print(\"\\n\u2705 GOOD: Limited retries with exponential backoff\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def safe_api_call(max_retries=3):\n",
    "        \"\"\"Safe API call with limited retries and backoff\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Simulate API call\n",
    "                if attempt < 2:  # Simulate failures\n",
    "                    raise Exception(\"API Error\")\n",
    "                return {\"success\": True, \"attempt\": attempt + 1}\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"  Failed after {max_retries} attempts\")\n",
    "                    raise\n",
    "                \n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"  Attempt {attempt + 1} failed, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Demonstrate\n",
    "    print(\"\\nDemonstration:\")\n",
    "    try:\n",
    "        result = safe_api_call()\n",
    "        print(f\"  Success on attempt {result['attempt']}\")\n",
    "    except:\n",
    "        print(\"  Final failure - stopping to prevent cost overrun\")\n",
    "\n",
    "\n",
    "class ContextWindowManager:\n",
    "    \"\"\"\n",
    "    Professional context window management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 4000, reserve_tokens: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize context manager\n",
    "        \n",
    "        Args:\n",
    "            max_tokens: Maximum context window size\n",
    "            reserve_tokens: Tokens to reserve for response\n",
    "        \"\"\"\n",
    "        self.max_tokens = max_tokens\n",
    "        self.reserve_tokens = reserve_tokens\n",
    "        self.effective_max = max_tokens - reserve_tokens\n",
    "        self.total_trimmed = 0\n",
    "        self.trim_count = 0\n",
    "    \n",
    "    def prepare_context(self, messages: List[Dict], new_message: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare context for API call\n",
    "        \n",
    "        Args:\n",
    "            messages: Current conversation history\n",
    "            new_message: New message to add\n",
    "        \n",
    "        Returns:\n",
    "            Optimized message list within token budget\n",
    "        \"\"\"\n",
    "        # Add new message temporarily\n",
    "        temp_messages = messages + [{\"role\": \"user\", \"content\": new_message}]\n",
    "        \n",
    "        # Calculate current size\n",
    "        current_tokens = self._estimate_tokens(temp_messages)\n",
    "        \n",
    "        if current_tokens <= self.effective_max:\n",
    "            return temp_messages\n",
    "        \n",
    "        # Need to trim\n",
    "        self.trim_count += 1\n",
    "        tokens_to_trim = current_tokens - self.effective_max\n",
    "        self.total_trimmed += tokens_to_trim\n",
    "        \n",
    "        # Trim strategy: Remove oldest messages but keep system\n",
    "        trimmed = self._trim_messages(temp_messages, self.effective_max)\n",
    "        \n",
    "        print(f\"\ud83d\udd04 Trimmed context: {current_tokens} \u2192 {self._estimate_tokens(trimmed)} tokens\")\n",
    "        \n",
    "        return trimmed\n",
    "    \n",
    "    def _estimate_tokens(self, messages: List[Dict]) -> int:\n",
    "        \"\"\"Estimate token count\"\"\"\n",
    "        return sum(len(m.get(\"content\", \"\")) // 4 for m in messages)\n",
    "    \n",
    "    def _trim_messages(self, messages: List[Dict], target_tokens: int) -> List[Dict]:\n",
    "        \"\"\"Trim messages to fit within token budget\"\"\"\n",
    "        # Keep system message if present\n",
    "        result = []\n",
    "        if messages and messages[0].get(\"role\") == \"system\":\n",
    "            result.append(messages[0])\n",
    "            messages = messages[1:]\n",
    "        \n",
    "        # Keep most recent messages that fit\n",
    "        for msg in reversed(messages):\n",
    "            test_result = [msg] + result[1:] if result else [msg]\n",
    "            if self._estimate_tokens(result[:1] + test_result) <= target_tokens:\n",
    "                result = result[:1] + test_result if result else test_result\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get context management statistics\"\"\"\n",
    "        return {\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"reserve_tokens\": self.reserve_tokens,\n",
    "            \"effective_max\": self.effective_max,\n",
    "            \"total_trimmed\": self.total_trimmed,\n",
    "            \"trim_count\": self.trim_count,\n",
    "            \"avg_trimmed\": self.total_trimmed / max(1, self.trim_count)\n",
    "        }\n",
    "\n",
    "\n",
    "def demonstrate_context_costs():\n",
    "    \"\"\"\n",
    "    Show the real cost impact of context management\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COST IMPACT COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    bad_tokens = bad_context_accumulation()\n",
    "    good_tokens = good_context_management()\n",
    "    \n",
    "    savings = bad_tokens - good_tokens\n",
    "    savings_percent = (savings / bad_tokens) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Bad approach: {bad_tokens:,} tokens\")\n",
    "    print(f\"Good approach: {good_tokens:,} tokens\")\n",
    "    print(f\"Tokens saved: {savings:,} ({savings_percent:.1f}%)\")\n",
    "    \n",
    "    # Cost calculation\n",
    "    cost_per_1k = 0.002\n",
    "    money_saved = (savings / 1000) * cost_per_1k\n",
    "    print(f\"Money saved on 20 messages: ${money_saved:.4f}\")\n",
    "    print(f\"Projected monthly savings (1000 conversations): ${money_saved * 50:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate the cost difference\n",
    "    demonstrate_context_costs()\n",
    "    \n",
    "    # Show strategies\n",
    "    smart_context_strategies()\n",
    "    \n",
    "    # Show safe retry pattern\n",
    "    safe_retry_pattern()\n",
    "    \n",
    "    # Professional context manager demo\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROFESSIONAL CONTEXT MANAGER DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    manager = ContextWindowManager(max_tokens=2000)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    ]\n",
    "    \n",
    "    # Simulate conversation\n",
    "    for i in range(10):\n",
    "        new_msg = f\"This is message {i} with some content to simulate real conversation length.\"\n",
    "        messages = manager.prepare_context(messages[:-1] if i > 0 else messages, new_msg)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Response to message {i}\"})\n",
    "    \n",
    "    stats = manager.get_stats()\n",
    "    print(\"\\nContext Manager Stats:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_rate_limiter.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: smart_rate_limiter.py\n",
    "\n",
    "\"\"\"\n",
    "Intelligent rate limiting that maximizes throughput while respecting limits.\n",
    "Includes user feedback and statistics tracking.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "\n",
    "class RateLimitTypes:\n",
    "    \"\"\"Different types of rate limits you'll encounter\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.limits = {\n",
    "            \"requests_per_minute\": \"How many API calls you can make\",\n",
    "            \"tokens_per_minute\": \"How much text you can process\", \n",
    "            \"requests_per_day\": \"Daily quota (free tiers)\",\n",
    "            \"concurrent_requests\": \"Parallel calls allowed\",\n",
    "            \"tokens_per_request\": \"Max size of single request\"\n",
    "        }\n",
    "    \n",
    "    def show_provider_limits(self) -> Dict:\n",
    "        \"\"\"Real limits from major providers (as of 2024)\"\"\"\n",
    "        \n",
    "        limits = {\n",
    "            \"OpenAI GPT-3.5\": {\n",
    "                \"tier_1\": {\"rpm\": 3, \"tpm\": 40000, \"max_tokens\": 4096},\n",
    "                \"tier_2\": {\"rpm\": 60, \"tpm\": 60000, \"max_tokens\": 4096},\n",
    "                \"tier_3\": {\"rpm\": 500, \"tpm\": 160000, \"max_tokens\": 4096}\n",
    "            },\n",
    "            \"OpenAI GPT-4\": {\n",
    "                \"tier_1\": {\"rpm\": 3, \"tpm\": 10000, \"max_tokens\": 8192},\n",
    "                \"tier_2\": {\"rpm\": 20, \"tpm\": 40000, \"max_tokens\": 8192},\n",
    "                \"tier_3\": {\"rpm\": 120, \"tpm\": 300000, \"max_tokens\": 8192}\n",
    "            },\n",
    "            \"Anthropic Claude\": {\n",
    "                \"default\": {\"rpm\": 50, \"tpm\": 100000, \"max_tokens\": 200000},\n",
    "                \"scale\": {\"rpm\": 1000, \"tpm\": 2000000, \"max_tokens\": 200000}\n",
    "            },\n",
    "            \"Google Gemini\": {\n",
    "                \"free\": {\"rpm\": 60, \"rpd\": 1500, \"tpm\": 1000000},\n",
    "                \"paid\": {\"rpm\": 360, \"rpd\": 30000, \"tpm\": 4000000}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return limits\n",
    "    \n",
    "    def print_limits(self):\n",
    "        \"\"\"Display provider limits in a readable format\"\"\"\n",
    "        limits = self.show_provider_limits()\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"PROVIDER RATE LIMITS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for provider, tiers in limits.items():\n",
    "            print(f\"\\n\ud83d\udcca {provider}:\")\n",
    "            for tier, limits in tiers.items():\n",
    "                print(f\"  {tier}:\")\n",
    "                for key, value in limits.items():\n",
    "                    if key == \"rpm\":\n",
    "                        print(f\"    Requests/min: {value}\")\n",
    "                    elif key == \"tpm\":\n",
    "                        print(f\"    Tokens/min: {value:,}\")\n",
    "                    elif key == \"rpd\":\n",
    "                        print(f\"    Requests/day: {value:,}\")\n",
    "                    elif key == \"max_tokens\":\n",
    "                        print(f\"    Max tokens: {value:,}\")\n",
    "\n",
    "\n",
    "class SmartRateLimiter:\n",
    "    \"\"\"\n",
    "    Intelligent rate limiting that maximizes throughput\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute: int = 60, tokens_per_minute: int = 40000):\n",
    "        \"\"\"\n",
    "        Initialize rate limiter\n",
    "        \n",
    "        Args:\n",
    "            requests_per_minute: RPM limit\n",
    "            tokens_per_minute: TPM limit\n",
    "        \"\"\"\n",
    "        self.rpm_limit = requests_per_minute\n",
    "        self.tpm_limit = tokens_per_minute\n",
    "        \n",
    "        # Track request times and token counts\n",
    "        self.request_times = deque()\n",
    "        self.token_counts = deque()\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_wait_time = 0\n",
    "        self.total_requests = 0\n",
    "        self.total_tokens = 0\n",
    "        self.rate_limit_hits = 0\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate token count for text\n",
    "        \n",
    "        Args:\n",
    "            text: Text to estimate\n",
    "        \n",
    "        Returns:\n",
    "            Estimated token count\n",
    "        \"\"\"\n",
    "        # Rough estimate: 1 token \u2248 4 characters\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def can_proceed(self, estimated_tokens: int) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Check if we can make a request now\n",
    "        \n",
    "        Args:\n",
    "            estimated_tokens: Estimated tokens for the request\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (can_proceed, reason_if_not)\n",
    "        \"\"\"\n",
    "        current_time = datetime.now()\n",
    "        minute_ago = current_time - timedelta(minutes=1)\n",
    "        \n",
    "        # Clean old entries\n",
    "        while self.request_times and self.request_times[0] < minute_ago:\n",
    "            self.request_times.popleft()\n",
    "            if self.token_counts:\n",
    "                self.token_counts.popleft()\n",
    "        \n",
    "        # Check request limit\n",
    "        if len(self.request_times) >= self.rpm_limit:\n",
    "            return False, f\"Request limit reached ({self.rpm_limit} RPM)\"\n",
    "        \n",
    "        # Check token limit\n",
    "        current_tokens = sum(self.token_counts)\n",
    "        if current_tokens + estimated_tokens > self.tpm_limit:\n",
    "            return False, f\"Token limit reached ({self.tpm_limit} TPM)\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def wait_if_needed(self, estimated_tokens: int) -> float:\n",
    "        \"\"\"\n",
    "        Smart waiting with user feedback\n",
    "        \n",
    "        Args:\n",
    "            estimated_tokens: Estimated tokens for the request\n",
    "        \n",
    "        Returns:\n",
    "            Number of seconds waited\n",
    "        \"\"\"\n",
    "        total_waited = 0\n",
    "        \n",
    "        while True:\n",
    "            can_proceed, reason = self.can_proceed(estimated_tokens)\n",
    "            \n",
    "            if can_proceed:\n",
    "                break\n",
    "            \n",
    "            self.rate_limit_hits += 1\n",
    "            \n",
    "            # Calculate optimal wait time\n",
    "            if self.request_times:\n",
    "                oldest_request = self.request_times[0]\n",
    "                wait_until = oldest_request + timedelta(minutes=1)\n",
    "                wait_seconds = (wait_until - datetime.now()).total_seconds()\n",
    "                \n",
    "                if wait_seconds > 0:\n",
    "                    # Provide helpful feedback\n",
    "                    current_rpm = len(self.request_times)\n",
    "                    current_tpm = sum(self.token_counts)\n",
    "                    \n",
    "                    print(f\"\\n\u23f3 Rate limit: {reason}\")\n",
    "                    print(f\"   Current usage: {current_rpm}/{self.rpm_limit} RPM, {current_tpm:,}/{self.tpm_limit:,} TPM\")\n",
    "                    print(f\"   Waiting {wait_seconds:.1f} seconds...\")\n",
    "                    print(f\"   (Request #{self.total_requests + 1})\")\n",
    "                    \n",
    "                    # Show progress bar for long waits\n",
    "                    if wait_seconds > 5:\n",
    "                        self._show_wait_progress(wait_seconds)\n",
    "                    else:\n",
    "                        time.sleep(wait_seconds)\n",
    "                    \n",
    "                    total_waited += wait_seconds\n",
    "                    self.total_wait_time += wait_seconds\n",
    "            else:\n",
    "                # Should not happen, but safety check\n",
    "                time.sleep(1)\n",
    "                total_waited += 1\n",
    "        \n",
    "        return total_waited\n",
    "    \n",
    "    def _show_wait_progress(self, wait_seconds: float):\n",
    "        \"\"\"Show progress bar while waiting\"\"\"\n",
    "        intervals = 20\n",
    "        interval_time = wait_seconds / intervals\n",
    "        \n",
    "        for i in range(intervals):\n",
    "            progress = (i + 1) / intervals\n",
    "            bar_length = 30\n",
    "            filled = int(bar_length * progress)\n",
    "            bar = \"\u2588\" * filled + \"\u2591\" * (bar_length - filled)\n",
    "            remaining = wait_seconds - (i * interval_time)\n",
    "            print(f\"\\r   [{bar}] {remaining:.1f}s remaining\", end=\"\")\n",
    "            time.sleep(interval_time)\n",
    "        print(\"\\r   \u2705 Ready to proceed!                        \")\n",
    "    \n",
    "    def record_request(self, actual_tokens: int):\n",
    "        \"\"\"\n",
    "        Record that a request was made\n",
    "        \n",
    "        Args:\n",
    "            actual_tokens: Actual token count used\n",
    "        \"\"\"\n",
    "        self.request_times.append(datetime.now())\n",
    "        self.token_counts.append(actual_tokens)\n",
    "        self.total_requests += 1\n",
    "        self.total_tokens += actual_tokens\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get rate limiting statistics\"\"\"\n",
    "        current_rpm = len(self.request_times)\n",
    "        current_tpm = sum(self.token_counts)\n",
    "        \n",
    "        # Calculate efficiency\n",
    "        if self.total_requests > 0:\n",
    "            avg_wait = self.total_wait_time / self.total_requests\n",
    "            efficiency = (self.total_requests / (self.total_requests + self.total_wait_time)) * 100\n",
    "        else:\n",
    "            avg_wait = 0\n",
    "            efficiency = 100\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"total_wait_time\": f\"{self.total_wait_time:.1f}s\",\n",
    "            \"avg_wait_per_request\": f\"{avg_wait:.2f}s\",\n",
    "            \"current_rpm\": current_rpm,\n",
    "            \"current_tpm\": current_tpm,\n",
    "            \"rpm_usage\": f\"{(current_rpm/self.rpm_limit)*100:.1f}%\",\n",
    "            \"tpm_usage\": f\"{(current_tpm/self.tpm_limit)*100:.1f}%\",\n",
    "            \"rate_limit_hits\": self.rate_limit_hits,\n",
    "            \"efficiency\": f\"{efficiency:.1f}%\"\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all statistics\"\"\"\n",
    "        self.request_times.clear()\n",
    "        self.token_counts.clear()\n",
    "        self.total_wait_time = 0\n",
    "        self.total_requests = 0\n",
    "        self.total_tokens = 0\n",
    "        self.rate_limit_hits = 0\n",
    "\n",
    "\n",
    "class AdaptiveRateLimiter:\n",
    "    \"\"\"\n",
    "    Advanced rate limiter that adapts to actual API responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize adaptive rate limiter\"\"\"\n",
    "        self.limiters = {}  # One limiter per model\n",
    "        self.performance_history = {}\n",
    "        self.last_429_time = {}  # Track when we last hit 429 errors\n",
    "    \n",
    "    def get_limiter(self, model: str) -> SmartRateLimiter:\n",
    "        \"\"\"Get or create limiter for a specific model\"\"\"\n",
    "        if model not in self.limiters:\n",
    "            # Set conservative defaults, will adapt based on responses\n",
    "            self.limiters[model] = SmartRateLimiter(\n",
    "                requests_per_minute=30,  # Start conservative\n",
    "                tokens_per_minute=30000\n",
    "            )\n",
    "        return self.limiters[model]\n",
    "    \n",
    "    def handle_response(self, model: str, success: bool, headers: Dict = None):\n",
    "        \"\"\"\n",
    "        Adapt limits based on API response\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            success: Whether request succeeded\n",
    "            headers: Response headers (may contain rate limit info)\n",
    "        \"\"\"\n",
    "        if not success:\n",
    "            # Hit rate limit, back off\n",
    "            if model in self.limiters:\n",
    "                limiter = self.limiters[model]\n",
    "                # Reduce limits by 20%\n",
    "                limiter.rpm_limit = int(limiter.rpm_limit * 0.8)\n",
    "                limiter.tpm_limit = int(limiter.tpm_limit * 0.8)\n",
    "                self.last_429_time[model] = datetime.now()\n",
    "                print(f\"\u26a0\ufe0f Rate limit hit for {model}, reducing to {limiter.rpm_limit} RPM\")\n",
    "        \n",
    "        elif headers:\n",
    "            # Some APIs provide rate limit info in headers\n",
    "            self._parse_rate_limit_headers(model, headers)\n",
    "    \n",
    "    def _parse_rate_limit_headers(self, model: str, headers: Dict):\n",
    "        \"\"\"Parse rate limit information from response headers\"\"\"\n",
    "        # Example headers (varies by provider):\n",
    "        # X-RateLimit-Limit-Requests\n",
    "        # X-RateLimit-Remaining-Requests\n",
    "        # X-RateLimit-Reset-Requests\n",
    "        \n",
    "        if \"x-ratelimit-limit-requests\" in headers:\n",
    "            limit = int(headers[\"x-ratelimit-limit-requests\"])\n",
    "            if model in self.limiters:\n",
    "                self.limiters[model].rpm_limit = limit\n",
    "        \n",
    "        if \"x-ratelimit-remaining-requests\" in headers:\n",
    "            remaining = int(headers[\"x-ratelimit-remaining-requests\"])\n",
    "            # Could use this to optimize request timing\n",
    "    \n",
    "    def should_increase_limits(self, model: str) -> bool:\n",
    "        \"\"\"Check if we should try increasing limits\"\"\"\n",
    "        if model not in self.last_429_time:\n",
    "            return True\n",
    "        \n",
    "        # Wait at least 5 minutes after a 429 before increasing\n",
    "        time_since_429 = datetime.now() - self.last_429_time[model]\n",
    "        return time_since_429 > timedelta(minutes=5)\n",
    "    \n",
    "    def optimize_limits(self, model: str):\n",
    "        \"\"\"Gradually increase limits if no errors\"\"\"\n",
    "        if self.should_increase_limits(model) and model in self.limiters:\n",
    "            limiter = self.limiters[model]\n",
    "            # Increase by 10%\n",
    "            limiter.rpm_limit = min(int(limiter.rpm_limit * 1.1), 500)  # Cap at 500\n",
    "            limiter.tpm_limit = min(int(limiter.tpm_limit * 1.1), 200000)  # Cap\n",
    "            print(f\"\ud83d\udcc8 Optimizing {model} limits to {limiter.rpm_limit} RPM\")\n",
    "\n",
    "\n",
    "def demonstrate_rate_limiting():\n",
    "    \"\"\"Demonstrate rate limiting in action\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"SMART RATE LIMITING DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create rate limiter with low limits for demo\n",
    "    limiter = SmartRateLimiter(requests_per_minute=5, tokens_per_minute=1000)\n",
    "    \n",
    "    print(f\"\\nLimits: {limiter.rpm_limit} RPM, {limiter.tpm_limit} TPM\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Simulate API calls\n",
    "    prompts = [\n",
    "        \"Short prompt\",\n",
    "        \"This is a medium length prompt with more tokens\",\n",
    "        \"This is a much longer prompt that contains significantly more tokens and will use up more of our token budget\",\n",
    "        \"Another short one\",\n",
    "        \"Medium prompt here\",\n",
    "        \"Final prompt\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n\ud83d\udcdd Request {i+1}: '{prompt[:30]}...'\")\n",
    "        \n",
    "        # Estimate tokens\n",
    "        estimated_tokens = limiter.estimate_tokens(prompt) * 10  # Multiply for demo\n",
    "        print(f\"   Estimated tokens: {estimated_tokens}\")\n",
    "        \n",
    "        # Wait if needed\n",
    "        wait_time = limiter.wait_if_needed(estimated_tokens)\n",
    "        if wait_time == 0:\n",
    "            print(\"   \u2705 No wait needed\")\n",
    "        \n",
    "        # Simulate API call\n",
    "        print(\"   Making API call...\")\n",
    "        time.sleep(0.5)  # Simulate API latency\n",
    "        \n",
    "        # Record request\n",
    "        actual_tokens = estimated_tokens + 50  # Simulate actual usage\n",
    "        limiter.record_request(actual_tokens)\n",
    "        print(f\"   \u2705 Complete! Used {actual_tokens} tokens\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RATE LIMITING STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stats = limiter.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "def demonstrate_adaptive_limiting():\n",
    "    \"\"\"Demonstrate adaptive rate limiting\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ADAPTIVE RATE LIMITING DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    adapter = AdaptiveRateLimiter()\n",
    "    \n",
    "    models = [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3-haiku\"]\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n\ud83e\udd16 Testing {model}\")\n",
    "        limiter = adapter.get_limiter(model)\n",
    "        print(f\"   Initial limits: {limiter.rpm_limit} RPM\")\n",
    "        \n",
    "        # Simulate successful requests\n",
    "        for i in range(3):\n",
    "            adapter.handle_response(model, success=True)\n",
    "        \n",
    "        # Try to optimize\n",
    "        adapter.optimize_limits(model)\n",
    "        \n",
    "        # Simulate rate limit hit\n",
    "        adapter.handle_response(model, success=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Show provider limits\n",
    "    rate_types = RateLimitTypes()\n",
    "    rate_types.print_limits()\n",
    "    \n",
    "    # Demonstrate smart rate limiting\n",
    "    print(\"\\n\")\n",
    "    demonstrate_rate_limiting()\n",
    "    \n",
    "    # Demonstrate adaptive rate limiting\n",
    "    demonstrate_adaptive_limiting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: cost_optimization.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: cost_optimization.py\n",
    "\n",
    "\"\"\"\n",
    "Battle-tested strategies to minimize API costs while maintaining quality.\n",
    "Includes model selection, caching, and batch processing.\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "\n",
    "def smart_model_selection(task_type: str, complexity: int, max_cost_per_request: float = 0.01) -> str:\n",
    "    \"\"\"\n",
    "    Choose the cheapest model that can handle the task\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of task (e.g., \"qa\", \"code\", \"creative\")\n",
    "        complexity: Complexity score 1-10\n",
    "        max_cost_per_request: Maximum acceptable cost\n",
    "    \n",
    "    Returns:\n",
    "        Recommended model name\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model capabilities and costs (prices as of 2024)\n",
    "    model_capabilities = {\n",
    "        \"gemini-pro\": {\n",
    "            \"cost_per_1k\": 0.0005,  # Very cheap!\n",
    "            \"good_for\": [\"simple_qa\", \"basic_chat\", \"translations\"],\n",
    "            \"complexity_score\": 6,\n",
    "            \"speed\": \"fast\",\n",
    "            \"context_window\": 32000\n",
    "        },\n",
    "        \"claude-3-haiku\": {\n",
    "            \"cost_per_1k\": 0.0015,\n",
    "            \"good_for\": [\"quick_responses\", \"high_volume\", \"summaries\"],\n",
    "            \"complexity_score\": 6,\n",
    "            \"speed\": \"very_fast\",\n",
    "            \"context_window\": 200000\n",
    "        },\n",
    "        \"gpt-3.5-turbo\": {\n",
    "            \"cost_per_1k\": 0.002,\n",
    "            \"good_for\": [\"simple_qa\", \"basic_chat\", \"summaries\", \"simple_code\"],\n",
    "            \"complexity_score\": 7,\n",
    "            \"speed\": \"fast\",\n",
    "            \"context_window\": 16000\n",
    "        },\n",
    "        \"claude-3-sonnet\": {\n",
    "            \"cost_per_1k\": 0.018,\n",
    "            \"good_for\": [\"complex_qa\", \"analysis\", \"creative\", \"code\"],\n",
    "            \"complexity_score\": 8,\n",
    "            \"speed\": \"medium\",\n",
    "            \"context_window\": 200000\n",
    "        },\n",
    "        \"gpt-4\": {\n",
    "            \"cost_per_1k\": 0.09,\n",
    "            \"good_for\": [\"complex_reasoning\", \"code_generation\", \"analysis\", \"math\"],\n",
    "            \"complexity_score\": 10,\n",
    "            \"speed\": \"slow\",\n",
    "            \"context_window\": 128000\n",
    "        },\n",
    "        \"claude-3-opus\": {\n",
    "            \"cost_per_1k\": 0.09,\n",
    "            \"good_for\": [\"complex_reasoning\", \"research\", \"long_documents\"],\n",
    "            \"complexity_score\": 10,\n",
    "            \"speed\": \"slow\",\n",
    "            \"context_window\": 200000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Task-specific recommendations\n",
    "    task_models = {\n",
    "        \"simple_qa\": [\"gemini-pro\", \"claude-3-haiku\", \"gpt-3.5-turbo\"],\n",
    "        \"code_generation\": [\"gpt-3.5-turbo\", \"claude-3-sonnet\", \"gpt-4\"],\n",
    "        \"creative_writing\": [\"claude-3-sonnet\", \"gpt-4\"],\n",
    "        \"data_analysis\": [\"gpt-3.5-turbo\", \"claude-3-sonnet\", \"gpt-4\"],\n",
    "        \"translation\": [\"gemini-pro\", \"gpt-3.5-turbo\"],\n",
    "        \"summarization\": [\"claude-3-haiku\", \"gpt-3.5-turbo\"],\n",
    "    }\n",
    "    \n",
    "    # Get suitable models for task\n",
    "    suitable_models = task_models.get(task_type, list(model_capabilities.keys()))\n",
    "    \n",
    "    # Filter by complexity\n",
    "    candidates = []\n",
    "    for model in suitable_models:\n",
    "        if model in model_capabilities:\n",
    "            model_info = model_capabilities[model]\n",
    "            if model_info[\"complexity_score\"] >= min(complexity, 10):\n",
    "                candidates.append((model, model_info))\n",
    "    \n",
    "    # Sort by cost\n",
    "    candidates.sort(key=lambda x: x[1][\"cost_per_1k\"])\n",
    "    \n",
    "    # Return cheapest suitable model\n",
    "    if candidates:\n",
    "        selected = candidates[0][0]\n",
    "        print(f\"\ud83d\udcca Model Selection:\")\n",
    "        print(f\"   Task: {task_type} (complexity: {complexity}/10)\")\n",
    "        print(f\"   Selected: {selected}\")\n",
    "        print(f\"   Cost: ${candidates[0][1]['cost_per_1k']:.4f}/1K tokens\")\n",
    "        print(f\"   Alternatives considered: {[c[0] for c in candidates[1:3]]}\")\n",
    "        return selected\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"\n",
    "    Cache responses to avoid repeated API calls\n",
    "    Saves money by reusing previous responses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_duration_hours: int = 24, max_cache_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Initialize cache\n",
    "        \n",
    "        Args:\n",
    "            cache_duration_hours: How long to keep cached responses\n",
    "            max_cache_size: Maximum number of cached items\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.cache_duration = timedelta(hours=cache_duration_hours)\n",
    "        self.max_cache_size = max_cache_size\n",
    "        \n",
    "        # Statistics\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        self.total_saved = 0.0\n",
    "        self.bytes_saved = 0\n",
    "    \n",
    "    def _get_cache_key(self, prompt: str, model: str = \"default\", temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generate unique cache key\"\"\"\n",
    "        content = f\"{prompt}_{model}_{temperature}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, prompt: str, model: str = \"gpt-3.5-turbo\", temperature: float = 0.7) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Try to get cached response\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to look up\n",
    "            model: Model name\n",
    "            temperature: Temperature setting\n",
    "        \n",
    "        Returns:\n",
    "            Cached response or None\n",
    "        \"\"\"\n",
    "        key = self._get_cache_key(prompt, model, temperature)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            # Check if still valid\n",
    "            if datetime.now() - entry[\"timestamp\"] < self.cache_duration:\n",
    "                self.cache_hits += 1\n",
    "                self.total_saved += entry[\"cost\"]\n",
    "                self.bytes_saved += len(str(entry[\"response\"]))\n",
    "                \n",
    "                # Update access time for LRU\n",
    "                entry[\"last_accessed\"] = datetime.now()\n",
    "                \n",
    "                print(f\"\ud83d\udcb0 Cache hit! Saved ${entry['cost']:.4f}\")\n",
    "                return entry[\"response\"]\n",
    "            else:\n",
    "                # Expired, remove it\n",
    "                del self.cache[key]\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, prompt: str, response: Any, cost: float, model: str = \"gpt-3.5-turbo\", temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Cache a response\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt\n",
    "            response: The response to cache\n",
    "            cost: Cost of this API call\n",
    "            model: Model name\n",
    "            temperature: Temperature setting\n",
    "        \"\"\"\n",
    "        # Check cache size\n",
    "        if len(self.cache) >= self.max_cache_size:\n",
    "            self._evict_oldest()\n",
    "        \n",
    "        key = self._get_cache_key(prompt, model, temperature)\n",
    "        self.cache[key] = {\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"last_accessed\": datetime.now(),\n",
    "            \"cost\": cost,\n",
    "            \"model\": model\n",
    "        }\n",
    "    \n",
    "    def _evict_oldest(self):\n",
    "        \"\"\"Remove least recently used item\"\"\"\n",
    "        if not self.cache:\n",
    "            return\n",
    "        \n",
    "        # Find least recently accessed\n",
    "        oldest_key = min(self.cache.keys(), \n",
    "                        key=lambda k: self.cache[k][\"last_accessed\"])\n",
    "        del self.cache[oldest_key]\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = self.cache_hits + self.cache_misses\n",
    "        hit_rate = (self.cache_hits / max(1, total_requests)) * 100\n",
    "        \n",
    "        # Calculate cache value\n",
    "        cache_value = sum(entry[\"cost\"] for entry in self.cache.values())\n",
    "        \n",
    "        return {\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_misses\": self.cache_misses,\n",
    "            \"hit_rate\": f\"{hit_rate:.1f}%\",\n",
    "            \"total_saved\": f\"${self.total_saved:.2f}\",\n",
    "            \"bytes_saved\": f\"{self.bytes_saved:,}\",\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"cache_value\": f\"${cache_value:.2f}\"\n",
    "        }\n",
    "    \n",
    "    def clear_expired(self):\n",
    "        \"\"\"Remove expired entries\"\"\"\n",
    "        now = datetime.now()\n",
    "        expired_keys = [\n",
    "            key for key, entry in self.cache.items()\n",
    "            if now - entry[\"timestamp\"] >= self.cache_duration\n",
    "        ]\n",
    "        for key in expired_keys:\n",
    "            del self.cache[key]\n",
    "        \n",
    "        return len(expired_keys)\n",
    "\n",
    "\n",
    "def batch_process_efficiently(items: List[str], batch_size: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Process multiple items in single API calls when possible\n",
    "    \n",
    "    Args:\n",
    "        items: List of items to process\n",
    "        batch_size: Items per API call\n",
    "    \n",
    "    Returns:\n",
    "        Results and cost information\n",
    "    \"\"\"\n",
    "    \n",
    "    total_cost = 0\n",
    "    results = []\n",
    "    api_calls = 0\n",
    "    \n",
    "    print(f\"\\n\ud83d\udce6 Batch Processing {len(items)} items\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i+batch_size]\n",
    "        api_calls += 1\n",
    "        \n",
    "        # Combine into single prompt\n",
    "        combined_prompt = \"Process each item separately and number the responses:\\n\\n\"\n",
    "        for j, item in enumerate(batch, 1):\n",
    "            combined_prompt += f\"{j}. {item}\\n\"\n",
    "        \n",
    "        # Estimate cost (simplified)\n",
    "        estimated_tokens = len(combined_prompt) // 4 + (100 * len(batch))  # Assume 100 tokens per response\n",
    "        batch_cost = (estimated_tokens / 1000) * 0.002  # GPT-3.5 pricing\n",
    "        total_cost += batch_cost\n",
    "        \n",
    "        print(f\"   Batch {api_calls}: {len(batch)} items, ~{estimated_tokens} tokens, ${batch_cost:.4f}\")\n",
    "        \n",
    "        # Simulate processing\n",
    "        batch_results = [f\"Processed: {item}\" for item in batch]\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    # Calculate savings\n",
    "    individual_cost = len(items) * ((150 / 1000) * 0.002)  # If processed individually\n",
    "    savings = individual_cost - total_cost\n",
    "    savings_percent = (savings / individual_cost) * 100\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcb0 Batch Processing Results:\")\n",
    "    print(f\"   Items processed: {len(items)}\")\n",
    "    print(f\"   API calls made: {api_calls}\")\n",
    "    print(f\"   Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"   Cost if individual: ${individual_cost:.4f}\")\n",
    "    print(f\"   Saved: ${savings:.4f} ({savings_percent:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"api_calls\": api_calls,\n",
    "        \"savings\": savings\n",
    "    }\n",
    "\n",
    "\n",
    "class CostOptimizer:\n",
    "    \"\"\"\n",
    "    Comprehensive cost optimization manager\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, daily_budget: float = 10.0):\n",
    "        \"\"\"\n",
    "        Initialize cost optimizer\n",
    "        \n",
    "        Args:\n",
    "            daily_budget: Maximum daily spending\n",
    "        \"\"\"\n",
    "        self.daily_budget = daily_budget\n",
    "        self.cache = ResponseCache()\n",
    "        self.model_usage = {}\n",
    "        self.optimization_stats = {\n",
    "            \"cache_savings\": 0,\n",
    "            \"model_downgrades\": 0,\n",
    "            \"batch_savings\": 0\n",
    "        }\n",
    "    \n",
    "    def optimize_request(self, prompt: str, task_type: str = \"simple_qa\") -> Dict:\n",
    "        \"\"\"\n",
    "        Optimize a request for cost\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to process\n",
    "            task_type: Type of task\n",
    "        \n",
    "        Returns:\n",
    "            Optimization recommendations\n",
    "        \"\"\"\n",
    "        recommendations = {}\n",
    "        \n",
    "        # 1. Check cache first\n",
    "        cached = self.cache.get(prompt)\n",
    "        if cached:\n",
    "            self.optimization_stats[\"cache_savings\"] += 1\n",
    "            recommendations[\"use_cache\"] = True\n",
    "            recommendations[\"cached_response\"] = cached\n",
    "            return recommendations\n",
    "        \n",
    "        # 2. Estimate complexity\n",
    "        complexity = self._estimate_complexity(prompt)\n",
    "        \n",
    "        # 3. Select optimal model\n",
    "        model = smart_model_selection(task_type, complexity)\n",
    "        recommendations[\"model\"] = model\n",
    "        \n",
    "        # 4. Check if we should batch\n",
    "        recommendations[\"can_batch\"] = len(prompt) < 500  # Short enough to batch\n",
    "        \n",
    "        # 5. Suggest optimizations\n",
    "        if complexity < 5:\n",
    "            recommendations[\"optimizations\"] = [\n",
    "                \"Consider using cheaper model\",\n",
    "                \"Enable aggressive caching\",\n",
    "                \"Batch with similar requests\"\n",
    "            ]\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _estimate_complexity(self, prompt: str) -> int:\n",
    "        \"\"\"Estimate task complexity from prompt\"\"\"\n",
    "        complexity = 3  # Base complexity\n",
    "        \n",
    "        # Increase for certain keywords\n",
    "        complex_keywords = [\"analyze\", \"explain\", \"compare\", \"debug\", \"optimize\", \"create\"]\n",
    "        for keyword in complex_keywords:\n",
    "            if keyword in prompt.lower():\n",
    "                complexity += 2\n",
    "        \n",
    "        # Increase for length\n",
    "        if len(prompt) > 500:\n",
    "            complexity += 2\n",
    "        if len(prompt) > 1000:\n",
    "            complexity += 2\n",
    "        \n",
    "        # Cap at 10\n",
    "        return min(complexity, 10)\n",
    "    \n",
    "    def get_optimization_report(self) -> str:\n",
    "        \"\"\"Generate optimization report\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(\"COST OPTIMIZATION REPORT\")\n",
    "        report.append(\"=\"*60)\n",
    "        \n",
    "        report.append(\"\\n\ud83d\udcca Cache Performance:\")\n",
    "        for key, value in cache_stats.items():\n",
    "            report.append(f\"   {key}: {value}\")\n",
    "        \n",
    "        report.append(\"\\n\ud83d\udcb0 Savings:\")\n",
    "        report.append(f\"   From caching: ${self.cache.total_saved:.2f}\")\n",
    "        report.append(f\"   From batching: ${self.optimization_stats['batch_savings']:.2f}\")\n",
    "        report.append(f\"   Model downgrades: {self.optimization_stats['model_downgrades']}\")\n",
    "        \n",
    "        report.append(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "        if cache_stats[\"cache_hits\"] < cache_stats[\"cache_misses\"]:\n",
    "            report.append(\"   \u2022 Increase cache duration or size\")\n",
    "        report.append(\"   \u2022 Batch similar requests together\")\n",
    "        report.append(\"   \u2022 Use cheaper models for simple tasks\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "def demonstrate_optimization():\n",
    "    \"\"\"Demonstrate cost optimization techniques\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COST OPTIMIZATION DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Model Selection\n",
    "    print(\"\\n1\ufe0f\u20e3 SMART MODEL SELECTION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    tasks = [\n",
    "        (\"What is 2+2?\", \"simple_qa\", 2),\n",
    "        (\"Write a Python function to sort a list\", \"code_generation\", 6),\n",
    "        (\"Explain quantum computing in detail\", \"complex_qa\", 9)\n",
    "    ]\n",
    "    \n",
    "    for prompt, task_type, complexity in tasks:\n",
    "        print(f\"\\nPrompt: '{prompt[:50]}...'\")\n",
    "        model = smart_model_selection(task_type, complexity)\n",
    "    \n",
    "    # 2. Caching\n",
    "    print(\"\\n2\ufe0f\u20e3 RESPONSE CACHING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cache = ResponseCache()\n",
    "    \n",
    "    # Simulate repeated requests\n",
    "    prompts = [\n",
    "        \"What is Python?\",\n",
    "        \"Explain AI\",\n",
    "        \"What is Python?\",  # Duplicate\n",
    "        \"How does ML work?\",\n",
    "        \"What is Python?\"   # Another duplicate\n",
    "    ]\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        cached = cache.get(prompt)\n",
    "        if not cached:\n",
    "            # Simulate API call\n",
    "            print(f\"\ud83d\udce4 API call for: '{prompt}'\")\n",
    "            cache.set(prompt, f\"Response to: {prompt}\", cost=0.002)\n",
    "        else:\n",
    "            print(f\"\ud83d\udcb0 Using cached response for: '{prompt}'\")\n",
    "    \n",
    "    print(f\"\\nCache stats: {cache.get_stats()}\")\n",
    "    \n",
    "    # 3. Batch Processing\n",
    "    print(\"\\n3\ufe0f\u20e3 BATCH PROCESSING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    items = [f\"Item {i}\" for i in range(25)]\n",
    "    batch_process_efficiently(items, batch_size=5)\n",
    "    \n",
    "    # 4. Complete Optimization\n",
    "    print(\"\\n4\ufe0f\u20e3 COMPLETE OPTIMIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    optimizer = CostOptimizer(daily_budget=5.0)\n",
    "    \n",
    "    test_prompts = [\n",
    "        (\"Hello\", \"simple_qa\"),\n",
    "        (\"Write a complex algorithm\", \"code_generation\"),\n",
    "        (\"Hello\", \"simple_qa\"),  # Duplicate - should cache\n",
    "        (\"Analyze this data\", \"data_analysis\")\n",
    "    ]\n",
    "    \n",
    "    for prompt, task in test_prompts:\n",
    "        print(f\"\\n\ud83d\udcdd Request: '{prompt}'\")\n",
    "        recommendations = optimizer.optimize_request(prompt, task)\n",
    "        \n",
    "        if recommendations.get(\"use_cache\"):\n",
    "            print(\"   \u2705 Using cached response!\")\n",
    "        else:\n",
    "            print(f\"   \ud83d\udcca Recommended model: {recommendations.get('model')}\")\n",
    "            print(f\"   \ud83d\udce6 Can batch: {recommendations.get('can_batch')}\")\n",
    "    \n",
    "    print(\"\\n\" + optimizer.get_optimization_report())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_optimization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: cost_monitor.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: cost_monitor.py\n",
    "\n",
    "\"\"\"\n",
    "Monitor and control API spending with budgets and alerts.\n",
    "Includes free alternatives for students and hobbyists.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class CostMonitor:\n",
    "    \"\"\"\n",
    "    Monitor and control API spending with comprehensive tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, daily_budget: float = 10.0, alert_threshold: float = 0.8):\n",
    "        \"\"\"\n",
    "        Initialize cost monitor\n",
    "        \n",
    "        Args:\n",
    "            daily_budget: Maximum daily spending allowed\n",
    "            alert_threshold: Alert when this fraction of budget is used\n",
    "        \"\"\"\n",
    "        self.daily_budget = daily_budget\n",
    "        self.alert_threshold = alert_threshold\n",
    "        \n",
    "        # Track costs by date\n",
    "        self.daily_costs = {}\n",
    "        \n",
    "        # Track by model\n",
    "        self.model_costs = {}\n",
    "        \n",
    "        # Overall statistics\n",
    "        self.total_spent = 0.0\n",
    "        self.request_count = 0\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "        # Alerts\n",
    "        self.alerts_triggered = []\n",
    "        \n",
    "    def track_request(self, cost: float, model: str, tokens: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Track a request and check budget\n",
    "        \n",
    "        Args:\n",
    "            cost: Cost of this request\n",
    "            model: Model used\n",
    "            tokens: Tokens used\n",
    "        \n",
    "        Returns:\n",
    "            Status dictionary with budget information\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If daily budget is exceeded\n",
    "        \"\"\"\n",
    "        today = datetime.now().date()\n",
    "        \n",
    "        # Initialize today's tracking if needed\n",
    "        if today not in self.daily_costs:\n",
    "            self.daily_costs[today] = {\n",
    "                \"cost\": 0,\n",
    "                \"requests\": 0,\n",
    "                \"tokens\": 0,\n",
    "                \"models\": {}\n",
    "            }\n",
    "        \n",
    "        # Update daily totals\n",
    "        self.daily_costs[today][\"cost\"] += cost\n",
    "        self.daily_costs[today][\"requests\"] += 1\n",
    "        self.daily_costs[today][\"tokens\"] += tokens\n",
    "        \n",
    "        # Track by model\n",
    "        if model not in self.daily_costs[today][\"models\"]:\n",
    "            self.daily_costs[today][\"models\"][model] = {\"cost\": 0, \"requests\": 0}\n",
    "        \n",
    "        self.daily_costs[today][\"models\"][model][\"cost\"] += cost\n",
    "        self.daily_costs[today][\"models\"][model][\"requests\"] += 1\n",
    "        \n",
    "        # Update global tracking\n",
    "        self.total_spent += cost\n",
    "        self.request_count += 1\n",
    "        self.total_tokens += tokens\n",
    "        \n",
    "        if model not in self.model_costs:\n",
    "            self.model_costs[model] = {\"cost\": 0, \"requests\": 0, \"tokens\": 0}\n",
    "        \n",
    "        self.model_costs[model][\"cost\"] += cost\n",
    "        self.model_costs[model][\"requests\"] += 1\n",
    "        self.model_costs[model][\"tokens\"] += tokens\n",
    "        \n",
    "        # Check budget\n",
    "        daily_spent = self.daily_costs[today][\"cost\"]\n",
    "        budget_percent = daily_spent / self.daily_budget\n",
    "        \n",
    "        status = {\n",
    "            \"daily_spent\": daily_spent,\n",
    "            \"daily_budget\": self.daily_budget,\n",
    "            \"remaining\": self.daily_budget - daily_spent,\n",
    "            \"percent_used\": budget_percent * 100,\n",
    "            \"status\": \"OK\"\n",
    "        }\n",
    "        \n",
    "        # Check if budget exceeded\n",
    "        if daily_spent > self.daily_budget:\n",
    "            status[\"status\"] = \"EXCEEDED\"\n",
    "            alert = f\"\u274c Daily budget exceeded! Spent ${daily_spent:.2f} / ${self.daily_budget:.2f}\"\n",
    "            self.alerts_triggered.append({\"time\": datetime.now(), \"message\": alert})\n",
    "            raise Exception(alert)\n",
    "        \n",
    "        # Check if approaching limit\n",
    "        elif budget_percent > self.alert_threshold:\n",
    "            status[\"status\"] = \"WARNING\"\n",
    "            alert = f\"\u26a0\ufe0f Warning: {budget_percent*100:.0f}% of daily budget used\"\n",
    "            print(alert)\n",
    "            self.alerts_triggered.append({\"time\": datetime.now(), \"message\": alert})\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    def get_daily_report(self, date: Optional[date] = None) -> str:\n",
    "        \"\"\"\n",
    "        Get spending report for a specific day\n",
    "        \n",
    "        Args:\n",
    "            date: Date to report on (None for today)\n",
    "        \n",
    "        Returns:\n",
    "            Formatted report string\n",
    "        \"\"\"\n",
    "        target_date = date or datetime.now().date()\n",
    "        \n",
    "        if target_date not in self.daily_costs:\n",
    "            return f\"No data for {target_date}\"\n",
    "        \n",
    "        data = self.daily_costs[target_date]\n",
    "        \n",
    "        report = []\n",
    "        report.append(f\"Daily Report: {target_date}\")\n",
    "        report.append(\"=\" * 50)\n",
    "        report.append(f\"Total Cost: ${data['cost']:.2f} / ${self.daily_budget:.2f}\")\n",
    "        report.append(f\"Requests: {data['requests']}\")\n",
    "        report.append(f\"Tokens: {data['tokens']:,}\")\n",
    "        \n",
    "        if data['requests'] > 0:\n",
    "            report.append(f\"Avg cost/request: ${data['cost']/data['requests']:.4f}\")\n",
    "            report.append(f\"Avg tokens/request: {data['tokens']/data['requests']:.0f}\")\n",
    "        \n",
    "        if data[\"models\"]:\n",
    "            report.append(\"\\nBy Model:\")\n",
    "            for model, stats in data[\"models\"].items():\n",
    "                report.append(f\"  {model}:\")\n",
    "                report.append(f\"    Cost: ${stats['cost']:.4f}\")\n",
    "                report.append(f\"    Requests: {stats['requests']}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "    \n",
    "    def get_full_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive spending report\"\"\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(\"API SPENDING REPORT\")\n",
    "        report.append(\"=\"*60)\n",
    "        \n",
    "        # Summary\n",
    "        report.append(f\"\\n\ud83d\udcca SUMMARY\")\n",
    "        report.append(f\"Total Spent: ${self.total_spent:.2f}\")\n",
    "        report.append(f\"Total Requests: {self.request_count}\")\n",
    "        report.append(f\"Total Tokens: {self.total_tokens:,}\")\n",
    "        \n",
    "        if self.request_count > 0:\n",
    "            report.append(f\"Average Cost: ${self.total_spent/self.request_count:.4f}/request\")\n",
    "            report.append(f\"Average Tokens: {self.total_tokens/self.request_count:.0f}/request\")\n",
    "        \n",
    "        # Daily breakdown\n",
    "        report.append(f\"\\n\ud83d\udcc5 DAILY BREAKDOWN\")\n",
    "        for day, data in sorted(self.daily_costs.items(), reverse=True)[:7]:  # Last 7 days\n",
    "            budget_percent = (data['cost'] / self.daily_budget) * 100\n",
    "            status = \"\u2705\" if budget_percent < 80 else \"\u26a0\ufe0f\" if budget_percent < 100 else \"\u274c\"\n",
    "            report.append(f\"\\n{day}: {status}\")\n",
    "            report.append(f\"  Cost: ${data['cost']:.2f} ({budget_percent:.0f}% of budget)\")\n",
    "            report.append(f\"  Requests: {data['requests']}\")\n",
    "        \n",
    "        # Model breakdown\n",
    "        if self.model_costs:\n",
    "            report.append(f\"\\n\ud83e\udd16 MODEL USAGE\")\n",
    "            sorted_models = sorted(self.model_costs.items(), \n",
    "                                 key=lambda x: x[1][\"cost\"], reverse=True)\n",
    "            \n",
    "            for model, stats in sorted_models:\n",
    "                cost_percent = (stats[\"cost\"] / self.total_spent) * 100\n",
    "                report.append(f\"\\n{model}:\")\n",
    "                report.append(f\"  Cost: ${stats['cost']:.2f} ({cost_percent:.1f}% of total)\")\n",
    "                report.append(f\"  Requests: {stats['requests']}\")\n",
    "                report.append(f\"  Avg: ${stats['cost']/stats['requests']:.4f}/request\")\n",
    "        \n",
    "        # Recent alerts\n",
    "        if self.alerts_triggered:\n",
    "            report.append(f\"\\n\u26a0\ufe0f RECENT ALERTS\")\n",
    "            for alert in self.alerts_triggered[-5:]:\n",
    "                report.append(f\"  {alert['time'].strftime('%Y-%m-%d %H:%M')}: {alert['message']}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(f\"\\n\ud83d\udca1 RECOMMENDATIONS\")\n",
    "        if self.total_spent > 0:\n",
    "            # Find most expensive model\n",
    "            if self.model_costs:\n",
    "                most_expensive = max(self.model_costs.items(), key=lambda x: x[1][\"cost\"])\n",
    "                if most_expensive[1][\"cost\"] / self.total_spent > 0.5:\n",
    "                    report.append(f\"  \u2022 Consider using cheaper models (50%+ spent on {most_expensive[0]})\")\n",
    "            \n",
    "            # Check daily patterns\n",
    "            avg_daily = self.total_spent / len(self.daily_costs)\n",
    "            if avg_daily > self.daily_budget * 0.8:\n",
    "                report.append(f\"  \u2022 Average daily spend (${avg_daily:.2f}) approaching budget\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "    \n",
    "    def export_data(self, filename: str = \"cost_data.json\"):\n",
    "        \"\"\"Export cost data to JSON file\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            \"export_time\": datetime.now().isoformat(),\n",
    "            \"total_spent\": self.total_spent,\n",
    "            \"request_count\": self.request_count,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"daily_budget\": self.daily_budget,\n",
    "            \"daily_costs\": {\n",
    "                str(day): info for day, info in self.daily_costs.items()\n",
    "            },\n",
    "            \"model_costs\": self.model_costs,\n",
    "            \"alerts\": [\n",
    "                {\"time\": alert[\"time\"].isoformat(), \"message\": alert[\"message\"]}\n",
    "                for alert in self.alerts_triggered\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"\ud83d\udcc1 Cost data exported to {filename}\")\n",
    "    \n",
    "    def predict_monthly_cost(self) -> float:\n",
    "        \"\"\"Predict monthly cost based on current usage\"\"\"\n",
    "        \n",
    "        if not self.daily_costs:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate average daily spend\n",
    "        avg_daily = self.total_spent / len(self.daily_costs)\n",
    "        \n",
    "        # Project to 30 days\n",
    "        projected = avg_daily * 30\n",
    "        \n",
    "        return projected\n",
    "\n",
    "\n",
    "def free_ai_options() -> Dict:\n",
    "    \"\"\"\n",
    "    Ways to use AI without spending money\n",
    "    Perfect for students and hobbyists!\n",
    "    \"\"\"\n",
    "    \n",
    "    options = {\n",
    "        \"Google Gemini\": {\n",
    "            \"free_tier\": \"60 requests/minute\",\n",
    "            \"limits\": \"1,500 requests/day\",\n",
    "            \"good_for\": \"Experimentation, learning, prototyping\",\n",
    "            \"setup\": \"Just need Google account\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Go to aistudio.google.com\n",
    "2. Sign in with Google account\n",
    "3. Get API key (no credit card!)\n",
    "4. Start building!\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50\u2b50\"\n",
    "        },\n",
    "        \n",
    "        \"OpenAI Free Credits\": {\n",
    "            \"free_tier\": \"$5 for new accounts (sometimes)\",\n",
    "            \"limits\": \"Expires after 3 months\",\n",
    "            \"good_for\": \"Initial testing, learning GPT\",\n",
    "            \"setup\": \"New phone number required\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Sign up at platform.openai.com\n",
    "2. Verify phone number\n",
    "3. Check for free credits\n",
    "4. Use wisely - it goes fast!\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE (limited)\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50\u2b50\u2b50\"\n",
    "        },\n",
    "        \n",
    "        \"Hugging Face Inference\": {\n",
    "            \"free_tier\": \"Rate-limited access to many models\",\n",
    "            \"limits\": \"Slow, queued requests\",\n",
    "            \"good_for\": \"Testing open models, learning\",\n",
    "            \"setup\": \"Free account\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Create account at huggingface.co\n",
    "2. Get API token\n",
    "3. Use Inference API\n",
    "4. Expect delays during peak times\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50\"\n",
    "        },\n",
    "        \n",
    "        \"Local Open Source (Ollama)\": {\n",
    "            \"free_tier\": \"Unlimited (your hardware)\",\n",
    "            \"limits\": \"Need decent GPU or lots of patience\",\n",
    "            \"good_for\": \"Privacy, unlimited use, learning\",\n",
    "            \"setup\": \"Install Ollama\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Download from ollama.ai\n",
    "2. Run: ollama pull llama2\n",
    "3. Run: ollama run llama2\n",
    "4. Use via API or CLI\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE (your electricity)\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50 (depends on model)\"\n",
    "        },\n",
    "        \n",
    "        \"Colab + Open Models\": {\n",
    "            \"free_tier\": \"Free GPU time (limited)\",\n",
    "            \"limits\": \"Session limits, disconnections\",\n",
    "            \"good_for\": \"Experiments, learning, notebooks\",\n",
    "            \"setup\": \"Google account + notebooks\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Go to colab.research.google.com\n",
    "2. Create new notebook\n",
    "3. Use Transformers library\n",
    "4. Load open models\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50\u2b50\"\n",
    "        },\n",
    "        \n",
    "        \"Replicate Free Tier\": {\n",
    "            \"free_tier\": \"Free predictions for public models\",\n",
    "            \"limits\": \"Very limited, mostly for testing\",\n",
    "            \"good_for\": \"Trying different models\",\n",
    "            \"setup\": \"GitHub account\",\n",
    "            \"how_to\": \"\"\"\n",
    "1. Sign up at replicate.com\n",
    "2. Get API token\n",
    "3. Use public models\n",
    "4. Watch usage carefully\n",
    "            \"\"\",\n",
    "            \"cost\": \"FREE (very limited)\",\n",
    "            \"quality\": \"\u2b50\u2b50\u2b50\u2b50\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"\ud83c\udf93 FREE AI OPTIONS FOR STUDENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, info in options.items():\n",
    "        print(f\"\\n\ud83d\udccc {name}\")\n",
    "        print(f\"   Cost: {info['cost']}\")\n",
    "        print(f\"   Quality: {info['quality']}\")\n",
    "        print(f\"   Free Tier: {info['free_tier']}\")\n",
    "        print(f\"   Good For: {info['good_for']}\")\n",
    "        print(f\"   Limits: {info['limits']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udca1 RECOMMENDATIONS BY USE CASE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"Just Learning\": [\"Google Gemini (best free option)\", \"Local Ollama\"],\n",
    "        \"Building Projects\": [\"Google Gemini\", \"OpenAI free credits\"],\n",
    "        \"Research/Experiments\": [\"Colab + Open Models\", \"Hugging Face\"],\n",
    "        \"Production Apps\": [\"Start with Gemini free tier\", \"Then upgrade as needed\"]\n",
    "    }\n",
    "    \n",
    "    for use_case, recs in recommendations.items():\n",
    "        print(f\"\\n{use_case}:\")\n",
    "        for rec in recs:\n",
    "            print(f\"  \u2022 {rec}\")\n",
    "    \n",
    "    return options\n",
    "\n",
    "\n",
    "def student_budget_strategies():\n",
    "    \"\"\"Budget-conscious strategies for students\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udcb0 STUDENT BUDGET STRATEGIES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"strategy\": \"Start Free, Upgrade Later\",\n",
    "            \"how\": \"Use Google Gemini free tier until you hit limits\",\n",
    "            \"savings\": \"$50-100/month\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Cache Everything\",\n",
    "            \"how\": \"Never make the same API call twice\",\n",
    "            \"savings\": \"50-70% reduction\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Use Cheap Models First\",\n",
    "            \"how\": \"Try Gemini/Haiku before GPT-4\",\n",
    "            \"savings\": \"10-100x cost difference\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Batch Requests\",\n",
    "            \"how\": \"Process multiple items per API call\",\n",
    "            \"savings\": \"30-50% reduction\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Local for Development\",\n",
    "            \"how\": \"Use Ollama locally, API for production\",\n",
    "            \"savings\": \"$20-50/month\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Share API Keys (Carefully!)\",\n",
    "            \"how\": \"Team projects can share costs\",\n",
    "            \"savings\": \"Split costs 3-4 ways\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Use University Resources\",\n",
    "            \"how\": \"Many universities provide compute credits\",\n",
    "            \"savings\": \"$100-500/month\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for s in strategies:\n",
    "        print(f\"\\n\ud83d\udccb {s['strategy']}\")\n",
    "        print(f\"   How: {s['how']}\")\n",
    "        print(f\"   Potential Savings: {s['savings']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udcda STUDENT STARTER STACK (All Free!)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "1. Development: Google Gemini (free tier)\n",
    "2. Experiments: Colab notebooks\n",
    "3. Local testing: Ollama with Llama 2\n",
    "4. Version control: GitHub (free)\n",
    "5. Deployment: Vercel/Netlify (free tier)\n",
    "\n",
    "Total Cost: $0/month\n",
    "Capabilities: Build full AI applications!\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def demonstrate_cost_monitoring():\n",
    "    \"\"\"Demonstrate cost monitoring in action\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"COST MONITORING DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create monitor\n",
    "    monitor = CostMonitor(daily_budget=5.0)\n",
    "    \n",
    "    # Simulate some API calls\n",
    "    api_calls = [\n",
    "        {\"cost\": 0.002, \"model\": \"gpt-3.5-turbo\", \"tokens\": 150},\n",
    "        {\"cost\": 0.003, \"model\": \"gpt-3.5-turbo\", \"tokens\": 200},\n",
    "        {\"cost\": 0.09, \"model\": \"gpt-4\", \"tokens\": 1000},\n",
    "        {\"cost\": 0.001, \"model\": \"gemini-pro\", \"tokens\": 100},\n",
    "        {\"cost\": 0.002, \"model\": \"gpt-3.5-turbo\", \"tokens\": 150},\n",
    "        {\"cost\": 0.05, \"model\": \"gpt-4\", \"tokens\": 500},\n",
    "        {\"cost\": 0.0005, \"model\": \"gemini-pro\", \"tokens\": 50},\n",
    "        {\"cost\": 0.002, \"model\": \"gpt-3.5-turbo\", \"tokens\": 150},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Tracking API Calls:\")\n",
    "    for i, call in enumerate(api_calls, 1):\n",
    "        print(f\"\\nCall {i}: {call['model']} (${call['cost']:.4f})\")\n",
    "        \n",
    "        try:\n",
    "            status = monitor.track_request(call[\"cost\"], call[\"model\"], call[\"tokens\"])\n",
    "            print(f\"  Budget: ${status['daily_spent']:.4f} / ${status['daily_budget']:.2f}\")\n",
    "            print(f\"  Status: {status['status']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {e}\")\n",
    "            break\n",
    "    \n",
    "    # Show reports\n",
    "    print(\"\\n\" + monitor.get_daily_report())\n",
    "    print(\"\\n\" + monitor.get_full_report())\n",
    "    \n",
    "    # Predict monthly cost\n",
    "    projected = monitor.predict_monthly_cost()\n",
    "    print(f\"\\n\ud83d\udcc8 Projected Monthly Cost: ${projected:.2f}\")\n",
    "    \n",
    "    # Export data\n",
    "    monitor.export_data(\"demo_cost_data.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Show free options\n",
    "    free_ai_options()\n",
    "    \n",
    "    # Student strategies\n",
    "    student_budget_strategies()\n",
    "    \n",
    "    # Demonstrate monitoring\n",
    "    print(\"\\n\")\n",
    "    demonstrate_cost_monitoring()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: chapter7_challenge_project.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.7\n",
    "# File: chapter7_challenge_project.py\n",
    "\n",
    "\"\"\"\n",
    "Chapter 7 Challenge Project: Multi-Provider AI Assistant Hub\n",
    "Build an AI assistant that intelligently switches between providers,\n",
    "manages costs, and handles rate limits.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Configuration\n",
    "# =======================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for each model\"\"\"\n",
    "    provider: str\n",
    "    name: str\n",
    "    input_cost_per_1k: float\n",
    "    output_cost_per_1k: float\n",
    "    rpm_limit: int\n",
    "    tpm_limit: int\n",
    "    complexity_score: int  # 1-10, how capable is this model\n",
    "    context_window: int\n",
    "\n",
    "\n",
    "class Provider(Enum):\n",
    "    \"\"\"Available providers\"\"\"\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"  \n",
    "    GOOGLE = \"google\"\n",
    "    LOCAL = \"local\"  # For Ollama or similar\n",
    "\n",
    "\n",
    "class AssistantHub:\n",
    "    \"\"\"\n",
    "    Your Multi-Provider AI Assistant Hub\n",
    "    Implements everything from Chapter 7!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the assistant hub\"\"\"\n",
    "        \n",
    "        # Model configurations\n",
    "        self.models = {\n",
    "            \"gpt-3.5-turbo\": ModelConfig(\n",
    "                provider=\"openai\",\n",
    "                name=\"gpt-3.5-turbo\",\n",
    "                input_cost_per_1k=0.0005,\n",
    "                output_cost_per_1k=0.0015,\n",
    "                rpm_limit=90,\n",
    "                tpm_limit=90000,\n",
    "                complexity_score=7,\n",
    "                context_window=16000\n",
    "            ),\n",
    "            \"gemini-pro\": ModelConfig(\n",
    "                provider=\"google\",\n",
    "                name=\"gemini-pro\",\n",
    "                input_cost_per_1k=0.000125,\n",
    "                output_cost_per_1k=0.000375,\n",
    "                rpm_limit=60,\n",
    "                tpm_limit=1000000,\n",
    "                complexity_score=6,\n",
    "                context_window=32000\n",
    "            ),\n",
    "            \"claude-3-haiku\": ModelConfig(\n",
    "                provider=\"anthropic\",\n",
    "                name=\"claude-3-haiku-20240307\",\n",
    "                input_cost_per_1k=0.00025,\n",
    "                output_cost_per_1k=0.00125,\n",
    "                rpm_limit=50,\n",
    "                tpm_limit=100000,\n",
    "                complexity_score=6,\n",
    "                context_window=200000\n",
    "            )\n",
    "            # TODO: Add more models\n",
    "        }\n",
    "        \n",
    "        # Initialize components\n",
    "        self.cache = {}  # Simple cache implementation\n",
    "        self.cache_duration = timedelta(hours=24)\n",
    "        \n",
    "        self.cost_tracker = {\n",
    "            \"total\": 0.0,\n",
    "            \"by_model\": {},\n",
    "            \"daily\": {}\n",
    "        }\n",
    "        \n",
    "        self.rate_limiters = {}  # One per model\n",
    "        self.conversation_history = []\n",
    "        self.available_providers = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "        \n",
    "        # Load API keys\n",
    "        self._load_api_keys()\n",
    "        \n",
    "        # Initialize rate limiters\n",
    "        self._init_rate_limiters()\n",
    "    \n",
    "    def _load_api_keys(self):\n",
    "        \"\"\"\n",
    "        Load API keys from environment variables\n",
    "        TODO: Implement secure key loading from Chapter 7.6\n",
    "        \"\"\"\n",
    "        self.api_keys = {}\n",
    "        \n",
    "        # Try to load from environment\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            self.api_keys[\"openai\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "            self.available_providers.append(\"openai\")\n",
    "            print(\"\u2705 OpenAI API key loaded\")\n",
    "        \n",
    "        if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "            self.api_keys[\"anthropic\"] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "            self.available_providers.append(\"anthropic\")\n",
    "            print(\"\u2705 Anthropic API key loaded\")\n",
    "        \n",
    "        if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            self.api_keys[\"google\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            self.available_providers.append(\"google\")\n",
    "            print(\"\u2705 Google API key loaded\")\n",
    "        \n",
    "        if not self.available_providers:\n",
    "            print(\"\u26a0\ufe0f No API keys found! Set environment variables:\")\n",
    "            print(\"  OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_API_KEY\")\n",
    "    \n",
    "    def _init_rate_limiters(self):\n",
    "        \"\"\"Initialize rate limiters for each model\"\"\"\n",
    "        for model_name, config in self.models.items():\n",
    "            self.rate_limiters[model_name] = {\n",
    "                \"request_times\": deque(),\n",
    "                \"token_counts\": deque(),\n",
    "                \"last_request\": None\n",
    "            }\n",
    "    \n",
    "    def _estimate_complexity(self, prompt: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate task complexity (1-10) based on prompt\n",
    "        \n",
    "        TODO: Improve this with better heuristics\n",
    "        \"\"\"\n",
    "        complexity = 3  # Base complexity\n",
    "        \n",
    "        # Length-based estimation\n",
    "        if len(prompt) > 500:\n",
    "            complexity += 2\n",
    "        if len(prompt) > 1000:\n",
    "            complexity += 2\n",
    "        \n",
    "        # Keyword-based estimation\n",
    "        complex_keywords = [\n",
    "            \"analyze\", \"explain in detail\", \"compare\",\n",
    "            \"write code\", \"debug\", \"optimize\", \n",
    "            \"create\", \"design\", \"comprehensive\"\n",
    "        ]\n",
    "        \n",
    "        for keyword in complex_keywords:\n",
    "            if keyword in prompt.lower():\n",
    "                complexity += 1\n",
    "        \n",
    "        # Cap at 10\n",
    "        return min(complexity, 10)\n",
    "    \n",
    "    def _select_model(self, prompt: str, max_cost: float = 0.01) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Select the best model for the task\n",
    "        \n",
    "        TODO: Implement smart selection logic\n",
    "        \"\"\"\n",
    "        complexity = self._estimate_complexity(prompt)\n",
    "        \n",
    "        # Filter models by availability and complexity\n",
    "        candidates = []\n",
    "        for model_name, config in self.models.items():\n",
    "            if config.provider in self.available_providers:\n",
    "                if config.complexity_score >= complexity:\n",
    "                    # Estimate cost\n",
    "                    estimated_tokens = len(prompt) // 4 + 200  # Input + output estimate\n",
    "                    estimated_cost = (estimated_tokens / 1000) * (\n",
    "                        config.input_cost_per_1k + config.output_cost_per_1k\n",
    "                    )\n",
    "                    \n",
    "                    if estimated_cost <= max_cost:\n",
    "                        candidates.append((model_name, estimated_cost))\n",
    "        \n",
    "        if not candidates:\n",
    "            print(\"\u26a0\ufe0f No suitable model found within budget\")\n",
    "            return None\n",
    "        \n",
    "        # Sort by cost and return cheapest\n",
    "        candidates.sort(key=lambda x: x[1])\n",
    "        selected = candidates[0][0]\n",
    "        \n",
    "        print(f\"\ud83d\udcca Selected model: {selected} (complexity: {complexity}/10)\")\n",
    "        return selected\n",
    "    \n",
    "    def _check_rate_limits(self, model: str, estimated_tokens: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if we can make a request without hitting rate limits\n",
    "        \n",
    "        TODO: Implement rate limit checking\n",
    "        \"\"\"\n",
    "        if model not in self.rate_limiters:\n",
    "            return True\n",
    "        \n",
    "        config = self.models[model]\n",
    "        limiter = self.rate_limiters[model]\n",
    "        \n",
    "        now = datetime.now()\n",
    "        minute_ago = now - timedelta(minutes=1)\n",
    "        \n",
    "        # Clean old entries\n",
    "        while limiter[\"request_times\"] and limiter[\"request_times\"][0] < minute_ago:\n",
    "            limiter[\"request_times\"].popleft()\n",
    "            if limiter[\"token_counts\"]:\n",
    "                limiter[\"token_counts\"].popleft()\n",
    "        \n",
    "        # Check limits\n",
    "        if len(limiter[\"request_times\"]) >= config.rpm_limit:\n",
    "            print(f\"\u23f3 Rate limit: {model} at {config.rpm_limit} RPM\")\n",
    "            return False\n",
    "        \n",
    "        current_tokens = sum(limiter[\"token_counts\"])\n",
    "        if current_tokens + estimated_tokens > config.tpm_limit:\n",
    "            print(f\"\u23f3 Token limit: {model} at {config.tpm_limit} TPM\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _get_cache_key(self, prompt: str, model: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        content = f\"{prompt}_{model}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def _check_cache(self, prompt: str, model: str) -> Optional[Any]:\n",
    "        \"\"\"Check if response is cached\"\"\"\n",
    "        key = self._get_cache_key(prompt, model)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            # Check if still valid\n",
    "            if datetime.now() - entry[\"timestamp\"] < self.cache_duration:\n",
    "                self.stats[\"cache_hits\"] += 1\n",
    "                print(f\"\ud83d\udcb0 Cache hit! Saved ${entry['cost']:.4f}\")\n",
    "                return entry[\"response\"]\n",
    "        \n",
    "        self.stats[\"cache_misses\"] += 1\n",
    "        return None\n",
    "    \n",
    "    def _cache_response(self, prompt: str, model: str, response: str, cost: float):\n",
    "        \"\"\"Cache a response\"\"\"\n",
    "        key = self._get_cache_key(prompt, model)\n",
    "        self.cache[key] = {\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"cost\": cost,\n",
    "            \"model\": model\n",
    "        }\n",
    "    \n",
    "    def _make_api_call(self, prompt: str, model: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Make actual API call to the selected model\n",
    "        \n",
    "        TODO: Implement actual API calls for each provider\n",
    "        \"\"\"\n",
    "        config = self.models[model]\n",
    "        \n",
    "        # Simulate API call for demo\n",
    "        print(f\"\ud83d\udd04 Making API call to {model}...\")\n",
    "        time.sleep(0.5)  # Simulate latency\n",
    "        \n",
    "        # TODO: Replace with actual API calls\n",
    "        # if config.provider == \"openai\":\n",
    "        #     response = self._call_openai(prompt, model)\n",
    "        # elif config.provider == \"anthropic\":\n",
    "        #     response = self._call_anthropic(prompt, model)\n",
    "        # elif config.provider == \"google\":\n",
    "        #     response = self._call_google(prompt, model)\n",
    "        \n",
    "        # Simulated response\n",
    "        response = {\n",
    "            \"text\": f\"[Simulated response from {model}] This is a response to: {prompt[:50]}...\",\n",
    "            \"tokens_used\": len(prompt) // 4 + 150,\n",
    "            \"model\": model\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _track_cost(self, model: str, tokens: int):\n",
    "        \"\"\"Track costs\"\"\"\n",
    "        config = self.models[model]\n",
    "        \n",
    "        # Estimate input/output split (rough)\n",
    "        input_tokens = tokens * 0.3\n",
    "        output_tokens = tokens * 0.7\n",
    "        \n",
    "        cost = (\n",
    "            (input_tokens / 1000) * config.input_cost_per_1k +\n",
    "            (output_tokens / 1000) * config.output_cost_per_1k\n",
    "        )\n",
    "        \n",
    "        # Update tracking\n",
    "        self.cost_tracker[\"total\"] += cost\n",
    "        \n",
    "        if model not in self.cost_tracker[\"by_model\"]:\n",
    "            self.cost_tracker[\"by_model\"][model] = 0\n",
    "        self.cost_tracker[\"by_model\"][model] += cost\n",
    "        \n",
    "        today = str(datetime.now().date())\n",
    "        if today not in self.cost_tracker[\"daily\"]:\n",
    "            self.cost_tracker[\"daily\"][today] = 0\n",
    "        self.cost_tracker[\"daily\"][today] += cost\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def _update_rate_limiter(self, model: str, tokens: int):\n",
    "        \"\"\"Update rate limiter after request\"\"\"\n",
    "        if model in self.rate_limiters:\n",
    "            limiter = self.rate_limiters[model]\n",
    "            limiter[\"request_times\"].append(datetime.now())\n",
    "            limiter[\"token_counts\"].append(tokens)\n",
    "            limiter[\"last_request\"] = datetime.now()\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Main chat interface\n",
    "        \n",
    "        This is where everything comes together!\n",
    "        \"\"\"\n",
    "        self.stats[\"total_requests\"] += 1\n",
    "        \n",
    "        # Step 1: Select best model\n",
    "        model = self._select_model(user_input)\n",
    "        if not model:\n",
    "            return \"Sorry, no suitable model available within budget constraints.\"\n",
    "        \n",
    "        # Step 2: Check cache\n",
    "        cached = self._check_cache(user_input, model)\n",
    "        if cached:\n",
    "            return cached\n",
    "        \n",
    "        # Step 3: Check rate limits\n",
    "        estimated_tokens = len(user_input) // 4 + 200\n",
    "        \n",
    "        if not self._check_rate_limits(model, estimated_tokens):\n",
    "            # Try fallback model\n",
    "            print(\"\ud83d\udd04 Trying fallback model...\")\n",
    "            for fallback_model in self.models.keys():\n",
    "                if fallback_model != model:\n",
    "                    if self._check_rate_limits(fallback_model, estimated_tokens):\n",
    "                        model = fallback_model\n",
    "                        print(f\"\u2705 Using fallback: {model}\")\n",
    "                        break\n",
    "            else:\n",
    "                return \"Rate limits exceeded on all models. Please wait a moment.\"\n",
    "        \n",
    "        # Step 4: Make API call\n",
    "        try:\n",
    "            response = self._make_api_call(user_input, model)\n",
    "            if not response:\n",
    "                self.stats[\"errors\"] += 1\n",
    "                return \"Failed to get response from API.\"\n",
    "            \n",
    "            response_text = response[\"text\"]\n",
    "            tokens_used = response[\"tokens_used\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats[\"errors\"] += 1\n",
    "            print(f\"\u274c API call failed: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Step 5: Track costs\n",
    "        cost = self._track_cost(model, tokens_used)\n",
    "        print(f\"\ud83d\udcb0 Cost: ${cost:.4f}\")\n",
    "        \n",
    "        # Step 6: Update rate limiter\n",
    "        self._update_rate_limiter(model, tokens_used)\n",
    "        \n",
    "        # Step 7: Cache response\n",
    "        self._cache_response(user_input, model, response_text, cost)\n",
    "        \n",
    "        # Step 8: Update statistics\n",
    "        self.stats[\"total_tokens\"] += tokens_used\n",
    "        \n",
    "        # Step 9: Update conversation history (limit size)\n",
    "        self.conversation_history.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"user\": user_input,\n",
    "            \"assistant\": response_text,\n",
    "            \"model\": model,\n",
    "            \"cost\": cost\n",
    "        })\n",
    "        \n",
    "        # Keep only last 20 exchanges\n",
    "        if len(self.conversation_history) > 20:\n",
    "            self.conversation_history = self.conversation_history[-20:]\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Generate statistics report\"\"\"\n",
    "        \n",
    "        cache_hit_rate = 0\n",
    "        if self.stats[\"cache_hits\"] + self.stats[\"cache_misses\"] > 0:\n",
    "            cache_hit_rate = (\n",
    "                self.stats[\"cache_hits\"] / \n",
    "                (self.stats[\"cache_hits\"] + self.stats[\"cache_misses\"])\n",
    "            ) * 100\n",
    "        \n",
    "        stats = {\n",
    "            \"total_cost\": f\"${self.cost_tracker['total']:.4f}\",\n",
    "            \"total_requests\": self.stats[\"total_requests\"],\n",
    "            \"total_tokens\": self.stats[\"total_tokens\"],\n",
    "            \"cache_hit_rate\": f\"{cache_hit_rate:.1f}%\",\n",
    "            \"cache_hits\": self.stats[\"cache_hits\"],\n",
    "            \"cache_misses\": self.stats[\"cache_misses\"],\n",
    "            \"errors\": self.stats[\"errors\"],\n",
    "            \"cost_by_model\": {\n",
    "                model: f\"${cost:.4f}\"\n",
    "                for model, cost in self.cost_tracker[\"by_model\"].items()\n",
    "            },\n",
    "            \"available_providers\": self.available_providers,\n",
    "            \"conversation_length\": len(self.conversation_history)\n",
    "        }\n",
    "        \n",
    "        # Add today's cost\n",
    "        today = str(datetime.now().date())\n",
    "        if today in self.cost_tracker[\"daily\"]:\n",
    "            stats[\"today_cost\"] = f\"${self.cost_tracker['daily'][today]:.4f}\"\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def export_conversation(self, filename: str):\n",
    "        \"\"\"Export conversation history\"\"\"\n",
    "        \n",
    "        export_data = {\n",
    "            \"export_time\": datetime.now().isoformat(),\n",
    "            \"statistics\": self.get_stats(),\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"timestamp\": conv[\"timestamp\"].isoformat(),\n",
    "                    \"user\": conv[\"user\"],\n",
    "                    \"assistant\": conv[\"assistant\"],\n",
    "                    \"model\": conv[\"model\"],\n",
    "                    \"cost\": conv[\"cost\"]\n",
    "                }\n",
    "                for conv in self.conversation_history\n",
    "            ],\n",
    "            \"total_cost\": self.cost_tracker[\"total\"]\n",
    "        }\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\ud83d\udcc1 Conversation exported to {filename}\")\n",
    "\n",
    "\n",
    "# ==================\n",
    "# Challenge Functions\n",
    "# ==================\n",
    "\n",
    "def challenge_basic():\n",
    "    \"\"\"Basic Challenge: Get it working with one provider\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASIC CHALLENGE: Single Provider\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    hub = AssistantHub()\n",
    "    \n",
    "    if not hub.available_providers:\n",
    "        print(\"\u274c No API keys configured. Please set environment variables.\")\n",
    "        return\n",
    "    \n",
    "    # Test basic functionality\n",
    "    test_prompts = [\n",
    "        \"What is Python?\",\n",
    "        \"Explain recursion briefly\",\n",
    "        \"What is Python?\",  # Test cache\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\n\ud83d\udcdd User: {prompt}\")\n",
    "        response = hub.chat(prompt)\n",
    "        print(f\"\ud83e\udd16 Assistant: {response[:100]}...\")\n",
    "    \n",
    "    # Check stats\n",
    "    print(\"\\n\ud83d\udcca Statistics:\")\n",
    "    stats = hub.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "def challenge_intermediate():\n",
    "    \"\"\"Intermediate Challenge: Multi-provider with failover\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERMEDIATE CHALLENGE: Multi-Provider with Caching\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    hub = AssistantHub()\n",
    "    \n",
    "    if len(hub.available_providers) < 2:\n",
    "        print(\"\u26a0\ufe0f This challenge requires at least 2 API providers configured.\")\n",
    "        print(\"  Currently available:\", hub.available_providers)\n",
    "    \n",
    "    # Test different complexity prompts\n",
    "    test_prompts = [\n",
    "        (\"Hello!\", \"simple\"),\n",
    "        (\"Explain machine learning\", \"medium\"),\n",
    "        (\"Write a Python sorting algorithm\", \"complex\"),\n",
    "        (\"Hello!\", \"simple\"),  # Test cache\n",
    "    ]\n",
    "    \n",
    "    for prompt, complexity in test_prompts:\n",
    "        print(f\"\\n\ud83d\udcdd User: {prompt} [{complexity}]\")\n",
    "        response = hub.chat(prompt)\n",
    "        print(f\"\ud83e\udd16 Response: {response[:100]}...\")\n",
    "    \n",
    "    # Export conversation\n",
    "    hub.export_conversation(\"intermediate_challenge.json\")\n",
    "    \n",
    "    # Show final stats\n",
    "    print(\"\\n\ud83d\udcca Final Statistics:\")\n",
    "    stats = hub.get_stats()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "def challenge_advanced():\n",
    "    \"\"\"Advanced Challenge: Complete implementation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ADVANCED CHALLENGE: Full Feature Implementation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    hub = AssistantHub()\n",
    "    \n",
    "    # Simulate a full conversation\n",
    "    prompts = [\n",
    "        \"Hello!\",  # Simple - should use cheap model\n",
    "        \"Explain quantum computing\",  # Medium\n",
    "        \"Write Python code for binary search\",  # Complex\n",
    "        \"What did I ask about first?\",  # Tests memory\n",
    "        \"Hello!\",  # Test cache again\n",
    "        \"Summarize our conversation\",  # Tests context\n",
    "    ]\n",
    "    \n",
    "    total_start_cost = hub.cost_tracker[\"total\"]\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Message {i}/{len(prompts)}\")\n",
    "        print(f\"User: {prompt}\")\n",
    "        \n",
    "        response = hub.chat(prompt)\n",
    "        print(f\"Assistant: {response[:150]}...\")\n",
    "        \n",
    "        # Show incremental cost\n",
    "        current_cost = hub.cost_tracker[\"total\"]\n",
    "        print(f\"Total cost so far: ${current_cost:.4f}\")\n",
    "    \n",
    "    # Final report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    hub.export_conversation(\"advanced_challenge.json\")\n",
    "    \n",
    "    final_stats = hub.get_stats()\n",
    "    for key, value in final_stats.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "def interactive_mode():\n",
    "    \"\"\"Interactive chat mode\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE MODE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Commands: 'quit', 'stats', 'export'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    hub = AssistantHub()\n",
    "    \n",
    "    if not hub.available_providers:\n",
    "        print(\"\u274c No API keys configured.\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n\ud83d\udc64 You: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif user_input.lower() == 'stats':\n",
    "            print(\"\\n\ud83d\udcca Current Statistics:\")\n",
    "            for key, value in hub.get_stats().items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'export':\n",
    "            hub.export_conversation(\"interactive_export.json\")\n",
    "            continue\n",
    "        elif not user_input:\n",
    "            continue\n",
    "        \n",
    "        response = hub.chat(user_input)\n",
    "        print(f\"\ud83e\udd16 Assistant: {response}\")\n",
    "    \n",
    "    # Final export\n",
    "    hub.export_conversation(\"interactive_session.json\")\n",
    "    print(\"\\n\ud83d\udc4b Goodbye! Session exported to interactive_session.json\")\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Main Entry Point\n",
    "# =======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"CHAPTER 7 CHALLENGE PROJECT\")\n",
    "    print(\"Multi-Provider AI Assistant Hub\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nChoose your challenge level:\")\n",
    "    print(\"1. Basic - Single provider implementation\")\n",
    "    print(\"2. Intermediate - Multi-provider with caching\")\n",
    "    print(\"3. Advanced - Complete feature implementation\")\n",
    "    print(\"4. Interactive - Chat mode\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1-4): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        challenge_basic()\n",
    "    elif choice == \"2\":\n",
    "        challenge_intermediate()\n",
    "    elif choice == \"3\":\n",
    "        challenge_advanced()\n",
    "    elif choice == \"4\":\n",
    "        interactive_mode()\n",
    "    else:\n",
    "        print(\"Invalid choice. Running basic challenge...\")\n",
    "        challenge_basic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_07_intro_ai_llm_solutions.ipynb**\n",
    "- Proceed to **Chapter 8**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}