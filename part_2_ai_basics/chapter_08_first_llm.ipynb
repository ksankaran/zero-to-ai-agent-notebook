{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Your First LLM Integration\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "## Overview\n",
    "In this chapter, you'll learn about:\n",
    "- Setting up OpenAI API (or alternative)\n",
    "- Making your first API call\n",
    "- Understanding API responses\n",
    "- Handling API errors gracefully\n",
    "- Building a simple chatbot\n",
    "- Saving conversation history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.1: Setting up OpenAI API (or alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: setup_api_key.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: setup_api_key.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_api_key():\n",
    "    \"\"\"Set up your OpenAI API key safely\"\"\"\n",
    "    \n",
    "    print(\"\ud83d\udd10 Let's set up your OpenAI API key!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if we already have a key saved\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        print(\"\u2705 You already have a .env file!\")\n",
    "        with open(env_file, 'r') as f:\n",
    "            if 'OPENAI_API_KEY' in f.read():\n",
    "                print(\"\u2705 Your API key is already set up!\")\n",
    "                return\n",
    "    \n",
    "    # Get the API key from user\n",
    "    print(\"\\nYou need your OpenAI API key from: https://platform.openai.com/api-keys\")\n",
    "    print(\"(It starts with 'sk-')\")\n",
    "    print()\n",
    "    \n",
    "    api_key = input(\"Paste your API key here: \").strip()\n",
    "    \n",
    "    # Basic validation\n",
    "    if not api_key.startswith('sk-'):\n",
    "        print(\"\u26a0\ufe0f  That doesn't look like an OpenAI key (should start with 'sk-')\")\n",
    "        print(\"But let's save it anyway - you can fix it later!\")\n",
    "    \n",
    "    # Save to .env file\n",
    "    with open('.env', 'w') as f:\n",
    "        f.write(f\"OPENAI_API_KEY={api_key}\\n\")\n",
    "    \n",
    "    print(\"\\n\u2705 API key saved to .env file!\")\n",
    "    print(\"\ud83d\udd12 Remember: NEVER share this file or commit it to Git!\")\n",
    "    \n",
    "    # Create .gitignore to keep it safe\n",
    "    with open('.gitignore', 'w') as f:\n",
    "        f.write(\".env\\n\")\n",
    "    \n",
    "    print(\"\u2705 Created .gitignore to keep your key safe!\")\n",
    "    print(\"\\n\ud83c\udf89 You're all set! Let's talk to AI!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_api_key()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: my_first_ai_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: my_first_ai_chat.py\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your API key from .env file\n",
    "def load_api_key():\n",
    "    \"\"\"Load API key from .env file\"\"\"\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "# Get your API key\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found! Run setup_api_key.py first!\")\n",
    "    exit()\n",
    "\n",
    "# THIS IS IT - YOUR FIRST AI CALL! \ud83d\ude80\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83e\udd16 AI Assistant Ready!\")\n",
    "print(\"Type 'quit' to exit\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Conversation loop\n",
    "while True:\n",
    "    # Get your message\n",
    "    user_message = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_message.lower() == 'quit':\n",
    "        print(\"\ud83d\udc4b Bye! You just built your first AI app!\")\n",
    "        break\n",
    "    \n",
    "    # \u2728 THE MAGIC HAPPENS HERE - WE CALL THE AI! \u2728\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the AI's response\n",
    "    ai_message = response.choices[0].message.content\n",
    "    \n",
    "    # Show the response\n",
    "    print(f\"\\nAI: {ai_message}\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Congratulations! You just had your first AI conversation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ai_chat_with_memory.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: ai_chat_with_memory.py\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load API key (same as before)\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found! Run setup_api_key.py first!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83e\udd16 AI Assistant with Memory!\")\n",
    "print(\"I'll remember our conversation now!\")\n",
    "print(\"Commands: 'quit' to exit, 'forget' to clear memory\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# This list will store our conversation history!\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, friendly assistant.\"}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_message.lower() == 'quit':\n",
    "        print(\"\ud83d\udc4b Bye! Thanks for chatting!\")\n",
    "        break\n",
    "    \n",
    "    if user_message.lower() == 'forget':\n",
    "        # Clear the conversation history\n",
    "        conversation = [conversation[0]]  # Keep only system message\n",
    "        print(\"\ud83e\uddf9 Memory cleared! Fresh start!\")\n",
    "        continue\n",
    "    \n",
    "    # Add user message to conversation history\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Send the ENTIRE conversation to the AI\n",
    "    # This is how it remembers what you talked about!\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation  # <- The magic! Send all messages!\n",
    "    )\n",
    "    \n",
    "    # Get and show the response\n",
    "    ai_message = response.choices[0].message.content\n",
    "    print(f\"\\nAI: {ai_message}\")\n",
    "    \n",
    "    # Add AI's response to conversation history\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "    \n",
    "    # Keep conversation from getting too long (optional)\n",
    "    if len(conversation) > 20:\n",
    "        # Keep system message and last 19 messages\n",
    "        conversation = [conversation[0]] + conversation[-19:]\n",
    "\n",
    "print(\"\\n\ud83c\udf89 You just built a chatbot with memory!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ai_chat_streaming.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: ai_chat_streaming.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "# Load API key\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83e\udd16 Streaming AI Chat!\")\n",
    "print(\"Watch the AI 'type' its response!\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_message.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    print(\"\\nAI: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Add stream=True to see the response as it's generated!\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "        stream=True  # \u2190 This makes it stream!\n",
    "    )\n",
    "    \n",
    "    # Print each chunk as it arrives\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # New line after response\n",
    "\n",
    "print(\"\\n\u2728 Pretty cool, right?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: try_claude.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: try_claude.py (if you have an Anthropic API key)\n",
    "\n",
    "import anthropic\n",
    "from pathlib import Path\n",
    "\n",
    "# Load API key from .env\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('ANTHROPIC_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"No Anthropic API key found. Add ANTHROPIC_API_KEY to your .env file!\")\n",
    "    exit()\n",
    "\n",
    "# Create Claude client\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "# Ask Claude something!\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short joke!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\ud83e\udd16 Claude says:\")\n",
    "print(message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: test_setup.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.1\n",
    "# File: test_setup.py\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\ud83e\uddea Testing Your AI Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test 1: Check if OpenAI is installed\n",
    "try:\n",
    "    import openai\n",
    "    print(\"\u2705 OpenAI package installed\")\n",
    "except ImportError:\n",
    "    print(\"\u274c OpenAI not installed. Run: pip install openai\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Test 2: Check for API key\n",
    "env_file = Path(\".env\")\n",
    "if not env_file.exists():\n",
    "    print(\"\u274c No .env file found. Run: python setup_api_key.py\")\n",
    "    sys.exit(1)\n",
    "\n",
    "api_key = None\n",
    "with open(env_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('OPENAI_API_KEY='):\n",
    "            api_key = line.split('=')[1].strip()\n",
    "            break\n",
    "\n",
    "if not api_key or api_key == 'your-key-here':\n",
    "    print(\"\u274c No valid API key in .env file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\u2705 API key found\")\n",
    "\n",
    "# Test 3: Try a real API call\n",
    "print(\"\ud83d\udd04 Testing API connection...\")\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'test successful' in 3 words or less\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    print(f\"\u2705 API call successful! AI said: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c API call failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\ud83c\udf89 All tests passed! You're ready to build AI apps!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.1 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1.1: Personality Bot\n",
    "\n",
    "Create a chatbot that:\n",
    "- Has a specific personality (like a pirate, poet, or chef)\n",
    "- Maintains that personality throughout the conversation\n",
    "- Remembers previous messages\n",
    "- Can switch personalities on command\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_1_8_1_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1.2: Conversation Saver\n",
    "\n",
    "Build a chat program that:\n",
    "- Saves each conversation to a text file\n",
    "- Names the file with the current date and time\n",
    "- Can load and continue a previous conversation\n",
    "- Shows how many messages have been exchanged\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_2_8_1_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1.3: Simple API Cost Calculator\n",
    "\n",
    "Create a tool that:\n",
    "- Counts how many tokens you use in each message\n",
    "- Estimates the cost of each API call\n",
    "- Keeps a running total of your spending\n",
    "- Warns you when you've spent more than $1\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_3_8_1_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.2: Making your first API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: anatomy_of_api_call.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: anatomy_of_api_call.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "# Load API key (you know this part!)\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Let's explore EVERY parameter you can use!\n",
    "response = client.chat.completions.create(\n",
    "    # 1. MODEL - Which AI brain to use\n",
    "    model=\"gpt-3.5-turbo\",  # Fast and cheap!\n",
    "    # model=\"gpt-4\",        # Smarter but slower\n",
    "    # model=\"gpt-3.5-turbo-1106\",  # Specific version\n",
    "    \n",
    "    # 2. MESSAGES - The conversation history\n",
    "    messages=[\n",
    "        # System message: Sets the AI's personality/role\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who explains things simply.\"},\n",
    "        \n",
    "        # User message: What the human says\n",
    "        {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "        \n",
    "        # Assistant message: Previous AI responses (for context)\n",
    "        # {\"role\": \"assistant\", \"content\": \"Previous response here...\"},\n",
    "    ],\n",
    "    \n",
    "    # 3. TEMPERATURE - Creativity control (0.0 to 2.0)\n",
    "    temperature=0.7,  # 0 = focused/deterministic, 2 = very creative/random\n",
    "    \n",
    "    # 4. MAX_TOKENS - Maximum response length\n",
    "    max_tokens=150,  # Roughly 1 token = 0.75 words\n",
    "    \n",
    "    # 5. TOP_P - Another way to control randomness (usually use temperature OR top_p, not both)\n",
    "    top_p=1.0,  # 0.1 = only most likely words, 1.0 = consider all words\n",
    "    \n",
    "    # 6. FREQUENCY_PENALTY - Reduce repetition (-2.0 to 2.0)\n",
    "    frequency_penalty=0.0,  # Positive = less repetition\n",
    "    \n",
    "    # 7. PRESENCE_PENALTY - Encourage new topics (-2.0 to 2.0)  \n",
    "    presence_penalty=0.0,  # Positive = talk about new things\n",
    "    \n",
    "    # 8. STOP - Stop sequences (where to cut off response)\n",
    "    stop=None,  # Can be a list like [\"\\n\", \".\", \"END\"]\n",
    "    \n",
    "    # 9. N - How many responses to generate\n",
    "    n=1,  # Generate multiple responses and pick the best!\n",
    "    \n",
    "    # 10. STREAM - Get response as it's generated\n",
    "    stream=False,  # True = see response word by word\n",
    "    \n",
    "    # 11. USER - Unique identifier for the user (for OpenAI's monitoring)\n",
    "    user=None,  # Can be a user ID for tracking\n",
    ")\n",
    "\n",
    "# Understanding the response structure\n",
    "print(\"\ud83d\udd0d API Response Structure:\")\n",
    "print(f\"ID: {response.id}\")\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Created at: {response.created}\")\n",
    "\n",
    "# The actual message\n",
    "message = response.choices[0].message\n",
    "print(f\"\\n\ud83d\udcac Response: {message.content}\")\n",
    "\n",
    "# Token usage (this is what costs money!)\n",
    "if response.usage:\n",
    "    print(f\"\\n\ud83d\udcca Token Usage:\")\n",
    "    print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "    print(f\"  Response tokens: {response.usage.completion_tokens}\")\n",
    "    print(f\"  Total tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "    # Cost calculation (GPT-3.5-turbo pricing)\n",
    "    cost = (response.usage.total_tokens / 1000) * 0.002\n",
    "    print(f\"  Estimated cost: ${cost:.6f}\")\n",
    "\n",
    "# Why the response was cut off (if applicable)\n",
    "print(f\"\\n\ud83d\uded1 Finish reason: {response.choices[0].finish_reason}\")\n",
    "# Possible values:\n",
    "# - \"stop\": Natural ending\n",
    "# - \"length\": Hit max_tokens limit  \n",
    "# - \"content_filter\": Blocked by safety filter\n",
    "# - \"null\": Still generating (if streaming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: temperature_experiments.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: temperature_experiments.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def test_temperature(prompt, temperature, description):\n",
    "    \"\"\"Test how temperature affects responses\"\"\"\n",
    "    print(f\"\\n\ud83c\udf21\ufe0f Temperature {temperature} - {description}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Same prompt, different temperatures\n",
    "prompt = \"Write a one-sentence story about a robot\"\n",
    "\n",
    "print(\"\ud83e\uddea Temperature Experiment: Same prompt, different creativity levels\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Temperature 0: Maximum consistency (almost deterministic)\n",
    "test_temperature(prompt, 0, \"Focused/Factual\")\n",
    "\n",
    "# Temperature 0.3: Slightly creative but still focused\n",
    "test_temperature(prompt, 0.3, \"Balanced/Professional\")\n",
    "\n",
    "# Temperature 0.7: Default - good balance\n",
    "test_temperature(prompt, 0.7, \"Creative/Natural\")\n",
    "\n",
    "# Temperature 1.0: More creative and varied\n",
    "test_temperature(prompt, 1.0, \"Very Creative\")\n",
    "\n",
    "# Temperature 1.5: Wild and unpredictable\n",
    "test_temperature(prompt, 1.5, \"Experimental/Wild\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udca1 When to use different temperatures:\")\n",
    "print(\"  \ud83d\udcca 0.0-0.3: Code generation, facts, math, analysis\")\n",
    "print(\"  \ud83d\udcdd 0.4-0.7: General chat, explanations, summaries\")\n",
    "print(\"  \ud83c\udfa8 0.8-1.2: Creative writing, brainstorming, stories\")\n",
    "print(\"  \ud83c\udfb2 1.3-2.0: Experimental, poetry, wild ideas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_personalities.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: simple_personalities.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Try different personalities with the SAME question\n",
    "question = \"How do I make friends?\"\n",
    "\n",
    "print(\"\ud83c\udfad Same Question, 3 Different Personalities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Personality 1: Friendly buddy\n",
    "print(\"\\n1\ufe0f\u20e3 FRIENDLY BUDDY:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly, casual buddy. Use simple language and be encouraging!\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Personality 2: Professional coach\n",
    "print(\"\\n2\ufe0f\u20e3 PROFESSIONAL COACH:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional life coach. Be formal and give structured advice.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Personality 3: Wise grandparent\n",
    "print(\"\\n3\ufe0f\u20e3 WISE GRANDPARENT:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a wise, caring grandparent. Share life wisdom and be warm.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udca1 See how the system message changes the AI's style?\")\n",
    "print(\"   You just learned how ChatGPT's 'Custom Instructions' work!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_conversation.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: smart_conversation.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\udcac Smart Conversation Manager\")\n",
    "print(\"Keeps only recent messages to save tokens!\")\n",
    "print(\"Commands: 'quit', 'count'\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Our conversation history\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "MAX_MESSAGES = 10  # Keep only last 10 messages\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'count':\n",
    "        print(f\"\ud83d\udcca Messages in memory: {len(conversation) - 1}\")  # -1 for system message\n",
    "        continue\n",
    "    \n",
    "    # Add user message\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Keep conversation from getting too long\n",
    "    if len(conversation) > MAX_MESSAGES:\n",
    "        # Keep system message (first) and recent messages\n",
    "        conversation = [conversation[0]] + conversation[-(MAX_MESSAGES-1):]\n",
    "        print(\"(Trimmed old messages to save tokens)\")\n",
    "    \n",
    "    # Make API call with conversation history\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation\n",
    "    )\n",
    "    \n",
    "    ai_message = response.choices[0].message.content\n",
    "    print(f\"\\nAI: {ai_message}\")\n",
    "    \n",
    "    # Add AI response to history\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "\n",
    "print(\"\\n\u2705 Smart conversation management - you're saving money already!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: streaming_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: streaming_demo.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83c\udf0a Streaming Chat Demo\")\n",
    "print(\"Watch the AI type its response!\")\n",
    "print(\"Type 'quit' to exit\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    print(\"\\nAI: \", end=\"\", flush=True)\n",
    "    \n",
    "    # The magic parameter: stream=True\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        stream=True  # \u2190 This makes it stream!\n",
    "    )\n",
    "    \n",
    "    # Print each piece as it arrives\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # New line after response is complete\n",
    "\n",
    "print(\"\\n\u2728 That's how ChatGPT does it!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: my_complete_chatbot.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: my_complete_chatbot.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found! Run setup_api_key.py first!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83e\udd16 Your Complete Chatbot\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Features:\")\n",
    "print(\"  \u2022 Adjustable temperature (creative vs focused)\")\n",
    "print(\"  \u2022 Streaming responses\")\n",
    "print(\"  \u2022 Conversation memory\")\n",
    "print(\"  \u2022 Save conversations\")\n",
    "print(\"\\nCommands: 'quit', 'save', 'temp <value>', 'clear'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Settings\n",
    "temperature = 0.7\n",
    "conversation = [{\"role\": \"system\", \"content\": \"You are a helpful, friendly assistant.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \").strip()\n",
    "    \n",
    "    # Handle commands\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"\ud83d\udc4b Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    elif user_input.lower() == 'clear':\n",
    "        conversation = [conversation[0]]  # Keep only system message\n",
    "        print(\"\ud83e\uddf9 Conversation cleared!\")\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'save':\n",
    "        # Save conversation to file\n",
    "        filename = f\"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({\n",
    "                \"conversation\": conversation,\n",
    "                \"temperature\": temperature,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }, f, indent=2)\n",
    "        print(f\"\ud83d\udcbe Saved to {filename}\")\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower().startswith('temp '):\n",
    "        # Change temperature\n",
    "        try:\n",
    "            new_temp = float(user_input.split()[1])\n",
    "            if 0 <= new_temp <= 2:\n",
    "                temperature = new_temp\n",
    "                print(f\"\ud83c\udf21\ufe0f Temperature set to {temperature}\")\n",
    "            else:\n",
    "                print(\"Temperature must be between 0 and 2\")\n",
    "        except:\n",
    "            print(\"Invalid temperature\")\n",
    "        continue\n",
    "    \n",
    "    # Regular chat message\n",
    "    if user_input:\n",
    "        # Add to conversation\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Keep conversation manageable (max 20 messages)\n",
    "        if len(conversation) > 20:\n",
    "            conversation = [conversation[0]] + conversation[-19:]\n",
    "        \n",
    "        print(\"\\nAI: \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Stream the response\n",
    "            stream = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=conversation,\n",
    "                temperature=temperature,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in stream:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    print(content, end=\"\", flush=True)\n",
    "                    full_response += content\n",
    "            \n",
    "            print()  # New line\n",
    "            \n",
    "            # Add AI response to conversation\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u274c Error: {e}\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Thanks for using your complete chatbot!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: mini_chatgpt.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.2\n",
    "# File: mini_chatgpt.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "class MiniChatGPT:\n",
    "    \"\"\"Your own mini version of ChatGPT!\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.conversations = {}  # Multiple conversation threads\n",
    "        self.current_conversation_id = None\n",
    "        self.settings = {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 500,\n",
    "            \"stream\": True\n",
    "        }\n",
    "    \n",
    "    def new_conversation(self, title=None):\n",
    "        \"\"\"Start a new conversation thread\"\"\"\n",
    "        conv_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        title = title or f\"Chat {conv_id}\"\n",
    "        \n",
    "        self.conversations[conv_id] = {\n",
    "            \"id\": conv_id,\n",
    "            \"title\": title,\n",
    "            \"messages\": [],\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"token_count\": 0,\n",
    "            \"cost\": 0.0\n",
    "        }\n",
    "        \n",
    "        self.current_conversation_id = conv_id\n",
    "        return conv_id\n",
    "    \n",
    "    def list_conversations(self):\n",
    "        \"\"\"List all conversations\"\"\"\n",
    "        if not self.conversations:\n",
    "            print(\"No conversations yet.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\ud83d\udcda Your Conversations:\")\n",
    "        for i, (conv_id, conv) in enumerate(self.conversations.items(), 1):\n",
    "            msg_count = len(conv[\"messages\"])\n",
    "            print(f\"  {i}. {conv['title']} ({msg_count} messages)\")\n",
    "            if conv_id == self.current_conversation_id:\n",
    "                print(\"     ^ Current\")\n",
    "    \n",
    "    def switch_conversation(self, conv_id):\n",
    "        \"\"\"Switch to a different conversation\"\"\"\n",
    "        if conv_id in self.conversations:\n",
    "            self.current_conversation_id = conv_id\n",
    "            print(f\"Switched to: {self.conversations[conv_id]['title']}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_current_messages(self):\n",
    "        \"\"\"Get messages for current conversation\"\"\"\n",
    "        if not self.current_conversation_id:\n",
    "            self.new_conversation()\n",
    "        \n",
    "        conv = self.conversations[self.current_conversation_id]\n",
    "        \n",
    "        # System message + conversation history\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are ChatGPT, a helpful AI assistant.\"}\n",
    "        ]\n",
    "        \n",
    "        # Add conversation messages (limit to last 20 for token management)\n",
    "        for msg in conv[\"messages\"][-20:]:\n",
    "            messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": msg[\"content\"]\n",
    "            })\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Send message and get response\"\"\"\n",
    "        if not self.current_conversation_id:\n",
    "            self.new_conversation()\n",
    "        \n",
    "        conv = self.conversations[self.current_conversation_id]\n",
    "        \n",
    "        # Add user message\n",
    "        conv[\"messages\"].append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Get messages for API\n",
    "        messages = self.get_current_messages()\n",
    "        \n",
    "        print(\"\\n\ud83e\udd16 ChatGPT: \", end=\"\", flush=True)\n",
    "        \n",
    "        full_response = \"\"\n",
    "        \n",
    "        try:\n",
    "            if self.settings[\"stream\"]:\n",
    "                # Streaming response\n",
    "                stream = self.client.chat.completions.create(\n",
    "                    model=self.settings[\"model\"],\n",
    "                    messages=messages,\n",
    "                    temperature=self.settings[\"temperature\"],\n",
    "                    max_tokens=self.settings[\"max_tokens\"],\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    if chunk.choices[0].delta.content:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        full_response += content\n",
    "                \n",
    "            else:\n",
    "                # Non-streaming response\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.settings[\"model\"],\n",
    "                    messages=messages,\n",
    "                    temperature=self.settings[\"temperature\"],\n",
    "                    max_tokens=self.settings[\"max_tokens\"]\n",
    "                )\n",
    "                \n",
    "                full_response = response.choices[0].message.content\n",
    "                print(full_response)\n",
    "                \n",
    "                # Track usage\n",
    "                if response.usage:\n",
    "                    conv[\"token_count\"] += response.usage.total_tokens\n",
    "                    conv[\"cost\"] += (response.usage.total_tokens / 1000) * 0.002\n",
    "            \n",
    "            print()  # New line\n",
    "            \n",
    "            # Add assistant response\n",
    "            conv[\"messages\"].append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": full_response,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return full_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u274c Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_all_conversations(self):\n",
    "        \"\"\"Save all conversations to file\"\"\"\n",
    "        filename = f\"conversations_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.conversations, f, indent=2)\n",
    "        \n",
    "        print(f\"\ud83d\udcbe Saved all conversations to {filename}\")\n",
    "    \n",
    "    def show_settings(self):\n",
    "        \"\"\"Show current settings\"\"\"\n",
    "        print(\"\\n\u2699\ufe0f Current Settings:\")\n",
    "        for key, value in self.settings.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    def change_setting(self, key, value):\n",
    "        \"\"\"Change a setting\"\"\"\n",
    "        if key in self.settings:\n",
    "            old_value = self.settings[key]\n",
    "            self.settings[key] = value\n",
    "            print(f\"\u2705 Changed {key}: {old_value} \u2192 {value}\")\n",
    "        else:\n",
    "            print(f\"\u274c Unknown setting: {key}\")\n",
    "    \n",
    "    def show_help(self):\n",
    "        \"\"\"Show available commands\"\"\"\n",
    "        print(\"\\n\ud83d\udcd6 Available Commands:\")\n",
    "        print(\"  'new' - Start new conversation\")\n",
    "        print(\"  'list' - List all conversations\")\n",
    "        print(\"  'switch' - Switch to another conversation\")\n",
    "        print(\"  'settings' - Show current settings\")\n",
    "        print(\"  'temp <value>' - Change temperature (0-2)\")\n",
    "        print(\"  'model <name>' - Change model\")\n",
    "        print(\"  'save' - Save all conversations\")\n",
    "        print(\"  'help' - Show this help\")\n",
    "        print(\"  'quit' - Exit\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the ChatGPT clone\"\"\"\n",
    "        print(\"\ud83d\ude80 Mini ChatGPT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Welcome to your own ChatGPT clone!\")\n",
    "        print(\"Type 'help' for commands or just start chatting!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Start first conversation\n",
    "        self.new_conversation(\"Welcome Chat\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n\ud83d\udc64 You: \").strip()\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Check for commands\n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"\\n\ud83d\udc4b Goodbye!\")\n",
    "                    break\n",
    "                \n",
    "                elif user_input.lower() == 'help':\n",
    "                    self.show_help()\n",
    "                \n",
    "                elif user_input.lower() == 'new':\n",
    "                    title = input(\"Conversation title (or Enter for default): \").strip()\n",
    "                    conv_id = self.new_conversation(title)\n",
    "                    print(f\"\u2728 Started new conversation: {self.conversations[conv_id]['title']}\")\n",
    "                \n",
    "                elif user_input.lower() == 'list':\n",
    "                    self.list_conversations()\n",
    "                \n",
    "                elif user_input.lower() == 'switch':\n",
    "                    self.list_conversations()\n",
    "                    choice = input(\"Choose conversation number: \").strip()\n",
    "                    try:\n",
    "                        idx = int(choice) - 1\n",
    "                        conv_list = list(self.conversations.keys())\n",
    "                        if 0 <= idx < len(conv_list):\n",
    "                            self.switch_conversation(conv_list[idx])\n",
    "                    except:\n",
    "                        print(\"Invalid choice\")\n",
    "                \n",
    "                elif user_input.lower() == 'settings':\n",
    "                    self.show_settings()\n",
    "                \n",
    "                elif user_input.lower().startswith('temp '):\n",
    "                    try:\n",
    "                        temp = float(user_input.split()[1])\n",
    "                        if 0 <= temp <= 2:\n",
    "                            self.change_setting('temperature', temp)\n",
    "                        else:\n",
    "                            print(\"Temperature must be between 0 and 2\")\n",
    "                    except:\n",
    "                        print(\"Invalid temperature value\")\n",
    "                \n",
    "                elif user_input.lower().startswith('model '):\n",
    "                    model = user_input[6:].strip()\n",
    "                    self.change_setting('model', model)\n",
    "                \n",
    "                elif user_input.lower() == 'save':\n",
    "                    self.save_all_conversations()\n",
    "                \n",
    "                else:\n",
    "                    # Regular chat message\n",
    "                    self.chat(user_input)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\u26a0\ufe0f Interrupted! Type 'quit' to exit.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n\u274c Error: {e}\")\n",
    "\n",
    "# Run Mini ChatGPT!\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = load_api_key()\n",
    "    if not api_key:\n",
    "        print(\"\u274c No API key found! Run setup_api_key.py first!\")\n",
    "        exit()\n",
    "    \n",
    "    chatgpt = MiniChatGPT(api_key)\n",
    "    chatgpt.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2.1: Temperature Tester\n",
    "\n",
    "Create a simple tool that:\n",
    "- Takes one prompt from the user\n",
    "- Generates 3 responses at different temperatures (0, 0.7, 1.5)\n",
    "- Shows all three responses\n",
    "- Lets the user pick their favorite\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_1_8_2_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2.2: Conversation Counter\n",
    "\n",
    "Build a chatbot that:\n",
    "- Counts how many messages have been sent\n",
    "- Shows the word count of each response\n",
    "- Estimates the cost (roughly $0.002 per 1000 tokens)\n",
    "- Saves the conversation when you quit\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_2_8_2_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2.3: Personality Switcher\n",
    "\n",
    "Create a chatbot that:\n",
    "- Has 3 different personalities (your choice!)\n",
    "- Lets you switch between them with a command\n",
    "- Each personality has a different temperature setting\n",
    "- Shows which personality is currently active\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_3_8_2_solution.py`\n",
    "\n",
    " **BONUS CHALLENGE: Complete the Mini ChatGPT!**\n",
    "Take the MiniChatGPT class boilerplate from above and implement all the methods to create a full ChatGPT clone with:\n",
    "- Multiple conversation threads (like ChatGPT's sidebar!)\n",
    "- Ability to switch between conversations\n",
    "- List all your conversations\n",
    "- Save everything to a file\n",
    "- Each conversation remembers its own history\n",
    "\n",
    "This is the ultimate test of everything you've learned - can you build your own ChatGPT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.3: Understanding API responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: response_inspector.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: response_inspector.py\n",
    "\n",
    "# Simple tools to understand API responses\n",
    "\n",
    "def inspect_response(response):\n",
    "    \"\"\"Print all the interesting parts of a response\"\"\"\n",
    "    print(\"\\n\ud83d\udd0d Response Inspector\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Response ID: {response.id}\")\n",
    "    print(f\"Model: {response.model}\")\n",
    "    print(f\"Created: {response.created}\")\n",
    "    \n",
    "    # The actual message\n",
    "    if response.choices:\n",
    "        message = response.choices[0].message.content\n",
    "        print(f\"\\nMessage: {message[:100]}...\")\n",
    "        print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "    \n",
    "    # Token usage\n",
    "    if hasattr(response, 'usage') and response.usage:\n",
    "        print(f\"\\n\ud83d\udcca Token Usage:\")\n",
    "        print(f\"  Prompt: {response.usage.prompt_tokens}\")\n",
    "        print(f\"  Response: {response.usage.completion_tokens}\")\n",
    "        print(f\"  Total: {response.usage.total_tokens}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def calculate_simple_cost(tokens, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Simple cost calculator\"\"\"\n",
    "    # Rough pricing (as of 2024)\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        return tokens * 0.002 / 1000  # $0.002 per 1K tokens\n",
    "    elif model == \"gpt-4\":\n",
    "        return tokens * 0.03 / 1000   # $0.03 per 1K tokens\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: explore_responses.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: explore_responses.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from response_inspector import inspect_response, calculate_simple_cost\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Try a simple request\n",
    "print(\"\ud83d\udce4 Sending request: 'Say hello in 5 words'\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hello in 5 words\"}],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Inspect what came back\n",
    "inspect_response(response)\n",
    "\n",
    "# Calculate cost\n",
    "if response.usage:\n",
    "    cost = calculate_simple_cost(response.usage.total_tokens)\n",
    "    print(f\"\\n\ud83d\udcb0 Estimated cost: ${cost:.6f}\")\n",
    "\n",
    "# What's in the response object?\n",
    "print(\"\\n\ud83d\udce6 Response object attributes:\")\n",
    "for attr in dir(response):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"  \u2022 {attr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: token_explorer.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: token_explorer.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Rough estimate: 1 token \u2248 4 characters or 0.75 words\"\"\"\n",
    "    return max(len(text) // 4, len(text.split()) * 4 // 3)\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Test different prompts\n",
    "test_prompts = [\n",
    "    \"Hi\",\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Write a haiku about coding\",\n",
    "    \"Explain quantum computing in simple terms\",\n",
    "]\n",
    "\n",
    "print(\"\ud83d\udd2c Token Usage Explorer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n\ud83d\udcdd Prompt: '{prompt}'\")\n",
    "    print(f\"\ud83d\udccf Estimated tokens: {estimate_tokens(prompt)}\")\n",
    "    \n",
    "    # Make API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    # Get actual token counts\n",
    "    if response.usage:\n",
    "        print(f\"\u2705 Actual prompt tokens: {response.usage.prompt_tokens}\")\n",
    "        print(f\"\u2705 Response tokens: {response.usage.completion_tokens}\")\n",
    "        print(f\"\u2705 Total tokens: {response.usage.total_tokens}\")\n",
    "        \n",
    "        # Cost\n",
    "        cost = response.usage.total_tokens * 0.002 / 1000\n",
    "        print(f\"\ud83d\udcb0 Cost: ${cost:.6f}\")\n",
    "    \n",
    "    # Show the response\n",
    "    message = response.choices[0].message.content\n",
    "    print(f\"\ud83d\udcac Response: {message[:50]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83d\udca1 Tip: Shorter prompts = lower costs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: finish_reason_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: finish_reason_demo.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def explain_finish_reason(reason):\n",
    "    \"\"\"Explain what each finish reason means\"\"\"\n",
    "    reasons = {\n",
    "        \"stop\": \"\u2705 Natural ending - AI completed its thought\",\n",
    "        \"length\": \"\u2702\ufe0f Hit token limit - response was cut off\",\n",
    "        \"content_filter\": \"\ud83d\udeab Blocked by safety filter\",\n",
    "        \"null\": \"\u23f3 Still generating (in streaming)\",\n",
    "    }\n",
    "    return reasons.get(reason, f\"\u2753 Unknown: {reason}\")\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\udd2c Finish Reason Explorer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Natural completion\n",
    "print(\"\\n1\ufe0f\u20e3 Natural completion test:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Count to 5\"}],\n",
    ")\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "print(explain_finish_reason(response.choices[0].finish_reason))\n",
    "\n",
    "# Test 2: Length limit\n",
    "print(\"\\n2\ufe0f\u20e3 Length limit test:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a long story\"}],\n",
    "    max_tokens=10  # Very short limit!\n",
    ")\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "print(explain_finish_reason(response.choices[0].finish_reason))\n",
    "\n",
    "# Test 3: Quick task\n",
    "print(\"\\n3\ufe0f\u20e3 Quick task test:\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's 2+2?\"}],\n",
    ")\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "print(explain_finish_reason(response.choices[0].finish_reason))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83d\udca1 Understanding finish reasons helps you:\")\n",
    "print(\"  \u2022 Know if responses are complete\")\n",
    "print(\"  \u2022 Detect when you need higher token limits\")\n",
    "print(\"  \u2022 Handle truncated responses properly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: response_comparison.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: response_comparison.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "prompt = \"Write a creative tagline for a coffee shop\"\n",
    "\n",
    "print(\"\u2615 Comparing Different Temperatures\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "temperatures = [0.2, 0.7, 1.2]\n",
    "total_tokens = 0\n",
    "total_cost = 0\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n\ud83c\udf21\ufe0f Temperature: {temp}\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp\n",
    "    )\n",
    "    \n",
    "    message = response.choices[0].message.content\n",
    "    tokens = response.usage.total_tokens\n",
    "    cost = tokens * 0.002 / 1000\n",
    "    \n",
    "    print(f\"Response: {message}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Cost: ${cost:.6f}\")\n",
    "    \n",
    "    total_tokens += tokens\n",
    "    total_cost += cost\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"\ud83d\udcca Totals:\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Total cost: ${total_cost:.6f}\")\n",
    "print(f\"  Average per response: ${total_cost/len(temperatures):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_logger.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.3\n",
    "# File: simple_logger.py\n",
    "\n",
    "import openai\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def log_response(prompt, response):\n",
    "    \"\"\"Log response details to a file\"\"\"\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response.choices[0].message.content,\n",
    "        \"model\": response.model,\n",
    "        \"tokens\": {\n",
    "            \"prompt\": response.usage.prompt_tokens,\n",
    "            \"completion\": response.usage.completion_tokens,\n",
    "            \"total\": response.usage.total_tokens\n",
    "        },\n",
    "        \"finish_reason\": response.choices[0].finish_reason,\n",
    "        \"cost_estimate\": response.usage.total_tokens * 0.002 / 1000\n",
    "    }\n",
    "    \n",
    "    # Append to log file\n",
    "    log_file = \"api_responses.json\"\n",
    "    \n",
    "    # Load existing logs\n",
    "    if Path(log_file).exists():\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    \n",
    "    # Add new entry\n",
    "    logs.append(log_entry)\n",
    "    \n",
    "    # Save\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "    \n",
    "    print(f\"\ud83d\udcdd Logged response to {log_file}\")\n",
    "    return log_entry\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\udcca Response Logger\")\n",
    "print(\"Type your prompts and I'll log all the details!\")\n",
    "print(\"Type 'quit' to exit, 'stats' to see statistics\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYour prompt: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'stats':\n",
    "        # Show statistics from log\n",
    "        if Path(\"api_responses.json\").exists():\n",
    "            with open(\"api_responses.json\", 'r') as f:\n",
    "                logs = json.load(f)\n",
    "            \n",
    "            total_tokens = sum(log['tokens']['total'] for log in logs)\n",
    "            total_cost = sum(log['cost_estimate'] for log in logs)\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcc8 Statistics from {len(logs)} requests:\")\n",
    "            print(f\"  Total tokens: {total_tokens:,}\")\n",
    "            print(f\"  Total cost: ${total_cost:.6f}\")\n",
    "            print(f\"  Average per request: ${total_cost/len(logs):.6f}\")\n",
    "        else:\n",
    "            print(\"No logs yet!\")\n",
    "        continue\n",
    "    \n",
    "    # Make API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}]\n",
    "    )\n",
    "    \n",
    "    # Show response\n",
    "    print(f\"\\n\ud83e\udd16 Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "    # Log it\n",
    "    log_entry = log_response(user_input, response)\n",
    "    print(f\"\ud83d\udcca Used {log_entry['tokens']['total']} tokens (${log_entry['cost_estimate']:.6f})\")\n",
    "\n",
    "print(\"\\n\ud83d\udc4b Check api_responses.json for your logged data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3.1: Token Predictor\n",
    "\n",
    "Create a tool that:\n",
    "- Takes a prompt from the user\n",
    "- Predicts how many tokens it will use\n",
    "- Makes the API call\n",
    "- Compares prediction vs actual\n",
    "- Keeps score of your accuracy\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_1_8_3_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3.2: Response Time Tracker\n",
    "\n",
    "Build a simple tool that:\n",
    "- Measures how long API calls take\n",
    "- Tests different prompt lengths\n",
    "- Shows if longer prompts take more time\n",
    "- Finds the sweet spot for speed vs detail\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_2_8_3_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3.3: Model Comparison\n",
    "\n",
    "Create a comparison tool that:\n",
    "- Sends the same prompt to different models (if available)\n",
    "- Compares token usage\n",
    "- Compares costs\n",
    "- Shows the differences in responses\n",
    "\n",
    " **\u00a0Solution**: `part_2_ai_basics/chapter_08_first_llm/exercise_3_8_3_solution.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.4: Handling API errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: error_types_demo.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.4\n",
    "# File: error_types_demo.py\n",
    "\n",
    "# Let's trigger different types of errors and see what happens!\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def test_error(description, test_function):\n",
    "    \"\"\"Test an error condition and show what happens\"\"\"\n",
    "    print(f\"\\n\ud83e\uddea Testing: {description}\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        test_function()\n",
    "        print(\"\u2705 No error occurred\")\n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        print(f\"\u274c Error Type: {error_type}\")\n",
    "        print(f\"\ud83d\udcdd Error Message: {str(e)}\")\n",
    "        \n",
    "        # Show useful error details if available\n",
    "        if hasattr(e, 'response'):\n",
    "            print(f\"\ud83d\udcca Status Code: {getattr(e, 'status_code', 'N/A')}\")\n",
    "        \n",
    "        return error_type\n",
    "    return None\n",
    "\n",
    "# Setup\n",
    "print(\"\ud83d\udd2c API Error Types Explorer\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Invalid API Key\n",
    "print(\"\\n1\ufe0f\u20e3 Invalid API Key Test:\")\n",
    "try:\n",
    "    bad_client = openai.OpenAI(api_key=\"sk-invalid-key-12345\")\n",
    "    response = bad_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\u274c {type(e).__name__}: {str(e)[:100]}...\")\n",
    "\n",
    "# Test 2: No API Key\n",
    "print(\"\\n2\ufe0f\u20e3 Missing API Key Test:\")\n",
    "try:\n",
    "    no_key_client = openai.OpenAI(api_key=None)\n",
    "    response = no_key_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\u274c {type(e).__name__}: {str(e)[:100]}...\")\n",
    "\n",
    "# Test 3: Invalid Model\n",
    "api_key = load_api_key()\n",
    "if api_key:\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    \n",
    "    print(\"\\n3\ufe0f\u20e3 Invalid Model Test:\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-99-ultra\",  # This model doesn't exist!\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c {type(e).__name__}: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Test 4: Invalid Parameters\n",
    "    print(\"\\n4\ufe0f\u20e3 Invalid Parameters Test:\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "            temperature=5.0  # Too high! Max is 2.0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c {type(e).__name__}: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Test 5: Empty Messages\n",
    "    print(\"\\n5\ufe0f\u20e3 Empty Messages Test:\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[]  # No messages!\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c {type(e).__name__}: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83d\udca1 Common error types:\")\n",
    "print(\"  \u2022 AuthenticationError: Bad API key\")\n",
    "print(\"  \u2022 NotFoundError: Invalid model or endpoint\")\n",
    "print(\"  \u2022 BadRequestError: Invalid parameters\")\n",
    "print(\"  \u2022 RateLimitError: Too many requests\")\n",
    "print(\"  \u2022 APIConnectionError: Network issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: basic_error_handling.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.4\n",
    "# File: basic_error_handling.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def safe_api_call(client, messages, max_tokens=None):\n",
    "    \"\"\"Make an API call with proper error handling\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content, None\n",
    "    \n",
    "    except openai.AuthenticationError:\n",
    "        return None, \"\u274c Invalid API key. Please check your credentials.\"\n",
    "    \n",
    "    except openai.RateLimitError as e:\n",
    "        return None, \"\u23f3 Rate limit hit. Please wait a moment and try again.\"\n",
    "    \n",
    "    except openai.BadRequestError as e:\n",
    "        return None, f\"\u274c Invalid request: {str(e)}\"\n",
    "    \n",
    "    except openai.APIConnectionError:\n",
    "        return None, \"\ud83c\udf10 Network error. Please check your internet connection.\"\n",
    "    \n",
    "    except openai.APITimeoutError:\n",
    "        return None, \"\u23f1\ufe0f Request timed out. Please try again.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any other errors\n",
    "        return None, f\"\u274c Unexpected error: {type(e).__name__}: {str(e)}\"\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found! Please set up your .env file.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\udee1\ufe0f Safe API Caller\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This handles errors gracefully!\")\n",
    "print(\"Type 'quit' to exit\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYour message: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Make safe API call\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    response, error = safe_api_call(client, messages)\n",
    "    \n",
    "    if error:\n",
    "        print(f\"\\n{error}\")\n",
    "        print(\"\ud83d\udca1 Tip: The application didn't crash! You can try again.\")\n",
    "    else:\n",
    "        print(f\"\\n\ud83e\udd16 Response: {response}\")\n",
    "\n",
    "print(\"\\n\ud83d\udc4b Goodbye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_retry.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.4\n",
    "# File: smart_retry.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def exponential_backoff(attempt):\n",
    "    \"\"\"Calculate wait time with exponential backoff\"\"\"\n",
    "    # 1st attempt: 1 sec, 2nd: 2 sec, 3rd: 4 sec, etc.\n",
    "    base_wait = 2 ** attempt\n",
    "    \n",
    "    # Add jitter to prevent thundering herd\n",
    "    jitter = random.uniform(0, 0.5)\n",
    "    \n",
    "    return min(base_wait + jitter, 32)  # Max 32 seconds\n",
    "\n",
    "def api_call_with_retry(client, messages, max_retries=3):\n",
    "    \"\"\"Make API call with automatic retry on failure\"\"\"\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\ud83d\udd04 Attempt {attempt + 1}/{max_retries}...\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            print(\"\u2705 Success!\")\n",
    "            return response.choices[0].message.content, None\n",
    "        \n",
    "        except openai.RateLimitError as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = exponential_backoff(attempt)\n",
    "                print(f\"\u23f3 Rate limit hit. Waiting {wait_time:.1f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"\u274c Rate limit persists after retries\")\n",
    "        \n",
    "        except openai.APITimeoutError as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = exponential_backoff(attempt)\n",
    "                print(f\"\u23f1\ufe0f Timeout. Retrying in {wait_time:.1f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"\u274c Timeout after all retries\")\n",
    "        \n",
    "        except openai.APIConnectionError as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = exponential_backoff(attempt)\n",
    "                print(f\"\ud83c\udf10 Connection error. Retrying in {wait_time:.1f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"\u274c Connection failed after all retries\")\n",
    "        \n",
    "        except openai.AuthenticationError:\n",
    "            # Don't retry auth errors - they won't fix themselves\n",
    "            print(\"\u274c Authentication failed - check your API key\")\n",
    "            return None, \"Invalid API key\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Don't retry unexpected errors\n",
    "            print(f\"\u274c Unexpected error: {type(e).__name__}\")\n",
    "            return None, str(e)\n",
    "    \n",
    "    # All retries failed\n",
    "    return None, f\"Failed after {max_retries} attempts: {str(last_error)}\"\n",
    "\n",
    "# Test the retry mechanism\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\udd04 Smart Retry Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing retry mechanism with different scenarios\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test 1: Normal request (should succeed on first try)\n",
    "print(\"\\n\ud83d\udcdd Test 1: Normal request\")\n",
    "messages = [{\"role\": \"user\", \"content\": \"Say hello!\"}]\n",
    "response, error = api_call_with_retry(client, messages)\n",
    "if response:\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "# Test 2: Simulate rate limit (you can trigger this with rapid requests)\n",
    "print(\"\\n\ud83d\udcdd Test 2: Multiple rapid requests\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nRequest {i+1}:\")\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"Count to {i+1}\"}]\n",
    "    response, error = api_call_with_retry(client, messages)\n",
    "    if response:\n",
    "        print(f\"Response: {response[:50]}...\")\n",
    "    else:\n",
    "        print(f\"Failed: {error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\ud83d\udca1 Retry best practices:\")\n",
    "print(\"  \u2022 Use exponential backoff (wait longer each retry)\")\n",
    "print(\"  \u2022 Add jitter to prevent synchronized retries\")\n",
    "print(\"  \u2022 Don't retry authentication errors\")\n",
    "print(\"  \u2022 Set a reasonable max retry limit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: rate_limit_handler.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.4\n",
    "# File: rate_limit_handler.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "class RateLimitHandler:\n",
    "    \"\"\"Handle rate limits intelligently\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.request_times = []\n",
    "        self.requests_per_minute = 20  # Conservative limit\n",
    "        self.last_rate_limit_time = None\n",
    "        \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Check if we need to wait before making a request\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Clean old request times (older than 1 minute)\n",
    "        one_minute_ago = now - timedelta(minutes=1)\n",
    "        self.request_times = [t for t in self.request_times if t > one_minute_ago]\n",
    "        \n",
    "        # Check if we're approaching the limit\n",
    "        if len(self.request_times) >= self.requests_per_minute:\n",
    "            # Calculate how long to wait\n",
    "            oldest_request = self.request_times[0]\n",
    "            wait_until = oldest_request + timedelta(minutes=1)\n",
    "            wait_seconds = (wait_until - now).total_seconds()\n",
    "            \n",
    "            if wait_seconds > 0:\n",
    "                print(f\"\u23f3 Approaching rate limit. Waiting {wait_seconds:.1f} seconds...\")\n",
    "                time.sleep(wait_seconds + 0.1)  # Add small buffer\n",
    "    \n",
    "    def make_request(self, messages):\n",
    "        \"\"\"Make a request with rate limit handling\"\"\"\n",
    "        self.wait_if_needed()\n",
    "        \n",
    "        try:\n",
    "            # Record request time\n",
    "            self.request_times.append(datetime.now())\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            # Check response headers for rate limit info\n",
    "            # Note: OpenAI Python client may not expose all headers\n",
    "            \n",
    "            return response.choices[0].message.content, None\n",
    "            \n",
    "        except openai.RateLimitError as e:\n",
    "            self.last_rate_limit_time = datetime.now()\n",
    "            \n",
    "            # Parse retry-after if available\n",
    "            retry_after = getattr(e, 'retry_after', None)\n",
    "            if retry_after:\n",
    "                print(f\"\u23f3 Rate limited. Retry after {retry_after} seconds\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                # Default wait\n",
    "                print(\"\u23f3 Rate limited. Waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "            \n",
    "            # Try once more\n",
    "            return self.make_request(messages)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, f\"Error: {str(e)}\"\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current rate limit status\"\"\"\n",
    "        now = datetime.now()\n",
    "        one_minute_ago = now - timedelta(minutes=1)\n",
    "        recent_requests = len([t for t in self.request_times if t > one_minute_ago])\n",
    "        \n",
    "        return {\n",
    "            'recent_requests': recent_requests,\n",
    "            'limit': self.requests_per_minute,\n",
    "            'available': self.requests_per_minute - recent_requests\n",
    "        }\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "rate_handler = RateLimitHandler(client)\n",
    "\n",
    "print(\"\ud83d\udea6 Rate Limit Handler Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rate limit: {rate_handler.requests_per_minute} requests/minute\")\n",
    "print(\"Commands: 'status', 'burst', 'quit'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    command = input(\"\\nEnter command or message: \").strip()\n",
    "    \n",
    "    if command.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    elif command.lower() == 'status':\n",
    "        status = rate_handler.get_status()\n",
    "        print(f\"\\n\ud83d\udcca Rate Limit Status:\")\n",
    "        print(f\"  Recent requests: {status['recent_requests']}\")\n",
    "        print(f\"  Limit: {status['limit']}/minute\")\n",
    "        print(f\"  Available: {status['available']}\")\n",
    "        continue\n",
    "    \n",
    "    elif command.lower() == 'burst':\n",
    "        # Test rate limiting with burst requests\n",
    "        print(\"\\n\ud83d\ude80 Sending burst of requests...\")\n",
    "        for i in range(5):\n",
    "            print(f\"\\nRequest {i+1}/5:\")\n",
    "            messages = [{\"role\": \"user\", \"content\": f\"Say '{i+1}'\"}]\n",
    "            response, error = rate_handler.make_request(messages)\n",
    "            \n",
    "            if response:\n",
    "                print(f\"\u2705 Response: {response}\")\n",
    "            else:\n",
    "                print(f\"\u274c Error: {error}\")\n",
    "            \n",
    "            # Show status\n",
    "            status = rate_handler.get_status()\n",
    "            print(f\"   [{status['recent_requests']}/{status['limit']} requests used]\")\n",
    "        continue\n",
    "    \n",
    "    # Normal message\n",
    "    messages = [{\"role\": \"user\", \"content\": command}]\n",
    "    response, error = rate_handler.make_request(messages)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n\ud83e\udd16 Response: {response}\")\n",
    "        status = rate_handler.get_status()\n",
    "        print(f\"\ud83d\udcca Requests: {status['recent_requests']}/{status['limit']}\")\n",
    "    else:\n",
    "        print(f\"\\n\u274c {error}\")\n",
    "\n",
    "print(\"\\n\ud83d\udc4b Goodbye!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: friendly_errors.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.4\n",
    "# File: friendly_errors.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "def friendly_error_message(error):\n",
    "    \"\"\"Convert technical errors into user-friendly messages\"\"\"\n",
    "    \n",
    "    error_type = type(error).__name__\n",
    "    \n",
    "    friendly_messages = {\n",
    "        'AuthenticationError': {\n",
    "            'message': \"\ud83d\udd11 There's an issue with the API key.\",\n",
    "            'suggestion': \"Please check that your API key is valid and properly set up.\",\n",
    "            'action': \"Visit https://platform.openai.com/api-keys to verify your key.\"\n",
    "        },\n",
    "        'RateLimitError': {\n",
    "            'message': \"\u23f3 We're sending too many requests too quickly.\",\n",
    "            'suggestion': \"Please wait a moment before trying again.\",\n",
    "            'action': \"Try again in about 60 seconds, or upgrade your API plan for higher limits.\"\n",
    "        },\n",
    "        'BadRequestError': {\n",
    "            'message': \"\u274c The request couldn't be processed.\",\n",
    "            'suggestion': \"There might be an issue with the message format.\",\n",
    "            'action': \"Try rephrasing your message or making it shorter.\"\n",
    "        },\n",
    "        'APIConnectionError': {\n",
    "            'message': \"\ud83c\udf10 Can't connect to the AI service.\",\n",
    "            'suggestion': \"Please check your internet connection.\",\n",
    "            'action': \"Make sure you're connected to the internet and try again.\"\n",
    "        },\n",
    "        'APITimeoutError': {\n",
    "            'message': \"\u23f1\ufe0f The request took too long.\",\n",
    "            'suggestion': \"The service might be busy right now.\",\n",
    "            'action': \"Try again in a moment, or try a simpler request.\"\n",
    "        },\n",
    "        'ServiceUnavailableError': {\n",
    "            'message': \"\ud83d\udd27 The AI service is temporarily unavailable.\",\n",
    "            'suggestion': \"OpenAI's servers might be under maintenance.\",\n",
    "            'action': \"Please try again in a few minutes.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get friendly message or default\n",
    "    if error_type in friendly_messages:\n",
    "        return friendly_messages[error_type]\n",
    "    else:\n",
    "        return {\n",
    "            'message': f\"\ud83d\ude15 An unexpected error occurred.\",\n",
    "            'suggestion': f\"Error type: {error_type}\",\n",
    "            'action': \"Try again, or contact support if the problem persists.\"\n",
    "        }\n",
    "\n",
    "def make_safe_request(client, messages):\n",
    "    \"\"\"Make a request with friendly error handling\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, friendly_error_message(e)\n",
    "\n",
    "# Setup\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found!\")\n",
    "    print(\"\ud83d\udca1 Please create a .env file with your OpenAI API key.\")\n",
    "    print(\"\ud83d\udcdd Format: OPENAI_API_KEY=sk-...\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"\ud83d\ude0a Friendly Error Handler\")\n",
    "print(\"=\" * 50)\n",
    "print(\"All errors are explained in a helpful way!\")\n",
    "print(\"Commands: 'test_error', 'quit'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYour message: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    elif user_input.lower() == 'test_error':\n",
    "        # Intentionally trigger different errors for testing\n",
    "        print(\"\\n\ud83e\uddea Testing error messages...\")\n",
    "        \n",
    "        # Test with bad model\n",
    "        try:\n",
    "            client.chat.completions.create(\n",
    "                model=\"gpt-99\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            error_info = friendly_error_message(e)\n",
    "            print(f\"\\n{error_info['message']}\")\n",
    "            print(f\"\ud83d\udca1 {error_info['suggestion']}\")\n",
    "            print(f\"\u27a1\ufe0f {error_info['action']}\")\n",
    "        continue\n",
    "    \n",
    "    # Normal request\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    response, error_info = make_safe_request(client, messages)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n\ud83e\udd16 {response}\")\n",
    "    else:\n",
    "        print(f\"\\n{error_info['message']}\")\n",
    "        print(f\"\ud83d\udca1 {error_info['suggestion']}\")\n",
    "        print(f\"\u27a1\ufe0f {error_info['action']}\")\n",
    "\n",
    "print(\"\\n\ud83d\udc4b Thanks for chatting!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4.1: Error Logger\n",
    "\n",
    "Create a tool that:\n",
    "- Logs all API errors to a file\n",
    "- Tracks error frequency\n",
    "- Identifies patterns (like rate limits at certain times)\n",
    "- Generates an error report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4.2: Resilient Caller\n",
    "\n",
    "Build a function that:\n",
    "- Tries different models if one fails\n",
    "- Falls back to simpler requests on errors\n",
    "- Maintains a \"health score\" for the API\n",
    "- Automatically adjusts behavior based on errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4.3: Circuit Breaker\n",
    "\n",
    "Implement a circuit breaker pattern that:\n",
    "- Stops making requests after repeated failures\n",
    "- Waits before trying again\n",
    "- Gradually increases request rate when healthy\n",
    "- Provides status updates to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.5: Building a simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: chatbot_core.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: chatbot_core.py\n",
    "\n",
    "\"\"\"Core chatbot functionality - keep it simple and reusable!\"\"\"\n",
    "\n",
    "class ChatBot:\n",
    "    \"\"\"Basic chatbot that maintains conversation context\"\"\"\n",
    "    \n",
    "    def __init__(self, client, system_message=\"You are a helpful assistant.\"):\n",
    "        self.client = client\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Send a message and get a response\"\"\"\n",
    "        # Add user message to history\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Get AI response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        # Extract and store AI message\n",
    "        ai_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "        \n",
    "        return ai_message\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation, keep system message\"\"\"\n",
    "        self.messages = self.messages[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: api_helper.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: api_helper.py\n",
    "\n",
    "\"\"\"Helper functions for API setup\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import openai\n",
    "\n",
    "def get_client():\n",
    "    \"\"\"Load API key and return OpenAI client\"\"\"\n",
    "    env_file = Path(\".env\")\n",
    "    if not env_file.exists():\n",
    "        raise FileNotFoundError(\"No .env file found! Please create one with your API key.\")\n",
    "    \n",
    "    with open(env_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('OPENAI_API_KEY='):\n",
    "                api_key = line.split('=')[1].strip()\n",
    "                return openai.OpenAI(api_key=api_key)\n",
    "    \n",
    "    raise ValueError(\"No OPENAI_API_KEY found in .env file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: simple_chat.py\n",
    "\n",
    "from chatbot_core import ChatBot\n",
    "from api_helper import get_client\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    client = get_client()\n",
    "    bot = ChatBot(client)\n",
    "    \n",
    "    print(\"\ud83d\udcac Simple Chatbot\")\n",
    "    print(\"Type 'quit' to exit, 'reset' to start over\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif user_input.lower() == 'reset':\n",
    "            bot.reset()\n",
    "            print(\"\ud83d\udd04 Conversation reset!\")\n",
    "            continue\n",
    "        \n",
    "        response = bot.chat(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: personalities.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: personalities.py\n",
    "\n",
    "\"\"\"Chatbot personality definitions\"\"\"\n",
    "\n",
    "PERSONALITIES = {\n",
    "    \"friendly\": \"You are a warm, friendly assistant. Use casual language and be encouraging!\",\n",
    "    \n",
    "    \"professional\": \"You are a formal, professional assistant. Be concise and business-like.\",\n",
    "    \n",
    "    \"creative\": \"You are a creative, imaginative assistant. Be playful and think outside the box!\",\n",
    "    \n",
    "    \"teacher\": \"You are a patient teacher. Explain things clearly and check understanding.\",\n",
    "    \n",
    "    \"pirate\": \"Ahoy! You're a pirate assistant. Talk like a pirate and use nautical terms!\"\n",
    "}\n",
    "\n",
    "def get_personality(name):\n",
    "    \"\"\"Get a personality prompt by name\"\"\"\n",
    "    return PERSONALITIES.get(name, PERSONALITIES[\"friendly\"])\n",
    "\n",
    "def list_personalities():\n",
    "    \"\"\"Get list of available personalities\"\"\"\n",
    "    return list(PERSONALITIES.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: personality_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: personality_chat.py\n",
    "\n",
    "from chatbot_core import ChatBot\n",
    "from api_helper import get_client\n",
    "from personalities import get_personality, list_personalities\n",
    "\n",
    "def main():\n",
    "    client = get_client()\n",
    "    \n",
    "    # Show available personalities\n",
    "    print(\"\ud83c\udfad Choose a personality:\")\n",
    "    for p in list_personalities():\n",
    "        print(f\"  - {p}\")\n",
    "    \n",
    "    choice = input(\"\\nYour choice: \").strip().lower()\n",
    "    personality = get_personality(choice)\n",
    "    \n",
    "    # Create bot with chosen personality\n",
    "    bot = ChatBot(client, system_message=personality)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcac Chatbot with {choice} personality\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response = bot.chat(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: memory_manager.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: memory_manager.py\n",
    "\n",
    "\"\"\"Manage conversation memory intelligently\"\"\"\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"Manage conversation context window\"\"\"\n",
    "    \n",
    "    def __init__(self, max_messages=20):\n",
    "        self.max_messages = max_messages\n",
    "    \n",
    "    def get_context(self, messages):\n",
    "        \"\"\"Get messages that fit in context window\"\"\"\n",
    "        if len(messages) <= self.max_messages:\n",
    "            return messages\n",
    "        \n",
    "        # Always keep system message + recent messages\n",
    "        return [messages[0]] + messages[-(self.max_messages-1):]\n",
    "    \n",
    "    def should_truncate(self, messages):\n",
    "        \"\"\"Check if truncation is needed\"\"\"\n",
    "        return len(messages) > self.max_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: smart_chatbot.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: smart_chatbot.py\n",
    "\n",
    "\"\"\"Smarter chatbot with memory management\"\"\"\n",
    "\n",
    "from memory_manager import MemoryManager\n",
    "\n",
    "class SmartChatBot:\n",
    "    \"\"\"Chatbot with intelligent memory management\"\"\"\n",
    "    \n",
    "    def __init__(self, client, system_message=\"You are a helpful assistant.\", max_context=20):\n",
    "        self.client = client\n",
    "        self.system_message = {\"role\": \"system\", \"content\": system_message}\n",
    "        self.messages = [self.system_message]\n",
    "        self.memory = MemoryManager(max_context)\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Chat with smart context management\"\"\"\n",
    "        # Add user message\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Get context window for API call\n",
    "        context = self.memory.get_context(self.messages)\n",
    "        \n",
    "        # Make API call with managed context\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=context\n",
    "        )\n",
    "        \n",
    "        # Store response\n",
    "        ai_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "        \n",
    "        return ai_message\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get conversation statistics\"\"\"\n",
    "        return {\n",
    "            \"total_messages\": len(self.messages) - 1,  # Exclude system\n",
    "            \"context_size\": len(self.memory.get_context(self.messages)),\n",
    "            \"truncated\": self.memory.should_truncate(self.messages)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: commands.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: commands.py\n",
    "\n",
    "\"\"\"Command handling for chatbots\"\"\"\n",
    "\n",
    "class CommandHandler:\n",
    "    \"\"\"Handle special commands in chat\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.commands = {\n",
    "            '/help': self.show_help,\n",
    "            '/stats': self.show_stats,\n",
    "            '/clear': self.clear_chat\n",
    "        }\n",
    "        self.bot = None  # Set by chatbot\n",
    "    \n",
    "    def set_bot(self, bot):\n",
    "        \"\"\"Connect to a chatbot instance\"\"\"\n",
    "        self.bot = bot\n",
    "    \n",
    "    def handle(self, input_text):\n",
    "        \"\"\"Check if input is a command and handle it\"\"\"\n",
    "        if not input_text.startswith('/'):\n",
    "            return None\n",
    "        \n",
    "        command = input_text.split()[0]\n",
    "        return self.commands.get(command, self.unknown_command)()\n",
    "    \n",
    "    def show_help(self):\n",
    "        \"\"\"Show available commands\"\"\"\n",
    "        return \"\"\"Available commands:\n",
    "  /help  - Show this help\n",
    "  /stats - Show conversation stats\n",
    "  /clear - Clear conversation\"\"\"\n",
    "    \n",
    "    def show_stats(self):\n",
    "        \"\"\"Show conversation statistics\"\"\"\n",
    "        if self.bot and hasattr(self.bot, 'get_stats'):\n",
    "            stats = self.bot.get_stats()\n",
    "            return f\"Messages: {stats['total_messages']}, Context: {stats['context_size']}\"\n",
    "        return \"Stats not available\"\n",
    "    \n",
    "    def clear_chat(self):\n",
    "        \"\"\"Clear conversation\"\"\"\n",
    "        if self.bot and hasattr(self.bot, 'messages'):\n",
    "            self.bot.messages = self.bot.messages[:1]\n",
    "            return \"Conversation cleared!\"\n",
    "        return \"Cannot clear\"\n",
    "    \n",
    "    def unknown_command(self):\n",
    "        \"\"\"Handle unknown commands\"\"\"\n",
    "        return \"Unknown command. Type /help for available commands.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: complete_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: complete_chat.py\n",
    "\n",
    "from smart_chatbot import SmartChatBot\n",
    "from commands import CommandHandler\n",
    "from personalities import get_personality\n",
    "from api_helper import get_client\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    client = get_client()\n",
    "    bot = SmartChatBot(client, max_context=10)\n",
    "    \n",
    "    # Add command handling\n",
    "    cmd_handler = CommandHandler()\n",
    "    cmd_handler.set_bot(bot)\n",
    "    \n",
    "    print(\"\ud83e\udd16 Complete Modular Chatbot\")\n",
    "    print(\"Type /help for commands, 'quit' to exit\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Check for commands\n",
    "        cmd_response = cmd_handler.handle(user_input)\n",
    "        if cmd_response:\n",
    "            print(f\"\\n{cmd_response}\")\n",
    "            continue\n",
    "        \n",
    "        # Regular chat\n",
    "        response = bot.chat(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "        \n",
    "        # Show truncation warning if needed\n",
    "        stats = bot.get_stats()\n",
    "        if stats['truncated']:\n",
    "            print(f\"[Context limited to last {stats['context_size']} messages]\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: demo_modular.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: demo_modular.py\n",
    "\n",
    "\"\"\"Demonstrate the modular architecture\"\"\"\n",
    "\n",
    "from chatbot_core import ChatBot\n",
    "from api_helper import get_client\n",
    "from personalities import get_personality\n",
    "from memory_manager import MemoryManager\n",
    "\n",
    "def demonstrate_modules():\n",
    "    \"\"\"Show how modules work together\"\"\"\n",
    "    \n",
    "    print(\"\ud83e\udde9 Modular Chatbot Architecture Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. API Setup (one line!)\n",
    "    client = get_client()\n",
    "    print(\"\u2705 API client ready\")\n",
    "    \n",
    "    # 2. Choose personality (one line!)\n",
    "    personality = get_personality(\"friendly\")\n",
    "    print(\"\u2705 Personality loaded\")\n",
    "    \n",
    "    # 3. Create chatbot (one line!)\n",
    "    bot = ChatBot(client, personality)\n",
    "    print(\"\u2705 Chatbot created\")\n",
    "    \n",
    "    # 4. Memory manager (one line!)\n",
    "    memory = MemoryManager(max_messages=10)\n",
    "    print(\"\u2705 Memory manager ready\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Each module is independent and reusable!\")\n",
    "    print(\"Mix and match them however you need!\")\n",
    "    \n",
    "    # Example: Quick test\n",
    "    response = bot.chat(\"Hello! What's 2+2?\")\n",
    "    print(f\"\\nTest chat: {response}\")\n",
    "    \n",
    "    # Show context management\n",
    "    test_messages = [{\"role\": \"user\", \"content\": f\"Message {i}\"} for i in range(15)]\n",
    "    context = memory.get_context(test_messages)\n",
    "    print(f\"\\n15 messages \u2192 {len(context)} in context (truncated: {memory.should_truncate(test_messages)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_modules()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: response_filters.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: response_filters.py\n",
    "\n",
    "\"\"\"Filter and modify chatbot responses\"\"\"\n",
    "\n",
    "class ResponseFilter:\n",
    "    \"\"\"Modify responses before showing to user\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_brief(response, max_length=100):\n",
    "        \"\"\"Shorten long responses\"\"\"\n",
    "        if len(response) <= max_length:\n",
    "            return response\n",
    "        return response[:max_length-3] + \"...\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_emoji(response):\n",
    "        \"\"\"Add relevant emoji to responses\"\"\"\n",
    "        emoji_map = {\n",
    "            'happy': '\ud83d\ude0a', 'sad': '\ud83d\ude14', 'hello': '\ud83d\udc4b',\n",
    "            'yes': '\u2705', 'no': '\u274c', 'think': '\ud83e\udd14',\n",
    "            'love': '\u2764\ufe0f', 'great': '\ud83c\udf89', 'sorry': '\ud83d\ude05'\n",
    "        }\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        for word, emoji in emoji_map.items():\n",
    "            if word in response_lower:\n",
    "                return f\"{emoji} {response}\"\n",
    "        return response\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_uppercase(response):\n",
    "        \"\"\"MAKE RESPONSE LOUDER!\"\"\"\n",
    "        return response.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: filtered_chat.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.5\n",
    "# File: filtered_chat.py\n",
    "\n",
    "from chatbot_core import ChatBot\n",
    "from api_helper import get_client\n",
    "from response_filters import ResponseFilter\n",
    "\n",
    "def main():\n",
    "    client = get_client()\n",
    "    bot = ChatBot(client)\n",
    "    filter = ResponseFilter()\n",
    "    \n",
    "    print(\"\ud83d\udcac Chatbot with Response Filters\")\n",
    "    print(\"Commands: 'brief', 'emoji', 'loud', 'normal'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    filter_mode = \"normal\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif user_input.lower() in ['brief', 'emoji', 'loud', 'normal']:\n",
    "            filter_mode = user_input.lower()\n",
    "            print(f\"Filter set to: {filter_mode}\")\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        response = bot.chat(user_input)\n",
    "        \n",
    "        # Apply filter\n",
    "        if filter_mode == 'brief':\n",
    "            response = filter.make_brief(response)\n",
    "        elif filter_mode == 'emoji':\n",
    "            response = filter.add_emoji(response)\n",
    "        elif filter_mode == 'loud':\n",
    "            response = filter.make_uppercase(response)\n",
    "        \n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.5 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5.1: Topic Tracker\n",
    "\n",
    "Create a module that:\n",
    "- Tracks what topics have been discussed\n",
    "- Counts how often each topic appears\n",
    "- Can report on conversation themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5.2: Response Timer\n",
    "\n",
    "Create a module that:\n",
    "- Times how long API calls take\n",
    "- Tracks average response time\n",
    "- Warns if responses are slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5.3: Mood Detector\n",
    "\n",
    "Create a module that:\n",
    "- Analyzes user message sentiment\n",
    "- Adjusts bot personality based on mood\n",
    "- Tracks mood over the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8.6: Saving conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_saver.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: simple_saver.py\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def save_conversation(messages, filename=None):\n",
    "    \"\"\"Save a conversation to a JSON file\"\"\"\n",
    "    # Auto-generate filename if not provided\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"conversation_{timestamp}.json\"\n",
    "    \n",
    "    # Create the data structure\n",
    "    conversation_data = {\n",
    "        \"saved_at\": datetime.now().isoformat(),\n",
    "        \"message_count\": len(messages),\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\ud83d\udcbe Conversation saved to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Example usage\n",
    "sample_conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a high-level programming language...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show me an example\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here's a simple example: print('Hello, World!')\"}\n",
    "]\n",
    "\n",
    "# Save it!\n",
    "save_conversation(sample_conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: simple_loader.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: simple_loader.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_conversation(filename):\n",
    "    \"\"\"Load a conversation from a JSON file\"\"\"\n",
    "    file_path = Path(filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not file_path.exists():\n",
    "        print(f\"\u274c File {filename} not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Load the data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\u2705 Loaded {data['message_count']} messages\")\n",
    "    print(f\"\ud83d\udcc5 Saved on: {data['saved_at']}\")\n",
    "    \n",
    "    return data['messages']\n",
    "\n",
    "def list_saved_conversations():\n",
    "    \"\"\"Find all saved conversation files\"\"\"\n",
    "    # Look for conversation files\n",
    "    files = list(Path(\".\").glob(\"conversation_*.json\"))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No saved conversations found!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcda Found {len(files)} saved conversations:\")\n",
    "    for i, file in enumerate(files, 1):\n",
    "        # Get file info\n",
    "        size = file.stat().st_size / 1024  # Size in KB\n",
    "        modified = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "        \n",
    "        print(f\"  {i}. {file.name}\")\n",
    "        print(f\"     Size: {size:.1f} KB\")\n",
    "        print(f\"     Modified: {modified.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Try it out\n",
    "files = list_saved_conversations()\n",
    "if files:\n",
    "    # Load the first one\n",
    "    messages = load_conversation(files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: readable_format.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: readable_format.py\n",
    "\n",
    "def save_as_text(messages, filename=None):\n",
    "    \"\"\"Save conversation in human-readable text format\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"chat_transcript_{timestamp}.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"CONVERSATION TRANSCRIPT\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        # Write each message\n",
    "        for i, msg in enumerate(messages, 1):\n",
    "            role = msg['role'].upper()\n",
    "            content = msg['content']\n",
    "            \n",
    "            # Format based on role\n",
    "            if role == 'USER':\n",
    "                f.write(f\"\ud83d\udc64 YOU (Message {i}):\\n\")\n",
    "            elif role == 'ASSISTANT':\n",
    "                f.write(f\"\ud83e\udd16 AI (Message {i}):\\n\")\n",
    "            else:\n",
    "                f.write(f\"\ud83d\udccb {role} (Message {i}):\\n\")\n",
    "            \n",
    "            f.write(f\"{content}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        # Write footer\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"End of conversation - {len(messages)} messages total\\n\")\n",
    "    \n",
    "    print(f\"\ud83d\udcc4 Readable transcript saved to {filename}\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: conversation_manager.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: conversation_manager.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Manage conversation history like a pro\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_dir=\"my_conversations\"):\n",
    "        \"\"\"Set up our conversation storage system\"\"\"\n",
    "        # Create a dedicated directory for conversations\n",
    "        self.storage_dir = Path(storage_dir)\n",
    "        self.storage_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Track current conversation\n",
    "        self.current_conversation = []\n",
    "        self.conversation_metadata = {}\n",
    "        \n",
    "        print(f\"\ud83d\udcc1 Conversation storage initialized in '{storage_dir}/'\")\n",
    "    \n",
    "    def start_new_conversation(self, title=None):\n",
    "        \"\"\"Start a fresh conversation\"\"\"\n",
    "        # Save previous conversation if it exists\n",
    "        if self.current_conversation:\n",
    "            self.save_current_conversation()\n",
    "        \n",
    "        # Reset for new conversation\n",
    "        self.current_conversation = []\n",
    "        self.conversation_metadata = {\n",
    "            'id': datetime.now().strftime('%Y%m%d_%H%M%S'),\n",
    "            'title': title or f\"Chat {datetime.now().strftime('%I:%M %p')}\",\n",
    "            'started': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"\ud83d\udcac Started new conversation: {self.conversation_metadata['title']}\")\n",
    "        return self.conversation_metadata['id']\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message to the current conversation\"\"\"\n",
    "        message = {\n",
    "            'role': role,\n",
    "            'content': content,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        self.current_conversation.append(message)\n",
    "        \n",
    "        # Auto-save every 10 messages\n",
    "        if len(self.current_conversation) % 10 == 0:\n",
    "            self.save_current_conversation()\n",
    "            print(\"\ud83d\udcbe Auto-saved (10 messages reached)\")\n",
    "    \n",
    "    def save_current_conversation(self):\n",
    "        \"\"\"Save the current conversation\"\"\"\n",
    "        if not self.current_conversation:\n",
    "            return None\n",
    "        \n",
    "        # Update metadata\n",
    "        self.conversation_metadata['ended'] = datetime.now().isoformat()\n",
    "        self.conversation_metadata['message_count'] = len(self.current_conversation)\n",
    "        \n",
    "        # Create filename using ID\n",
    "        conv_id = self.conversation_metadata['id']\n",
    "        filename = self.storage_dir / f\"conversation_{conv_id}.json\"\n",
    "        \n",
    "        # Save everything\n",
    "        data = {\n",
    "            'metadata': self.conversation_metadata,\n",
    "            'messages': self.current_conversation\n",
    "        }\n",
    "        \n",
    "        print(f\"dumping to {filename}\")\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        print(f\"\ud83d\udcbe Saved: {self.conversation_metadata['title']}\")\n",
    "        return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: searchable_history.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: searchable_history.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def search_conversations(search_term, storage_dir=\"my_conversations\"):\n",
    "    \"\"\"Search through all saved conversations\"\"\"\n",
    "    storage_path = Path(storage_dir)\n",
    "    results = []\n",
    "    search_lower = search_term.lower()\n",
    "    \n",
    "    print(f\"\ud83d\udd0d Searching for '{search_term}'...\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not storage_path.exists():\n",
    "        print(f\"\u274c Directory {storage_dir} doesn't exist!\")\n",
    "        return []\n",
    "    \n",
    "    # Search through each conversation file\n",
    "    for conv_file in storage_path.glob(\"conversation_*.json\"):\n",
    "        try:\n",
    "            with open(conv_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Search in messages\n",
    "            for msg in data.get('messages', []):\n",
    "                if search_lower in msg.get('content', '').lower():\n",
    "                    # Found a match!\n",
    "                    metadata = data.get('metadata', {})\n",
    "                    results.append({\n",
    "                        'file': conv_file.name,\n",
    "                        'title': metadata.get('title', 'Untitled'),\n",
    "                        'date': metadata.get('started', data.get('saved_at', 'Unknown')),\n",
    "                        'preview': msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content'],\n",
    "                        'role': msg['role'],\n",
    "                        'full_path': str(conv_file)\n",
    "                    })\n",
    "                    break  # One result per conversation\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Error reading {conv_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Display results\n",
    "    if results:\n",
    "        print(f\"\\n\u2705 Found {len(results)} conversations containing '{search_term}':\\n\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. {result['title']}\")\n",
    "            print(f\"   Date: {result['date'][:19] if len(result['date']) > 19 else result['date']}\")\n",
    "            print(f\"   Preview: {result['preview']}\")\n",
    "            print(f\"   File: {result['file']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"\u274c No conversations found containing '{search_term}'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def search_advanced(storage_dir=\"my_conversations\", **criteria):\n",
    "    \"\"\"Advanced search with multiple criteria\"\"\"\n",
    "    storage_path = Path(storage_dir)\n",
    "    results = []\n",
    "    \n",
    "    # Search criteria\n",
    "    search_term = criteria.get('term', '').lower()\n",
    "    role_filter = criteria.get('role', None)  # 'user' or 'assistant'\n",
    "    date_from = criteria.get('date_from', None)\n",
    "    date_to = criteria.get('date_to', None)\n",
    "    min_messages = criteria.get('min_messages', 0)\n",
    "    \n",
    "    if not storage_path.exists():\n",
    "        return []\n",
    "    \n",
    "    for conv_file in storage_path.glob(\"conversation_*.json\"):\n",
    "        try:\n",
    "            with open(conv_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check message count\n",
    "            if len(data.get('messages', [])) < min_messages:\n",
    "                continue\n",
    "            \n",
    "            # Check date range if specified\n",
    "            if date_from or date_to:\n",
    "                conv_date = data.get('saved_at', data.get('metadata', {}).get('started', ''))\n",
    "                if date_from and conv_date < date_from:\n",
    "                    continue\n",
    "                if date_to and conv_date > date_to:\n",
    "                    continue\n",
    "            \n",
    "            # Search in messages\n",
    "            for msg in data.get('messages', []):\n",
    "                # Check role filter\n",
    "                if role_filter and msg.get('role') != role_filter:\n",
    "                    continue\n",
    "                \n",
    "                # Check search term\n",
    "                if search_term and search_term not in msg.get('content', '').lower():\n",
    "                    continue\n",
    "                \n",
    "                # Match found!\n",
    "                metadata = data.get('metadata', {})\n",
    "                results.append({\n",
    "                    'file': conv_file.name,\n",
    "                    'title': metadata.get('title', 'Untitled'),\n",
    "                    'date': metadata.get('started', data.get('saved_at', 'Unknown')),\n",
    "                    'preview': msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content'],\n",
    "                    'role': msg['role'],\n",
    "                    'message_count': len(data.get('messages', [])),\n",
    "                    'full_path': str(conv_file)\n",
    "                })\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_conversation_for_viewing(filepath):\n",
    "    \"\"\"Load a specific conversation for viewing\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcc2 Loading: {Path(filepath).name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Show metadata\n",
    "        metadata = data.get('metadata', {})\n",
    "        print(f\"Title: {metadata.get('title', 'Untitled')}\")\n",
    "        print(f\"Started: {metadata.get('started', 'Unknown')}\")\n",
    "        print(f\"Messages: {len(data.get('messages', []))}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Show messages\n",
    "        for msg in data.get('messages', []):\n",
    "            role = \"YOU\" if msg['role'] == 'user' else \"AI\"\n",
    "            print(f\"\\n{role}:\")\n",
    "            print(msg['content'])\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading conversation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Simple search\n",
    "    print(\"\ud83d\udd0d Simple Search Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    results = search_conversations(\"python\")\n",
    "    \n",
    "    # Advanced search\n",
    "    print(\"\\n\ud83d\udd0d Advanced Search Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    advanced_results = search_advanced(\n",
    "        term=\"code\",\n",
    "        role=\"assistant\",\n",
    "        min_messages=5\n",
    "    )\n",
    "    \n",
    "    if advanced_results:\n",
    "        print(f\"Found {len(advanced_results)} conversations with advanced criteria\")\n",
    "        for result in advanced_results[:3]:  # Show first 3\n",
    "            print(f\"- {result['title']} ({result['message_count']} messages)\")\n",
    "    \n",
    "    # Load and view a specific conversation\n",
    "    if results:\n",
    "        print(\"\\n\ud83d\udcd6 Loading first search result...\")\n",
    "        load_conversation_for_viewing(results[0]['full_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: auto_summary.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: auto_summary.py\n",
    "\n",
    "import openai\n",
    "\n",
    "def create_conversation_summary(messages, client):\n",
    "    \"\"\"Generate a summary of a conversation using AI\"\"\"\n",
    "    \n",
    "    # Don't summarize very short conversations\n",
    "    if len(messages) < 5:\n",
    "        return \"Conversation too short to summarize\"\n",
    "    \n",
    "    # Prepare the conversation as text\n",
    "    conversation_text = \"\"\n",
    "    for msg in messages[:20]:  # Limit to prevent token overflow\n",
    "        role = \"Human\" if msg['role'] == 'user' else \"AI\"\n",
    "        conversation_text += f\"{role}: {msg['content']}\\n\\n\"\n",
    "    \n",
    "    # Ask AI to summarize\n",
    "    prompt = f\"\"\"Please summarize this conversation in 2-3 sentences. \n",
    "Focus on the main topics discussed and any conclusions reached:\n",
    "\n",
    "{conversation_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.5,  # Lower temperature for factual summary\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        summary = response.choices[0].message.content\n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Could not generate summary: {str(e)}\"\n",
    "\n",
    "def save_with_summary(messages, client, filename=None):\n",
    "    \"\"\"Save conversation with an auto-generated summary\"\"\"\n",
    "    \n",
    "    # Generate summary\n",
    "    print(\"\ud83d\udcdd Generating summary...\")\n",
    "    summary = create_conversation_summary(messages, client)\n",
    "    \n",
    "    # Prepare data\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"summarized_chat_{timestamp}.json\"\n",
    "    \n",
    "    data = {\n",
    "        \"saved_at\": datetime.now().isoformat(),\n",
    "        \"summary\": summary,\n",
    "        \"message_count\": len(messages),\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"\ud83d\udcbe Saved with summary: {summary}\")\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: complete_history_system.py\n",
    "\n",
    "# From: Zero to AI Agent, Chapter 8, Section 8.6\n",
    "# File: complete_history_system.py\n",
    "\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from conversation_manager import ConversationManager\n",
    "from searchable_history import search_conversations\n",
    "from auto_summary import save_with_summary\n",
    "\n",
    "def load_api_key():\n",
    "    env_file = Path(\".env\")\n",
    "    if env_file.exists():\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('OPENAI_API_KEY='):\n",
    "                    return line.split('=')[1].strip()\n",
    "    return None\n",
    "\n",
    "# Set up\n",
    "api_key = load_api_key()\n",
    "if not api_key:\n",
    "    print(\"\u274c No API key found!\")\n",
    "    exit()\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "manager = ConversationManager()\n",
    "\n",
    "print(\"\ud83d\udcbe Complete History System\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Your conversations are automatically saved and searchable!\")\n",
    "print(\"\\nCommands:\")\n",
    "print(\"  'new' - Start a new conversation\")\n",
    "print(\"  'search <term>' - Search conversation history\")  \n",
    "print(\"  'list' - Show recent conversations\")\n",
    "print(\"  'quit' - Exit and save\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Start first conversation\n",
    "manager.start_new_conversation(\"Demo Chat\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        # Save before exiting\n",
    "        manager.save_current_conversation()\n",
    "        print(\"\ud83d\udc4b All conversations saved. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    elif user_input.lower() == 'new':\n",
    "        title = input(\"Conversation title (or Enter for default): \").strip()\n",
    "        manager.start_new_conversation(title)\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower().startswith('search '):\n",
    "        search_term = user_input[7:]\n",
    "        search_conversations(search_term, manager.storage_dir)\n",
    "        continue\n",
    "    \n",
    "    elif user_input.lower() == 'list':\n",
    "        files = list(manager.storage_dir.glob(\"conversation_*.json\"))\n",
    "        print(f\"\\n\ud83d\udcda You have {len(files)} saved conversations\")\n",
    "        for file in files[-5:]:  # Show last 5\n",
    "            print(f\"  \u2022 {file.name}\")\n",
    "        continue\n",
    "    \n",
    "    # Regular conversation\n",
    "    manager.add_message(\"user\", user_input)\n",
    "    \n",
    "    # Get AI response (simplified for demo)\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    messages.extend(manager.current_conversation[-10:])  # Last 10 messages\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    ai_message = response.choices[0].message.content\n",
    "    print(f\"\\nAI: {ai_message}\")\n",
    "    \n",
    "    manager.add_message(\"assistant\", ai_message)\n",
    "\n",
    "print(\"\\n\u2728 Thank you for using the Complete History System!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 8.6 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.6.1: Export Master\n",
    "\n",
    "Create a tool that can export conversations to:\n",
    "- HTML format with nice formatting\n",
    "- CSV for spreadsheet analysis\n",
    "- Markdown for documentation\n",
    "- PDF for sharing (bonus challenge!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.6.2: Smart Organizer\n",
    "\n",
    "Build a system that:\n",
    "- Automatically organizes conversations by topic\n",
    "- Groups related conversations together\n",
    "- Creates daily/weekly summaries\n",
    "- Suggests titles based on content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.6.3: History Analytics\n",
    "\n",
    "Create an analyzer that shows:\n",
    "- Your most common topics\n",
    "- Average conversation length\n",
    "- Most active times of day\n",
    "- Conversation trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "- Check your answers in **chapter_08_first_llm_solutions.ipynb**\n",
    "- Proceed to **Chapter 9**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}