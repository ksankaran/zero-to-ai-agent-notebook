{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Introduction to AI and Large Language Models - Solutions\n",
    "**From: Zero to AI Agent**\n",
    "\n",
    "**Try the exercises in the main notebook first before viewing solutions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.1 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.1: AI or Not AI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_1_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: AI or Not AI?\n",
    "Determine if each system uses AI or traditional programming.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_systems():\n",
    "    \"\"\"\n",
    "    Analyzes 10 systems to determine if they use AI or traditional programming.\n",
    "    Key principle: If it adapts based on data or improves over time, it's probably AI!\n",
    "    \"\"\"\n",
    "    \n",
    "    systems = [\n",
    "        {\n",
    "            \"number\": 1,\n",
    "            \"name\": \"A calculator app that adds numbers\",\n",
    "            \"answer\": \"Traditional Programming\",\n",
    "            \"icon\": \"\u2699\ufe0f\",\n",
    "            \"reason\": \"Follows fixed mathematical rules, no learning involved\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 2,\n",
    "            \"name\": \"Google Photos finding all pictures of your dog\",\n",
    "            \"answer\": \"AI\",\n",
    "            \"icon\": \"\ud83e\udd16\",\n",
    "            \"reason\": \"Learned to recognize dogs (and specific dogs!) from millions of images\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 3,\n",
    "            \"name\": \"A website login that checks if password matches\",\n",
    "            \"answer\": \"Traditional Programming\",\n",
    "            \"icon\": \"\u2699\ufe0f\",\n",
    "            \"reason\": \"Simple comparison: does input equal stored password?\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 4,\n",
    "            \"name\": \"Spotify creating your 'Discover Weekly' playlist\",\n",
    "            \"answer\": \"AI\",\n",
    "            \"icon\": \"\ud83e\udd16\",\n",
    "            \"reason\": \"Learns your music taste from listening patterns\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 5,\n",
    "            \"name\": \"An alarm clock that rings at 7 AM\",\n",
    "            \"answer\": \"Traditional Programming\",\n",
    "            \"icon\": \"\u2699\ufe0f\",\n",
    "            \"reason\": \"Fixed rule: if time = 7:00 AM, then ring\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 6,\n",
    "            \"name\": \"Your phone's face unlock\",\n",
    "            \"answer\": \"AI\",\n",
    "            \"icon\": \"\ud83e\udd16\",\n",
    "            \"reason\": \"Learned to recognize faces, adapts to changes (glasses, beard, etc.)\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 7,\n",
    "            \"name\": \"A thermostat that turns on at 70\u00b0F\",\n",
    "            \"answer\": \"Traditional Programming\",\n",
    "            \"icon\": \"\u2699\ufe0f\",\n",
    "            \"reason\": \"Simple rule: if temperature < 70\u00b0F, turn on heat\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 8,\n",
    "            \"name\": \"Gmail's spam filter\",\n",
    "            \"answer\": \"AI\",\n",
    "            \"icon\": \"\ud83e\udd16\",\n",
    "            \"reason\": \"Learned patterns of spam from millions of emails\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 9,\n",
    "            \"name\": \"A video game where enemies always patrol the same path\",\n",
    "            \"answer\": \"Traditional Programming\",\n",
    "            \"icon\": \"\u2699\ufe0f\",\n",
    "            \"reason\": \"Pre-programmed movement patterns\"\n",
    "        },\n",
    "        {\n",
    "            \"number\": 10,\n",
    "            \"name\": \"YouTube's recommendation algorithm\",\n",
    "            \"answer\": \"AI\",\n",
    "            \"icon\": \"\ud83e\udd16\",\n",
    "            \"reason\": \"Learns what you like based on viewing history and similar users\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return systems\n",
    "\n",
    "\n",
    "def print_results():\n",
    "    \"\"\"Displays the analysis results in a clear format.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXERCISE 1 SOLUTION: AI or Traditional Programming?\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    systems = analyze_systems()\n",
    "    \n",
    "    # Count totals\n",
    "    ai_count = sum(1 for s in systems if s[\"answer\"] == \"AI\")\n",
    "    traditional_count = len(systems) - ai_count\n",
    "    \n",
    "    # Display each system\n",
    "    for system in systems:\n",
    "        print(f\"\\n{system['number']}. {system['name']}\")\n",
    "        print(f\"   Answer: {system['icon']} {system['answer']}\")\n",
    "        print(f\"   Why: {system['reason']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"SUMMARY: {ai_count} AI systems, {traditional_count} Traditional Programming systems\")\n",
    "    print(\"\\n\ud83d\udca1 Quick Rule: If it adapts based on data or improves over time, it's probably AI!\")\n",
    "    \n",
    "    # Additional insights\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY PATTERNS TO REMEMBER:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n\u2705 Signs of AI:\")\n",
    "    print(\"  \u2022 Learns from examples\")\n",
    "    print(\"  \u2022 Improves over time\")\n",
    "    print(\"  \u2022 Recognizes patterns\")\n",
    "    print(\"  \u2022 Makes predictions\")\n",
    "    print(\"  \u2022 Adapts to user behavior\")\n",
    "    \n",
    "    print(\"\\n\u2699\ufe0f Signs of Traditional Programming:\")\n",
    "    print(\"  \u2022 Follows fixed rules\")\n",
    "    print(\"  \u2022 Same output for same input\")\n",
    "    print(\"  \u2022 No learning involved\")\n",
    "    print(\"  \u2022 Simple if-then logic\")\n",
    "    print(\"  \u2022 Doesn't improve with use\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.2: Categorizing AI Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_1_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Categorizing AI Types\n",
    "Match AI applications to their learning type:\n",
    "- Supervised Learning: Learns from labeled examples\n",
    "- Unsupervised Learning: Finds patterns without labels\n",
    "- Reinforcement Learning: Learns through trial and error with rewards\n",
    "\"\"\"\n",
    "\n",
    "def categorize_applications():\n",
    "    \"\"\"\n",
    "    Categorizes AI applications by their learning type.\n",
    "    Returns a list of applications with their classifications.\n",
    "    \"\"\"\n",
    "    \n",
    "    applications = [\n",
    "        {\n",
    "            \"id\": \"A\",\n",
    "            \"description\": \"An email filter trained on examples of spam and not-spam emails\",\n",
    "            \"type\": \"Supervised Learning\",\n",
    "            \"reason\": \"Has labeled examples (spam or not spam) to learn from\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"B\",\n",
    "            \"description\": \"A system that groups customers by shopping behavior without predefined categories\",\n",
    "            \"type\": \"Unsupervised Learning\",\n",
    "            \"reason\": \"Finds natural groupings without being told what groups should exist\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"C\",\n",
    "            \"description\": \"A robot learning to walk by trying different movements and getting points for distance traveled\",\n",
    "            \"type\": \"Reinforcement Learning\",\n",
    "            \"reason\": \"Gets rewards (distance) for good attempts, learns from success/failure\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"D\",\n",
    "            \"description\": \"A model that predicts house prices from past sales data with known prices\",\n",
    "            \"type\": \"Supervised Learning\",\n",
    "            \"reason\": \"Learns from examples with known prices (labels)\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"E\",\n",
    "            \"description\": \"An AI finding hidden patterns in genetic data without knowing what diseases to look for\",\n",
    "            \"type\": \"Unsupervised Learning\",\n",
    "            \"reason\": \"Discovers patterns without predefined disease categories\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"F\",\n",
    "            \"description\": \"A game-playing AI that improves by winning/losing thousands of games\",\n",
    "            \"type\": \"Reinforcement Learning\",\n",
    "            \"reason\": \"Learns from rewards (winning) and penalties (losing)\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"G\",\n",
    "            \"description\": \"A photo app that learned to identify faces after seeing millions of labeled face images\",\n",
    "            \"type\": \"Supervised Learning\",\n",
    "            \"reason\": \"Trained on millions of images labeled as 'face' or 'not face'\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return applications\n",
    "\n",
    "\n",
    "def display_by_category():\n",
    "    \"\"\"Displays applications grouped by learning type.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 2 SOLUTION: Types of Machine Learning\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    applications = categorize_applications()\n",
    "    \n",
    "    # Group by type\n",
    "    supervised = [app for app in applications if app[\"type\"] == \"Supervised Learning\"]\n",
    "    unsupervised = [app for app in applications if app[\"type\"] == \"Unsupervised Learning\"]\n",
    "    reinforcement = [app for app in applications if app[\"type\"] == \"Reinforcement Learning\"]\n",
    "    \n",
    "    # Display Supervised Learning\n",
    "    print(\"\\n\ud83d\udcda SUPERVISED LEARNING (Learning with a teacher)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Pattern: Has labeled training data with correct answers\")\n",
    "    print(\"\\nApplications:\")\n",
    "    for app in supervised:\n",
    "        print(f\"\\n{app['id']}. {app['description']}\")\n",
    "        print(f\"   Why: {app['reason']}\")\n",
    "    \n",
    "    # Display Unsupervised Learning\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\udd0d UNSUPERVISED LEARNING (Finding patterns alone)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Pattern: No labels, discovers hidden structure in data\")\n",
    "    print(\"\\nApplications:\")\n",
    "    for app in unsupervised:\n",
    "        print(f\"\\n{app['id']}. {app['description']}\")\n",
    "        print(f\"   Why: {app['reason']}\")\n",
    "    \n",
    "    # Display Reinforcement Learning\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83c\udfae REINFORCEMENT LEARNING (Learning by doing)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Pattern: Learns through trial and error with rewards/penalties\")\n",
    "    print(\"\\nApplications:\")\n",
    "    for app in reinforcement:\n",
    "        print(f\"\\n{app['id']}. {app['description']}\")\n",
    "        print(f\"   Why: {app['reason']}\")\n",
    "    \n",
    "    # Summary and quick reference\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUICK REFERENCE GUIDE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n\ud83d\udca1 How to identify each type:\")\n",
    "    print(\"\\n1. SUPERVISED LEARNING:\")\n",
    "    print(\"   \u2022 Has training data with correct answers/labels\")\n",
    "    print(\"   \u2022 Examples: spam/not-spam, cat/dog, price predictions\")\n",
    "    print(\"   \u2022 Think: Learning with a teacher showing right answers\")\n",
    "    \n",
    "    print(\"\\n2. UNSUPERVISED LEARNING:\")\n",
    "    print(\"   \u2022 No labels, just raw data\")\n",
    "    print(\"   \u2022 Finds hidden patterns or groups\")\n",
    "    print(\"   \u2022 Examples: customer segmentation, anomaly detection\")\n",
    "    print(\"   \u2022 Think: Explorer finding patterns on their own\")\n",
    "    \n",
    "    print(\"\\n3. REINFORCEMENT LEARNING:\")\n",
    "    print(\"   \u2022 Learns by trying actions and getting feedback\")\n",
    "    print(\"   \u2022 Rewards for good actions, penalties for bad\")\n",
    "    print(\"   \u2022 Examples: game AI, robotics, trading bots\")\n",
    "    print(\"   \u2022 Think: Learning to ride a bike through practice\")\n",
    "\n",
    "\n",
    "def show_answer_key():\n",
    "    \"\"\"Shows a simple answer key format.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANSWER KEY (Quick Reference)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    applications = categorize_applications()\n",
    "    \n",
    "    print(\"\\nSupervised Learning: \", end=\"\")\n",
    "    supervised_ids = [app['id'] for app in applications if app['type'] == \"Supervised Learning\"]\n",
    "    print(\", \".join(supervised_ids))\n",
    "    \n",
    "    print(\"Unsupervised Learning: \", end=\"\")\n",
    "    unsupervised_ids = [app['id'] for app in applications if app['type'] == \"Unsupervised Learning\"]\n",
    "    print(\", \".join(unsupervised_ids))\n",
    "    \n",
    "    print(\"Reinforcement Learning: \", end=\"\")\n",
    "    reinforcement_ids = [app['id'] for app in applications if app['type'] == \"Reinforcement Learning\"]\n",
    "    print(\", \".join(reinforcement_ids))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_by_category()\n",
    "    show_answer_key()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1.3: Design Your Own AI Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_7_1_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Design Your Own AI Application\n",
    "Example AI application design to inspire your thinking.\n",
    "\"\"\"\n",
    "\n",
    "def create_example_application():\n",
    "    \"\"\"\n",
    "    Creates a detailed example of an AI application design.\n",
    "    This shows one possible solution to inspire creative thinking.\n",
    "    \"\"\"\n",
    "    \n",
    "    ai_application = {\n",
    "        \"name\": \"Smart Fridge Chef\",\n",
    "        \n",
    "        \"problem\": \"\"\"I never know what to cook with the random ingredients in my fridge, \n",
    "and food often goes to waste. I also struggle to maintain a balanced diet and \n",
    "often resort to ordering takeout when I can't think of what to make.\"\"\",\n",
    "        \n",
    "        \"ai_type\": \"Supervised Learning\",\n",
    "        \n",
    "        \"ai_type_reason\": \"\"\"The system would learn from thousands of labeled recipes \n",
    "(ingredients \u2192 dish name + instructions). Each recipe is labeled with ingredients, \n",
    "cooking time, difficulty level, nutritional info, and user ratings. This is \n",
    "classic supervised learning - learning from examples with known outputs.\"\"\",\n",
    "        \n",
    "        \"data_needed\": [\n",
    "            \"Thousands of recipes with complete ingredient lists\",\n",
    "            \"User ratings and reviews for each recipe\",\n",
    "            \"Ingredient substitution database (e.g., buttermilk \u2192 milk + lemon)\",\n",
    "            \"Nutritional information for all ingredients\",\n",
    "            \"Typical shelf life and storage info for ingredients\",\n",
    "            \"Cooking technique videos/instructions\",\n",
    "            \"Dietary restriction mappings (vegan, gluten-free, etc.)\",\n",
    "            \"Seasonal ingredient availability data\",\n",
    "            \"Price data for missing ingredients\"\n",
    "        ],\n",
    "        \n",
    "        \"inputs\": [\n",
    "            \"Photo of fridge contents (computer vision to identify ingredients)\",\n",
    "            \"Manual ingredient list entry option\",\n",
    "            \"Dietary restrictions and allergies\",\n",
    "            \"Available cooking time\",\n",
    "            \"Number of servings needed\",\n",
    "            \"Cooking skill level (beginner/intermediate/expert)\",\n",
    "            \"Kitchen equipment available\",\n",
    "            \"Preferred cuisine types\",\n",
    "            \"Nutritional goals (high protein, low carb, etc.)\"\n",
    "        ],\n",
    "        \n",
    "        \"outputs\": [\n",
    "            \"Top 5 recipe recommendations ranked by match percentage\",\n",
    "            \"Shopping list for missing minor ingredients (with cost estimate)\",\n",
    "            \"Step-by-step cooking instructions with timers\",\n",
    "            \"Nutritional breakdown of each recipe\",\n",
    "            \"Estimated prep and cooking time\",\n",
    "            \"Difficulty rating for each recipe\",\n",
    "            \"Video tutorials for complex techniques\",\n",
    "            \"Meal prep suggestions for leftovers\",\n",
    "            \"Wine/beverage pairing suggestions\"\n",
    "        ],\n",
    "        \n",
    "        \"why_ai\": \"\"\"Traditional programming would require impossibly complex rules \n",
    "for every ingredient combination. You'd need millions of if-then statements like \n",
    "'if has_tomatoes and has_basil and has_mozzarella then suggest_caprese'. \n",
    "\n",
    "AI excels here because it can:\n",
    "1. Learn subtle patterns (ingredients that pair well together)\n",
    "2. Handle partial matches (missing one ingredient? Find alternatives)\n",
    "3. Personalize over time (learn YOUR preferences)\n",
    "4. Discover non-obvious combinations (fusion cuisine)\n",
    "5. Consider multiple factors simultaneously (nutrition + taste + time + skill)\n",
    "6. Improve with user feedback (ratings make it better)\"\"\"\n",
    "    }\n",
    "    \n",
    "    return ai_application\n",
    "\n",
    "\n",
    "def display_application_design():\n",
    "    \"\"\"Displays the AI application design in a structured format.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EXERCISE 3 SOLUTION: AI Application Design Example\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    app = create_example_application()\n",
    "    \n",
    "    print(f\"\\n\ud83c\udf73 APPLICATION NAME: {app['name']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccc PROBLEM IT SOLVES:\")\n",
    "    print(app['problem'])\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca TYPE OF AI: {app['ai_type']}\")\n",
    "    print(f\"\\nWhy this type?\")\n",
    "    print(app['ai_type_reason'])\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc1 DATA IT WOULD NEED TO LEARN FROM:\")\n",
    "    for i, data in enumerate(app['data_needed'], 1):\n",
    "        print(f\"  {i}. {data}\")\n",
    "    \n",
    "    print(\"\\n\u27a1\ufe0f INPUTS (What users provide):\")\n",
    "    for i, input_item in enumerate(app['inputs'], 1):\n",
    "        print(f\"  {i}. {input_item}\")\n",
    "    \n",
    "    print(\"\\n\u2b05\ufe0f OUTPUTS (What the AI produces):\")\n",
    "    for i, output in enumerate(app['outputs'], 1):\n",
    "        print(f\"  {i}. {output}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 WHY AI INSTEAD OF TRADITIONAL PROGRAMMING?\")\n",
    "    print(app['why_ai'])\n",
    "\n",
    "\n",
    "def provide_additional_ideas():\n",
    "    \"\"\"Provides more AI application ideas to inspire creativity.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MORE AI APPLICATION IDEAS TO INSPIRE YOU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ideas = [\n",
    "        {\n",
    "            \"name\": \"Personal Energy Coach\",\n",
    "            \"problem\": \"Feeling tired at random times, poor sleep quality\",\n",
    "            \"ai_approach\": \"Track activities, sleep, food \u2192 predict energy levels\",\n",
    "            \"type\": \"Supervised Learning (energy levels from lifestyle data)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Plant Health Monitor\",\n",
    "            \"problem\": \"House plants dying despite best efforts\",\n",
    "            \"ai_approach\": \"Image analysis + environment data \u2192 care instructions\",\n",
    "            \"type\": \"Supervised Learning (plant images \u2192 health diagnosis)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Wardrobe Optimizer\",\n",
    "            \"problem\": \"Never know what to wear, clothes go unworn\",\n",
    "            \"ai_approach\": \"Weather + calendar + style preferences \u2192 outfit suggestions\",\n",
    "            \"type\": \"Supervised + Reinforcement (learns your actual choices)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Study Buddy AI\",\n",
    "            \"problem\": \"Inefficient studying, forgetting material\",\n",
    "            \"ai_approach\": \"Track study patterns \u2192 optimize review schedule\",\n",
    "            \"type\": \"Reinforcement Learning (rewards for retention)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Mood Predictor\",\n",
    "            \"problem\": \"Unexpected mood swings affecting productivity\",\n",
    "            \"ai_approach\": \"Daily patterns + activities \u2192 mood forecast\",\n",
    "            \"type\": \"Unsupervised (finding patterns) + Supervised (prediction)\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for idea in ideas:\n",
    "        print(f\"\\n\ud83d\udca1 {idea['name']}\")\n",
    "        print(f\"   Problem: {idea['problem']}\")\n",
    "        print(f\"   Approach: {idea['ai_approach']}\")\n",
    "        print(f\"   AI Type: {idea['type']}\")\n",
    "\n",
    "\n",
    "def evaluation_criteria():\n",
    "    \"\"\"Shows what makes a good AI application idea.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WHAT MAKES A GOOD AI APPLICATION?\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n\u2705 Good candidates for AI have:\")\n",
    "    print(\"  \u2022 Patterns to learn (not just rules to follow)\")\n",
    "    print(\"  \u2022 Lots of available training data\")\n",
    "    print(\"  \u2022 Clear success metrics (you can measure if it's working)\")\n",
    "    print(\"  \u2022 Complexity that makes rules impractical\")\n",
    "    print(\"  \u2022 Opportunity for personalization\")\n",
    "    print(\"  \u2022 Room for improvement over time\")\n",
    "    \n",
    "    print(\"\\n\u274c Poor candidates for AI:\")\n",
    "    print(\"  \u2022 Simple calculations (use traditional programming)\")\n",
    "    print(\"  \u2022 Tasks requiring 100% accuracy (AI can make mistakes)\")\n",
    "    print(\"  \u2022 Problems with clear, simple rules\")\n",
    "    print(\"  \u2022 Situations with no historical data\")\n",
    "    print(\"  \u2022 Tasks requiring human judgment/ethics\")\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf Remember: AI is best for pattern recognition, prediction, and personalization!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_application_design()\n",
    "    provide_additional_ideas()\n",
    "    evaluation_criteria()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YOUR TURN!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNow design YOUR OWN AI application using this template:\")\n",
    "    print(\"1. What problem does it solve?\")\n",
    "    print(\"2. What type of AI would it use?\")\n",
    "    print(\"3. What data would it need?\")\n",
    "    print(\"4. What inputs/outputs would it have?\")\n",
    "    print(\"5. Why is AI better than traditional programming for this?\")\n",
    "    print(\"\\nBe creative! The best AI ideas solve real problems you face daily.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.1: Token Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_2_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: Token Estimation\n",
    "Understanding token counting for different types of text.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str, method: str = \"average\") -> int:\n",
    "    \"\"\"\n",
    "    Estimate token count for text using different methods.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to estimate tokens for\n",
    "        method: Estimation method ('chars', 'words', or 'average')\n",
    "    \n",
    "    Returns:\n",
    "        Estimated token count\n",
    "    \"\"\"\n",
    "    if method == \"chars\":\n",
    "        # Method 1: ~4 characters per token\n",
    "        return math.ceil(len(text) / 4)\n",
    "    elif method == \"words\":\n",
    "        # Method 2: ~3/4 words per token (or 4/3 tokens per word)\n",
    "        word_count = len(text.split())\n",
    "        return math.ceil(word_count * 4 / 3)\n",
    "    else:  # average\n",
    "        # Use average of both methods for better accuracy\n",
    "        char_estimate = len(text) / 4\n",
    "        word_estimate = len(text.split()) * 4 / 3\n",
    "        return math.ceil((char_estimate + word_estimate) / 2)\n",
    "\n",
    "\n",
    "def analyze_tokenization_examples():\n",
    "    \"\"\"Analyze token counts for various text examples.\"\"\"\n",
    "    \n",
    "    examples = [\n",
    "        (\"Hello, world!\", \"Simple greeting\"),\n",
    "        (\"The quick brown fox jumps over the lazy dog.\", \"Pangram\"),\n",
    "        (\"def calculate_sum(a, b): return a + b\", \"Python function\"),\n",
    "        (\"https://www.example.com/path/to/page\", \"URL\"),\n",
    "        (\"user@example.com\", \"Email address\"),\n",
    "        (\"The year 2024 marks an important milestone.\", \"Text with numbers\"),\n",
    "        (\"\ud83c\udf89 Emojis are fun! \ud83d\ude80\", \"Text with emojis\"),\n",
    "        (\"import numpy as np\\nimport pandas as pd\", \"Python imports\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TOKEN ESTIMATION EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    for text, description in examples:\n",
    "        char_tokens = estimate_tokens(text, \"chars\")\n",
    "        word_tokens = estimate_tokens(text, \"words\")\n",
    "        avg_tokens = estimate_tokens(text, \"average\")\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        print(f\"Length: {len(text)} chars, {len(text.split())} words\")\n",
    "        print(f\"Token estimates:\")\n",
    "        print(f\"  By characters: ~{char_tokens} tokens\")\n",
    "        print(f\"  By words: ~{word_tokens} tokens\")\n",
    "        print(f\"  Average: ~{avg_tokens} tokens\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Exercise specific examples\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 1 SOLUTIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    exercise_texts = [\n",
    "        (\"Hello, world!\", 3, 4),\n",
    "        (\"The quick brown fox jumps over the lazy dog.\", 10, 11),\n",
    "        (\" \".join([\"word\"] * 200), 267, 270),  # 200-word email simulation\n",
    "        (\"def calculate_sum(a, b): return a + b\", 12, 15),\n",
    "    ]\n",
    "    \n",
    "    for text, min_expected, max_expected in exercise_texts:\n",
    "        if len(text) > 50:\n",
    "            display_text = f\"{text[:47]}...\"\n",
    "        else:\n",
    "            display_text = text\n",
    "            \n",
    "        estimated = estimate_tokens(text)\n",
    "        \n",
    "        print(f\"\\nText: '{display_text}'\")\n",
    "        print(f\"Estimated tokens: {estimated}\")\n",
    "        print(f\"Expected range: {min_expected}-{max_expected} tokens\")\n",
    "        \n",
    "        if min_expected <= estimated <= max_expected:\n",
    "            print(\"\u2705 Estimate within expected range!\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f Estimate outside expected range\")\n",
    "    \n",
    "    # Special case: Estimate for entire section\n",
    "    section_words = 2800  # Approximate word count of section\n",
    "    section_tokens = math.ceil(section_words * 4 / 3)\n",
    "    print(f\"\\nEntire section (~{section_words} words): ~{section_tokens} tokens\")\n",
    "\n",
    "\n",
    "def demonstrate_tokenization_patterns():\n",
    "    \"\"\"Show common tokenization patterns.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMMON TOKENIZATION PATTERNS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    patterns = {\n",
    "        \"Single words\": [\n",
    "            (\"cat\", 1),\n",
    "            (\"running\", 1),\n",
    "            (\"extraordinary\", 2),  # Longer uncommon words may split\n",
    "        ],\n",
    "        \"Numbers\": [\n",
    "            (\"42\", 1),\n",
    "            (\"3.14159\", 2),  # Decimals often split\n",
    "            (\"1,000,000\", 3),  # Formatted numbers split more\n",
    "        ],\n",
    "        \"Punctuation\": [\n",
    "            (\"Hello!\", 2),  # Word + punctuation\n",
    "            (\"Hi, there!\", 4),  # Each punctuation typically separate\n",
    "        ],\n",
    "        \"Code\": [\n",
    "            (\"print()\", 3),  # Function calls\n",
    "            (\"if x > 5:\", 6),  # Conditionals\n",
    "            (\"list_comp = [x**2 for x in range(10)]\", 15),  # Complex code\n",
    "        ],\n",
    "        \"Special characters\": [\n",
    "            (\"hello@world\", 3),\n",
    "            (\"user_name\", 2),\n",
    "            (\"kebab-case\", 3),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, examples in patterns.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for text, typical_tokens in examples:\n",
    "            estimated = estimate_tokens(text)\n",
    "            print(f\"  '{text}' \u2192 typically {typical_tokens} tokens (estimated: {estimated})\")\n",
    "\n",
    "\n",
    "def token_cost_calculator(token_count: int, model: str = \"gpt-3.5-turbo\") -> float:\n",
    "    \"\"\"\n",
    "    Calculate API cost based on token count.\n",
    "    \n",
    "    Args:\n",
    "        token_count: Number of tokens\n",
    "        model: Model name for pricing\n",
    "    \n",
    "    Returns:\n",
    "        Estimated cost in dollars\n",
    "    \"\"\"\n",
    "    # Simplified pricing (as of 2024, check current prices)\n",
    "    pricing = {\n",
    "        \"gpt-3.5-turbo\": 0.002,  # $0.002 per 1K tokens\n",
    "        \"gpt-4\": 0.06,  # $0.06 per 1K tokens\n",
    "        \"claude-3-haiku\": 0.0015,  # $0.0015 per 1K tokens\n",
    "    }\n",
    "    \n",
    "    price_per_1k = pricing.get(model, 0.002)\n",
    "    cost = (token_count / 1000) * price_per_1k\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all token estimation demonstrations.\"\"\"\n",
    "    \n",
    "    # Basic token analysis\n",
    "    analyze_tokenization_examples()\n",
    "    \n",
    "    # Show patterns\n",
    "    demonstrate_tokenization_patterns()\n",
    "    \n",
    "    # Cost calculation example\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOKEN COST EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    conversation_tokens = 1000\n",
    "    for model in [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-3-haiku\"]:\n",
    "        cost = token_cost_calculator(conversation_tokens, model)\n",
    "        print(f\"{model}: {conversation_tokens} tokens = ${cost:.4f}\")\n",
    "    \n",
    "    # Practical tip\n",
    "    print(\"\\n\ud83d\udca1 PRACTICAL TIPS:\")\n",
    "    print(\"\u2022 Use token counting to estimate costs BEFORE making API calls\")\n",
    "    print(\"\u2022 Different models may tokenize the same text differently\")\n",
    "    print(\"\u2022 Code and URLs often use more tokens than expected\")\n",
    "    print(\"\u2022 Consider token limits when designing prompts\")\n",
    "    print(\"\u2022 Reserve tokens for the response when calculating input limits\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.2: Choosing the Right Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_2_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Choosing the Right Parameters\n",
    "Understanding when to use different temperature settings for various tasks.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParameterProfile:\n",
    "    \"\"\"Profile for LLM parameters for different use cases.\"\"\"\n",
    "    temperature: Tuple[float, float]  # (min, max) range\n",
    "    max_tokens: int\n",
    "    top_p: float\n",
    "    frequency_penalty: float\n",
    "    presence_penalty: float\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "def get_parameter_profiles() -> Dict[str, ParameterProfile]:\n",
    "    \"\"\"\n",
    "    Get recommended parameter profiles for different scenarios.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of scenario -> ParameterProfile\n",
    "    \"\"\"\n",
    "    \n",
    "    profiles = {\n",
    "        \"legal_contract\": ParameterProfile(\n",
    "            temperature=(0.0, 0.2),\n",
    "            max_tokens=2000,\n",
    "            top_p=0.1,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            reasoning=(\n",
    "                \"Legal language requires precision and consistency. \"\n",
    "                \"Low temperature ensures deterministic, formal output. \"\n",
    "                \"Low top_p further restricts to most likely (standard) phrases.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"creative_story\": ParameterProfile(\n",
    "            temperature=(0.9, 1.2),\n",
    "            max_tokens=1500,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0.5,\n",
    "            presence_penalty=0.5,\n",
    "            reasoning=(\n",
    "                \"Creative writing benefits from high randomness. \"\n",
    "                \"High temperature enables unexpected word choices. \"\n",
    "                \"Penalties reduce repetition and encourage variety.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"technical_translation\": ParameterProfile(\n",
    "            temperature=(0.1, 0.3),\n",
    "            max_tokens=3000,\n",
    "            top_p=0.3,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            reasoning=(\n",
    "                \"Technical translation needs accuracy over creativity. \"\n",
    "                \"Low temperature maintains terminology consistency. \"\n",
    "                \"No penalties to allow technical term repetition.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"product_descriptions\": ParameterProfile(\n",
    "            temperature=(0.7, 0.8),\n",
    "            max_tokens=500,\n",
    "            top_p=0.9,\n",
    "            frequency_penalty=0.3,\n",
    "            presence_penalty=0.2,\n",
    "            reasoning=(\n",
    "                \"Product descriptions need variety but remain coherent. \"\n",
    "                \"Moderate temperature balances creativity with clarity. \"\n",
    "                \"Light penalties avoid repetitive phrases.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"code_generation\": ParameterProfile(\n",
    "            temperature=(0.0, 0.2),\n",
    "            max_tokens=1000,\n",
    "            top_p=0.1,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            reasoning=(\n",
    "                \"Code must be syntactically correct and functional. \"\n",
    "                \"Near-zero temperature ensures valid syntax. \"\n",
    "                \"Low top_p sticks to common coding patterns.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"brainstorming\": ParameterProfile(\n",
    "            temperature=(0.8, 1.0),\n",
    "            max_tokens=800,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0.7,\n",
    "            presence_penalty=0.7,\n",
    "            reasoning=(\n",
    "                \"Brainstorming needs maximum diversity of ideas. \"\n",
    "                \"High temperature encourages creative combinations. \"\n",
    "                \"High penalties push for unique, varied suggestions.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"customer_service\": ParameterProfile(\n",
    "            temperature=(0.3, 0.5),\n",
    "            max_tokens=300,\n",
    "            top_p=0.7,\n",
    "            frequency_penalty=0.2,\n",
    "            presence_penalty=0.1,\n",
    "            reasoning=(\n",
    "                \"Customer service needs consistent, helpful tone. \"\n",
    "                \"Low-moderate temperature keeps responses professional. \"\n",
    "                \"Some variation prevents robotic feel.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"data_extraction\": ParameterProfile(\n",
    "            temperature=(0.0, 0.1),\n",
    "            max_tokens=500,\n",
    "            top_p=0.1,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            reasoning=(\n",
    "                \"Data extraction requires maximum accuracy. \"\n",
    "                \"Near-zero temperature prevents hallucination. \"\n",
    "                \"Deterministic output for consistent parsing.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"educational_content\": ParameterProfile(\n",
    "            temperature=(0.4, 0.6),\n",
    "            max_tokens=1500,\n",
    "            top_p=0.8,\n",
    "            frequency_penalty=0.2,\n",
    "            presence_penalty=0.1,\n",
    "            reasoning=(\n",
    "                \"Educational content needs clarity with engagement. \"\n",
    "                \"Moderate temperature keeps explanations interesting. \"\n",
    "                \"Light penalties maintain term consistency.\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"poetry_generation\": ParameterProfile(\n",
    "            temperature=(0.9, 1.3),\n",
    "            max_tokens=500,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0.6,\n",
    "            presence_penalty=0.8,\n",
    "            reasoning=(\n",
    "                \"Poetry thrives on unexpected word combinations. \"\n",
    "                \"High temperature enables creative expression. \"\n",
    "                \"Strong penalties encourage unique imagery.\"\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "\n",
    "def analyze_exercise_scenarios():\n",
    "    \"\"\"Analyze the specific scenarios from Exercise 2.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 2 SOLUTIONS: CHOOSING THE RIGHT PARAMETERS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    exercise_scenarios = [\n",
    "        (\"A. Writing legal contract language\", \"legal_contract\"),\n",
    "        (\"B. Generating creative story ideas\", \"creative_story\"),\n",
    "        (\"C. Translating technical documentation\", \"technical_translation\"),\n",
    "        (\"D. Writing varied product descriptions\", \"product_descriptions\"),\n",
    "        (\"E. Solving coding problems\", \"code_generation\"),\n",
    "        (\"F. Brainstorming business names\", \"brainstorming\"),\n",
    "    ]\n",
    "    \n",
    "    profiles = get_parameter_profiles()\n",
    "    \n",
    "    for scenario_name, profile_key in exercise_scenarios:\n",
    "        profile = profiles[profile_key]\n",
    "        \n",
    "        print(f\"\\n{scenario_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"\ud83c\udf21\ufe0f Temperature: {profile.temperature[0]:.1f} - {profile.temperature[1]:.1f}\")\n",
    "        print(f\"\ud83d\udcca Reasoning: {profile.reasoning}\")\n",
    "        \n",
    "        # Additional recommendations\n",
    "        if profile.temperature[1] < 0.3:\n",
    "            print(\"\u26a1 Type: DETERMINISTIC - Prioritizes accuracy and consistency\")\n",
    "        elif profile.temperature[1] < 0.7:\n",
    "            print(\"\u2696\ufe0f Type: BALANCED - Mix of consistency and variety\")\n",
    "        else:\n",
    "            print(\"\ud83c\udfa8 Type: CREATIVE - Prioritizes novelty and diversity\")\n",
    "\n",
    "\n",
    "def demonstrate_temperature_effects():\n",
    "    \"\"\"Show how temperature affects output.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEMPERATURE EFFECTS DEMONSTRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    prompt = \"Complete this sentence: The future of AI is\"\n",
    "    \n",
    "    temperature_examples = [\n",
    "        (0.0, [\n",
    "            \"The future of AI is bright and full of potential.\",\n",
    "            \"The future of AI is bright and full of potential.\",  # Same!\n",
    "            \"The future of AI is bright and full of potential.\",  # Same!\n",
    "        ]),\n",
    "        (0.5, [\n",
    "            \"The future of AI is promising and transformative.\",\n",
    "            \"The future of AI is full of exciting possibilities.\",\n",
    "            \"The future of AI is both exciting and challenging.\",\n",
    "        ]),\n",
    "        (1.0, [\n",
    "            \"The future of AI is like a kaleidoscope of possibilities.\",\n",
    "            \"The future of AI is unwritten, waiting for bold innovators.\",\n",
    "            \"The future of AI is a symphony yet to be composed.\",\n",
    "        ]),\n",
    "        (1.5, [\n",
    "            \"The future of AI is pizza-shaped with quantum sprinkles.\",\n",
    "            \"The future of AI is dancing on moonbeams of silicon dreams.\",\n",
    "            \"The future of AI is yesterday's tomorrow singing backwards.\",\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    \n",
    "    for temp, outputs in temperature_examples:\n",
    "        print(f\"\\n\ud83c\udf21\ufe0f Temperature: {temp}\")\n",
    "        print(\"Possible outputs:\")\n",
    "        for i, output in enumerate(outputs, 1):\n",
    "            print(f\"  {i}. {output}\")\n",
    "        \n",
    "        if temp == 0.0:\n",
    "            print(\"  Note: Deterministic - same output every time\")\n",
    "        elif temp < 0.5:\n",
    "            print(\"  Note: Conservative - safe, predictable choices\")\n",
    "        elif temp < 1.0:\n",
    "            print(\"  Note: Balanced - good variety while coherent\")\n",
    "        else:\n",
    "            print(\"  Note: Wild - may produce nonsense\")\n",
    "\n",
    "\n",
    "def parameter_decision_tree():\n",
    "    \"\"\"Interactive parameter selection guide.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PARAMETER SELECTION DECISION TREE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    questions = [\n",
    "        (\"Does the output need to be factually accurate?\", {\n",
    "            \"yes\": (\"temp\", 0.0, 0.3),\n",
    "            \"no\": (\"next\", 1)\n",
    "        }),\n",
    "        (\"Is creativity more important than consistency?\", {\n",
    "            \"yes\": (\"temp\", 0.8, 1.2),\n",
    "            \"no\": (\"next\", 2)\n",
    "        }),\n",
    "        (\"Will users see multiple outputs (need variety)?\", {\n",
    "            \"yes\": (\"temp\", 0.5, 0.8),\n",
    "            \"no\": (\"temp\", 0.2, 0.5)\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nQuick Temperature Selection Guide:\")\n",
    "    print(\"\\n1. Does the output need to be factually accurate?\")\n",
    "    print(\"   YES \u2192 Use 0.0-0.3 (data extraction, translation, code)\")\n",
    "    print(\"   NO  \u2192 Continue to question 2\")\n",
    "    \n",
    "    print(\"\\n2. Is creativity more important than consistency?\")\n",
    "    print(\"   YES \u2192 Use 0.8-1.2 (stories, brainstorming, poetry)\")\n",
    "    print(\"   NO  \u2192 Continue to question 3\")\n",
    "    \n",
    "    print(\"\\n3. Will users see multiple outputs (need variety)?\")\n",
    "    print(\"   YES \u2192 Use 0.5-0.8 (product descriptions, content generation)\")\n",
    "    print(\"   NO  \u2192 Use 0.2-0.5 (customer service, explanations)\")\n",
    "\n",
    "\n",
    "def best_practices():\n",
    "    \"\"\"Show best practices for parameter selection.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BEST PRACTICES FOR PARAMETER SELECTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    practices = [\n",
    "        (\"Start Conservative\", \n",
    "         \"Begin with lower temperature and increase if needed\"),\n",
    "        \n",
    "        (\"Test Systematically\", \n",
    "         \"Try 3-5 different temperatures with same prompt\"),\n",
    "        \n",
    "        (\"Consider Your Audience\", \n",
    "         \"B2B \u2192 lower temp, B2C \u2192 moderate temp, Creative \u2192 higher temp\"),\n",
    "        \n",
    "        (\"Use Temperature with Top-p\", \n",
    "         \"Temperature=0.8 + top_p=0.9 often better than temp=1.0 alone\"),\n",
    "        \n",
    "        (\"Adjust for Model Size\", \n",
    "         \"Larger models handle higher temperatures better\"),\n",
    "        \n",
    "        (\"Document Your Choices\", \n",
    "         \"Record which parameters work for which use cases\"),\n",
    "        \n",
    "        (\"Monitor Over Time\", \n",
    "         \"Model updates may require parameter adjustments\"),\n",
    "    ]\n",
    "    \n",
    "    for i, (practice, description) in enumerate(practices, 1):\n",
    "        print(f\"\\n{i}. {practice}\")\n",
    "        print(f\"   {description}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\ud83d\udca1 GOLDEN RULE: There's no perfect temperature!\")\n",
    "    print(\"   Test with your specific use case and adjust based on results.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all parameter selection demonstrations.\"\"\"\n",
    "    \n",
    "    # Exercise solutions\n",
    "    analyze_exercise_scenarios()\n",
    "    \n",
    "    # Show temperature effects\n",
    "    demonstrate_temperature_effects()\n",
    "    \n",
    "    # Decision guide\n",
    "    parameter_decision_tree()\n",
    "    \n",
    "    # Best practices\n",
    "    best_practices()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2.3: Identifying Good vs Bad LLM Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_7_2_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Identifying Good vs Bad LLM Tasks\n",
    "Understanding what LLMs excel at versus what they struggle with.\n",
    "\"\"\"\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class TaskSuitability(Enum):\n",
    "    \"\"\"Categories for LLM task suitability.\"\"\"\n",
    "    GREAT = \"\u2705 Great for LLMs\"\n",
    "    OKAY = \"\u26a0\ufe0f Okay with Caveats\"\n",
    "    BAD = \"\u274c Bad Idea\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskAnalysis:\n",
    "    \"\"\"Analysis of a task's suitability for LLMs.\"\"\"\n",
    "    task: str\n",
    "    suitability: TaskSuitability\n",
    "    reasoning: str\n",
    "    alternatives: str = \"\"\n",
    "    best_practices: str = \"\"\n",
    "\n",
    "\n",
    "def analyze_llm_tasks() -> Dict[str, TaskAnalysis]:\n",
    "    \"\"\"\n",
    "    Analyze various tasks for LLM suitability.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of task -> TaskAnalysis\n",
    "    \"\"\"\n",
    "    \n",
    "    tasks = {\n",
    "        \"blog_draft\": TaskAnalysis(\n",
    "            task=\"Writing a first draft of a blog post\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs excel at generating coherent, structured text. \"\n",
    "                \"They can create outlines, expand ideas, and maintain consistent tone.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Provide clear topic and target audience\\n\"\n",
    "                \"\u2022 Review and fact-check output\\n\"\n",
    "                \"\u2022 Add personal insights and experiences\\n\"\n",
    "                \"\u2022 Verify any statistics or claims\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"compound_interest\": TaskAnalysis(\n",
    "            task=\"Calculating compound interest over 30 years\",\n",
    "            suitability=TaskSuitability.BAD,\n",
    "            reasoning=(\n",
    "                \"LLMs can't perform precise mathematical calculations reliably. \"\n",
    "                \"They approximate based on training patterns, leading to errors.\"\n",
    "            ),\n",
    "            alternatives=(\n",
    "                \"\u2022 Use a calculator or spreadsheet\\n\"\n",
    "                \"\u2022 Write or use a simple Python function\\n\"\n",
    "                \"\u2022 Use financial calculator tools\\n\"\n",
    "                \"\u2022 Ask LLM to write the formula, then calculate yourself\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"email_professional\": TaskAnalysis(\n",
    "            task=\"Checking if an email sounds professional\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs understand tone, formality, and professional communication. \"\n",
    "                \"They can identify casual language, suggest improvements, and ensure clarity.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Provide context about recipient and purpose\\n\"\n",
    "                \"\u2022 Ask for specific feedback (tone, clarity, structure)\\n\"\n",
    "                \"\u2022 Consider industry-specific conventions\\n\"\n",
    "                \"\u2022 Review suggestions critically\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"stock_prices\": TaskAnalysis(\n",
    "            task=\"Getting today's stock prices\",\n",
    "            suitability=TaskSuitability.BAD,\n",
    "            reasoning=(\n",
    "                \"LLMs don't have real-time data access. \"\n",
    "                \"Training data has a cutoff date, so current information is unavailable.\"\n",
    "            ),\n",
    "            alternatives=(\n",
    "                \"\u2022 Use financial APIs (Yahoo Finance, Alpha Vantage)\\n\"\n",
    "                \"\u2022 Check financial websites directly\\n\"\n",
    "                \"\u2022 Use specialized trading platforms\\n\"\n",
    "                \"\u2022 Some LLMs with web access can search for current data\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"explain_concept\": TaskAnalysis(\n",
    "            task=\"Explaining a complex concept simply\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs excel at adapting explanations for different audiences. \"\n",
    "                \"They can use analogies, break down components, and adjust complexity.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Specify the audience level (child, beginner, expert)\\n\"\n",
    "                \"\u2022 Ask for analogies or examples\\n\"\n",
    "                \"\u2022 Request step-by-step breakdowns\\n\"\n",
    "                \"\u2022 Verify technical accuracy for specialized topics\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"test_data\": TaskAnalysis(\n",
    "            task=\"Generating test data for your application\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs can generate realistic, varied test data quickly. \"\n",
    "                \"They understand data patterns and can create edge cases.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Provide clear schema/format requirements\\n\"\n",
    "                \"\u2022 Specify constraints and validation rules\\n\"\n",
    "                \"\u2022 Request both typical and edge cases\\n\"\n",
    "                \"\u2022 Generate in batches to ensure variety\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"medical_diagnosis\": TaskAnalysis(\n",
    "            task=\"Making medical diagnoses\",\n",
    "            suitability=TaskSuitability.BAD,\n",
    "            reasoning=(\n",
    "                \"Medical diagnosis requires professional expertise and liability. \"\n",
    "                \"LLMs can hallucinate symptoms or conditions. Lives are at stake.\"\n",
    "            ),\n",
    "            alternatives=(\n",
    "                \"\u2022 Consult qualified medical professionals\\n\"\n",
    "                \"\u2022 Use for educational understanding only\\n\"\n",
    "                \"\u2022 Can help prepare questions for doctors\\n\"\n",
    "                \"\u2022 Never replace professional medical advice\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"document_summary\": TaskAnalysis(\n",
    "            task=\"Summarizing a long document\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs excel at identifying key points and condensing information. \"\n",
    "                \"They maintain context and can adjust summary length/detail.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Specify desired summary length\\n\"\n",
    "                \"\u2022 Indicate key aspects to focus on\\n\"\n",
    "                \"\u2022 Check that critical points aren't omitted\\n\"\n",
    "                \"\u2022 Consider chunking very long documents\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"password_security\": TaskAnalysis(\n",
    "            task=\"Checking if a password is secure\",\n",
    "            suitability=TaskSuitability.BAD,\n",
    "            reasoning=(\n",
    "                \"Never send passwords to LLMs - security risk! \"\n",
    "                \"Also, password strength requires algorithmic checking, not language analysis.\"\n",
    "            ),\n",
    "            alternatives=(\n",
    "                \"\u2022 Use dedicated password strength libraries\\n\"\n",
    "                \"\u2022 Implement client-side validation\\n\"\n",
    "                \"\u2022 Use services like Have I Been Pwned API\\n\"\n",
    "                \"\u2022 Never transmit actual passwords to third parties\"\n",
    "            )\n",
    "        ),\n",
    "        \n",
    "        \"shakespeare_poetry\": TaskAnalysis(\n",
    "            task=\"Writing poetry in Shakespeare's style\",\n",
    "            suitability=TaskSuitability.GREAT,\n",
    "            reasoning=(\n",
    "                \"LLMs are excellent at mimicking writing styles. \"\n",
    "                \"They understand meter, rhyme schemes, and archaic language patterns.\"\n",
    "            ),\n",
    "            best_practices=(\n",
    "                \"\u2022 Specify form (sonnet, soliloquy, etc.)\\n\"\n",
    "                \"\u2022 Request specific themes or subjects\\n\"\n",
    "                \"\u2022 Can combine with modern topics for humor\\n\"\n",
    "                \"\u2022 Review for authentic feel and rhythm\"\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "\n",
    "def categorize_exercise_tasks():\n",
    "    \"\"\"Solve Exercise 3: Categorize the specific tasks.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 3 SOLUTION: IDENTIFYING GOOD VS BAD LLM TASKS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Map exercise numbers to task keys\n",
    "    exercise_mapping = [\n",
    "        (1, \"blog_draft\"),\n",
    "        (2, \"compound_interest\"),\n",
    "        (3, \"email_professional\"),\n",
    "        (4, \"stock_prices\"),\n",
    "        (5, \"explain_concept\"),\n",
    "        (6, \"test_data\"),\n",
    "        (7, \"medical_diagnosis\"),\n",
    "        (8, \"document_summary\"),\n",
    "        (9, \"password_security\"),\n",
    "        (10, \"shakespeare_poetry\")\n",
    "    ]\n",
    "    \n",
    "    tasks = analyze_llm_tasks()\n",
    "    \n",
    "    # Group by suitability\n",
    "    great_tasks = []\n",
    "    okay_tasks = []\n",
    "    bad_tasks = []\n",
    "    \n",
    "    for num, task_key in exercise_mapping:\n",
    "        task = tasks[task_key]\n",
    "        if task.suitability == TaskSuitability.GREAT:\n",
    "            great_tasks.append((num, task))\n",
    "        elif task.suitability == TaskSuitability.OKAY:\n",
    "            okay_tasks.append((num, task))\n",
    "        else:\n",
    "            bad_tasks.append((num, task))\n",
    "    \n",
    "    # Display categorized results\n",
    "    print(\"\\n\u2705 GREAT FOR LLMs:\")\n",
    "    print(\"-\" * 50)\n",
    "    for num, task in great_tasks:\n",
    "        print(f\"{num}. {task.task}\")\n",
    "        print(f\"   Why: {task.reasoning[:100]}...\")\n",
    "    \n",
    "    print(\"\\n\u26a0\ufe0f OKAY WITH CAVEATS:\")\n",
    "    print(\"-\" * 50)\n",
    "    if okay_tasks:\n",
    "        for num, task in okay_tasks:\n",
    "            print(f\"{num}. {task.task}\")\n",
    "            print(f\"   Caveat: {task.reasoning[:100]}...\")\n",
    "    else:\n",
    "        print(\"(None in this exercise)\")\n",
    "    \n",
    "    print(\"\\n\u274c BAD IDEA:\")\n",
    "    print(\"-\" * 50)\n",
    "    for num, task in bad_tasks:\n",
    "        print(f\"{num}. {task.task}\")\n",
    "        print(f\"   Why: {task.reasoning[:100]}...\")\n",
    "\n",
    "\n",
    "def detailed_task_analysis():\n",
    "    \"\"\"Provide detailed analysis for each task.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED TASK ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    tasks = analyze_llm_tasks()\n",
    "    \n",
    "    # Sort tasks by suitability\n",
    "    sorted_tasks = sorted(tasks.values(), \n",
    "                         key=lambda x: (x.suitability.value, x.task))\n",
    "    \n",
    "    current_suitability = None\n",
    "    \n",
    "    for task in sorted_tasks:\n",
    "        if task.suitability != current_suitability:\n",
    "            print(f\"\\n{task.suitability.value}\")\n",
    "            print(\"=\" * 50)\n",
    "            current_suitability = task.suitability\n",
    "        \n",
    "        print(f\"\\n\ud83d\udccb Task: {task.task}\")\n",
    "        print(f\"\ud83d\udcad Reasoning: {task.reasoning}\")\n",
    "        \n",
    "        if task.best_practices:\n",
    "            print(f\"\u2728 Best Practices:\\n{task.best_practices}\")\n",
    "        \n",
    "        if task.alternatives:\n",
    "            print(f\"\ud83d\udd04 Alternatives:\\n{task.alternatives}\")\n",
    "\n",
    "\n",
    "def task_selection_framework():\n",
    "    \"\"\"Framework for deciding if a task is suitable for LLMs.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LLM TASK SUITABILITY FRAMEWORK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf GREAT FOR LLMs - All these conditions:\")\n",
    "    print(\"\u2022 Involves natural language processing\")\n",
    "    print(\"\u2022 Doesn't require real-time data\")\n",
    "    print(\"\u2022 Benefits from pattern recognition\")\n",
    "    print(\"\u2022 Allows for some variation in output\")\n",
    "    print(\"\u2022 Not life-critical or high-stakes\")\n",
    "    \n",
    "    print(\"\\n\u26a0\ufe0f USE WITH CAUTION - Any of these:\")\n",
    "    print(\"\u2022 Requires some factual accuracy\")\n",
    "    print(\"\u2022 Involves subjective judgment\")\n",
    "    print(\"\u2022 Needs cultural/contextual awareness\")\n",
    "    print(\"\u2022 Has legal/ethical implications\")\n",
    "    print(\"\u2022 Output needs human review\")\n",
    "    \n",
    "    print(\"\\n\u274c AVOID FOR LLMs - Any of these:\")\n",
    "    print(\"\u2022 Requires precise calculations\")\n",
    "    print(\"\u2022 Needs real-time/current data\")\n",
    "    print(\"\u2022 Involves personal/sensitive data\")\n",
    "    print(\"\u2022 Has safety/health implications\")\n",
    "    print(\"\u2022 Requires guaranteed accuracy\")\n",
    "    print(\"\u2022 Needs deterministic output\")\n",
    "\n",
    "\n",
    "def practical_examples():\n",
    "    \"\"\"Show practical examples of good task design.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PRACTICAL EXAMPLES: TURNING BAD TASKS INTO GOOD ONES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    transformations = [\n",
    "        {\n",
    "            \"bad\": \"Calculate my tax return\",\n",
    "            \"good\": \"Explain tax deductions I might be eligible for\",\n",
    "            \"why\": \"Shifted from calculation to explanation\"\n",
    "        },\n",
    "        {\n",
    "            \"bad\": \"Diagnose my symptoms\",\n",
    "            \"good\": \"Help me describe symptoms clearly for my doctor\",\n",
    "            \"why\": \"Shifted from diagnosis to communication aid\"\n",
    "        },\n",
    "        {\n",
    "            \"bad\": \"Tell me tomorrow's weather\",\n",
    "            \"good\": \"Explain how weather patterns work\",\n",
    "            \"why\": \"Shifted from real-time data to general knowledge\"\n",
    "        },\n",
    "        {\n",
    "            \"bad\": \"Is this contract legally binding?\",\n",
    "            \"good\": \"What are common elements in contracts I should review?\",\n",
    "            \"why\": \"Shifted from legal advice to educational information\"\n",
    "        },\n",
    "        {\n",
    "            \"bad\": \"Debug my production code\",\n",
    "            \"good\": \"Suggest debugging strategies for this type of error\",\n",
    "            \"why\": \"Shifted from specific fix to general approach\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, transform in enumerate(transformations, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"\u274c Bad: {transform['bad']}\")\n",
    "        print(f\"\u2705 Good: {transform['good']}\")\n",
    "        print(f\"\ud83d\udca1 Why: {transform['why']}\")\n",
    "\n",
    "\n",
    "def key_insights():\n",
    "    \"\"\"Summarize key insights about LLM task suitability.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    insights = [\n",
    "        (\"Language Tasks\", \"LLMs excel at anything involving natural language\"),\n",
    "        (\"Pattern Recognition\", \"Great at finding and applying patterns from training\"),\n",
    "        (\"Creative Generation\", \"Excellent for creative and varied content\"),\n",
    "        (\"Style Mimicry\", \"Can adopt any writing style or tone\"),\n",
    "        (\"Knowledge Synthesis\", \"Good at combining information in new ways\"),\n",
    "        \n",
    "        (\"No Real-Time Data\", \"Cannot access current information\"),\n",
    "        (\"No True Calculation\", \"Approximate math, not compute\"),\n",
    "        (\"No Critical Decisions\", \"Never use for life-impacting choices\"),\n",
    "        (\"No Personal Data\", \"Don't send sensitive information\"),\n",
    "        (\"No Guaranteed Facts\", \"Always verify important information\")\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\u2705 LLMs ARE GREAT AT:\")\n",
    "    for title, desc in insights[:5]:\n",
    "        print(f\"\u2022 {title}: {desc}\")\n",
    "    \n",
    "    print(\"\\n\u274c LLMs CANNOT/SHOULD NOT:\")\n",
    "    for title, desc in insights[5:]:\n",
    "        print(f\"\u2022 {title}: {desc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\ud83c\udfaf REMEMBER: LLMs are powerful tools for language tasks,\")\n",
    "    print(\"   not magic oracles for all problems!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all task suitability demonstrations.\"\"\"\n",
    "    \n",
    "    # Exercise solution\n",
    "    categorize_exercise_tasks()\n",
    "    \n",
    "    # Detailed analysis\n",
    "    detailed_task_analysis()\n",
    "    \n",
    "    # Framework\n",
    "    task_selection_framework()\n",
    "    \n",
    "    # Practical examples\n",
    "    practical_examples()\n",
    "    \n",
    "    # Key insights\n",
    "    key_insights()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.3 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.1: Trace the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_3_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: Trace the Flow\n",
    "Understanding how LLMs process text from input to output.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def trace_prompt_flow(prompt: str = \"The weather today is\"):\n",
    "    \"\"\"\n",
    "    Trace how an LLM processes a prompt through each stage.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The input prompt to trace\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 1: TRACING LLM PROMPT FLOW\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Tokenization\n",
    "    trace_tokenization(prompt)\n",
    "    \n",
    "    # Step 2: Pattern Recognition\n",
    "    trace_pattern_activation(prompt)\n",
    "    \n",
    "    # Step 3: Likely Completions\n",
    "    trace_completions(prompt)\n",
    "    \n",
    "    # Step 4: Missing Information\n",
    "    trace_missing_info(prompt)\n",
    "    \n",
    "    # Step 5: Complete Flow Visualization\n",
    "    visualize_complete_flow(prompt)\n",
    "\n",
    "\n",
    "def trace_tokenization(prompt: str):\n",
    "    \"\"\"Trace the tokenization process.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcdd STEP 1: TOKENIZATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simulated tokenization (actual varies by model)\n",
    "    tokens = [\"The\", \" weather\", \" today\", \" is\"]\n",
    "    token_ids = [464, 1969, 1651, 318]  # Example token IDs\n",
    "    \n",
    "    print(\"Text \u2192 Tokens \u2192 Numbers:\")\n",
    "    print(f\"'{prompt}'\")\n",
    "    print(\"    \u2193\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"    \u2193\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 Key Points:\")\n",
    "    print(\"\u2022 Each token represents a piece of text\")\n",
    "    print(\"\u2022 Common words are single tokens\")\n",
    "    print(\"\u2022 Spaces often included in tokens\")\n",
    "    print(\"\u2022 Model has ~50,000 possible tokens\")\n",
    "    \n",
    "    # Show position encoding\n",
    "    print(\"\\n\ud83d\udccd Position Encoding Added:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"  Position {i}: '{token}' + position_embedding[{i}]\")\n",
    "\n",
    "\n",
    "def trace_pattern_activation(prompt: str):\n",
    "    \"\"\"Trace which patterns might activate.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83e\udde0 STEP 2: PATTERN ACTIVATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    patterns = {\n",
    "        \"Weather Discussion\": {\n",
    "            \"strength\": 0.95,\n",
    "            \"triggers\": [\"weather\", \"today\"],\n",
    "            \"associations\": [\"sunny\", \"cloudy\", \"rainy\", \"temperature\", \"forecast\"]\n",
    "        },\n",
    "        \"Time Reference\": {\n",
    "            \"strength\": 0.7,\n",
    "            \"triggers\": [\"today\"],\n",
    "            \"associations\": [\"current\", \"now\", \"this morning\", \"this afternoon\"]\n",
    "        },\n",
    "        \"Incomplete Sentence\": {\n",
    "            \"strength\": 0.9,\n",
    "            \"triggers\": [\"is\" + \"incomplete\"],\n",
    "            \"associations\": [\"continuation needed\", \"descriptive phrase expected\"]\n",
    "        },\n",
    "        \"News/Report Style\": {\n",
    "            \"strength\": 0.6,\n",
    "            \"triggers\": [\"The\", \"weather\"],\n",
    "            \"associations\": [\"formal tone\", \"informative\", \"descriptive\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Patterns Recognized (via Attention Mechanism):\")\n",
    "    \n",
    "    for pattern_name, pattern_info in patterns.items():\n",
    "        print(f\"\\n\ud83c\udfaf {pattern_name}\")\n",
    "        print(f\"   Activation Strength: {pattern_info['strength']:.1%}\")\n",
    "        print(f\"   Triggered by: {', '.join(pattern_info['triggers'])}\")\n",
    "        print(f\"   Associates with: {', '.join(pattern_info['associations'][:3])}...\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 Attention Scores (simplified):\")\n",
    "    attention_matrix = [\n",
    "        [\"The\", \"weather\", \"today\", \"is\"],\n",
    "        [0.1, 0.3, 0.2, 0.1],  # \"The\" attends to...\n",
    "        [0.2, 0.8, 0.4, 0.3],  # \"weather\" attends to...\n",
    "        [0.1, 0.5, 0.7, 0.2],  # \"today\" attends to...\n",
    "        [0.3, 0.6, 0.5, 0.9],  # \"is\" attends to...\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nWord relationships (attention weights):\")\n",
    "    for i, word in enumerate(attention_matrix[0]):\n",
    "        print(f\"  '{word}' strongly attends to:\", end=\" \")\n",
    "        weights = [attention_matrix[j+1][i] for j in range(4)]\n",
    "        max_idx = weights.index(max(weights))\n",
    "        print(f\"'{attention_matrix[0][max_idx]}'\")\n",
    "\n",
    "\n",
    "def trace_completions(prompt: str):\n",
    "    \"\"\"Trace likely completions and their probabilities.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83c\udfb2 STEP 3: LIKELY COMPLETIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simulated next token probabilities\n",
    "    completions = [\n",
    "        (\"sunny\", 0.15, \"Common weather descriptor\"),\n",
    "        (\"cloudy\", 0.12, \"Common weather descriptor\"),\n",
    "        (\"beautiful\", 0.10, \"Positive descriptor\"),\n",
    "        (\"rainy\", 0.08, \"Weather condition\"),\n",
    "        (\"cold\", 0.07, \"Temperature descriptor\"),\n",
    "        (\"perfect\", 0.06, \"Positive descriptor\"),\n",
    "        (\"expected\", 0.05, \"Forecast language\"),\n",
    "        (\"unpredictable\", 0.04, \"Weather characteristic\"),\n",
    "        (\"[other]\", 0.33, \"Long tail of possibilities\")\n",
    "    ]\n",
    "    \n",
    "    print(\"Next Token Probability Distribution:\")\n",
    "    print(\"(After softmax transformation)\")\n",
    "    \n",
    "    for token, prob, category in completions[:8]:\n",
    "        bar_length = int(prob * 100)\n",
    "        bar = \"\u2588\" * bar_length\n",
    "        print(f\"  {token:15} {prob:5.1%} {bar:20} ({category})\")\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfb0 Sampling Process (at temperature=0.7):\")\n",
    "    print(\"1. Apply temperature scaling to logits\")\n",
    "    print(\"2. Convert to probabilities via softmax\")\n",
    "    print(\"3. Sample from distribution\")\n",
    "    print(\"4. Selected: 'sunny' (example)\")\n",
    "    \n",
    "    # Show how temperature affects distribution\n",
    "    print(\"\\n\ud83c\udf21\ufe0f Temperature Effects:\")\n",
    "    temps = [\n",
    "        (0.0, \"Deterministic: Always pick 'sunny' (highest prob)\"),\n",
    "        (0.5, \"Conservative: Mostly 'sunny' or 'cloudy'\"),\n",
    "        (1.0, \"Balanced: Natural probability distribution\"),\n",
    "        (1.5, \"Creative: More uniform, unexpected choices possible\")\n",
    "    ]\n",
    "    \n",
    "    for temp, desc in temps:\n",
    "        print(f\"  T={temp}: {desc}\")\n",
    "\n",
    "\n",
    "def trace_missing_info(prompt: str):\n",
    "    \"\"\"Identify missing information the LLM doesn't have.\"\"\"\n",
    "    \n",
    "    print(\"\\n\u2753 STEP 4: MISSING INFORMATION ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    missing = [\n",
    "        {\n",
    "            \"info\": \"Actual Current Date\",\n",
    "            \"impact\": \"Cannot know what 'today' refers to\",\n",
    "            \"llm_behavior\": \"Will complete generically or assume\"\n",
    "        },\n",
    "        {\n",
    "            \"info\": \"Geographic Location\",\n",
    "            \"impact\": \"No specific location for weather\",\n",
    "            \"llm_behavior\": \"May describe generic/typical weather\"\n",
    "        },\n",
    "        {\n",
    "            \"info\": \"Real-time Weather Data\",\n",
    "            \"impact\": \"No access to actual weather conditions\",\n",
    "            \"llm_behavior\": \"Will generate plausible but fictional weather\"\n",
    "        },\n",
    "        {\n",
    "            \"info\": \"User's Intent\",\n",
    "            \"impact\": \"Unclear if asking or stating\",\n",
    "            \"llm_behavior\": \"Will assume most likely continuation\"\n",
    "        },\n",
    "        {\n",
    "            \"info\": \"Desired Format\",\n",
    "            \"impact\": \"Formal report? Casual chat?\",\n",
    "            \"llm_behavior\": \"Will use most common style from training\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Information the LLM DOESN'T Have:\")\n",
    "    \n",
    "    for item in missing:\n",
    "        print(f\"\\n\u274c {item['info']}\")\n",
    "        print(f\"   Impact: {item['impact']}\")\n",
    "        print(f\"   LLM Behavior: {item['llm_behavior']}\")\n",
    "    \n",
    "    print(\"\\n\u26a0\ufe0f Result: LLM will generate statistically plausible\")\n",
    "    print(\"         but not factually accurate weather description\")\n",
    "\n",
    "\n",
    "def visualize_complete_flow(prompt: str):\n",
    "    \"\"\"Visualize the complete processing flow.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udd04 COMPLETE PROCESSING FLOW\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    flow = \"\"\"\n",
    "    \"The weather today is\"\n",
    "           \u2193\n",
    "    [1] TOKENIZATION\n",
    "    [The] [weather] [today] [is]\n",
    "           \u2193\n",
    "    [2] EMBEDDING\n",
    "    Vector representations in 768D space\n",
    "           \u2193\n",
    "    [3] ATTENTION LAYERS (\u00d712)\n",
    "    Each token attends to others\n",
    "    Patterns emerge: \"weather report\"\n",
    "           \u2193\n",
    "    [4] PATTERN RECOGNITION\n",
    "    - Weather description context \u2713\n",
    "    - Incomplete sentence \u2713\n",
    "    - Present tense \u2713\n",
    "           \u2193\n",
    "    [5] NEXT TOKEN PREDICTION\n",
    "    Probability distribution over vocabulary\n",
    "    Top choices: sunny(15%), cloudy(12%), beautiful(10%)\n",
    "           \u2193\n",
    "    [6] SAMPLING (temperature=0.7)\n",
    "    Selected: \"sunny\"\n",
    "           \u2193\n",
    "    [7] APPEND & REPEAT\n",
    "    \"The weather today is sunny\"\n",
    "           \u2193\n",
    "    Continue until stop token or limit\n",
    "           \u2193\n",
    "    [8] OUTPUT\n",
    "    \"The weather today is sunny and pleasant, \n",
    "     with temperatures reaching...\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(flow)\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf KEY INSIGHTS:\")\n",
    "    print(\"\u2022 LLM doesn't 'know' the weather - it predicts likely text\")\n",
    "    print(\"\u2022 Each step is mathematical transformation, not understanding\")\n",
    "    print(\"\u2022 Quality depends on training patterns, not real-time data\")\n",
    "    print(\"\u2022 Confident output \u2260 factual accuracy\")\n",
    "\n",
    "\n",
    "def demonstrate_variations():\n",
    "    \"\"\"Show how slight changes affect the flow.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udd00 PROMPT VARIATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    variations = [\n",
    "        (\"The weather today is\", \"Generic completion expected\"),\n",
    "        (\"The weather today in Tokyo is\", \"Location-specific patterns activate\"),\n",
    "        (\"The weather forecast for today is\", \"Shifts to prediction language\"),\n",
    "        (\"Today's weather is\", \"Less formal, same meaning\"),\n",
    "        (\"The weather today will be\", \"Future tense changes predictions\")\n",
    "    ]\n",
    "    \n",
    "    print(\"How small changes affect processing:\\n\")\n",
    "    \n",
    "    for variant, effect in variations:\n",
    "        print(f\"\ud83d\udcdd '{variant}'\")\n",
    "        print(f\"   \u2192 {effect}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 Each variation activates slightly different patterns,\")\n",
    "    print(\"   leading to different completion probabilities!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the complete tracing exercise.\"\"\"\n",
    "    \n",
    "    # Main exercise\n",
    "    trace_prompt_flow(\"The weather today is\")\n",
    "    \n",
    "    # Show variations\n",
    "    demonstrate_variations()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 1 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You've traced a prompt through the entire LLM pipeline!\")\n",
    "    print(\"   From text \u2192 tokens \u2192 patterns \u2192 predictions \u2192 output\")\n",
    "    print(\"\\n\ud83c\udf93 Remember: LLMs are sophisticated pattern matchers,\")\n",
    "    print(\"   not weather services or fact databases!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.2: Context Window Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_3_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Context Window Planning\n",
    "Strategies for managing limited context windows effectively.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContextStrategy:\n",
    "    \"\"\"Strategy for managing context within token limits.\"\"\"\n",
    "    name: str\n",
    "    approach: str\n",
    "    implementation: str\n",
    "    pros: List[str]\n",
    "    cons: List[str]\n",
    "    token_allocation: Dict[str, int]\n",
    "\n",
    "\n",
    "def plan_context_strategies(context_limit: int = 4000):\n",
    "    \"\"\"\n",
    "    Plan strategies for different context window challenges.\n",
    "    \n",
    "    Args:\n",
    "        context_limit: Available context window in tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 2: CONTEXT WINDOW PLANNING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Context Window Limit: {context_limit} tokens\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Solve each scenario\n",
    "    solve_long_conversation()\n",
    "    solve_large_document()\n",
    "    solve_persistent_chat()\n",
    "    \n",
    "    # General strategies\n",
    "    show_context_management_techniques()\n",
    "    \n",
    "    # Best practices\n",
    "    show_best_practices()\n",
    "\n",
    "\n",
    "def solve_long_conversation():\n",
    "    \"\"\"Solution A: Having a 10,000 token conversation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca SCENARIO A: 10,000 TOKEN CONVERSATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    strategy = ContextStrategy(\n",
    "        name=\"Sliding Window with Summary\",\n",
    "        approach=\"Keep recent messages + summary of older ones\",\n",
    "        implementation=\"\"\"\n",
    "1. Maintain full recent context (last 2,500 tokens)\n",
    "2. Summarize older messages (500 token summary)\n",
    "3. Keep system prompt (100 tokens)\n",
    "4. Reserve space for response (900 tokens)\n",
    "        \"\"\",\n",
    "        pros=[\n",
    "            \"Preserves recent context perfectly\",\n",
    "            \"Maintains conversation continuity\",\n",
    "            \"Old information still accessible via summary\",\n",
    "            \"Efficient token usage\"\n",
    "        ],\n",
    "        cons=[\n",
    "            \"Summary may lose nuance\",\n",
    "            \"Requires periodic summarization calls\",\n",
    "            \"Important details might be condensed\"\n",
    "        ],\n",
    "        token_allocation={\n",
    "            \"System Prompt\": 100,\n",
    "            \"Summary of Old\": 500,\n",
    "            \"Recent Messages\": 2500,\n",
    "            \"Response Buffer\": 900,\n",
    "            \"Total\": 4000\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Strategy: {strategy.name}\")\n",
    "    print(f\"\\nApproach: {strategy.approach}\")\n",
    "    print(f\"\\nImplementation:{strategy.implementation}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Token Allocation:\")\n",
    "    for component, tokens in strategy.token_allocation.items():\n",
    "        if component != \"Total\":\n",
    "            percentage = (tokens / 4000) * 100\n",
    "            bar = \"\u2588\" * int(percentage / 2)\n",
    "            print(f\"  {component:20} {tokens:4} tokens ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Pros:\")\n",
    "    for pro in strategy.pros:\n",
    "        print(f\"  \u2022 {pro}\")\n",
    "    \n",
    "    print(f\"\\n\u274c Cons:\")\n",
    "    for con in strategy.cons:\n",
    "        print(f\"  \u2022 {con}\")\n",
    "    \n",
    "    # Show implementation code\n",
    "    print(\"\\n\ud83d\udcbb Implementation Example:\")\n",
    "    print(\"\"\"\n",
    "def manage_long_conversation(messages, max_tokens=4000):\n",
    "    # Calculate current token usage\n",
    "    total_tokens = sum(estimate_tokens(m['content']) for m in messages)\n",
    "    \n",
    "    if total_tokens <= max_tokens:\n",
    "        return messages  # Everything fits\n",
    "    \n",
    "    # Keep system message\n",
    "    result = [messages[0]] if messages[0]['role'] == 'system' else []\n",
    "    \n",
    "    # Summarize older messages\n",
    "    old_messages = messages[1:-10]  # All but last 10\n",
    "    if old_messages:\n",
    "        summary = summarize_messages(old_messages)\n",
    "        result.append({\"role\": \"system\", \"content\": f\"Previous conversation: {summary}\"})\n",
    "    \n",
    "    # Add recent messages\n",
    "    result.extend(messages[-10:])\n",
    "    \n",
    "    return result\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def solve_large_document():\n",
    "    \"\"\"Solution B: Analyzing a 50,000 token document.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc4 SCENARIO B: 50,000 TOKEN DOCUMENT ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"name\": \"Chunking with Overlap\",\n",
    "            \"description\": \"Divide into overlapping chunks\",\n",
    "            \"chunks\": math.ceil(50000 / 3000),  # 3000 tokens per chunk\n",
    "            \"overlap\": 500,\n",
    "            \"process\": [\n",
    "                \"1. Split document into 3,000 token chunks\",\n",
    "                \"2. Keep 500 token overlap between chunks\",\n",
    "                \"3. Process each chunk independently\",\n",
    "                \"4. Combine results with meta-analysis\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Hierarchical Summarization\",\n",
    "            \"description\": \"Multi-level summary approach\",\n",
    "            \"levels\": 3,\n",
    "            \"process\": [\n",
    "                \"1. Split into 10 sections (5,000 tokens each)\",\n",
    "                \"2. Summarize each to 400 tokens\",\n",
    "                \"3. Combine summaries (4,000 tokens)\",\n",
    "                \"4. Generate final analysis\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Extractive + Focus\",\n",
    "            \"description\": \"Extract key sections then deep dive\",\n",
    "            \"process\": [\n",
    "                \"1. Quick scan for key sections (headers, conclusions)\",\n",
    "                \"2. Extract most relevant 3,500 tokens\",\n",
    "                \"3. Focused analysis on extracts\",\n",
    "                \"4. Reference back to specific sections as needed\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Multiple Strategies Available:\\n\")\n",
    "    \n",
    "    for i, strategy in enumerate(strategies, 1):\n",
    "        print(f\"{i}. {strategy['name']}\")\n",
    "        print(f\"   {strategy['description']}\")\n",
    "        print(f\"\\n   Process:\")\n",
    "        for step in strategy[\"process\"]:\n",
    "            print(f\"   {step}\")\n",
    "        print()\n",
    "    \n",
    "    # Detailed implementation of best approach\n",
    "    print(\"\ud83c\udfc6 RECOMMENDED APPROACH: Chunking with Overlap\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"\"\"\n",
    "def analyze_large_document(document, chunk_size=3000, overlap=500):\n",
    "    chunks = []\n",
    "    position = 0\n",
    "    \n",
    "    # Create overlapping chunks\n",
    "    while position < len(document):\n",
    "        chunk_end = position + chunk_size\n",
    "        chunk = document[position:chunk_end]\n",
    "        \n",
    "        chunks.append({\n",
    "            'content': chunk,\n",
    "            'start': position,\n",
    "            'end': chunk_end\n",
    "        })\n",
    "        \n",
    "        position += (chunk_size - overlap)\n",
    "    \n",
    "    # Process each chunk\n",
    "    analyses = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = f\\\"\\\"\\\"\n",
    "        Analyzing part {i+1}/{len(chunks)} of document.\n",
    "        Context from previous: {analyses[-1][:200] if analyses else 'None'}\n",
    "        \n",
    "        Text:\n",
    "        {chunk['content']}\n",
    "        \n",
    "        Provide key points and analysis:\n",
    "        \\\"\\\"\\\"\n",
    "        \n",
    "        analysis = llm_call(prompt, max_tokens=500)\n",
    "        analyses.append(analysis)\n",
    "    \n",
    "    # Final synthesis\n",
    "    final_prompt = f\\\"\\\"\\\"\n",
    "    Synthesize these analyses into a coherent summary:\n",
    "    {' '.join(analyses)}\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    return llm_call(final_prompt)\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Token Budget per Chunk:\")\n",
    "    print(\"  Document chunk: 3,000 tokens\")\n",
    "    print(\"  Context/prompt: 500 tokens\")\n",
    "    print(\"  Response space: 500 tokens\")\n",
    "    print(\"  Total per call: 4,000 tokens\")\n",
    "    print(f\"\\n  Total chunks needed: ~{math.ceil(50000/2500)} (with overlap)\")\n",
    "\n",
    "\n",
    "def solve_persistent_chat():\n",
    "    \"\"\"Solution C: Maintaining chat history over multiple sessions.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcac SCENARIO C: PERSISTENT CHAT HISTORY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Strategy: Hierarchical Memory System\\n\")\n",
    "    \n",
    "    memory_layers = [\n",
    "        {\n",
    "            \"layer\": \"Working Memory\",\n",
    "            \"tokens\": 2000,\n",
    "            \"content\": \"Current conversation (last 5-10 messages)\",\n",
    "            \"persistence\": \"In-context\"\n",
    "        },\n",
    "        {\n",
    "            \"layer\": \"Session Summary\",\n",
    "            \"tokens\": 500,\n",
    "            \"content\": \"Summary of current session\",\n",
    "            \"persistence\": \"Generated at intervals\"\n",
    "        },\n",
    "        {\n",
    "            \"layer\": \"Long-term Memory\",\n",
    "            \"tokens\": 300,\n",
    "            \"content\": \"Key facts, preferences, important history\",\n",
    "            \"persistence\": \"Database/file storage\"\n",
    "        },\n",
    "        {\n",
    "            \"layer\": \"Context Prompt\",\n",
    "            \"tokens\": 200,\n",
    "            \"content\": \"System prompt + user preferences\",\n",
    "            \"persistence\": \"Configuration\"\n",
    "        },\n",
    "        {\n",
    "            \"layer\": \"Response Buffer\",\n",
    "            \"tokens\": 1000,\n",
    "            \"content\": \"Space for generation\",\n",
    "            \"persistence\": \"N/A\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"Memory Architecture:\")\n",
    "    total = 0\n",
    "    for layer in memory_layers:\n",
    "        total += layer[\"tokens\"]\n",
    "        print(f\"\\n\ud83d\udcc1 {layer['layer']}\")\n",
    "        print(f\"   Tokens: {layer['tokens']}\")\n",
    "        print(f\"   Content: {layer['content']}\")\n",
    "        print(f\"   Persistence: {layer['persistence']}\")\n",
    "    \n",
    "    print(f\"\\n   Total: {total} tokens\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcbe Implementation:\")\n",
    "    print(\"\"\"\n",
    "class PersistentChatbot:\n",
    "    def __init__(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.working_memory = []\n",
    "        self.session_summary = \"\"\n",
    "        self.long_term_memory = self.load_user_memory()\n",
    "    \n",
    "    def load_user_memory(self):\n",
    "        # Load from database/file\n",
    "        return {\n",
    "            \"name\": \"User's name\",\n",
    "            \"preferences\": {...},\n",
    "            \"key_facts\": [...],\n",
    "            \"conversation_summaries\": [...]\n",
    "        }\n",
    "    \n",
    "    def build_context(self, new_message):\n",
    "        context = []\n",
    "        \n",
    "        # 1. System prompt + long-term memory\n",
    "        context.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a helpful assistant. User info: {self.long_term_memory}\"\n",
    "        })\n",
    "        \n",
    "        # 2. Session summary if exists\n",
    "        if self.session_summary:\n",
    "            context.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Current session summary: {self.session_summary}\"\n",
    "            })\n",
    "        \n",
    "        # 3. Recent messages (working memory)\n",
    "        context.extend(self.working_memory[-10:])\n",
    "        \n",
    "        # 4. New message\n",
    "        context.append({\"role\": \"user\", \"content\": new_message})\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def save_to_long_term(self):\n",
    "        # Periodically extract and save important information\n",
    "        important_facts = extract_key_information(self.working_memory)\n",
    "        self.long_term_memory['key_facts'].extend(important_facts)\n",
    "        save_to_database(self.user_id, self.long_term_memory)\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def show_context_management_techniques():\n",
    "    \"\"\"Show various context management techniques.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CONTEXT MANAGEMENT TECHNIQUES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    techniques = [\n",
    "        {\n",
    "            \"name\": \"Sliding Window\",\n",
    "            \"description\": \"Keep only recent N messages\",\n",
    "            \"code\": \"messages = messages[-20:]\",\n",
    "            \"use_case\": \"Simple conversations\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Summarization\",\n",
    "            \"description\": \"Replace old messages with summary\",\n",
    "            \"code\": \"summary = summarize(messages[:-10])\\nmessages = [summary] + messages[-10:]\",\n",
    "            \"use_case\": \"Long conversations\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Importance Scoring\",\n",
    "            \"description\": \"Keep only important messages\",\n",
    "            \"code\": \"messages = [m for m in messages if m['importance'] > threshold]\",\n",
    "            \"use_case\": \"Information-dense chats\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Compression\",\n",
    "            \"description\": \"Compress message content\",\n",
    "            \"code\": \"messages = [compress(m) for m in messages]\",\n",
    "            \"use_case\": \"Verbose content\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Chunking\",\n",
    "            \"description\": \"Process in segments\",\n",
    "            \"code\": \"for chunk in chunks(document, 3000):\\n    process(chunk)\",\n",
    "            \"use_case\": \"Large documents\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for technique in techniques:\n",
    "        print(f\"\\n\ud83d\udccb {technique['name']}\")\n",
    "        print(f\"   {technique['description']}\")\n",
    "        print(f\"   Use case: {technique['use_case']}\")\n",
    "        print(f\"   Example:\")\n",
    "        for line in technique['code'].split('\\n'):\n",
    "            print(f\"      {line}\")\n",
    "\n",
    "\n",
    "def show_best_practices():\n",
    "    \"\"\"Show best practices for context management.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BEST PRACTICES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    practices = [\n",
    "        \"\ud83c\udfaf Always reserve tokens for response (20-25% of limit)\",\n",
    "        \"\ud83d\udcca Track token usage proactively, don't wait for errors\",\n",
    "        \"\ud83d\udd04 Implement graceful degradation when approaching limits\",\n",
    "        \"\ud83d\udcbe Store important information externally\",\n",
    "        \"\ud83d\udcdd Summarize proactively, not reactively\",\n",
    "        \"\ud83c\udfa8 Design prompts to be context-efficient\",\n",
    "        \"\ud83d\udd0d Use retrieval for reference material\",\n",
    "        \"\u26a1 Cache frequently used context\",\n",
    "        \"\ud83d\udcc8 Monitor and optimize token usage patterns\",\n",
    "        \"\ud83d\udee1\ufe0f Have fallback strategies for context overflow\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nKey Guidelines:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  {practice}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udccf Token Budgeting Formula:\")\n",
    "    print(\"  Available = Context_Limit - Response_Buffer\")\n",
    "    print(\"  Usable = Available * 0.9  (safety margin)\")\n",
    "    print(\"  Per_Message = Usable / Expected_Conversation_Length\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run context window planning exercise.\"\"\"\n",
    "    \n",
    "    plan_context_strategies(context_limit=4000)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 2 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You now understand how to manage limited context windows\")\n",
    "    print(\"   for various scenarios: long chats, large docs, and persistence!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3.3: Understanding Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_7_3_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Understanding Failures\n",
    "Why LLMs fail at certain tasks - explained through their mechanisms.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FailureAnalysis:\n",
    "    \"\"\"Analysis of why an LLM fails at a specific task.\"\"\"\n",
    "    scenario: str\n",
    "    root_cause: str\n",
    "    mechanism_explanation: str\n",
    "    observable_behavior: str\n",
    "    mitigation: str\n",
    "\n",
    "\n",
    "def analyze_llm_failures():\n",
    "    \"\"\"\n",
    "    Analyze why LLMs fail at specific tasks based on their mechanisms.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 3: UNDERSTANDING LLM FAILURES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Why do LLMs fail? Let's trace each failure to its root cause.\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    failures = get_failure_analyses()\n",
    "    \n",
    "    for i, (scenario, analysis) in enumerate(failures.items(), 1):\n",
    "        print(f\"\\n{i}. {analysis.scenario}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd0d ROOT CAUSE:\")\n",
    "        print(f\"   {analysis.root_cause}\")\n",
    "        \n",
    "        print(f\"\\n\u2699\ufe0f MECHANISM EXPLANATION:\")\n",
    "        for line in analysis.mechanism_explanation.split('\\n'):\n",
    "            print(f\"   {line}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udc41\ufe0f OBSERVABLE BEHAVIOR:\")\n",
    "        print(f\"   {analysis.observable_behavior}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udca1 MITIGATION:\")\n",
    "        print(f\"   {analysis.mitigation}\")\n",
    "    \n",
    "    # Show the common pattern\n",
    "    show_common_failure_patterns()\n",
    "    \n",
    "    # Demonstrate with examples\n",
    "    demonstrate_failure_examples()\n",
    "\n",
    "\n",
    "def get_failure_analyses() -> Dict[str, FailureAnalysis]:\n",
    "    \"\"\"Get detailed analyses of each failure scenario.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"arithmetic\": FailureAnalysis(\n",
    "            scenario=\"Can't do exact arithmetic on large numbers\",\n",
    "            root_cause=\"LLMs learn statistical patterns, not mathematical operations\",\n",
    "            mechanism_explanation=\"\"\"\n",
    "LLMs see \"123 \u00d7 456\" as tokens, not numbers.\n",
    "They predict likely text patterns from training.\n",
    "No actual calculation happens - just pattern matching.\n",
    "They've seen \"2+2=4\" millions of times, so they get it right.\n",
    "They haven't seen \"123,456 \u00d7 789,012\" so they approximate.\"\"\",\n",
    "            observable_behavior=\"Gets simple math right, fails on complex/unusual calculations\",\n",
    "            mitigation=\"Use tools/code for calculations, LLM for explaining the process\"\n",
    "        ),\n",
    "        \n",
    "        \"citations\": FailureAnalysis(\n",
    "            scenario=\"Makes up fake citations\",\n",
    "            root_cause=\"Pattern completion without factual grounding\",\n",
    "            mechanism_explanation=\"\"\"\n",
    "LLMs learn the PATTERN of citations: (Author, Year, Journal).\n",
    "When asked for citations, they generate plausible-looking ones.\n",
    "No connection to real publications database.\n",
    "They combine: real author names + likely years + journal names.\n",
    "Result looks real but is statistically generated fiction.\"\"\",\n",
    "            observable_behavior=\"Citations follow correct format but don't exist\",\n",
    "            mitigation=\"Always verify citations, use retrieval-augmented generation\"\n",
    "        ),\n",
    "        \n",
    "        \"contradictions\": FailureAnalysis(\n",
    "            scenario=\"Contradicts itself in long conversations\",\n",
    "            root_cause=\"No persistent state or memory between tokens\",\n",
    "            mechanism_explanation=\"\"\"\n",
    "Each response is generated token-by-token.\n",
    "No memory of what it 'decided' earlier.\n",
    "Context window truncation loses old statements.\n",
    "Statistical generation doesn't enforce logical consistency.\n",
    "Different prompt contexts activate different patterns.\"\"\",\n",
    "            observable_behavior=\"Says one thing early, opposite later, doesn't notice\",\n",
    "            mitigation=\"Track important claims, remind model of prior statements\"\n",
    "        ),\n",
    "        \n",
    "        \"preferences\": FailureAnalysis(\n",
    "            scenario=\"Can't learn your preferences permanently\",\n",
    "            root_cause=\"No weight updates after training, stateless operation\",\n",
    "            mechanism_explanation=\"\"\"\n",
    "LLM weights are frozen after training.\n",
    "Each conversation starts fresh - no memory.\n",
    "Can't update internal parameters based on your feedback.\n",
    "'Learning' only exists within current context window.\n",
    "When context clears, all 'learning' is lost.\"\"\",\n",
    "            observable_behavior=\"Forgets preferences between sessions, repeats same mistakes\",\n",
    "            mitigation=\"Store preferences externally, include in system prompt\"\n",
    "        ),\n",
    "        \n",
    "        \"confident_errors\": FailureAnalysis(\n",
    "            scenario=\"Says factually wrong things confidently\",\n",
    "            root_cause=\"No truth mechanism - only pattern likelihood\",\n",
    "            mechanism_explanation=\"\"\"\n",
    "LLMs optimize for likely/plausible text, not truth.\n",
    "Confidence is a linguistic pattern, not certainty measure.\n",
    "\"The capital of France is Paris\" - pattern learned from data.\n",
    "\"The capital of [Made-up Country] is [Plausible City Name]\" - same pattern.\n",
    "No difference in mechanism between true and false statements.\"\"\",\n",
    "            observable_behavior=\"States fiction with same confidence as facts\",\n",
    "            mitigation=\"Always fact-check important claims, especially specific details\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def show_common_failure_patterns():\n",
    "    \"\"\"Show common patterns across all failures.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMMON FAILURE PATTERNS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    patterns = [\n",
    "        {\n",
    "            \"pattern\": \"Pattern Without Understanding\",\n",
    "            \"explanation\": \"LLMs match patterns without grasping meaning\",\n",
    "            \"examples\": [\"Math notation\", \"Citation format\", \"Code syntax\"]\n",
    "        },\n",
    "        {\n",
    "            \"pattern\": \"Statistical Plausibility \u2260 Truth\",\n",
    "            \"explanation\": \"Generates what's likely, not what's true\",\n",
    "            \"examples\": [\"Fake citations\", \"Plausible but wrong facts\", \"Confident errors\"]\n",
    "        },\n",
    "        {\n",
    "            \"pattern\": \"No Persistent State\",\n",
    "            \"explanation\": \"Each generation starts fresh, no memory\",\n",
    "            \"examples\": [\"Contradictions\", \"Forgotten context\", \"Lost preferences\"]\n",
    "        },\n",
    "        {\n",
    "            \"pattern\": \"Context Window Limitations\",\n",
    "            \"explanation\": \"Can only 'see' limited tokens at once\",\n",
    "            \"examples\": [\"Long document issues\", \"Conversation drift\", \"Lost details\"]\n",
    "        },\n",
    "        {\n",
    "            \"pattern\": \"Training Data Boundaries\",\n",
    "            \"explanation\": \"Can't go beyond what was in training\",\n",
    "            \"examples\": [\"Current events\", \"Personal information\", \"New developments\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for pattern_info in patterns:\n",
    "        print(f\"\\n\ud83d\udd0d {pattern_info['pattern']}\")\n",
    "        print(f\"   {pattern_info['explanation']}\")\n",
    "        print(f\"   Examples: {', '.join(pattern_info['examples'])}\")\n",
    "\n",
    "\n",
    "def demonstrate_failure_examples():\n",
    "    \"\"\"Demonstrate failures with concrete examples.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CONCRETE FAILURE EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    examples = [\n",
    "        {\n",
    "            \"task\": \"Calculate 12,847 \u00d7 9,736\",\n",
    "            \"llm_output\": \"125,234,872 (likely wrong)\",\n",
    "            \"actual\": \"125,094,392\",\n",
    "            \"why\": \"No computation, just pattern matching\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Cite the seminal paper on transformer architecture\",\n",
    "            \"llm_output\": \"Vaswani et al., 2017 \u2713 (happens to be right)\",\n",
    "            \"actual\": \"'Attention is All You Need', Vaswani et al., 2017\",\n",
    "            \"why\": \"This one is memorized, but could easily make up others\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"What's the weather in Tokyo?\",\n",
    "            \"llm_output\": \"It's sunny and 22\u00b0C (completely made up)\",\n",
    "            \"actual\": \"No way to know without real-time data\",\n",
    "            \"why\": \"Generates plausible weather, not actual weather\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Remember I prefer Python over Java\",\n",
    "            \"llm_output\": \"I'll remember that! (no, it won't)\",\n",
    "            \"actual\": \"Forgotten by next conversation\",\n",
    "            \"why\": \"No weight updates, no persistent memory\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for ex in examples:\n",
    "        print(f\"\\n\ud83d\udcdd Task: {ex['task']}\")\n",
    "        print(f\"   LLM Output: {ex['llm_output']}\")\n",
    "        print(f\"   Reality: {ex['actual']}\")\n",
    "        print(f\"   \u2753 Why: {ex['why']}\")\n",
    "\n",
    "\n",
    "def create_diagnostic_tests():\n",
    "    \"\"\"Create tests to identify LLM limitations.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DIAGNOSTIC TESTS FOR LLM LIMITATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    tests = [\n",
    "        {\n",
    "            \"name\": \"Arithmetic Test\",\n",
    "            \"prompt\": \"What is 48,293 \u00d7 7,456?\",\n",
    "            \"expected_behavior\": \"Wrong answer or refusal\",\n",
    "            \"reveals\": \"Lack of computation ability\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Citation Test\",\n",
    "            \"prompt\": \"Cite 3 papers about unicorn biology from Nature journal\",\n",
    "            \"expected_behavior\": \"Generates fake citations\",\n",
    "            \"reveals\": \"Pattern generation without factual grounding\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Consistency Test\",\n",
    "            \"prompt\": \"Part 1: Is X better than Y?\\nPart 2 (later): Compare Y to X\",\n",
    "            \"expected_behavior\": \"May contradict earlier stance\",\n",
    "            \"reveals\": \"No persistent beliefs or memory\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Current Events Test\",\n",
    "            \"prompt\": \"What happened in the news yesterday?\",\n",
    "            \"expected_behavior\": \"Generic or outdated response\",\n",
    "            \"reveals\": \"No real-time information access\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Self-Knowledge Test\",\n",
    "            \"prompt\": \"What did we discuss 10 messages ago?\",\n",
    "            \"expected_behavior\": \"Accurate if in context, lost if outside window\",\n",
    "            \"reveals\": \"Context window limitations\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTests to Reveal LLM Limitations:\\n\")\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\ud83e\uddea {test['name']}\")\n",
    "        print(f\"   Prompt: '{test['prompt']}'\")\n",
    "        print(f\"   Expected: {test['expected_behavior']}\")\n",
    "        print(f\"   Reveals: {test['reveals']}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def explain_implications():\n",
    "    \"\"\"Explain practical implications of these failures.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRACTICAL IMPLICATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    implications = [\n",
    "        (\"Never use LLMs for:\", [\n",
    "            \"Critical calculations without verification\",\n",
    "            \"Source of truth for facts without checking\",\n",
    "            \"Medical/legal advice without professional review\",\n",
    "            \"Real-time information needs\",\n",
    "            \"Maintaining long-term user state\"\n",
    "        ]),\n",
    "        \n",
    "        (\"Always verify:\", [\n",
    "            \"Mathematical results\",\n",
    "            \"Citations and references\",\n",
    "            \"Specific facts and figures\",\n",
    "            \"Technical specifications\",\n",
    "            \"Current information\"\n",
    "        ]),\n",
    "        \n",
    "        (\"Design around limitations:\", [\n",
    "            \"Use tools for computation\",\n",
    "            \"Implement external memory systems\",\n",
    "            \"Fact-check important claims\",\n",
    "            \"Keep context windows managed\",\n",
    "            \"Store user preferences separately\"\n",
    "        ]),\n",
    "        \n",
    "        (\"LLMs excel at:\", [\n",
    "            \"Pattern-based text generation\",\n",
    "            \"Style and tone matching\",\n",
    "            \"Creative ideation\",\n",
    "            \"Language transformation\",\n",
    "            \"Explaining concepts (verify accuracy)\"\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    for category, items in implications:\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  \u2022 {item}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run failure analysis exercise.\"\"\"\n",
    "    \n",
    "    # Analyze failures\n",
    "    analyze_llm_failures()\n",
    "    \n",
    "    # Create diagnostic tests\n",
    "    create_diagnostic_tests()\n",
    "    \n",
    "    # Explain implications\n",
    "    explain_implications()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 3 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You now understand WHY LLMs fail at specific tasks\")\n",
    "    print(\"   and can design around these limitations!\")\n",
    "    print(\"\\n\ud83c\udf93 Remember: These aren't bugs, they're fundamental\")\n",
    "    print(\"   consequences of how LLMs work - pattern matching, not reasoning!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.4 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.1: Prompt Improvement Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_4_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: Prompt Improvement Challenge\n",
    "Transform weak prompts into effective ones using prompt engineering techniques.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def improve_prompts():\n",
    "    \"\"\"\n",
    "    Improve weak prompts using prompt engineering best practices.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 1: PROMPT IMPROVEMENT CHALLENGE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    weak_prompts = [\n",
    "        \"Write about space\",\n",
    "        \"Fix this: def func(x): return x/0\",\n",
    "        \"Translate: Hello\",\n",
    "        \"Make a list\",\n",
    "        \"Explain AI\"\n",
    "    ]\n",
    "    \n",
    "    for i, weak in enumerate(weak_prompts, 1):\n",
    "        print(f\"\\n{i}. WEAK PROMPT: \\\"{weak}\\\"\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        improved = improve_single_prompt(weak, i)\n",
    "        \n",
    "        print(\"\u2728 IMPROVED VERSION:\")\n",
    "        print(\"```\")\n",
    "        print(improved[\"prompt\"])\n",
    "        print(\"```\")\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca IMPROVEMENTS APPLIED:\")\n",
    "        for improvement in improved[\"improvements\"]:\n",
    "            print(f\"  \u2022 {improvement}\")\n",
    "        \n",
    "        print(\"\\n\ud83d\udca1 WHY THIS IS BETTER:\")\n",
    "        print(f\"  {improved['reasoning']}\")\n",
    "    \n",
    "    # Show general principles\n",
    "    show_improvement_principles()\n",
    "\n",
    "\n",
    "def improve_single_prompt(weak_prompt: str, number: int) -> Dict:\n",
    "    \"\"\"Improve a single weak prompt.\"\"\"\n",
    "    \n",
    "    improvements = {\n",
    "        1: {  # \"Write about space\"\n",
    "            \"prompt\": \"\"\"Write a 300-word educational article about space exploration for high school students.\n",
    "\n",
    "Include:\n",
    "- 2-3 recent achievements in space exploration\n",
    "- Why space exploration matters for humanity\n",
    "- One specific mission or technology in detail\n",
    "\n",
    "Style: Engaging but informative, avoid technical jargon.\n",
    "Format: Include a compelling opening hook and a forward-looking conclusion.\"\"\",\n",
    "            \"improvements\": [\n",
    "                \"Added specific audience (high school students)\",\n",
    "                \"Defined length (300 words)\",\n",
    "                \"Listed specific content requirements\",\n",
    "                \"Clarified tone and style\",\n",
    "                \"Specified structure expectations\"\n",
    "            ],\n",
    "            \"reasoning\": \"The original prompt was too vague. The improved version provides context, constraints, and clear expectations, leading to more useful and targeted output.\"\n",
    "        },\n",
    "        \n",
    "        2: {  # \"Fix this: def func(x): return x/0\"\n",
    "            \"prompt\": \"\"\"Debug and fix this Python function that's causing a ZeroDivisionError:\n",
    "\n",
    "```python\n",
    "def func(x):\n",
    "    return x/0\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "1. Identify the error and explain why it occurs\n",
    "2. Provide a corrected version with proper error handling\n",
    "3. Include at least 2 different approaches to handle division by zero\n",
    "4. Add docstring and type hints\n",
    "5. Write a simple test to verify the fix\n",
    "\n",
    "Context: This function is part of a calculator application where x represents user input.\"\"\",\n",
    "            \"improvements\": [\n",
    "                \"Specified the exact error to address\",\n",
    "                \"Required explanation of the problem\",\n",
    "                \"Asked for multiple solutions\",\n",
    "                \"Added code quality requirements\",\n",
    "                \"Provided application context\"\n",
    "            ],\n",
    "            \"reasoning\": \"Simply saying 'fix this' doesn't guide the response. The improved version teaches debugging methodology and ensures robust error handling.\"\n",
    "        },\n",
    "        \n",
    "        3: {  # \"Translate: Hello\"\n",
    "            \"prompt\": \"\"\"Translate \"Hello\" into the following languages, including pronunciation guide and cultural context:\n",
    "\n",
    "Languages:\n",
    "1. Spanish\n",
    "2. French  \n",
    "3. Japanese\n",
    "4. Arabic\n",
    "5. Mandarin Chinese\n",
    "\n",
    "For each translation, provide:\n",
    "- The written form (with script for non-Latin alphabets)\n",
    "- Phonetic pronunciation for English speakers\n",
    "- Formal vs informal variations if applicable\n",
    "- Cultural note about appropriate usage\n",
    "\n",
    "Example format:\n",
    "Language: [Translation] | Pronunciation: [Guide] | Context: [When/how to use]\"\"\",\n",
    "            \"improvements\": [\n",
    "                \"Specified target languages\",\n",
    "                \"Requested pronunciation guides\",\n",
    "                \"Asked for cultural context\",\n",
    "                \"Provided output format\",\n",
    "                \"Included formal/informal distinctions\"\n",
    "            ],\n",
    "            \"reasoning\": \"A single word translation lacks context. The improved version provides practical, usable information for real communication.\"\n",
    "        },\n",
    "        \n",
    "        4: {  # \"Make a list\"\n",
    "            \"prompt\": \"\"\"Create a prioritized checklist for launching a small online business.\n",
    "\n",
    "Specifications:\n",
    "- 15-20 actionable items\n",
    "- Organized into 3 phases: Planning, Setup, and Launch\n",
    "- Each item should be specific and measurable\n",
    "- Include time estimates (hours/days/weeks)\n",
    "- Mark critical path items with [CRITICAL]\n",
    "- Add brief explanation for why each item matters\n",
    "\n",
    "Format as a numbered list with clear phase headers.\n",
    "Target audience: First-time entrepreneur with limited budget.\"\"\",\n",
    "            \"improvements\": [\n",
    "                \"Defined the type and purpose of list\",\n",
    "                \"Specified item count and organization\",\n",
    "                \"Required actionable, measurable items\",\n",
    "                \"Added time estimates requirement\",\n",
    "                \"Identified target audience\"\n",
    "            ],\n",
    "            \"reasoning\": \"'Make a list' could mean anything. The improved version creates a valuable, actionable resource with clear structure and purpose.\"\n",
    "        },\n",
    "        \n",
    "        5: {  # \"Explain AI\"\n",
    "            \"prompt\": \"\"\"Explain artificial intelligence to a curious 12-year-old using everyday analogies.\n",
    "\n",
    "Cover these key points:\n",
    "1. What AI is (and isn't)\n",
    "2. How AI learns from examples\n",
    "3. One specific example they use daily (like YouTube recommendations)\n",
    "4. Why AI sometimes makes mistakes\n",
    "5. How AI might help them in the future\n",
    "\n",
    "Requirements:\n",
    "- Use at least 2 relatable analogies\n",
    "- Avoid technical terms or explain them simply\n",
    "- Keep it under 400 words\n",
    "- End with an engaging question to spark further curiosity\n",
    "\n",
    "Tone: Friendly, encouraging, and wonder-inducing.\"\"\",\n",
    "            \"improvements\": [\n",
    "                \"Defined specific audience and their level\",\n",
    "                \"Listed key points to cover\",\n",
    "                \"Required relatable analogies\",\n",
    "                \"Set appropriate length\",\n",
    "                \"Specified engaging tone\"\n",
    "            ],\n",
    "            \"reasoning\": \"'Explain AI' is too broad. The improved version creates an age-appropriate, engaging explanation that actually teaches understanding.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return improvements[number]\n",
    "\n",
    "\n",
    "def show_improvement_principles():\n",
    "    \"\"\"Show general principles for improving prompts.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROMPT IMPROVEMENT PRINCIPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    principles = [\n",
    "        {\n",
    "            \"principle\": \"Be Specific\",\n",
    "            \"bad\": \"Write something\",\n",
    "            \"good\": \"Write a 500-word blog post about...\",\n",
    "            \"why\": \"Specificity reduces ambiguity\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Define the Audience\",\n",
    "            \"bad\": \"Explain quantum physics\",\n",
    "            \"good\": \"Explain quantum physics to a high school student\",\n",
    "            \"why\": \"Audience determines language and depth\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Set Clear Constraints\",\n",
    "            \"bad\": \"Make it good\",\n",
    "            \"good\": \"Make it engaging, under 200 words, with examples\",\n",
    "            \"why\": \"Constraints guide creativity\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Provide Context\",\n",
    "            \"bad\": \"Fix this code\",\n",
    "            \"good\": \"Fix this code that processes user payments\",\n",
    "            \"why\": \"Context informs appropriate solutions\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Specify Format\",\n",
    "            \"bad\": \"List some ideas\",\n",
    "            \"good\": \"List 5 ideas as bullet points with brief explanations\",\n",
    "            \"why\": \"Format specifications ensure usable output\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Include Examples\",\n",
    "            \"bad\": \"Format this data\",\n",
    "            \"good\": \"Format this data like this example: Name | Age | City\",\n",
    "            \"why\": \"Examples clarify expectations\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for p in principles:\n",
    "        print(f\"\\n\ud83d\udccb {p['principle']}\")\n",
    "        print(f\"   \u274c Bad: \\\"{p['bad']}\\\"\")\n",
    "        print(f\"   \u2705 Good: \\\"{p['good']}\\\"\")\n",
    "        print(f\"   \ud83d\udca1 Why: {p['why']}\")\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \"\"\"Create a reusable template for well-structured prompts.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"UNIVERSAL PROMPT TEMPLATE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    template = \"\"\"\n",
    "[ROLE/CONTEXT]\n",
    "You are a [specific role] helping with [specific task].\n",
    "\n",
    "[TASK]\n",
    "[Clear, specific description of what you want]\n",
    "\n",
    "[REQUIREMENTS]\n",
    "- [Specific requirement 1]\n",
    "- [Specific requirement 2]\n",
    "- [Specific requirement 3]\n",
    "\n",
    "[CONSTRAINTS]\n",
    "- Length: [word/token count]\n",
    "- Style: [tone/formality level]\n",
    "- Format: [output structure]\n",
    "\n",
    "[EXAMPLES] (if needed)\n",
    "Input: [example input]\n",
    "Output: [example output]\n",
    "\n",
    "[ADDITIONAL CONTEXT] (if relevant)\n",
    "[Any background information that helps]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Use this template to structure your prompts:\")\n",
    "    print(template)\n",
    "    \n",
    "    print(\"\\n\u2705 Example using the template:\")\n",
    "    \n",
    "    example = \"\"\"\n",
    "[ROLE/CONTEXT]\n",
    "You are a technical documentation writer helping with API documentation.\n",
    "\n",
    "[TASK]\n",
    "Write clear documentation for a REST API endpoint that creates new user accounts.\n",
    "\n",
    "[REQUIREMENTS]\n",
    "- Include endpoint URL, HTTP method, and authentication\n",
    "- Document all request parameters with types and validation rules\n",
    "- Show example request and response bodies\n",
    "- List possible error codes and their meanings\n",
    "\n",
    "[CONSTRAINTS]\n",
    "- Length: 300-400 words\n",
    "- Style: Technical but accessible to junior developers\n",
    "- Format: Markdown with code blocks\n",
    "\n",
    "[EXAMPLES]\n",
    "Similar to Stripe's API documentation style\n",
    "\n",
    "[ADDITIONAL CONTEXT]\n",
    "This is for a SaaS application's public API used by third-party developers.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(example)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run prompt improvement exercise.\"\"\"\n",
    "    \n",
    "    # Improve prompts\n",
    "    improve_prompts()\n",
    "    \n",
    "    # Show template\n",
    "    create_prompt_template()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 1 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You've learned to transform vague prompts into effective ones!\")\n",
    "    print(\"   Remember: Specific, Contextual, Constrained, and Formatted!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.2: Few-Shot Template Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_4_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Few-Shot Template Creation\n",
    "Create effective few-shot prompts for various tasks.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def create_few_shot_templates():\n",
    "    \"\"\"\n",
    "    Create few-shot prompt templates for different tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 2: FEW-SHOT TEMPLATE CREATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Task 1: Date Extraction\n",
    "    date_extraction_template()\n",
    "    \n",
    "    # Task 2: Customer Support Classification\n",
    "    support_classification_template()\n",
    "    \n",
    "    # Task 3: Informal to Formal Conversion\n",
    "    formality_conversion_template()\n",
    "    \n",
    "    # Best practices\n",
    "    show_few_shot_best_practices()\n",
    "\n",
    "\n",
    "def date_extraction_template():\n",
    "    \"\"\"Create few-shot template for extracting dates from text.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc5 TASK 1: EXTRACTING DATES FROM TEXT\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    template = \"\"\"Extract all dates from the given text and format them as YYYY-MM-DD.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Text: \"The meeting is scheduled for January 15th, 2024.\"\n",
    "Dates: 2024-01-15\n",
    "\n",
    "Text: \"We launched on 03/20/2023 and had our first update by April 2023.\"\n",
    "Dates: 2023-03-20, 2023-04-01 (assuming first of month when day not specified)\n",
    "\n",
    "Text: \"The contract expires next Tuesday, which is the 5th of December.\"\n",
    "Dates: 2024-12-05 (assuming current year when not specified)\n",
    "\n",
    "Text: \"Founded in 1995, the company went public on July 4, 2010.\"\n",
    "Dates: 1995-01-01 (year only), 2010-07-04\n",
    "\n",
    "Text: \"The deadline was yesterday, and the review is tomorrow.\"\n",
    "Dates: [Relative dates - need current date context]\n",
    "\n",
    "Now extract dates from this text:\n",
    "Text: {input_text}\n",
    "Dates:\"\"\"\n",
    "    \n",
    "    print(\"TEMPLATE:\")\n",
    "    print(template)\n",
    "    \n",
    "    # Show how to use it\n",
    "    print(\"\\n\ud83d\udcbb USAGE EXAMPLE:\")\n",
    "    usage = \"\"\"\n",
    "def extract_dates(text):\n",
    "    prompt = template.format(input_text=text)\n",
    "    # Send to LLM\n",
    "    response = llm_call(prompt, temperature=0.1)\n",
    "    # Parse response\n",
    "    dates = parse_date_response(response)\n",
    "    return dates\n",
    "\n",
    "# Test\n",
    "text = \"The project started on May 1st and ends December 31, 2024\"\n",
    "dates = extract_dates(text)\n",
    "# Returns: [\"2024-05-01\", \"2024-12-31\"]\n",
    "    \"\"\"\n",
    "    print(usage)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca KEY FEATURES:\")\n",
    "    print(\"\u2022 Multiple examples showing different date formats\")\n",
    "    print(\"\u2022 Handles partial dates (year only, month-year)\")\n",
    "    print(\"\u2022 Shows how to handle relative dates\")\n",
    "    print(\"\u2022 Clear output format specification\")\n",
    "    print(\"\u2022 Edge cases included\")\n",
    "\n",
    "\n",
    "def support_classification_template():\n",
    "    \"\"\"Create few-shot template for classifying customer support tickets.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83c\udfab TASK 2: CLASSIFYING CUSTOMER SUPPORT TICKETS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    template = \"\"\"Classify the customer support ticket into one of these categories:\n",
    "- TECHNICAL: Software bugs, errors, technical issues\n",
    "- BILLING: Payment, subscription, refund issues\n",
    "- FEATURE: Feature requests, suggestions\n",
    "- ACCOUNT: Login, password, account management\n",
    "- OTHER: Doesn't fit other categories\n",
    "\n",
    "Also assign priority: HIGH, MEDIUM, or LOW\n",
    "\n",
    "Examples:\n",
    "\n",
    "Ticket: \"I can't log into my account. I've tried resetting my password three times.\"\n",
    "Category: ACCOUNT\n",
    "Priority: HIGH\n",
    "\n",
    "Ticket: \"When will you add dark mode? It would really help with eye strain.\"\n",
    "Category: FEATURE\n",
    "Priority: LOW\n",
    "\n",
    "Ticket: \"I was charged twice for my monthly subscription. Please refund the duplicate charge.\"\n",
    "Category: BILLING\n",
    "Priority: HIGH\n",
    "\n",
    "Ticket: \"The app crashes whenever I try to upload a file larger than 10MB.\"\n",
    "Category: TECHNICAL\n",
    "Priority: HIGH\n",
    "\n",
    "Ticket: \"How do I change my notification settings? I can't find the option.\"\n",
    "Category: ACCOUNT\n",
    "Priority: LOW\n",
    "\n",
    "Ticket: \"Your service is great! Just wanted to say thanks.\"\n",
    "Category: OTHER\n",
    "Priority: LOW\n",
    "\n",
    "Now classify this ticket:\n",
    "Ticket: {ticket_text}\n",
    "Category:\n",
    "Priority:\"\"\"\n",
    "    \n",
    "    print(\"TEMPLATE:\")\n",
    "    print(template)\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf CLASSIFICATION SCHEMA:\")\n",
    "    schema = {\n",
    "        \"TECHNICAL\": [\"bug\", \"error\", \"crash\", \"slow\", \"broken\", \"not working\"],\n",
    "        \"BILLING\": [\"charge\", \"payment\", \"refund\", \"invoice\", \"subscription\", \"price\"],\n",
    "        \"FEATURE\": [\"add\", \"request\", \"would be nice\", \"suggestion\", \"improve\"],\n",
    "        \"ACCOUNT\": [\"login\", \"password\", \"email\", \"profile\", \"settings\", \"can't access\"],\n",
    "        \"OTHER\": [\"thanks\", \"praise\", \"general question\", \"feedback\"]\n",
    "    }\n",
    "    \n",
    "    for category, keywords in schema.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"  Keywords: {', '.join(keywords[:4])}...\")\n",
    "    \n",
    "    print(\"\\n\u26a1 PRIORITY RULES:\")\n",
    "    print(\"\u2022 HIGH: Can't use service, money issues, data loss\")\n",
    "    print(\"\u2022 MEDIUM: Functionality impaired but workarounds exist\")\n",
    "    print(\"\u2022 LOW: Questions, suggestions, minor inconveniences\")\n",
    "\n",
    "\n",
    "def formality_conversion_template():\n",
    "    \"\"\"Create few-shot template for converting informal to formal text.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udc54 TASK 3: INFORMAL TO FORMAL BUSINESS LANGUAGE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    template = \"\"\"Convert informal text to professional business language while preserving the meaning.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Informal: \"Hey, just wanted to check if you got my email about the project?\"\n",
    "Formal: \"I am writing to inquire whether you have received my email regarding the project.\"\n",
    "\n",
    "Informal: \"Can we push the meeting? Something came up.\"\n",
    "Formal: \"Would it be possible to reschedule our meeting? An urgent matter has arisen that requires my immediate attention.\"\n",
    "\n",
    "Informal: \"The numbers look kinda weird. Mind taking another look?\"\n",
    "Formal: \"Upon review, the figures appear to contain some inconsistencies. Could you please verify the data?\"\n",
    "\n",
    "Informal: \"Thanks for getting back to me so fast!\"\n",
    "Formal: \"Thank you for your prompt response.\"\n",
    "\n",
    "Informal: \"Sorry for the late reply - things have been crazy here.\"\n",
    "Formal: \"I apologize for the delayed response. We have been experiencing an unusually high volume of activity.\"\n",
    "\n",
    "Informal: \"Let me know if you need anything else!\"\n",
    "Formal: \"Please do not hesitate to contact me if you require any additional assistance.\"\n",
    "\n",
    "Now convert this text:\n",
    "Informal: {informal_text}\n",
    "Formal:\"\"\"\n",
    "    \n",
    "    print(\"TEMPLATE:\")\n",
    "    print(template)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcdd TRANSFORMATION PATTERNS:\")\n",
    "    \n",
    "    patterns = [\n",
    "        (\"Contractions\", \"don't \u2192 do not, can't \u2192 cannot\"),\n",
    "        (\"Casual greetings\", \"Hey \u2192 Good morning/afternoon\"),\n",
    "        (\"Colloquialisms\", \"kinda \u2192 somewhat, stuff \u2192 matters\"),\n",
    "        (\"Informal phrases\", \"ASAP \u2192 at your earliest convenience\"),\n",
    "        (\"Casual closings\", \"Thanks! \u2192 Thank you for your consideration\"),\n",
    "        (\"Slang/shortcuts\", \"info \u2192 information, docs \u2192 documents\")\n",
    "    ]\n",
    "    \n",
    "    for pattern, example in patterns:\n",
    "        print(f\"\\n\u2022 {pattern}:\")\n",
    "        print(f\"  {example}\")\n",
    "    \n",
    "    print(\"\\n\u2728 ADVANCED TEMPLATE VARIATION:\")\n",
    "    advanced = \"\"\"\n",
    "# For different formality levels:\n",
    "\n",
    "Level 1 (Casual Professional):\n",
    "- Friendly but professional\n",
    "- Some contractions OK\n",
    "- Personal pronouns acceptable\n",
    "\n",
    "Level 2 (Standard Business):\n",
    "- No contractions\n",
    "- Third person preferred\n",
    "- Standard business phrases\n",
    "\n",
    "Level 3 (Highly Formal):\n",
    "- Very formal language\n",
    "- Passive voice when appropriate\n",
    "- Traditional business conventions\n",
    "    \"\"\"\n",
    "    print(advanced)\n",
    "\n",
    "\n",
    "def show_few_shot_best_practices():\n",
    "    \"\"\"Show best practices for creating few-shot templates.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEW-SHOT TEMPLATE BEST PRACTICES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    practices = [\n",
    "        {\n",
    "            \"principle\": \"Diverse Examples\",\n",
    "            \"description\": \"Include 3-6 examples covering different cases\",\n",
    "            \"why\": \"Shows pattern variations and edge cases\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Consistent Format\",\n",
    "            \"description\": \"Use exact same format for all examples\",\n",
    "            \"why\": \"Makes pattern recognition easier for LLM\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Progressive Complexity\",\n",
    "            \"description\": \"Start simple, add complexity gradually\",\n",
    "            \"why\": \"Builds understanding step by step\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Include Edge Cases\",\n",
    "            \"description\": \"Show handling of exceptions and special cases\",\n",
    "            \"why\": \"Prevents failures on unusual inputs\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Clear Delimiters\",\n",
    "            \"description\": \"Use clear markers between sections\",\n",
    "            \"why\": \"Helps LLM parse structure correctly\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Output Specification\",\n",
    "            \"description\": \"Show exact desired output format\",\n",
    "            \"why\": \"Ensures consistent, parseable responses\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for p in practices:\n",
    "        print(f\"\\n\ud83d\udccb {p['principle']}\")\n",
    "        print(f\"   {p['description']}\")\n",
    "        print(f\"   Why: {p['why']}\")\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf TEMPLATE STRUCTURE:\")\n",
    "    structure = \"\"\"\n",
    "[TASK DESCRIPTION]\n",
    "Clear explanation of what to do\n",
    "\n",
    "[EXAMPLES]\n",
    "Input: [example 1 input]\n",
    "Output: [example 1 output]\n",
    "\n",
    "Input: [example 2 input]\n",
    "Output: [example 2 output]\n",
    "\n",
    "Input: [example 3 input]\n",
    "Output: [example 3 output]\n",
    "\n",
    "[ACTUAL TASK]\n",
    "Input: {user_input}\n",
    "Output:\n",
    "    \"\"\"\n",
    "    print(structure)\n",
    "\n",
    "\n",
    "def create_universal_few_shot_template():\n",
    "    \"\"\"Create a universal few-shot template generator.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"UNIVERSAL FEW-SHOT TEMPLATE GENERATOR\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    code = '''\n",
    "class FewShotTemplate:\n",
    "    \"\"\"Universal few-shot template generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, task_description: str):\n",
    "        self.task_description = task_description\n",
    "        self.examples = []\n",
    "    \n",
    "    def add_example(self, input_text: str, output_text: str, note: str = \"\"):\n",
    "        \"\"\"Add an example to the template.\"\"\"\n",
    "        self.examples.append({\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"note\": note\n",
    "        })\n",
    "    \n",
    "    def generate_prompt(self, user_input: str) -> str:\n",
    "        \"\"\"Generate the complete few-shot prompt.\"\"\"\n",
    "        prompt = f\"{self.task_description}\\\\n\\\\n\"\n",
    "        \n",
    "        if self.examples:\n",
    "            prompt += \"Examples:\\\\n\\\\n\"\n",
    "            for i, ex in enumerate(self.examples, 1):\n",
    "                prompt += f\"Example {i}:\\\\n\"\n",
    "                prompt += f\"Input: {ex['input']}\\\\n\"\n",
    "                prompt += f\"Output: {ex['output']}\\\\n\"\n",
    "                if ex['note']:\n",
    "                    prompt += f\"Note: {ex['note']}\\\\n\"\n",
    "                prompt += \"\\\\n\"\n",
    "        \n",
    "        prompt += f\"Now process this:\\\\n\"\n",
    "        prompt += f\"Input: {user_input}\\\\n\"\n",
    "        prompt += f\"Output:\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "# Usage example\n",
    "template = FewShotTemplate(\"Extract key points from text as bullet points\")\n",
    "template.add_example(\n",
    "    \"The weather is nice today. It's sunny and warm.\",\n",
    "    \"\u2022 Weather is nice\\\\n\u2022 Sunny conditions\\\\n\u2022 Warm temperature\"\n",
    ")\n",
    "template.add_example(\n",
    "    \"The project deadline is Friday. We need to finish the report.\",\n",
    "    \"\u2022 Project deadline: Friday\\\\n\u2022 Report needs completion\"\n",
    ")\n",
    "\n",
    "prompt = template.generate_prompt(\"New user input here\")\n",
    "    '''\n",
    "    \n",
    "    print(\"IMPLEMENTATION:\")\n",
    "    print(code)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run few-shot template creation exercise.\"\"\"\n",
    "    \n",
    "    # Create templates\n",
    "    create_few_shot_templates()\n",
    "    \n",
    "    # Universal generator\n",
    "    create_universal_few_shot_template()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 2 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You've learned to create effective few-shot templates!\")\n",
    "    print(\"   Remember: Clear examples + consistent format = better results!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.3: Role-Based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_7_4_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Role-Based Prompting\n",
    "Create system prompts for different AI assistant personalities.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AssistantRole:\n",
    "    \"\"\"Definition of an AI assistant role.\"\"\"\n",
    "    name: str\n",
    "    system_prompt: str\n",
    "    example_interactions: List[Dict[str, str]]\n",
    "    key_behaviors: List[str]\n",
    "    avoid_behaviors: List[str]\n",
    "\n",
    "\n",
    "def create_role_based_prompts():\n",
    "    \"\"\"\n",
    "    Create system prompts for different AI assistant roles.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 3: ROLE-BASED PROMPTING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create each role\n",
    "    socratic_tutor = create_socratic_tutor()\n",
    "    security_reviewer = create_security_code_reviewer()\n",
    "    creative_partner = create_creative_writing_partner()\n",
    "    \n",
    "    # Display each role\n",
    "    display_role(socratic_tutor)\n",
    "    display_role(security_reviewer)\n",
    "    display_role(creative_partner)\n",
    "    \n",
    "    # Show role design principles\n",
    "    show_role_design_principles()\n",
    "    \n",
    "    # Bonus roles\n",
    "    show_bonus_roles()\n",
    "\n",
    "\n",
    "def create_socratic_tutor() -> AssistantRole:\n",
    "    \"\"\"Create a Socratic tutor who guides through questions.\"\"\"\n",
    "    \n",
    "    return AssistantRole(\n",
    "        name=\"Socratic Tutor\",\n",
    "        system_prompt=\"\"\"You are a Socratic tutor who helps students learn by asking thoughtful questions rather than providing direct answers. Your goal is to guide students to discover knowledge themselves through critical thinking.\n",
    "\n",
    "Core Principles:\n",
    "1. Never give direct answers initially - always respond with guiding questions\n",
    "2. Break complex problems into smaller, manageable questions\n",
    "3. Help students identify what they already know\n",
    "4. Guide them to connect existing knowledge to new concepts\n",
    "5. Only provide direct instruction after several rounds of questioning, if the student is stuck\n",
    "\n",
    "Question Types to Use:\n",
    "- Clarification: \"What do you mean by...?\"\n",
    "- Assumption examination: \"What assumptions are you making?\"\n",
    "- Evidence and reasoning: \"What evidence supports that?\"\n",
    "- Perspective shifting: \"How would this look from another angle?\"\n",
    "- Implications: \"What follows from what you said?\"\n",
    "- Questions about questions: \"Why is that question important?\"\n",
    "\n",
    "Interaction Style:\n",
    "- Patient and encouraging\n",
    "- Celebrate insights and breakthroughs\n",
    "- Acknowledge effort, not just correct answers\n",
    "- Use \"I wonder...\" and \"What if...\" to spark curiosity\n",
    "\n",
    "When a student is frustrated:\n",
    "- Provide smaller stepping stones\n",
    "- Offer multiple choice questions to guide thinking\n",
    "- Give hints through analogies\n",
    "- Affirm their progress so far\n",
    "\n",
    "Remember: Your goal is not to show knowledge but to develop the student's thinking skills.\"\"\",\n",
    "        \n",
    "        example_interactions=[\n",
    "            {\n",
    "                \"student\": \"What is recursion in programming?\",\n",
    "                \"tutor\": \"That's a great question! Before we dive in, can you think of any real-world situations where something refers to itself? For example, have you ever seen a mirror reflecting another mirror?\"\n",
    "            },\n",
    "            {\n",
    "                \"student\": \"I guess... like nested Russian dolls?\",\n",
    "                \"tutor\": \"Excellent example! Russian dolls contain smaller versions of themselves. Now, thinking about that concept, what do you think might happen if a function in programming could call itself? What would be necessary to prevent it from continuing forever like infinite mirror reflections?\"\n",
    "            },\n",
    "            {\n",
    "                \"student\": \"It would need a way to stop?\",\n",
    "                \"tutor\": \"Exactly! You've just discovered what we call a 'base case.' Now, can you imagine a simple task that might be easier to solve by breaking it into a smaller version of the same task?\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        key_behaviors=[\n",
    "            \"Ask guiding questions instead of lecturing\",\n",
    "            \"Build on student's existing knowledge\",\n",
    "            \"Encourage exploration and mistakes\",\n",
    "            \"Use analogies and real-world examples\",\n",
    "            \"Validate thinking process, not just answers\"\n",
    "        ],\n",
    "        \n",
    "        avoid_behaviors=[\n",
    "            \"Giving answers too quickly\",\n",
    "            \"Using complex jargon without exploration\",\n",
    "            \"Showing impatience with wrong answers\",\n",
    "            \"Asking yes/no questions exclusively\",\n",
    "            \"Making students feel inadequate\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_security_code_reviewer() -> AssistantRole:\n",
    "    \"\"\"Create a code reviewer focusing on security.\"\"\"\n",
    "    \n",
    "    return AssistantRole(\n",
    "        name=\"Security-Focused Code Reviewer\",\n",
    "        system_prompt=\"\"\"You are a senior security engineer conducting code reviews with a focus on identifying and preventing security vulnerabilities. Your reviews are thorough, constructive, and educational.\n",
    "\n",
    "Review Priorities (in order):\n",
    "1. Critical security vulnerabilities (injection, auth bypass, data exposure)\n",
    "2. Authentication and authorization issues\n",
    "3. Input validation and sanitization\n",
    "4. Cryptographic weaknesses\n",
    "5. Configuration security\n",
    "6. Dependencies and third-party risks\n",
    "7. Code quality affecting security\n",
    "\n",
    "For Each Issue Found:\n",
    "- Severity: CRITICAL/HIGH/MEDIUM/LOW\n",
    "- Vulnerability type (OWASP category if applicable)\n",
    "- Specific line numbers and code sections\n",
    "- Clear explanation of the risk\n",
    "- Proof of concept or attack scenario\n",
    "- Recommended fix with code example\n",
    "- References to security best practices\n",
    "\n",
    "Review Style:\n",
    "- Start with a security summary\n",
    "- Group findings by severity\n",
    "- Provide actionable recommendations\n",
    "- Include positive feedback on good security practices\n",
    "- Educate, don't just criticize\n",
    "\n",
    "Security Checklist:\n",
    "\u25a1 SQL/NoSQL injection vulnerabilities\n",
    "\u25a1 XSS (stored, reflected, DOM-based)\n",
    "\u25a1 Authentication/session management\n",
    "\u25a1 Access control and authorization\n",
    "\u25a1 Sensitive data exposure\n",
    "\u25a1 Security misconfiguration\n",
    "\u25a1 Vulnerable dependencies\n",
    "\u25a1 Insufficient logging and monitoring\n",
    "\u25a1 Rate limiting and DOS protection\n",
    "\u25a1 CSRF protections\n",
    "\n",
    "Output Format:\n",
    "```\n",
    "SECURITY REVIEW SUMMARY\n",
    "=======================\n",
    "Risk Level: [CRITICAL/HIGH/MEDIUM/LOW]\n",
    "Issues Found: X Critical, Y High, Z Medium, W Low\n",
    "\n",
    "CRITICAL ISSUES\n",
    "---------------\n",
    "[Detailed findings]\n",
    "\n",
    "RECOMMENDATIONS\n",
    "---------------\n",
    "[Priority actions]\n",
    "```\n",
    "\n",
    "Remember: The goal is to improve security while helping developers learn secure coding practices.\"\"\",\n",
    "        \n",
    "        example_interactions=[\n",
    "            {\n",
    "                \"developer\": \"Review this login function:\\ndef login(username, password):\\n    query = f\\\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\\\"\\n    return db.execute(query)\",\n",
    "                \"reviewer\": \"\"\"SECURITY REVIEW - CRITICAL ISSUES FOUND\n",
    "=============================================\n",
    "Risk Level: CRITICAL\n",
    "Issues Found: 2 Critical, 1 High\n",
    "\n",
    "CRITICAL ISSUE #1: SQL Injection\n",
    "---------------------------------\n",
    "Line 2: query = f\\\"SELECT * FROM users WHERE username='{username}'...\\\"\n",
    "\n",
    "Vulnerability: Direct string interpolation in SQL query allows SQL injection\n",
    "Attack Example: username = \"admin' OR '1'='1' --\"\n",
    "Impact: Complete database compromise, authentication bypass\n",
    "\n",
    "FIX REQUIRED:\n",
    "```python\n",
    "def login(username, password):\n",
    "    query = \"SELECT * FROM users WHERE username = ? AND password = ?\"\n",
    "    return db.execute(query, (username, hash_password(password)))\n",
    "```\n",
    "\n",
    "CRITICAL ISSUE #2: Plain Text Password Storage\n",
    "----------------------------------------------\n",
    "The query suggests passwords are stored in plain text.\n",
    "\n",
    "FIX REQUIRED: Implement proper password hashing:\n",
    "```python\n",
    "import bcrypt\n",
    "hashed = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
    "```\n",
    "\n",
    "Would you like me to explain the full secure authentication flow?\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        key_behaviors=[\n",
    "            \"Prioritize security issues by real-world impact\",\n",
    "            \"Provide specific, actionable fixes\",\n",
    "            \"Educate about security principles\",\n",
    "            \"Include attack scenarios for context\",\n",
    "            \"Reference security standards (OWASP, CWE)\"\n",
    "        ],\n",
    "        \n",
    "        avoid_behaviors=[\n",
    "            \"Being condescending about security mistakes\",\n",
    "            \"Providing fixes without explanation\",\n",
    "            \"Ignoring minor issues that could escalate\",\n",
    "            \"Overwhelming with too many low-priority issues\",\n",
    "            \"Using security jargon without explanation\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_creative_writing_partner() -> AssistantRole:\n",
    "    \"\"\"Create a creative writing partner for brainstorming.\"\"\"\n",
    "    \n",
    "    return AssistantRole(\n",
    "        name=\"Creative Writing Brainstorm Partner\",\n",
    "        system_prompt=\"\"\"You are an enthusiastic creative writing partner who helps writers brainstorm ideas, develop stories, and overcome creative blocks. You're collaborative, inspiring, and love exploring possibilities.\n",
    "\n",
    "Brainstorming Approach:\n",
    "- \"Yes, and...\" mindset - build on every idea\n",
    "- No idea is too wild during brainstorming\n",
    "- Connect unexpected elements for originality\n",
    "- Ask \"What if?\" frequently\n",
    "- Explore multiple directions before settling\n",
    "\n",
    "Creative Techniques to Use:\n",
    "1. Mind mapping from a central concept\n",
    "2. \"Worst idea first\" to break mental blocks\n",
    "3. Random word/image association\n",
    "4. Genre blending and mashups\n",
    "5. Reversing expectations and tropes\n",
    "6. \"But, therefore\" story structure\n",
    "7. Character-driven vs plot-driven exploration\n",
    "\n",
    "For Story Development:\n",
    "- Characters: Motivations, flaws, growth arcs\n",
    "- World: Rules, atmosphere, unique elements\n",
    "- Conflict: Internal vs external, stakes\n",
    "- Theme: What's the deeper meaning?\n",
    "- Voice: Tone, style, perspective\n",
    "\n",
    "Interaction Style:\n",
    "- Enthusiastic and energetic\n",
    "- Use vivid, sensory language\n",
    "- Offer multiple options, not single solutions\n",
    "- Ask provocative questions\n",
    "- Share excitement about possibilities\n",
    "- Celebrate creative breakthroughs\n",
    "\n",
    "When helping with blocks:\n",
    "- Suggest writing exercises\n",
    "- Change perspective or POV\n",
    "- Explore the opposite approach\n",
    "- Break into smaller pieces\n",
    "- Use constraints as creative fuel\n",
    "\n",
    "Output Format Examples:\n",
    "\"Ooh, what if we took that idea and...\"\n",
    "\"I love that! It reminds me of... but with a twist where...\"\n",
    "\"Let's explore three wildly different directions...\"\n",
    "\"What would happen if we reversed...\"\n",
    "\n",
    "Remember: Your role is to expand possibilities, not judge or limit. Every session should leave the writer excited and full of ideas!\"\"\",\n",
    "        \n",
    "        example_interactions=[\n",
    "            {\n",
    "                \"writer\": \"I want to write about a detective story but I'm stuck on making it unique.\",\n",
    "                \"partner\": \"\"\"Ooh, detective stories are ripe for fresh twists! Let's brainstorm some wild angles:\n",
    "\n",
    "What if...\n",
    "1. The detective is actually the murder victim, solving their own death as a ghost that only one person can see?\n",
    "\n",
    "2. It's set in a world where everyone can read minds EXCEPT the detective - making them the only person who can actually solve crimes?\n",
    "\n",
    "3. The detective is a sentient AI in a smart home, solving a murder using only IoT devices as witnesses?\n",
    "\n",
    "Let's go deeper - what aspect of traditional detective stories do you want to flip on its head? The lone wolf investigator? The reliable narrator? The idea that crimes should be solved at all?\n",
    "\n",
    "Also, what genres could we mash up with detective fiction? Detective + cosmic horror? Detective + romantic comedy? Detective + cooking show?\n",
    "\n",
    "Which direction sparks something for you? Or should we get even wilder?\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        key_behaviors=[\n",
    "            \"Generate multiple ideas rapidly\",\n",
    "            \"Build enthusiasm and energy\",\n",
    "            \"Connect disparate concepts\",\n",
    "            \"Ask 'what if' questions constantly\",\n",
    "            \"Celebrate creative risks\"\n",
    "        ],\n",
    "        \n",
    "        avoid_behaviors=[\n",
    "            \"Dismissing ideas as 'too weird'\",\n",
    "            \"Focusing on marketability during brainstorming\",\n",
    "            \"Being overly practical or logical\",\n",
    "            \"Providing single 'correct' answers\",\n",
    "            \"Bringing up potential problems too early\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def display_role(role: AssistantRole):\n",
    "    \"\"\"Display a role configuration.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\ud83c\udfad {role.name.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nSYSTEM PROMPT:\")\n",
    "    print(\"-\"*50)\n",
    "    print(role.system_prompt)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcdd EXAMPLE INTERACTION:\")\n",
    "    print(\"-\"*50)\n",
    "    for interaction in role.example_interactions[:1]:  # Show first example\n",
    "        print(f\"User: {interaction['student' if 'student' in interaction else 'developer' if 'developer' in interaction else 'writer']}\")\n",
    "        print(f\"\\nAssistant: {interaction['tutor' if 'tutor' in interaction else 'reviewer' if 'reviewer' in interaction else 'partner']}\")\n",
    "    \n",
    "    print(\"\\n\u2705 KEY BEHAVIORS:\")\n",
    "    for behavior in role.key_behaviors:\n",
    "        print(f\"\u2022 {behavior}\")\n",
    "    \n",
    "    print(\"\\n\u274c AVOID:\")\n",
    "    for behavior in role.avoid_behaviors:\n",
    "        print(f\"\u2022 {behavior}\")\n",
    "\n",
    "\n",
    "def show_role_design_principles():\n",
    "    \"\"\"Show principles for designing effective role prompts.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ROLE DESIGN PRINCIPLES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    principles = [\n",
    "        {\n",
    "            \"principle\": \"Clear Identity\",\n",
    "            \"description\": \"Define who the AI is and their expertise\",\n",
    "            \"example\": \"You are a senior security engineer with 10 years experience...\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Specific Objectives\",\n",
    "            \"description\": \"State clear goals for the interaction\",\n",
    "            \"example\": \"Your goal is to help students discover knowledge themselves...\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Behavioral Guidelines\",\n",
    "            \"description\": \"Define how to act, not just what to know\",\n",
    "            \"example\": \"Always respond with guiding questions, never direct answers...\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Output Format\",\n",
    "            \"description\": \"Specify structure and style of responses\",\n",
    "            \"example\": \"Start with a summary, then detail findings by severity...\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Edge Case Handling\",\n",
    "            \"description\": \"Define behavior for difficult situations\",\n",
    "            \"example\": \"When a student is frustrated, provide smaller stepping stones...\"\n",
    "        },\n",
    "        {\n",
    "            \"principle\": \"Tone and Voice\",\n",
    "            \"description\": \"Establish consistent personality\",\n",
    "            \"example\": \"Be enthusiastic, use 'What if...' frequently...\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for p in principles:\n",
    "        print(f\"\\n\ud83d\udccb {p['principle']}\")\n",
    "        print(f\"   {p['description']}\")\n",
    "        print(f\"   Example: {p['example']}\")\n",
    "\n",
    "\n",
    "def show_bonus_roles():\n",
    "    \"\"\"Show additional role examples.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BONUS ROLE EXAMPLES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    bonus_roles = [\n",
    "        {\n",
    "            \"name\": \"Devil's Advocate\",\n",
    "            \"prompt_snippet\": \"Challenge every assertion respectfully. Ask for evidence. Point out logical fallacies. Explore counterarguments.\",\n",
    "            \"use_case\": \"Testing ideas, improving arguments\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Technical Translator\",\n",
    "            \"prompt_snippet\": \"Explain complex technical concepts using everyday analogies. No jargon without explanation.\",\n",
    "            \"use_case\": \"Making technical content accessible\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Empathetic Listener\",\n",
    "            \"prompt_snippet\": \"Focus on understanding emotions. Reflect feelings back. Ask open-ended questions. Never judge.\",\n",
    "            \"use_case\": \"Emotional support, therapy apps\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data Analyst\",\n",
    "            \"prompt_snippet\": \"Focus on patterns, correlations, and insights. Always ask for statistical significance. Suggest visualizations.\",\n",
    "            \"use_case\": \"Data interpretation and analysis\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Product Manager\",\n",
    "            \"prompt_snippet\": \"Focus on user value, feasibility, and business impact. Ask about metrics and success criteria.\",\n",
    "            \"use_case\": \"Product development discussions\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for role in bonus_roles:\n",
    "        print(f\"\\n\ud83c\udfad {role['name']}\")\n",
    "        print(f\"   Snippet: {role['prompt_snippet']}\")\n",
    "        print(f\"   Use Case: {role['use_case']}\")\n",
    "\n",
    "\n",
    "def create_role_template():\n",
    "    \"\"\"Create a reusable template for role creation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"REUSABLE ROLE TEMPLATE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    template = \"\"\"\n",
    "# [ROLE NAME] System Prompt\n",
    "\n",
    "You are a [SPECIFIC IDENTITY] who [PRIMARY PURPOSE].\n",
    "\n",
    "## Core Principles\n",
    "1. [Key principle 1]\n",
    "2. [Key principle 2]\n",
    "3. [Key principle 3]\n",
    "\n",
    "## Behavioral Guidelines\n",
    "- Always: [What to always do]\n",
    "- Never: [What to never do]\n",
    "- When [situation], then [response]\n",
    "\n",
    "## Communication Style\n",
    "- Tone: [Formal/Casual/etc]\n",
    "- Language: [Technical/Simple/etc]\n",
    "- Structure: [How to organize responses]\n",
    "\n",
    "## Specific Techniques\n",
    "- [Technique 1]: [How and when to use]\n",
    "- [Technique 2]: [How and when to use]\n",
    "\n",
    "## Output Format\n",
    "[Specify exact format for responses]\n",
    "\n",
    "## Edge Cases\n",
    "- If [edge case 1], then [handle like this]\n",
    "- If [edge case 2], then [handle like this]\n",
    "\n",
    "Remember: [Key reminder about role]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"TEMPLATE:\")\n",
    "    print(template)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run role-based prompting exercise.\"\"\"\n",
    "    \n",
    "    # Create roles\n",
    "    create_role_based_prompts()\n",
    "    \n",
    "    # Show template\n",
    "    create_role_template()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXERCISE 3 COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n\u2705 You've mastered role-based prompting!\")\n",
    "    print(\"   Remember: Clear identity + specific behaviors = consistent character!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4.4: Completion Control Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_4_7_4_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 4 Solution: Completion Control Experiment\n",
    "Experiment with different parameters to control LLM output.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Results from a parameter experiment.\"\"\"\n",
    "    temperature: float\n",
    "    max_tokens: int\n",
    "    stop_sequences: List[str]\n",
    "    prompt: str\n",
    "    output: str\n",
    "    observations: List[str]\n",
    "    metrics: Dict[str, Any]\n",
    "\n",
    "\n",
    "def run_completion_experiments():\n",
    "    \"\"\"\n",
    "    Run experiments with different completion control parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 4: COMPLETION CONTROL EXPERIMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Base prompt for all experiments\n",
    "    base_prompt = \"Write a description of a mysterious forest. The forest\"\n",
    "    \n",
    "    # Experiment 1: Temperature variations\n",
    "    temperature_experiment(base_prompt)\n",
    "    \n",
    "    # Experiment 2: Max tokens variations\n",
    "    max_tokens_experiment(base_prompt)\n",
    "    \n",
    "    # Experiment 3: Stop sequences\n",
    "    stop_sequence_experiment(base_prompt)\n",
    "    \n",
    "    # Experiment 4: Combined parameters\n",
    "    combined_parameters_experiment(base_prompt)\n",
    "    \n",
    "    # Analysis and insights\n",
    "    show_parameter_insights()\n",
    "    \n",
    "    # Practical applications\n",
    "    show_practical_applications()\n",
    "\n",
    "\n",
    "def temperature_experiment(base_prompt: str):\n",
    "    \"\"\"Experiment with different temperature settings.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83c\udf21\ufe0f EXPERIMENT 1: TEMPERATURE VARIATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Base prompt: \\\"{base_prompt}\\\"\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "    \n",
    "    results = []\n",
    "    for temp in temperatures:\n",
    "        result = ExperimentResult(\n",
    "            temperature=temp,\n",
    "            max_tokens=50,\n",
    "            stop_sequences=[],\n",
    "            prompt=base_prompt,\n",
    "            output=simulate_completion(base_prompt, temp, 50, []),\n",
    "            observations=analyze_temperature_output(temp),\n",
    "            metrics=calculate_metrics(temp)\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Display results\n",
    "    for result in results:\n",
    "        print(f\"\\n\ud83d\udcca Temperature: {result.temperature}\")\n",
    "        print(f\"Output: {result.output}\")\n",
    "        print(\"Observations:\")\n",
    "        for obs in result.observations:\n",
    "            print(f\"  \u2022 {obs}\")\n",
    "        print(f\"Metrics: Creativity={result.metrics['creativity']}, \"\n",
    "              f\"Coherence={result.metrics['coherence']}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udd0d TEMPERATURE INSIGHTS:\")\n",
    "    print(\"\u2022 T=0.0: Deterministic, same output every time, very predictable\")\n",
    "    print(\"\u2022 T=0.5: Slightly varied, maintains coherence, good for factual content\")\n",
    "    print(\"\u2022 T=1.0: Balanced creativity and coherence, good for general use\")\n",
    "    print(\"\u2022 T=1.5: Highly creative, may lose coherence, good for brainstorming\")\n",
    "\n",
    "\n",
    "def simulate_completion(prompt: str, temperature: float, \n",
    "                        max_tokens: int, stop_sequences: List[str]) -> str:\n",
    "    \"\"\"Simulate LLM completion with different parameters.\"\"\"\n",
    "    \n",
    "    # Simulated outputs based on temperature\n",
    "    outputs = {\n",
    "        0.0: \"is dark and dense with ancient trees. The tall pines block most sunlight, creating shadows everywhere. A narrow path winds through the undergrowth.\",\n",
    "        0.5: \"holds ancient secrets within its shadowy depths. Twisted oaks and towering pines create a canopy so thick that daylight barely penetrates.\",\n",
    "        1.0: \"whispers with voices of forgotten ages. Luminescent fungi paint ethereal patterns on bark while mist dances between gnarled roots that seem to breathe.\",\n",
    "        1.5: \"dreams in colors that don't exist, where time flows backward and trees sing lullabies to sleeping dragons made of starlight and shadow.\"\n",
    "    }\n",
    "    \n",
    "    output = outputs.get(temperature, outputs[1.0])\n",
    "    \n",
    "    # Apply max_tokens limit\n",
    "    words = output.split()\n",
    "    if max_tokens < 50:\n",
    "        words = words[:max_tokens // 4]  # Rough token estimation\n",
    "    output = \" \".join(words)\n",
    "    \n",
    "    # Apply stop sequences\n",
    "    for stop in stop_sequences:\n",
    "        if stop in output:\n",
    "            output = output.split(stop)[0] + stop\n",
    "            break\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def analyze_temperature_output(temperature: float) -> List[str]:\n",
    "    \"\"\"Analyze characteristics of output at different temperatures.\"\"\"\n",
    "    \n",
    "    analyses = {\n",
    "        0.0: [\n",
    "            \"Highly predictable word choices\",\n",
    "            \"Conventional descriptions\",\n",
    "            \"Consistent style\",\n",
    "            \"Safe, expected imagery\",\n",
    "            \"Suitable for technical documentation\"\n",
    "        ],\n",
    "        0.5: [\n",
    "            \"Some variety in word choice\",\n",
    "            \"Mostly conventional with occasional surprises\",\n",
    "            \"Good balance of consistency\",\n",
    "            \"Clear and coherent\",\n",
    "            \"Suitable for professional content\"\n",
    "        ],\n",
    "        1.0: [\n",
    "            \"Creative word combinations\",\n",
    "            \"Mix of expected and unexpected\",\n",
    "            \"Natural flow with variety\",\n",
    "            \"Engaging imagery\",\n",
    "            \"Suitable for creative writing\"\n",
    "        ],\n",
    "        1.5: [\n",
    "            \"Highly unpredictable\",\n",
    "            \"Unusual word combinations\",\n",
    "            \"May include surreal elements\",\n",
    "            \"Risk of incoherence\",\n",
    "            \"Suitable for brainstorming only\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return analyses.get(temperature, analyses[1.0])\n",
    "\n",
    "\n",
    "def calculate_metrics(temperature: float) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate metrics for different parameter settings.\"\"\"\n",
    "    \n",
    "    # Simulated metrics\n",
    "    creativity = min(10, temperature * 6.67)  # 0-10 scale\n",
    "    coherence = max(0, 10 - temperature * 5)  # 0-10 scale\n",
    "    predictability = max(0, 10 - temperature * 6.67)  # 0-10 scale\n",
    "    \n",
    "    return {\n",
    "        \"creativity\": round(creativity, 1),\n",
    "        \"coherence\": round(coherence, 1),\n",
    "        \"predictability\": round(predictability, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "def max_tokens_experiment(base_prompt: str):\n",
    "    \"\"\"Experiment with different max_tokens settings.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83d\udccf EXPERIMENT 2: MAX_TOKENS VARIATIONS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    token_limits = [50, 200, 500]\n",
    "    \n",
    "    for limit in token_limits:\n",
    "        print(f\"\\n\ud83c\udfaf Max Tokens: {limit}\")\n",
    "        \n",
    "        # Estimate output characteristics\n",
    "        if limit == 50:\n",
    "            print(\"Output: Short, ~2-3 sentences\")\n",
    "            print(\"Use case: Quick summaries, brief responses\")\n",
    "            print(\"Cost: Minimal\")\n",
    "            print(\"Speed: Very fast\")\n",
    "        elif limit == 200:\n",
    "            print(\"Output: Medium, ~1 paragraph\")\n",
    "            print(\"Use case: Standard responses, explanations\")\n",
    "            print(\"Cost: Moderate\")\n",
    "            print(\"Speed: Fast\")\n",
    "        else:  # 500\n",
    "            print(\"Output: Long, ~2-3 paragraphs\")\n",
    "            print(\"Use case: Detailed explanations, stories\")\n",
    "            print(\"Cost: Higher\")\n",
    "            print(\"Speed: Slower\")\n",
    "        \n",
    "        print(\"Considerations:\")\n",
    "        print(f\"  \u2022 Response may end mid-sentence if limit reached\")\n",
    "        print(f\"  \u2022 Actual tokens \u2260 words (roughly 1 token = 0.75 words)\")\n",
    "        print(f\"  \u2022 Include buffer for unexpected verbosity\")\n",
    "\n",
    "\n",
    "def stop_sequence_experiment(base_prompt: str):\n",
    "    \"\"\"Experiment with different stop sequences.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83d\uded1 EXPERIMENT 3: STOP SEQUENCES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    stop_configs = [\n",
    "        {\n",
    "            \"sequences\": [],\n",
    "            \"description\": \"No stop sequences\",\n",
    "            \"output\": \"continues until max_tokens or natural end\",\n",
    "            \"use_case\": \"Free-form generation\"\n",
    "        },\n",
    "        {\n",
    "            \"sequences\": [\"\\n\"],\n",
    "            \"description\": \"Stop at newline\",\n",
    "            \"output\": \"single line/paragraph only\",\n",
    "            \"use_case\": \"Single-line responses, titles\"\n",
    "        },\n",
    "        {\n",
    "            \"sequences\": [\".\", \"!\", \"?\"],\n",
    "            \"description\": \"Stop at sentence end\",\n",
    "            \"output\": \"exactly one sentence\",\n",
    "            \"use_case\": \"Concise responses, definitions\"\n",
    "        },\n",
    "        {\n",
    "            \"sequences\": [\"END\", \"---\"],\n",
    "            \"description\": \"Custom markers\",\n",
    "            \"output\": \"continues until marker\",\n",
    "            \"use_case\": \"Structured templates, forms\"\n",
    "        },\n",
    "        {\n",
    "            \"sequences\": [\"</answer>\"],\n",
    "            \"description\": \"XML-style tags\",\n",
    "            \"output\": \"structured response sections\",\n",
    "            \"use_case\": \"Parsing specific content\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for config in stop_configs:\n",
    "        print(f\"\\n\ud83d\udccd Stop Sequences: {config['sequences'] if config['sequences'] else 'None'}\")\n",
    "        print(f\"   Description: {config['description']}\")\n",
    "        print(f\"   Behavior: {config['output']}\")\n",
    "        print(f\"   Use Case: {config['use_case']}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 STOP SEQUENCE TIPS:\")\n",
    "    print(\"\u2022 Order matters - first match wins\")\n",
    "    print(\"\u2022 Include the stop sequence in output\")\n",
    "    print(\"\u2022 Use unique markers to avoid premature stops\")\n",
    "    print(\"\u2022 Test with your specific content type\")\n",
    "\n",
    "\n",
    "def combined_parameters_experiment(base_prompt: str):\n",
    "    \"\"\"Experiment with combined parameter effects.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfa8 EXPERIMENT 4: COMBINED PARAMETERS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    combinations = [\n",
    "        {\n",
    "            \"name\": \"Factual Assistant\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 150,\n",
    "            \"stop_sequences\": [\"\\n\\n\"],\n",
    "            \"use_case\": \"Technical documentation, facts\",\n",
    "            \"characteristics\": [\"Precise\", \"Consistent\", \"Brief\", \"Structured\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Creative Writer\",\n",
    "            \"temperature\": 0.9,\n",
    "            \"max_tokens\": 500,\n",
    "            \"stop_sequences\": [\"THE END\"],\n",
    "            \"use_case\": \"Stories, creative content\",\n",
    "            \"characteristics\": [\"Imaginative\", \"Varied\", \"Flowing\", \"Expansive\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Chatbot\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 200,\n",
    "            \"stop_sequences\": [],\n",
    "            \"use_case\": \"Conversational AI\",\n",
    "            \"characteristics\": [\"Natural\", \"Balanced\", \"Responsive\", \"Friendly\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Code Generator\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 300,\n",
    "            \"stop_sequences\": [\"```\", \"# End\"],\n",
    "            \"use_case\": \"Programming assistance\",\n",
    "            \"characteristics\": [\"Syntactically correct\", \"Functional\", \"Complete\", \"Commented\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Brainstorming Bot\",\n",
    "            \"temperature\": 1.2,\n",
    "            \"max_tokens\": 100,\n",
    "            \"stop_sequences\": [],\n",
    "            \"use_case\": \"Idea generation\",\n",
    "            \"characteristics\": [\"Wild\", \"Unexpected\", \"Brief bursts\", \"Provocative\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for combo in combinations:\n",
    "        print(f\"\\n\ud83c\udfaf {combo['name']}\")\n",
    "        print(f\"   Temperature: {combo['temperature']}\")\n",
    "        print(f\"   Max Tokens: {combo['max_tokens']}\")\n",
    "        print(f\"   Stop Sequences: {combo['stop_sequences']}\")\n",
    "        print(f\"   Use Case: {combo['use_case']}\")\n",
    "        print(f\"   Characteristics: {', '.join(combo['characteristics'])}\")\n",
    "\n",
    "\n",
    "def show_parameter_insights():\n",
    "    \"\"\"Show key insights about parameter interactions.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PARAMETER INTERACTION INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    insights = [\n",
    "        {\n",
    "            \"observation\": \"Temperature vs Max Tokens\",\n",
    "            \"insight\": \"High temperature needs more tokens to develop creative ideas fully\",\n",
    "            \"recommendation\": \"If temp > 1.0, use max_tokens > 200\"\n",
    "        },\n",
    "        {\n",
    "            \"observation\": \"Stop Sequences vs Temperature\",\n",
    "            \"insight\": \"High temperature may generate stop sequences unexpectedly\",\n",
    "            \"recommendation\": \"Use unique stop sequences with high temperature\"\n",
    "        },\n",
    "        {\n",
    "            \"observation\": \"Task vs Parameters\",\n",
    "            \"insight\": \"Match parameters to task requirements, not preferences\",\n",
    "            \"recommendation\": \"Create parameter presets for common tasks\"\n",
    "        },\n",
    "        {\n",
    "            \"observation\": \"Cost vs Quality\",\n",
    "            \"insight\": \"Lower temperature often needs fewer tokens for good results\",\n",
    "            \"recommendation\": \"Start with temp=0.3-0.5 for cost optimization\"\n",
    "        },\n",
    "        {\n",
    "            \"observation\": \"Consistency vs Variety\",\n",
    "            \"insight\": \"Production systems need predictability, creative tasks need variety\",\n",
    "            \"recommendation\": \"Use temp<0.3 for production, temp>0.7 for creative\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for item in insights:\n",
    "        print(f\"\\n\ud83d\udd0d {item['observation']}\")\n",
    "        print(f\"   Insight: {item['insight']}\")\n",
    "        print(f\"   \ud83d\udcdd Recommendation: {item['recommendation']}\")\n",
    "\n",
    "\n",
    "def show_practical_applications():\n",
    "    \"\"\"Show practical applications of parameter control.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PRACTICAL APPLICATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    applications = [\n",
    "        {\n",
    "            \"scenario\": \"Customer Service Bot\",\n",
    "            \"config\": \"temp=0.3, max_tokens=150, stop=['\\nCustomer:', '\\nAgent:']\",\n",
    "            \"why\": \"Consistent, professional, controlled length\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Blog Post Generator\",\n",
    "            \"config\": \"temp=0.7, max_tokens=800, stop=['</article>']\",\n",
    "            \"why\": \"Creative but coherent, full length allowed\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Data Extraction\",\n",
    "            \"config\": \"temp=0.0, max_tokens=100, stop=['\\\\n', ',']\",\n",
    "            \"why\": \"Deterministic parsing, structured output\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Code Documentation\",\n",
    "            \"config\": \"temp=0.2, max_tokens=200, stop=['```', '\\\"\\\"\\\"']\",\n",
    "            \"why\": \"Accurate, concise, properly formatted\"\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Story Writing\",\n",
    "            \"config\": \"temp=0.9, max_tokens=1000, stop=['THE END']\",\n",
    "            \"why\": \"Creative freedom with defined ending\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for app in applications:\n",
    "        print(f\"\\n\ud83d\udcf1 {app['scenario']}\")\n",
    "        print(f\"   Config: {app['config']}\")\n",
    "        print(f\"   Why: {app['why']}\")\n",
    "\n",
    "\n",
    "def create_parameter_testing_framework():\n",
    "    \"\"\"Create a framework for systematic parameter testing.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PARAMETER TESTING FRAMEWORK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    code = '''\n",
    "class ParameterTester:\n",
    "    \"\"\"Framework for systematic LLM parameter testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_prompt: str):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.results = []\n",
    "    \n",
    "    def test_temperature_range(self, temps: List[float], \n",
    "                               fixed_tokens: int = 100):\n",
    "        \"\"\"Test across temperature range.\"\"\"\n",
    "        for temp in temps:\n",
    "            output = llm_call(\n",
    "                prompt=self.base_prompt,\n",
    "                temperature=temp,\n",
    "                max_tokens=fixed_tokens\n",
    "            )\n",
    "            self.results.append({\n",
    "                \"temperature\": temp,\n",
    "                \"output\": output,\n",
    "                \"word_count\": len(output.split()),\n",
    "                \"unique_words\": len(set(output.split()))\n",
    "            })\n",
    "    \n",
    "    def test_token_limits(self, limits: List[int], \n",
    "                         fixed_temp: float = 0.7):\n",
    "        \"\"\"Test different token limits.\"\"\"\n",
    "        for limit in limits:\n",
    "            output = llm_call(\n",
    "                prompt=self.base_prompt,\n",
    "                temperature=fixed_temp,\n",
    "                max_tokens=limit\n",
    "            )\n",
    "            self.results.append({\n",
    "                \"max_tokens\": limit,\n",
    "                \"output\": output,\n",
    "                \"actual_length\": len(output),\n",
    "                \"truncated\": not output.endswith(('.', '!', '?'))\n",
    "            })\n",
    "    \n",
    "    def analyze_results(self) -> Dict:\n",
    "        \"\"\"Analyze test results.\"\"\"\n",
    "        return {\n",
    "            \"total_tests\": len(self.results),\n",
    "            \"avg_length\": sum(len(r[\"output\"]) for r in self.results) / len(self.results),\n",
    "            \"variety_score\": self.calculate_variety(),\n",
    "            \"optimal_settings\": self.find_optimal()\n",
    "        }\n",
    "    \n",
    "    def export_results(self, filename: str):\n",
    "        \"\"\"Export results for analysis.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "\n",
    "# Usage\n",
    "tester = ParameterTester(\"Write about artificial intelligence\")\n",
    "tester.test_temperature_range([0, 0.3, 0.5, 0.7, 1.0, 1.3])\n",
    "tester.test_token_limits([50, 100, 200, 500])\n",
    "analysis = tester.analyze_results()\n",
    "    '''\n",
    "    \n",
    "    print(\"IMPLEMENTATION:\")\n",
    "    print(code)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run completion control experiments.\"\"\"\n",
    "    \n",
    "    # Run experiments\n",
    "    run_completion_experiments()\n",
    "    \n",
    "    # Testing framework\n",
    "    create_parameter_testing_framework()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXERCISE 4 COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n\u2705 You now understand how parameters control LLM output!\")\n",
    "    print(\"   Remember: Test systematically, document results, create presets!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.5 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.1: Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_5_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: Cost Calculator\n",
    "Calculate monthly costs for different providers based on usage patterns.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Provider:\n",
    "    \"\"\"LLM Provider configuration.\"\"\"\n",
    "    name: str\n",
    "    models: Dict[str, Dict[str, float]]  # model -> {input_cost, output_cost}\n",
    "    free_tier: Dict[str, int]  # model -> free tokens\n",
    "    rate_limits: Dict[str, int]  # model -> requests per minute\n",
    "\n",
    "\n",
    "def create_cost_calculator():\n",
    "    \"\"\"Create a comprehensive cost calculator for LLM providers.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 1: LLM COST CALCULATOR\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define providers\n",
    "    providers = setup_providers()\n",
    "    \n",
    "    # Define usage scenarios\n",
    "    scenarios = define_usage_scenarios()\n",
    "    \n",
    "    # Calculate costs for each scenario\n",
    "    for scenario in scenarios:\n",
    "        print(f\"\\n\ud83d\udcca Scenario: {scenario['name']}\")\n",
    "        print(f\"   Daily messages: {scenario['daily_messages']}\")\n",
    "        print(f\"   Avg input tokens: {scenario['avg_input_tokens']}\")\n",
    "        print(f\"   Avg output tokens: {scenario['avg_output_tokens']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for provider in providers:\n",
    "            monthly_cost = calculate_monthly_cost(\n",
    "                provider,\n",
    "                scenario['daily_messages'],\n",
    "                scenario['avg_input_tokens'],\n",
    "                scenario['avg_output_tokens'],\n",
    "                scenario['model_preference']\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n   {provider.name}:\")\n",
    "            for model, cost in monthly_cost.items():\n",
    "                if cost > 0:\n",
    "                    print(f\"      {model}: ${cost:.2f}/month\")\n",
    "\n",
    "\n",
    "def setup_providers() -> List[Provider]:\n",
    "    \"\"\"Setup provider configurations with current pricing.\"\"\"\n",
    "    \n",
    "    return [\n",
    "        Provider(\n",
    "            name=\"OpenAI\",\n",
    "            models={\n",
    "                \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},  # per 1K tokens\n",
    "                \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "                \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
    "            },\n",
    "            free_tier={\"gpt-3.5-turbo\": 0},\n",
    "            rate_limits={\"gpt-3.5-turbo\": 90, \"gpt-4\": 40}\n",
    "        ),\n",
    "        Provider(\n",
    "            name=\"Anthropic\",\n",
    "            models={\n",
    "                \"claude-3-haiku\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "                \"claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n",
    "                \"claude-3-opus\": {\"input\": 0.015, \"output\": 0.075}\n",
    "            },\n",
    "            free_tier={},\n",
    "            rate_limits={\"claude-3-haiku\": 50, \"claude-3-sonnet\": 40}\n",
    "        ),\n",
    "        Provider(\n",
    "            name=\"Google\",\n",
    "            models={\n",
    "                \"gemini-pro\": {\"input\": 0.000125, \"output\": 0.000375},\n",
    "                \"gemini-ultra\": {\"input\": 0.007, \"output\": 0.021}\n",
    "            },\n",
    "            free_tier={\"gemini-pro\": 60},  # free requests per minute\n",
    "            rate_limits={\"gemini-pro\": 60}\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def define_usage_scenarios() -> List[Dict]:\n",
    "    \"\"\"Define different chatbot usage scenarios.\"\"\"\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"name\": \"Small Startup (Customer Support)\",\n",
    "            \"daily_messages\": 100,\n",
    "            \"avg_input_tokens\": 150,\n",
    "            \"avg_output_tokens\": 200,\n",
    "            \"model_preference\": \"cheapest\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medium Business (Sales Assistant)\",\n",
    "            \"daily_messages\": 500,\n",
    "            \"avg_input_tokens\": 200,\n",
    "            \"avg_output_tokens\": 300,\n",
    "            \"model_preference\": \"balanced\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Enterprise (Technical Support)\",\n",
    "            \"daily_messages\": 2000,\n",
    "            \"avg_input_tokens\": 300,\n",
    "            \"avg_output_tokens\": 500,\n",
    "            \"model_preference\": \"quality\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AI Coding Assistant\",\n",
    "            \"daily_messages\": 50,\n",
    "            \"avg_input_tokens\": 500,\n",
    "            \"avg_output_tokens\": 800,\n",
    "            \"model_preference\": \"quality\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Content Generation Platform\",\n",
    "            \"daily_messages\": 1000,\n",
    "            \"avg_input_tokens\": 100,\n",
    "            \"avg_output_tokens\": 1000,\n",
    "            \"model_preference\": \"balanced\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "def calculate_monthly_cost(provider: Provider, daily_messages: int,\n",
    "                          avg_input: int, avg_output: int, \n",
    "                          preference: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate monthly costs for a provider.\"\"\"\n",
    "    \n",
    "    monthly_messages = daily_messages * 30\n",
    "    total_input_tokens = monthly_messages * avg_input\n",
    "    total_output_tokens = monthly_messages * avg_output\n",
    "    \n",
    "    costs = {}\n",
    "    \n",
    "    for model, pricing in provider.models.items():\n",
    "        # Skip expensive models for \"cheapest\" preference\n",
    "        if preference == \"cheapest\" and (\"gpt-4\" in model or \"opus\" in model):\n",
    "            continue\n",
    "        \n",
    "        # Skip cheap models for \"quality\" preference  \n",
    "        if preference == \"quality\" and (\"haiku\" in model or \"gpt-3.5\" in model):\n",
    "            continue\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_cost = (total_input_tokens / 1000) * pricing[\"input\"]\n",
    "        output_cost = (total_output_tokens / 1000) * pricing[\"output\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        costs[model] = total_cost\n",
    "    \n",
    "    return costs\n",
    "\n",
    "\n",
    "def create_detailed_calculator():\n",
    "    \"\"\"Create a detailed cost calculation class.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED COST CALCULATOR IMPLEMENTATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    code = '''\n",
    "class LLMCostCalculator:\n",
    "    \"\"\"Comprehensive LLM cost calculator with optimization suggestions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.providers = self._load_provider_data()\n",
    "        self.usage_history = []\n",
    "    \n",
    "    def calculate_cost(self, provider: str, model: str, \n",
    "                      input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate cost for a single request.\"\"\"\n",
    "        pricing = self.providers[provider][\"models\"][model]\n",
    "        input_cost = (input_tokens / 1000) * pricing[\"input_per_1k\"]\n",
    "        output_cost = (output_tokens / 1000) * pricing[\"output_per_1k\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def estimate_monthly_budget(self, usage_pattern: Dict) -> Dict:\n",
    "        \"\"\"Estimate monthly budget across providers.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for provider in self.providers:\n",
    "            provider_costs = {}\n",
    "            \n",
    "            for model in self.providers[provider][\"models\"]:\n",
    "                daily_cost = 0\n",
    "                \n",
    "                for interaction in usage_pattern[\"daily_interactions\"]:\n",
    "                    cost = self.calculate_cost(\n",
    "                        provider, model,\n",
    "                        interaction[\"input_tokens\"],\n",
    "                        interaction[\"output_tokens\"]\n",
    "                    )\n",
    "                    daily_cost += cost * interaction[\"frequency\"]\n",
    "                \n",
    "                provider_costs[model] = {\n",
    "                    \"daily\": daily_cost,\n",
    "                    \"monthly\": daily_cost * 30,\n",
    "                    \"yearly\": daily_cost * 365\n",
    "                }\n",
    "            \n",
    "            results[provider] = provider_costs\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize_costs(self, current_usage: Dict) -> List[Dict]:\n",
    "        \"\"\"Provide cost optimization recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Analyze current spending\n",
    "        current_cost = self.calculate_current_cost(current_usage)\n",
    "        \n",
    "        # Check for cheaper alternatives\n",
    "        if current_usage[\"model\"] == \"gpt-4\":\n",
    "            gpt35_cost = self.calculate_with_model(\"gpt-3.5-turbo\", current_usage)\n",
    "            if gpt35_cost < current_cost * 0.5:\n",
    "                recommendations.append({\n",
    "                    \"action\": \"Switch to GPT-3.5-Turbo for simple tasks\",\n",
    "                    \"savings\": f\"${(current_cost - gpt35_cost):.2f}/month\",\n",
    "                    \"impact\": \"Minimal for non-complex tasks\"\n",
    "                })\n",
    "        \n",
    "        # Check for batching opportunities\n",
    "        if current_usage[\"requests_per_day\"] > 100:\n",
    "            recommendations.append({\n",
    "                \"action\": \"Batch similar requests together\",\n",
    "                \"savings\": \"10-20% on token usage\",\n",
    "                \"impact\": \"Slightly delayed responses\"\n",
    "            })\n",
    "        \n",
    "        # Check for caching opportunities\n",
    "        if current_usage[\"unique_queries_ratio\"] < 0.7:\n",
    "            recommendations.append({\n",
    "                \"action\": \"Implement response caching\",\n",
    "                \"savings\": f\"{(1 - current_usage['unique_queries_ratio']) * 100:.0f}%\",\n",
    "                \"impact\": \"Instant responses for repeated queries\"\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def compare_providers(self, usage_scenario: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Create comparison table for providers.\"\"\"\n",
    "        comparison = []\n",
    "        \n",
    "        for provider in self.providers:\n",
    "            for model in self.providers[provider][\"models\"]:\n",
    "                row = {\n",
    "                    \"Provider\": provider,\n",
    "                    \"Model\": model,\n",
    "                    \"Monthly Cost\": self.calculate_monthly(provider, model, usage_scenario),\n",
    "                    \"Rate Limit\": self.providers[provider][\"rate_limits\"].get(model),\n",
    "                    \"Context Window\": self.providers[provider][\"context_windows\"].get(model),\n",
    "                    \"Best For\": self.providers[provider][\"best_for\"].get(model)\n",
    "                }\n",
    "                comparison.append(row)\n",
    "        \n",
    "        return pd.DataFrame(comparison).sort_values(\"Monthly Cost\")\n",
    "\n",
    "# Usage Example\n",
    "calculator = LLMCostCalculator()\n",
    "\n",
    "usage = {\n",
    "    \"daily_interactions\": [\n",
    "        {\"type\": \"simple_query\", \"frequency\": 50, \"input_tokens\": 100, \"output_tokens\": 150},\n",
    "        {\"type\": \"complex_analysis\", \"frequency\": 10, \"input_tokens\": 500, \"output_tokens\": 800},\n",
    "        {\"type\": \"code_generation\", \"frequency\": 5, \"input_tokens\": 300, \"output_tokens\": 600}\n",
    "    ]\n",
    "}\n",
    "\n",
    "budget = calculator.estimate_monthly_budget(usage)\n",
    "print(f\"OpenAI GPT-3.5: ${budget['OpenAI']['gpt-3.5-turbo']['monthly']:.2f}/month\")\n",
    "print(f\"Anthropic Claude-3-Haiku: ${budget['Anthropic']['claude-3-haiku']['monthly']:.2f}/month\")\n",
    "\n",
    "recommendations = calculator.optimize_costs(current_usage)\n",
    "for rec in recommendations:\n",
    "    print(f\"\ud83d\udca1 {rec['action']}: Save {rec['savings']}\")\n",
    "    '''\n",
    "    \n",
    "    print(code)\n",
    "\n",
    "\n",
    "def show_cost_optimization_strategies():\n",
    "    \"\"\"Show various cost optimization strategies.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COST OPTIMIZATION STRATEGIES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"strategy\": \"Model Downgrading\",\n",
    "            \"description\": \"Use cheaper models when possible\",\n",
    "            \"savings\": \"50-90%\",\n",
    "            \"example\": \"GPT-4 \u2192 GPT-3.5-Turbo for simple tasks\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Response Caching\",\n",
    "            \"description\": \"Cache common queries\",\n",
    "            \"savings\": \"20-40%\",\n",
    "            \"example\": \"FAQ responses, repeated calculations\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Prompt Optimization\",\n",
    "            \"description\": \"Shorter, more efficient prompts\",\n",
    "            \"savings\": \"10-30%\",\n",
    "            \"example\": \"Remove unnecessary context, use abbreviations\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Batch Processing\",\n",
    "            \"description\": \"Combine multiple requests\",\n",
    "            \"savings\": \"15-25%\",\n",
    "            \"example\": \"Process 10 items in one call vs 10 calls\"\n",
    "        },\n",
    "        {\n",
    "            \"strategy\": \"Hybrid Approach\",\n",
    "            \"description\": \"Mix expensive and cheap models\",\n",
    "            \"savings\": \"30-50%\",\n",
    "            \"example\": \"GPT-3.5 filters, GPT-4 for complex only\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for s in strategies:\n",
    "        print(f\"\\n\ud83d\udcb0 {s['strategy']}\")\n",
    "        print(f\"   Description: {s['description']}\")\n",
    "        print(f\"   Potential Savings: {s['savings']}\")\n",
    "        print(f\"   Example: {s['example']}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run cost calculator exercise.\"\"\"\n",
    "    \n",
    "    # Basic calculator\n",
    "    create_cost_calculator()\n",
    "    \n",
    "    # Detailed implementation\n",
    "    create_detailed_calculator()\n",
    "    \n",
    "    # Optimization strategies\n",
    "    show_cost_optimization_strategies()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 1 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You can now calculate and optimize LLM costs!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.2: Provider Comparison Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_5_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Provider Comparison Matrix\n",
    "Build a comparison matrix for your specific use case.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProviderEvaluation:\n",
    "    \"\"\"Evaluation of a provider for specific use case.\"\"\"\n",
    "    provider: str\n",
    "    cost_score: float  # 1-10, 10 being cheapest\n",
    "    performance_score: float  # 1-10, 10 being best\n",
    "    features_score: float  # 1-10, 10 being most features\n",
    "    limitations_score: float  # 1-10, 10 being least limitations\n",
    "    overall_score: float  # Weighted average\n",
    "\n",
    "\n",
    "def create_provider_comparison_matrix():\n",
    "    \"\"\"Create comprehensive provider comparison matrix.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 2: PROVIDER COMPARISON MATRIX\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define use cases\n",
    "    use_cases = [\n",
    "        \"Customer Service Chatbot\",\n",
    "        \"Code Generation Assistant\",\n",
    "        \"Content Writing Platform\",\n",
    "        \"Data Analysis Tool\",\n",
    "        \"Educational Tutor\"\n",
    "    ]\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        print(f\"\\n\ud83d\udcca USE CASE: {use_case}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create comparison for this use case\n",
    "        comparison = create_use_case_comparison(use_case)\n",
    "        display_comparison_matrix(comparison)\n",
    "        \n",
    "        # Provide recommendations\n",
    "        provide_recommendations(use_case, comparison)\n",
    "\n",
    "\n",
    "def create_use_case_comparison(use_case: str) -> Dict:\n",
    "    \"\"\"Create detailed comparison for specific use case.\"\"\"\n",
    "    \n",
    "    providers_data = {\n",
    "        \"OpenAI\": {\n",
    "            \"models\": [\"GPT-3.5-Turbo\", \"GPT-4\", \"GPT-4-Turbo\"],\n",
    "            \"cost\": get_cost_rating(use_case, \"OpenAI\"),\n",
    "            \"performance\": get_performance_rating(use_case, \"OpenAI\"),\n",
    "            \"features\": {\n",
    "                \"function_calling\": True,\n",
    "                \"json_mode\": True,\n",
    "                \"vision\": True,\n",
    "                \"streaming\": True,\n",
    "                \"fine_tuning\": True,\n",
    "                \"embeddings\": True\n",
    "            },\n",
    "            \"limitations\": {\n",
    "                \"rate_limits\": \"Medium\",\n",
    "                \"context_window\": \"16K-128K\",\n",
    "                \"data_privacy\": \"Standard\",\n",
    "                \"geographic_restrictions\": \"Some countries blocked\"\n",
    "            }\n",
    "        },\n",
    "        \"Anthropic\": {\n",
    "            \"models\": [\"Claude-3-Haiku\", \"Claude-3-Sonnet\", \"Claude-3-Opus\"],\n",
    "            \"cost\": get_cost_rating(use_case, \"Anthropic\"),\n",
    "            \"performance\": get_performance_rating(use_case, \"Anthropic\"),\n",
    "            \"features\": {\n",
    "                \"function_calling\": False,\n",
    "                \"json_mode\": False,\n",
    "                \"vision\": True,\n",
    "                \"streaming\": True,\n",
    "                \"fine_tuning\": False,\n",
    "                \"embeddings\": False\n",
    "            },\n",
    "            \"limitations\": {\n",
    "                \"rate_limits\": \"Low\",\n",
    "                \"context_window\": \"200K\",\n",
    "                \"data_privacy\": \"Enhanced\",\n",
    "                \"geographic_restrictions\": \"Limited availability\"\n",
    "            }\n",
    "        },\n",
    "        \"Google\": {\n",
    "            \"models\": [\"Gemini-Pro\", \"Gemini-Ultra\", \"Gemini-Nano\"],\n",
    "            \"cost\": get_cost_rating(use_case, \"Google\"),\n",
    "            \"performance\": get_performance_rating(use_case, \"Google\"),\n",
    "            \"features\": {\n",
    "                \"function_calling\": True,\n",
    "                \"json_mode\": True,\n",
    "                \"vision\": True,\n",
    "                \"streaming\": True,\n",
    "                \"fine_tuning\": False,\n",
    "                \"embeddings\": True\n",
    "            },\n",
    "            \"limitations\": {\n",
    "                \"rate_limits\": \"High (free tier)\",\n",
    "                \"context_window\": \"32K-1M\",\n",
    "                \"data_privacy\": \"Google standard\",\n",
    "                \"geographic_restrictions\": \"Wide availability\"\n",
    "            }\n",
    "        },\n",
    "        \"Open Source\": {\n",
    "            \"models\": [\"Llama-2\", \"Mistral\", \"Mixtral\"],\n",
    "            \"cost\": get_cost_rating(use_case, \"Open Source\"),\n",
    "            \"performance\": get_performance_rating(use_case, \"Open Source\"),\n",
    "            \"features\": {\n",
    "                \"function_calling\": \"Depends\",\n",
    "                \"json_mode\": \"Depends\",\n",
    "                \"vision\": \"Limited\",\n",
    "                \"streaming\": True,\n",
    "                \"fine_tuning\": True,\n",
    "                \"embeddings\": True\n",
    "            },\n",
    "            \"limitations\": {\n",
    "                \"rate_limits\": \"None (self-hosted)\",\n",
    "                \"context_window\": \"4K-32K\",\n",
    "                \"data_privacy\": \"Full control\",\n",
    "                \"geographic_restrictions\": \"None\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return providers_data\n",
    "\n",
    "\n",
    "def get_cost_rating(use_case: str, provider: str) -> Dict:\n",
    "    \"\"\"Get cost ratings for specific use case and provider.\"\"\"\n",
    "    \n",
    "    # Cost ratings matrix (simplified)\n",
    "    cost_matrix = {\n",
    "        \"Customer Service Chatbot\": {\n",
    "            \"OpenAI\": 7,  # GPT-3.5 is cost-effective\n",
    "            \"Anthropic\": 8,  # Haiku is very cheap\n",
    "            \"Google\": 9,  # Gemini Pro has good free tier\n",
    "            \"Open Source\": 10  # Self-hosting after initial setup\n",
    "        },\n",
    "        \"Code Generation Assistant\": {\n",
    "            \"OpenAI\": 6,  # GPT-4 needed, expensive\n",
    "            \"Anthropic\": 5,  # Opus needed, expensive\n",
    "            \"Google\": 7,  # Gemini Pro decent\n",
    "            \"Open Source\": 8  # Good open models available\n",
    "        },\n",
    "        \"Content Writing Platform\": {\n",
    "            \"OpenAI\": 7,\n",
    "            \"Anthropic\": 7,\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 6\n",
    "        },\n",
    "        \"Data Analysis Tool\": {\n",
    "            \"OpenAI\": 6,\n",
    "            \"Anthropic\": 6,\n",
    "            \"Google\": 7,\n",
    "            \"Open Source\": 5\n",
    "        },\n",
    "        \"Educational Tutor\": {\n",
    "            \"OpenAI\": 7,\n",
    "            \"Anthropic\": 8,\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cost_matrix.get(use_case, {}).get(provider, 5)\n",
    "\n",
    "\n",
    "def get_performance_rating(use_case: str, provider: str) -> Dict:\n",
    "    \"\"\"Get performance ratings for specific use case and provider.\"\"\"\n",
    "    \n",
    "    performance_matrix = {\n",
    "        \"Customer Service Chatbot\": {\n",
    "            \"OpenAI\": 9,\n",
    "            \"Anthropic\": 9,\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 7\n",
    "        },\n",
    "        \"Code Generation Assistant\": {\n",
    "            \"OpenAI\": 10,  # GPT-4 excels\n",
    "            \"Anthropic\": 9,  # Claude very good\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 7\n",
    "        },\n",
    "        \"Content Writing Platform\": {\n",
    "            \"OpenAI\": 9,\n",
    "            \"Anthropic\": 10,  # Claude excels at writing\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 7\n",
    "        },\n",
    "        \"Data Analysis Tool\": {\n",
    "            \"OpenAI\": 9,\n",
    "            \"Anthropic\": 8,\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 6\n",
    "        },\n",
    "        \"Educational Tutor\": {\n",
    "            \"OpenAI\": 9,\n",
    "            \"Anthropic\": 10,  # Claude great at explanations\n",
    "            \"Google\": 8,\n",
    "            \"Open Source\": 7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return performance_matrix.get(use_case, {}).get(provider, 5)\n",
    "\n",
    "\n",
    "def display_comparison_matrix(providers_data: Dict):\n",
    "    \"\"\"Display comparison matrix in readable format.\"\"\"\n",
    "    \n",
    "    # Create summary scores\n",
    "    summary = []\n",
    "    \n",
    "    for provider, data in providers_data.items():\n",
    "        # Count features\n",
    "        features_count = sum(1 for v in data[\"features\"].values() \n",
    "                           if v is True or v == \"Depends\")\n",
    "        \n",
    "        # Calculate limitation score (inverse)\n",
    "        limitation_factors = {\n",
    "            \"rate_limits\": {\"Low\": 3, \"Medium\": 5, \"High\": 8, \"None\": 10},\n",
    "            \"context_window\": {\"4K-32K\": 5, \"16K-128K\": 7, \"32K-1M\": 9, \"200K\": 10},\n",
    "            \"data_privacy\": {\"Standard\": 5, \"Enhanced\": 8, \"Google standard\": 6, \"Full control\": 10}\n",
    "        }\n",
    "        \n",
    "        limitation_score = 7  # Default\n",
    "        \n",
    "        summary.append({\n",
    "            \"Provider\": provider,\n",
    "            \"Cost (1-10)\": data[\"cost\"],\n",
    "            \"Performance (1-10)\": data[\"performance\"],\n",
    "            \"Features\": f\"{features_count}/6\",\n",
    "            \"Best Model\": data[\"models\"][0] if data[\"models\"] else \"N/A\",\n",
    "            \"Key Advantage\": get_key_advantage(provider)\n",
    "        })\n",
    "    \n",
    "    # Display as table\n",
    "    for row in summary:\n",
    "        print(f\"\\n{row['Provider']}:\")\n",
    "        for key, value in row.items():\n",
    "            if key != \"Provider\":\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "def get_key_advantage(provider: str) -> str:\n",
    "    \"\"\"Get key advantage for each provider.\"\"\"\n",
    "    \n",
    "    advantages = {\n",
    "        \"OpenAI\": \"Most mature ecosystem, best tooling\",\n",
    "        \"Anthropic\": \"Longest context, best safety\",\n",
    "        \"Google\": \"Great free tier, multimodal\",\n",
    "        \"Open Source\": \"Full control, no restrictions\"\n",
    "    }\n",
    "    \n",
    "    return advantages.get(provider, \"N/A\")\n",
    "\n",
    "\n",
    "def provide_recommendations(use_case: str, providers_data: Dict):\n",
    "    \"\"\"Provide specific recommendations for use case.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 RECOMMENDATIONS FOR {use_case}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    recommendations = {\n",
    "        \"Customer Service Chatbot\": [\n",
    "            \"Start with: Google Gemini Pro (free tier)\",\n",
    "            \"Scale to: OpenAI GPT-3.5-Turbo\",\n",
    "            \"Enterprise: Anthropic Claude-3-Haiku\",\n",
    "            \"Self-hosted: Llama-2-13B-Chat\"\n",
    "        ],\n",
    "        \"Code Generation Assistant\": [\n",
    "            \"Best quality: OpenAI GPT-4\",\n",
    "            \"Cost-effective: Anthropic Claude-3-Sonnet\",\n",
    "            \"Free option: Google Gemini Pro\",\n",
    "            \"Open source: Mixtral-8x7B-Instruct\"\n",
    "        ],\n",
    "        \"Content Writing Platform\": [\n",
    "            \"Best quality: Anthropic Claude-3-Opus\",\n",
    "            \"Balanced: OpenAI GPT-4-Turbo\",\n",
    "            \"Budget: Google Gemini Pro\",\n",
    "            \"Open source: Mistral-7B-Instruct\"\n",
    "        ],\n",
    "        \"Data Analysis Tool\": [\n",
    "            \"Best: OpenAI GPT-4 (function calling)\",\n",
    "            \"Alternative: Google Gemini Pro\",\n",
    "            \"Budget: OpenAI GPT-3.5-Turbo\",\n",
    "            \"Open source: CodeLlama-13B\"\n",
    "        ],\n",
    "        \"Educational Tutor\": [\n",
    "            \"Best: Anthropic Claude (safe, detailed)\",\n",
    "            \"Alternative: OpenAI GPT-4\",\n",
    "            \"Budget: Google Gemini Pro\",\n",
    "            \"Open source: Llama-2-70B-Chat\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for rec in recommendations.get(use_case, []):\n",
    "        print(f\"  \u2022 {rec}\")\n",
    "\n",
    "\n",
    "def create_detailed_comparison_framework():\n",
    "    \"\"\"Create detailed comparison framework code.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DETAILED COMPARISON FRAMEWORK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    code = '''\n",
    "class ProviderComparator:\n",
    "    \"\"\"Comprehensive provider comparison framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_case: Dict):\n",
    "        self.use_case = use_case\n",
    "        self.weights = use_case.get(\"weights\", {\n",
    "            \"cost\": 0.3,\n",
    "            \"performance\": 0.3,\n",
    "            \"features\": 0.2,\n",
    "            \"limitations\": 0.2\n",
    "        })\n",
    "        self.providers = {}\n",
    "    \n",
    "    def add_provider(self, name: str, config: Dict):\n",
    "        \"\"\"Add provider to comparison.\"\"\"\n",
    "        self.providers[name] = {\n",
    "            \"config\": config,\n",
    "            \"scores\": self.calculate_scores(config)\n",
    "        }\n",
    "    \n",
    "    def calculate_scores(self, config: Dict) -> Dict:\n",
    "        \"\"\"Calculate scores for provider.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Cost score (based on per-token pricing)\n",
    "        avg_cost = (config[\"input_cost\"] + config[\"output_cost\"]) / 2\n",
    "        scores[\"cost\"] = 10 * (1 / (1 + avg_cost * 100))  # Inverse cost\n",
    "        \n",
    "        # Performance score (based on benchmarks)\n",
    "        scores[\"performance\"] = config.get(\"benchmark_score\", 5)\n",
    "        \n",
    "        # Features score\n",
    "        feature_count = len([f for f in config.get(\"features\", []) if f])\n",
    "        scores[\"features\"] = min(10, feature_count * 2)\n",
    "        \n",
    "        # Limitations score\n",
    "        limitation_count = len(config.get(\"limitations\", []))\n",
    "        scores[\"limitations\"] = max(0, 10 - limitation_count * 2)\n",
    "        \n",
    "        # Overall weighted score\n",
    "        scores[\"overall\"] = sum(\n",
    "            scores[key] * self.weights.get(key, 0.25)\n",
    "            for key in [\"cost\", \"performance\", \"features\", \"limitations\"]\n",
    "        )\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate comparison dataframe.\"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for name, provider in self.providers.items():\n",
    "            row = {\"Provider\": name}\n",
    "            row.update(provider[\"scores\"])\n",
    "            data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df.sort_values(\"overall\", ascending=False)\n",
    "    \n",
    "    def recommend(self, constraints: Dict = None) -> str:\n",
    "        \"\"\"Recommend best provider given constraints.\"\"\"\n",
    "        filtered = self.providers.copy()\n",
    "        \n",
    "        if constraints:\n",
    "            if \"max_cost\" in constraints:\n",
    "                filtered = {k: v for k, v in filtered.items()\n",
    "                           if v[\"config\"][\"avg_cost\"] <= constraints[\"max_cost\"]}\n",
    "            \n",
    "            if \"min_performance\" in constraints:\n",
    "                filtered = {k: v for k, v in filtered.items()\n",
    "                           if v[\"scores\"][\"performance\"] >= constraints[\"min_performance\"]}\n",
    "            \n",
    "            if \"required_features\" in constraints:\n",
    "                for feature in constraints[\"required_features\"]:\n",
    "                    filtered = {k: v for k, v in filtered.items()\n",
    "                               if feature in v[\"config\"].get(\"features\", [])}\n",
    "        \n",
    "        if not filtered:\n",
    "            return \"No providers match your constraints\"\n",
    "        \n",
    "        # Return provider with highest overall score\n",
    "        best = max(filtered.items(), key=lambda x: x[1][\"scores\"][\"overall\"])\n",
    "        return best[0]\n",
    "\n",
    "# Usage Example\n",
    "comparator = ProviderComparator({\n",
    "    \"name\": \"Customer Service Bot\",\n",
    "    \"weights\": {\n",
    "        \"cost\": 0.4,  # Cost is most important\n",
    "        \"performance\": 0.3,\n",
    "        \"features\": 0.2,\n",
    "        \"limitations\": 0.1\n",
    "    }\n",
    "})\n",
    "\n",
    "comparator.add_provider(\"OpenAI\", {\n",
    "    \"input_cost\": 0.0005,\n",
    "    \"output_cost\": 0.0015,\n",
    "    \"benchmark_score\": 9,\n",
    "    \"features\": [\"streaming\", \"function_calling\", \"json_mode\"],\n",
    "    \"limitations\": [\"rate_limits\"]\n",
    "})\n",
    "\n",
    "comparator.add_provider(\"Anthropic\", {\n",
    "    \"input_cost\": 0.00025,\n",
    "    \"output_cost\": 0.00125,\n",
    "    \"benchmark_score\": 9,\n",
    "    \"features\": [\"streaming\", \"long_context\"],\n",
    "    \"limitations\": [\"no_function_calling\", \"limited_availability\"]\n",
    "})\n",
    "\n",
    "df = comparator.compare()\n",
    "print(df)\n",
    "\n",
    "best = comparator.recommend(constraints={\"max_cost\": 0.001})\n",
    "print(f\"Recommended: {best}\")\n",
    "    '''\n",
    "    \n",
    "    print(code)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run provider comparison exercise.\"\"\"\n",
    "    \n",
    "    # Create comparison matrices\n",
    "    create_provider_comparison_matrix()\n",
    "    \n",
    "    # Detailed framework\n",
    "    create_detailed_comparison_framework()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 2 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You can now compare providers systematically for any use case!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.3: Migration Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_3_7_5_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Migration Planning\n",
    "Design a migration plan from OpenAI to an open-source model.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MigrationPhase:\n",
    "    \"\"\"A phase in the migration plan.\"\"\"\n",
    "    name: str\n",
    "    duration: int  # days\n",
    "    tasks: List[str]\n",
    "    risks: List[str]\n",
    "    success_criteria: List[str]\n",
    "    rollback_plan: str\n",
    "\n",
    "\n",
    "def create_migration_plan():\n",
    "    \"\"\"Create comprehensive migration plan from OpenAI to open-source.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 3: MIGRATION PLANNING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Migration: OpenAI GPT \u2192 Open Source LLM (Llama-2/Mistral)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Define migration phases\n",
    "    phases = create_migration_phases()\n",
    "    \n",
    "    # Display migration timeline\n",
    "    display_migration_timeline(phases)\n",
    "    \n",
    "    # Identify challenges\n",
    "    identify_migration_challenges()\n",
    "    \n",
    "    # Create mitigation strategies\n",
    "    create_mitigation_strategies()\n",
    "    \n",
    "    # Provide migration checklist\n",
    "    provide_migration_checklist()\n",
    "    \n",
    "    # Show code migration examples\n",
    "    show_code_migration_examples()\n",
    "\n",
    "\n",
    "def create_migration_phases() -> List[MigrationPhase]:\n",
    "    \"\"\"Define migration phases.\"\"\"\n",
    "    \n",
    "    phases = [\n",
    "        MigrationPhase(\n",
    "            name=\"Phase 1: Assessment & Planning\",\n",
    "            duration=14,\n",
    "            tasks=[\n",
    "                \"Audit current OpenAI usage patterns\",\n",
    "                \"Document all API endpoints and features used\",\n",
    "                \"Measure baseline performance metrics\",\n",
    "                \"Evaluate open-source alternatives\",\n",
    "                \"Select target model (Llama-2, Mistral, etc.)\",\n",
    "                \"Estimate infrastructure requirements\",\n",
    "                \"Calculate ROI and cost savings\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"Underestimating complexity\",\n",
    "                \"Missing critical features\",\n",
    "                \"Incomplete usage audit\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"Complete API usage documentation\",\n",
    "                \"Model selection justified with benchmarks\",\n",
    "                \"Infrastructure requirements defined\",\n",
    "                \"Budget approved\"\n",
    "            ],\n",
    "            rollback_plan=\"Continue with OpenAI while re-evaluating\"\n",
    "        ),\n",
    "        \n",
    "        MigrationPhase(\n",
    "            name=\"Phase 2: Infrastructure Setup\",\n",
    "            duration=21,\n",
    "            tasks=[\n",
    "                \"Provision GPU servers or cloud instances\",\n",
    "                \"Install model serving framework (vLLM, TGI, etc.)\",\n",
    "                \"Deploy selected open-source model\",\n",
    "                \"Setup monitoring and logging\",\n",
    "                \"Configure load balancing\",\n",
    "                \"Implement security measures\",\n",
    "                \"Setup backup and recovery\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"Hardware compatibility issues\",\n",
    "                \"Insufficient compute resources\",\n",
    "                \"Security vulnerabilities\",\n",
    "                \"Complex deployment\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"Model successfully deployed\",\n",
    "                \"Response times < 2 seconds\",\n",
    "                \"99.9% uptime achieved in testing\",\n",
    "                \"Security audit passed\"\n",
    "            ],\n",
    "            rollback_plan=\"Maintain OpenAI as primary, use open-source as backup\"\n",
    "        ),\n",
    "        \n",
    "        MigrationPhase(\n",
    "            name=\"Phase 3: Feature Parity Development\",\n",
    "            duration=30,\n",
    "            tasks=[\n",
    "                \"Implement prompt translation layer\",\n",
    "                \"Recreate function calling if needed\",\n",
    "                \"Build response formatting modules\",\n",
    "                \"Develop quality assurance tests\",\n",
    "                \"Create performance benchmarks\",\n",
    "                \"Build fallback mechanisms\",\n",
    "                \"Implement caching layer\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"Feature gaps discovered\",\n",
    "                \"Quality degradation\",\n",
    "                \"Performance issues\",\n",
    "                \"Increased complexity\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"All critical features replicated\",\n",
    "                \"Quality metrics within 10% of OpenAI\",\n",
    "                \"Automated testing suite complete\",\n",
    "                \"Fallback system operational\"\n",
    "            ],\n",
    "            rollback_plan=\"Keep OpenAI for features that can't be replicated\"\n",
    "        ),\n",
    "        \n",
    "        MigrationPhase(\n",
    "            name=\"Phase 4: Parallel Testing\",\n",
    "            duration=30,\n",
    "            tasks=[\n",
    "                \"Run A/B tests with real traffic\",\n",
    "                \"Compare outputs systematically\",\n",
    "                \"Measure user satisfaction\",\n",
    "                \"Monitor system performance\",\n",
    "                \"Collect edge cases\",\n",
    "                \"Tune model parameters\",\n",
    "                \"Optimize infrastructure\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"User experience degradation\",\n",
    "                \"Unexpected edge cases\",\n",
    "                \"Performance bottlenecks\",\n",
    "                \"Higher error rates\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"95% output quality parity\",\n",
    "                \"Response time within 150% of OpenAI\",\n",
    "                \"Error rate < 2%\",\n",
    "                \"Positive user feedback\"\n",
    "            ],\n",
    "            rollback_plan=\"Route traffic back to OpenAI if issues arise\"\n",
    "        ),\n",
    "        \n",
    "        MigrationPhase(\n",
    "            name=\"Phase 5: Gradual Migration\",\n",
    "            duration=21,\n",
    "            tasks=[\n",
    "                \"Start with 10% traffic migration\",\n",
    "                \"Monitor all metrics closely\",\n",
    "                \"Gradually increase to 50%\",\n",
    "                \"Address issues as they arise\",\n",
    "                \"Train team on new system\",\n",
    "                \"Update documentation\",\n",
    "                \"Prepare full cutover\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"Scaling issues\",\n",
    "                \"Team resistance\",\n",
    "                \"Documentation gaps\",\n",
    "                \"Customer complaints\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"50% traffic on open-source\",\n",
    "                \"SLA requirements met\",\n",
    "                \"Team fully trained\",\n",
    "                \"Documentation complete\"\n",
    "            ],\n",
    "            rollback_plan=\"Reduce traffic percentage, fix issues, retry\"\n",
    "        ),\n",
    "        \n",
    "        MigrationPhase(\n",
    "            name=\"Phase 6: Full Migration & Optimization\",\n",
    "            duration=14,\n",
    "            tasks=[\n",
    "                \"Complete traffic migration\",\n",
    "                \"Decommission OpenAI integration\",\n",
    "                \"Optimize resource usage\",\n",
    "                \"Implement cost tracking\",\n",
    "                \"Setup continuous improvement\",\n",
    "                \"Celebrate success! \ud83c\udf89\"\n",
    "            ],\n",
    "            risks=[\n",
    "                \"Premature decommissioning\",\n",
    "                \"Optimization breaking functionality\",\n",
    "                \"Team burnout\"\n",
    "            ],\n",
    "            success_criteria=[\n",
    "                \"100% traffic on open-source\",\n",
    "                \"Cost savings realized\",\n",
    "                \"Team satisfaction high\",\n",
    "                \"System stable for 2 weeks\"\n",
    "            ],\n",
    "            rollback_plan=\"Emergency OpenAI API keys ready if critical issues\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return phases\n",
    "\n",
    "\n",
    "def display_migration_timeline(phases: List[MigrationPhase]):\n",
    "    \"\"\"Display migration timeline.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcc5 MIGRATION TIMELINE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_days = sum(phase.duration for phase in phases)\n",
    "    print(f\"Total Duration: {total_days} days (~{total_days // 30} months)\")\n",
    "    print()\n",
    "    \n",
    "    start_date = datetime.now()\n",
    "    \n",
    "    for i, phase in enumerate(phases, 1):\n",
    "        end_date = start_date + timedelta(days=phase.duration)\n",
    "        print(f\"\ud83d\udccd {phase.name}\")\n",
    "        print(f\"   Duration: {phase.duration} days\")\n",
    "        print(f\"   Start: {start_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   End: {end_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   Key Tasks: {len(phase.tasks)} tasks\")\n",
    "        print()\n",
    "        start_date = end_date\n",
    "\n",
    "\n",
    "def identify_migration_challenges():\n",
    "    \"\"\"Identify key migration challenges.\"\"\"\n",
    "    \n",
    "    print(\"\\n\u26a0\ufe0f KEY MIGRATION CHALLENGES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    challenges = {\n",
    "        \"Technical Challenges\": [\n",
    "            \"Prompt compatibility - OpenAI prompts may not work\",\n",
    "            \"Feature gaps - No native function calling\",\n",
    "            \"Performance differences - May be slower\",\n",
    "            \"Context window limitations - Often smaller\",\n",
    "            \"Output quality variance - Less consistent\"\n",
    "        ],\n",
    "        \"Infrastructure Challenges\": [\n",
    "            \"GPU requirements - Expensive hardware needed\",\n",
    "            \"Scaling complexity - Manual scaling required\",\n",
    "            \"Monitoring gaps - Less mature tooling\",\n",
    "            \"Security responsibility - You own security now\",\n",
    "            \"Deployment complexity - More moving parts\"\n",
    "        ],\n",
    "        \"Organizational Challenges\": [\n",
    "            \"Team training required - New skills needed\",\n",
    "            \"Support burden - No vendor support\",\n",
    "            \"Documentation needs - Must create your own\",\n",
    "            \"Change resistance - Team may prefer OpenAI\",\n",
    "            \"Risk tolerance - Management concerns\"\n",
    "        ],\n",
    "        \"Cost Challenges\": [\n",
    "            \"Upfront investment - Hardware/cloud costs\",\n",
    "            \"Hidden costs - DevOps, monitoring, maintenance\",\n",
    "            \"Opportunity cost - Engineering time\",\n",
    "            \"Training costs - Team education\",\n",
    "            \"Dual running costs - During migration\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in challenges.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  \u2022 {item}\")\n",
    "\n",
    "\n",
    "def create_mitigation_strategies():\n",
    "    \"\"\"Create strategies to handle migration challenges.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 MITIGATION STRATEGIES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"challenge\": \"Prompt Compatibility\",\n",
    "            \"strategy\": \"Build prompt translation layer\",\n",
    "            \"implementation\": \"\"\"\n",
    "# Prompt translator example\n",
    "def translate_prompt(openai_prompt: str) -> str:\n",
    "    # OpenAI system prompts \u2192 Llama format\n",
    "    if \"You are\" in openai_prompt:\n",
    "        return f\"<s>[INST] <<SYS>>\\\\n{openai_prompt}\\\\n<</SYS>>\\\\n[/INST]\"\n",
    "    return openai_prompt\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"challenge\": \"Feature Gaps\",\n",
    "            \"strategy\": \"Implement missing features externally\",\n",
    "            \"implementation\": \"\"\"\n",
    "# Function calling wrapper\n",
    "def function_calling_wrapper(prompt: str, functions: List):\n",
    "    # Add function descriptions to prompt\n",
    "    enhanced_prompt = f\"{prompt}\\\\n\\\\nAvailable functions: {functions}\"\n",
    "    response = llm_call(enhanced_prompt)\n",
    "    # Parse response for function calls\n",
    "    return parse_function_calls(response)\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"challenge\": \"Performance Issues\",\n",
    "            \"strategy\": \"Implement caching and optimization\",\n",
    "            \"implementation\": \"\"\"\n",
    "# Performance optimization\n",
    "class OptimizedLLM:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.batch_queue = []\n",
    "    \n",
    "    def cached_inference(self, prompt: str):\n",
    "        if prompt in self.cache:\n",
    "            return self.cache[prompt]\n",
    "        response = self.llm_call(prompt)\n",
    "        self.cache[prompt] = response\n",
    "        return response\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"challenge\": \"Quality Variance\",\n",
    "            \"strategy\": \"Implement quality assurance layer\",\n",
    "            \"implementation\": \"\"\"\n",
    "# Quality assurance\n",
    "def quality_check(response: str, expected_quality: float = 0.8):\n",
    "    score = calculate_quality_score(response)\n",
    "    if score < expected_quality:\n",
    "        # Retry with different parameters or fallback\n",
    "        return retry_with_adjustments(response)\n",
    "    return response\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for s in strategies:\n",
    "        print(f\"\\n\ud83c\udfaf Challenge: {s['challenge']}\")\n",
    "        print(f\"   Strategy: {s['strategy']}\")\n",
    "        print(f\"   Implementation:\")\n",
    "        print(s['implementation'])\n",
    "\n",
    "\n",
    "def provide_migration_checklist():\n",
    "    \"\"\"Provide comprehensive migration checklist.\"\"\"\n",
    "    \n",
    "    print(\"\\n\u2705 MIGRATION CHECKLIST\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    checklist = {\n",
    "        \"Pre-Migration\": [\n",
    "            \"\u25a1 Complete API usage audit\",\n",
    "            \"\u25a1 Document all prompts and patterns\",\n",
    "            \"\u25a1 Benchmark current performance\",\n",
    "            \"\u25a1 Calculate expected cost savings\",\n",
    "            \"\u25a1 Get stakeholder buy-in\",\n",
    "            \"\u25a1 Select target model\",\n",
    "            \"\u25a1 Plan infrastructure\",\n",
    "            \"\u25a1 Create rollback plan\"\n",
    "        ],\n",
    "        \"During Migration\": [\n",
    "            \"\u25a1 Setup monitoring dashboards\",\n",
    "            \"\u25a1 Implement A/B testing\",\n",
    "            \"\u25a1 Create prompt translation\",\n",
    "            \"\u25a1 Build quality checks\",\n",
    "            \"\u25a1 Train the team\",\n",
    "            \"\u25a1 Document everything\",\n",
    "            \"\u25a1 Maintain OpenAI backup\",\n",
    "            \"\u25a1 Collect user feedback\"\n",
    "        ],\n",
    "        \"Post-Migration\": [\n",
    "            \"\u25a1 Monitor performance metrics\",\n",
    "            \"\u25a1 Track cost savings\",\n",
    "            \"\u25a1 Optimize resource usage\",\n",
    "            \"\u25a1 Update documentation\",\n",
    "            \"\u25a1 Plan improvements\",\n",
    "            \"\u25a1 Share learnings\",\n",
    "            \"\u25a1 Celebrate success!\",\n",
    "            \"\u25a1 Plan next optimizations\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for phase, items in checklist.items():\n",
    "        print(f\"\\n{phase}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "\n",
    "\n",
    "def show_code_migration_examples():\n",
    "    \"\"\"Show actual code migration examples.\"\"\"\n",
    "    \n",
    "    print(\"\\n\ud83d\udcbb CODE MIGRATION EXAMPLES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\n1\ufe0f\u20e3 OPENAI \u2192 OPEN SOURCE API WRAPPER:\")\n",
    "    \n",
    "    code_example = '''\n",
    "class LLMProvider:\n",
    "    \"\"\"Unified interface for different LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"openai\"):\n",
    "        self.provider = provider\n",
    "        \n",
    "        if provider == \"openai\":\n",
    "            import openai\n",
    "            self.client = openai.OpenAI()\n",
    "        elif provider == \"llama\":\n",
    "            from llama_cpp import Llama\n",
    "            self.client = Llama(model_path=\"./models/llama-2-7b.gguf\")\n",
    "        elif provider == \"vllm\":\n",
    "            from vllm import LLM\n",
    "            self.client = LLM(model=\"meta-llama/Llama-2-7b-hf\")\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Unified completion interface.\"\"\"\n",
    "        \n",
    "        if self.provider == \"openai\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                **kwargs\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        elif self.provider == \"llama\":\n",
    "            response = self.client(\n",
    "                prompt,\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 512),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7)\n",
    "            )\n",
    "            return response[\"choices\"][0][\"text\"]\n",
    "            \n",
    "        elif self.provider == \"vllm\":\n",
    "            outputs = self.client.generate([prompt], **kwargs)\n",
    "            return outputs[0].outputs[0].text\n",
    "\n",
    "# Usage - same interface regardless of provider!\n",
    "llm = LLMProvider(provider=\"llama\")  # Easy switch!\n",
    "response = llm.complete(\"Explain quantum computing\")\n",
    "    '''\n",
    "    \n",
    "    print(code_example)\n",
    "    \n",
    "    print(\"\\n2\ufe0f\u20e3 GRADUAL MIGRATION WITH FALLBACK:\")\n",
    "    \n",
    "    fallback_code = '''\n",
    "class MigrationLLM:\n",
    "    \"\"\"LLM with automatic fallback during migration.\"\"\"\n",
    "    \n",
    "    def __init__(self, primary=\"llama\", fallback=\"openai\"):\n",
    "        self.primary_llm = LLMProvider(primary)\n",
    "        self.fallback_llm = LLMProvider(fallback)\n",
    "        self.use_fallback_probability = 0.1  # Start with 10% on new\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Complete with automatic fallback.\"\"\"\n",
    "        \n",
    "        import random\n",
    "        \n",
    "        # Gradually increase usage of new system\n",
    "        use_primary = random.random() > self.use_fallback_probability\n",
    "        \n",
    "        if use_primary:\n",
    "            try:\n",
    "                response = self.primary_llm.complete(prompt, **kwargs)\n",
    "                # Log success\n",
    "                self.log_success(\"primary\")\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                # Log failure and fallback\n",
    "                self.log_failure(\"primary\", e)\n",
    "                return self.fallback_llm.complete(prompt, **kwargs)\n",
    "        else:\n",
    "            # Still testing with fallback\n",
    "            return self.fallback_llm.complete(prompt, **kwargs)\n",
    "    \n",
    "    def increase_primary_usage(self, increment: float = 0.1):\n",
    "        \"\"\"Gradually increase primary usage.\"\"\"\n",
    "        self.use_fallback_probability = max(0, \n",
    "            self.use_fallback_probability - increment)\n",
    "    '''\n",
    "    \n",
    "    print(fallback_code)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run migration planning exercise.\"\"\"\n",
    "    \n",
    "    # Create comprehensive migration plan\n",
    "    create_migration_plan()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 3 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You now have a complete migration plan from OpenAI to open-source!\")\n",
    "    print(\"   Remember: Migration is a marathon, not a sprint. Plan carefully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5.4: API Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_4_7_5_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 4 Solution: API Abstraction\n",
    "Write a wrapper class that works with multiple LLM providers.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Provider(Enum):\n",
    "    \"\"\"Supported LLM providers.\"\"\"\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    GOOGLE = \"google\"\n",
    "    COHERE = \"cohere\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Unified response format.\"\"\"\n",
    "    text: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "    latency: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "\n",
    "class BaseLLMProvider(ABC):\n",
    "    \"\"\"Abstract base class for LLM providers.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def complete(self, prompt: str, **kwargs) -> LLMResponse:\n",
    "        \"\"\"Generate completion.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def chat(self, messages: List[Dict], **kwargs) -> LLMResponse:\n",
    "        \"\"\"Chat completion.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check if provider is available.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OpenAIProvider(BaseLLMProvider):\n",
    "    \"\"\"OpenAI provider implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.pricing = {\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015},\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03}\n",
    "        }\n",
    "        \n",
    "        # Import OpenAI library\n",
    "        try:\n",
    "            import openai\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"OpenAI library not installed. Run: pip install openai\")\n",
    "            self.available = False\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI initialization error: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> LLMResponse:\n",
    "        \"\"\"OpenAI completion.\"\"\"\n",
    "        return self.chat([{\"role\": \"user\", \"content\": prompt}], **kwargs)\n",
    "    \n",
    "    def chat(self, messages: List[Dict], **kwargs) -> LLMResponse:\n",
    "        \"\"\"OpenAI chat completion.\"\"\"\n",
    "        if not self.available:\n",
    "            raise Exception(\"OpenAI provider not available\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=kwargs.get(\"model\", self.model),\n",
    "                messages=messages,\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 512),\n",
    "                top_p=kwargs.get(\"top_p\", 1.0),\n",
    "                frequency_penalty=kwargs.get(\"frequency_penalty\", 0),\n",
    "                presence_penalty=kwargs.get(\"presence_penalty\", 0)\n",
    "            )\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Calculate cost\n",
    "            model = kwargs.get(\"model\", self.model)\n",
    "            if model in self.pricing:\n",
    "                input_cost = (response.usage.prompt_tokens / 1000) * \\\n",
    "                           self.pricing[model][\"input\"]\n",
    "                output_cost = (response.usage.completion_tokens / 1000) * \\\n",
    "                            self.pricing[model][\"output\"]\n",
    "                total_cost = input_cost + output_cost\n",
    "            else:\n",
    "                total_cost = 0\n",
    "            \n",
    "            return LLMResponse(\n",
    "                text=response.choices[0].message.content,\n",
    "                provider=\"OpenAI\",\n",
    "                model=model,\n",
    "                tokens_used=response.usage.total_tokens,\n",
    "                cost=total_cost,\n",
    "                latency=latency,\n",
    "                metadata={\n",
    "                    \"finish_reason\": response.choices[0].finish_reason,\n",
    "                    \"response_id\": response.id\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"OpenAI API error: {e}\")\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate tokens for OpenAI.\"\"\"\n",
    "        # Rough estimation: 1 token \u2248 4 characters\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check availability.\"\"\"\n",
    "        return self.available\n",
    "\n",
    "\n",
    "class AnthropicProvider(BaseLLMProvider):\n",
    "    \"\"\"Anthropic provider implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"claude-3-haiku-20240307\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.pricing = {\n",
    "            \"claude-3-haiku\": {\"input\": 0.00025, \"output\": 0.00125},\n",
    "            \"claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015},\n",
    "            \"claude-3-opus\": {\"input\": 0.015, \"output\": 0.075}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            import anthropic\n",
    "            self.client = anthropic.Anthropic(api_key=api_key)\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"Anthropic library not installed. Run: pip install anthropic\")\n",
    "            self.available = False\n",
    "        except Exception as e:\n",
    "            print(f\"Anthropic initialization error: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> LLMResponse:\n",
    "        \"\"\"Anthropic completion.\"\"\"\n",
    "        return self.chat([{\"role\": \"user\", \"content\": prompt}], **kwargs)\n",
    "    \n",
    "    def chat(self, messages: List[Dict], **kwargs) -> LLMResponse:\n",
    "        \"\"\"Anthropic chat completion.\"\"\"\n",
    "        if not self.available:\n",
    "            raise Exception(\"Anthropic provider not available\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Convert messages format if needed\n",
    "            if messages[0].get(\"role\") == \"system\":\n",
    "                system = messages[0][\"content\"]\n",
    "                messages = messages[1:]\n",
    "            else:\n",
    "                system = None\n",
    "            \n",
    "            response = self.client.messages.create(\n",
    "                model=kwargs.get(\"model\", self.model),\n",
    "                messages=messages,\n",
    "                max_tokens=kwargs.get(\"max_tokens\", 512),\n",
    "                temperature=kwargs.get(\"temperature\", 0.7),\n",
    "                system=system\n",
    "            )\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Estimate cost (simplified)\n",
    "            tokens_used = len(prompt) // 4 + len(response.content[0].text) // 4\n",
    "            cost = (tokens_used / 1000) * 0.001  # Simplified pricing\n",
    "            \n",
    "            return LLMResponse(\n",
    "                text=response.content[0].text,\n",
    "                provider=\"Anthropic\",\n",
    "                model=kwargs.get(\"model\", self.model),\n",
    "                tokens_used=tokens_used,\n",
    "                cost=cost,\n",
    "                latency=latency,\n",
    "                metadata={\n",
    "                    \"response_id\": response.id,\n",
    "                    \"stop_reason\": response.stop_reason\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Anthropic API error: {e}\")\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate tokens for Anthropic.\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def is_available(self) -> bool:\n",
    "        \"\"\"Check availability.\"\"\"\n",
    "        return self.available\n",
    "\n",
    "\n",
    "class UnifiedLLM:\n",
    "    \"\"\"\n",
    "    Unified LLM interface that works with multiple providers.\n",
    "    Easy switching, fallback support, and unified response format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.providers: Dict[Provider, BaseLLMProvider] = {}\n",
    "        self.primary_provider: Optional[Provider] = None\n",
    "        self.fallback_chain: List[Provider] = []\n",
    "        self.usage_stats: Dict[str, Any] = {\n",
    "            \"total_requests\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_cost\": 0.0,\n",
    "            \"provider_usage\": {},\n",
    "            \"errors\": []\n",
    "        }\n",
    "    \n",
    "    def add_provider(self, provider: Provider, config: Dict) -> None:\n",
    "        \"\"\"Add a provider with configuration.\"\"\"\n",
    "        \n",
    "        if provider == Provider.OPENAI:\n",
    "            self.providers[provider] = OpenAIProvider(\n",
    "                api_key=config[\"api_key\"],\n",
    "                model=config.get(\"model\", \"gpt-3.5-turbo\")\n",
    "            )\n",
    "        elif provider == Provider.ANTHROPIC:\n",
    "            self.providers[provider] = AnthropicProvider(\n",
    "                api_key=config[\"api_key\"],\n",
    "                model=config.get(\"model\", \"claude-3-haiku-20240307\")\n",
    "            )\n",
    "        # Add more providers as needed\n",
    "        \n",
    "        # Set as primary if first provider\n",
    "        if self.primary_provider is None and \\\n",
    "           self.providers[provider].is_available():\n",
    "            self.primary_provider = provider\n",
    "        \n",
    "        # Add to fallback chain\n",
    "        if provider not in self.fallback_chain and \\\n",
    "           self.providers[provider].is_available():\n",
    "            self.fallback_chain.append(provider)\n",
    "    \n",
    "    def set_primary(self, provider: Provider) -> None:\n",
    "        \"\"\"Set primary provider.\"\"\"\n",
    "        if provider in self.providers and \\\n",
    "           self.providers[provider].is_available():\n",
    "            self.primary_provider = provider\n",
    "        else:\n",
    "            raise ValueError(f\"Provider {provider} not available\")\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Generate completion with automatic fallback.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            **kwargs: Additional parameters\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse with unified format\n",
    "        \"\"\"\n",
    "        \n",
    "        self.usage_stats[\"total_requests\"] += 1\n",
    "        \n",
    "        # Try primary provider first\n",
    "        if self.primary_provider:\n",
    "            try:\n",
    "                response = self.providers[self.primary_provider].complete(\n",
    "                    prompt, **kwargs\n",
    "                )\n",
    "                self._update_stats(response)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                self.usage_stats[\"errors\"].append({\n",
    "                    \"provider\": self.primary_provider.value,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "        \n",
    "        # Try fallback providers\n",
    "        for provider in self.fallback_chain:\n",
    "            if provider == self.primary_provider:\n",
    "                continue  # Already tried\n",
    "            \n",
    "            try:\n",
    "                response = self.providers[provider].complete(prompt, **kwargs)\n",
    "                self._update_stats(response)\n",
    "                print(f\"\u26a0\ufe0f Used fallback provider: {provider.value}\")\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                self.usage_stats[\"errors\"].append({\n",
    "                    \"provider\": provider.value,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"All providers failed\")\n",
    "    \n",
    "    def chat(self, messages: List[Dict], **kwargs) -> LLMResponse:\n",
    "        \"\"\"Chat completion with automatic fallback.\"\"\"\n",
    "        \n",
    "        self.usage_stats[\"total_requests\"] += 1\n",
    "        \n",
    "        # Try primary provider\n",
    "        if self.primary_provider:\n",
    "            try:\n",
    "                response = self.providers[self.primary_provider].chat(\n",
    "                    messages, **kwargs\n",
    "                )\n",
    "                self._update_stats(response)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                self.usage_stats[\"errors\"].append({\n",
    "                    \"provider\": self.primary_provider.value,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": time.time()\n",
    "                })\n",
    "        \n",
    "        # Try fallbacks\n",
    "        for provider in self.fallback_chain:\n",
    "            if provider == self.primary_provider:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                response = self.providers[provider].chat(messages, **kwargs)\n",
    "                self._update_stats(response)\n",
    "                print(f\"\u26a0\ufe0f Used fallback provider: {provider.value}\")\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"All providers failed\")\n",
    "    \n",
    "    def _update_stats(self, response: LLMResponse):\n",
    "        \"\"\"Update usage statistics.\"\"\"\n",
    "        self.usage_stats[\"total_tokens\"] += response.tokens_used\n",
    "        self.usage_stats[\"total_cost\"] += response.cost\n",
    "        \n",
    "        if response.provider not in self.usage_stats[\"provider_usage\"]:\n",
    "            self.usage_stats[\"provider_usage\"][response.provider] = {\n",
    "                \"requests\": 0,\n",
    "                \"tokens\": 0,\n",
    "                \"cost\": 0.0,\n",
    "                \"avg_latency\": 0.0\n",
    "            }\n",
    "        \n",
    "        stats = self.usage_stats[\"provider_usage\"][response.provider]\n",
    "        stats[\"requests\"] += 1\n",
    "        stats[\"tokens\"] += response.tokens_used\n",
    "        stats[\"cost\"] += response.cost\n",
    "        \n",
    "        # Update average latency\n",
    "        prev_avg = stats[\"avg_latency\"]\n",
    "        stats[\"avg_latency\"] = (prev_avg * (stats[\"requests\"] - 1) + \n",
    "                                response.latency) / stats[\"requests\"]\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        return self.usage_stats\n",
    "    \n",
    "    def estimate_cost(self, prompt: str, provider: Optional[Provider] = None) -> float:\n",
    "        \"\"\"Estimate cost before making request.\"\"\"\n",
    "        \n",
    "        if provider is None:\n",
    "            provider = self.primary_provider\n",
    "        \n",
    "        if provider and provider in self.providers:\n",
    "            tokens = self.providers[provider].estimate_tokens(prompt)\n",
    "            # Simplified cost estimation\n",
    "            return (tokens / 1000) * 0.002  # Average cost\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def compare_providers(self, prompt: str) -> Dict[str, LLMResponse]:\n",
    "        \"\"\"Compare response from all available providers.\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for provider, llm in self.providers.items():\n",
    "            if llm.is_available():\n",
    "                try:\n",
    "                    response = llm.complete(prompt)\n",
    "                    results[provider.value] = response\n",
    "                except Exception as e:\n",
    "                    results[provider.value] = f\"Error: {e}\"\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def demonstrate_usage():\n",
    "    \"\"\"Demonstrate the unified LLM interface.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"UNIFIED LLM INTERFACE DEMONSTRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Example usage code\n",
    "    usage_code = '''\n",
    "# Initialize unified interface\n",
    "llm = UnifiedLLM()\n",
    "\n",
    "# Add multiple providers\n",
    "llm.add_provider(Provider.OPENAI, {\n",
    "    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"model\": \"gpt-3.5-turbo\"\n",
    "})\n",
    "\n",
    "llm.add_provider(Provider.ANTHROPIC, {\n",
    "    \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"model\": \"claude-3-haiku-20240307\"\n",
    "})\n",
    "\n",
    "# Set primary provider\n",
    "llm.set_primary(Provider.OPENAI)\n",
    "\n",
    "# Simple completion - automatically handles fallback!\n",
    "response = llm.complete(\n",
    "    \"Explain machine learning in one sentence\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.text}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")\n",
    "print(f\"Latency: {response.latency:.2f}s\")\n",
    "\n",
    "# Chat completion\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "# Compare providers\n",
    "comparison = llm.compare_providers(\"What is 2+2?\")\n",
    "for provider, result in comparison.items():\n",
    "    if isinstance(result, LLMResponse):\n",
    "        print(f\"{provider}: {result.text}\")\n",
    "\n",
    "# Get usage statistics\n",
    "stats = llm.get_stats()\n",
    "print(f\"Total requests: {stats['total_requests']}\")\n",
    "print(f\"Total cost: ${stats['total_cost']:.4f}\")\n",
    "\n",
    "# Estimate cost before making request\n",
    "estimated_cost = llm.estimate_cost(\"Long prompt here...\" * 100)\n",
    "print(f\"Estimated cost: ${estimated_cost:.6f}\")\n",
    "    '''\n",
    "    \n",
    "    print(\"USAGE EXAMPLE:\")\n",
    "    print(usage_code)\n",
    "    \n",
    "    # Show migration simplicity\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MIGRATION SIMPLICITY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    migration_example = '''\n",
    "# Switching providers is ONE LINE!\n",
    "\n",
    "# Before migration (using OpenAI)\n",
    "llm.set_primary(Provider.OPENAI)\n",
    "response = llm.complete(\"Hello world\")\n",
    "\n",
    "# After migration (using Anthropic)\n",
    "llm.set_primary(Provider.ANTHROPIC)\n",
    "response = llm.complete(\"Hello world\")  # Same code!\n",
    "\n",
    "# Or gradually migrate with weighted selection\n",
    "import random\n",
    "\n",
    "def gradual_migration(prompt: str, anthropic_weight: float = 0.1):\n",
    "    \"\"\"Gradually increase Anthropic usage.\"\"\"\n",
    "    if random.random() < anthropic_weight:\n",
    "        llm.set_primary(Provider.ANTHROPIC)\n",
    "    else:\n",
    "        llm.set_primary(Provider.OPENAI)\n",
    "    \n",
    "    return llm.complete(prompt)\n",
    "\n",
    "# Start with 10% on Anthropic, increase over time\n",
    "response = gradual_migration(\"Test prompt\", anthropic_weight=0.1)\n",
    "    '''\n",
    "    \n",
    "    print(migration_example)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run API abstraction exercise.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXERCISE 4: API ABSTRACTION LAYER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Note about the implementation\n",
    "    print(\"\\n\ud83d\udcdd IMPLEMENTATION OVERVIEW:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"This solution provides:\")\n",
    "    print(\"\u2022 Unified interface for multiple providers\")\n",
    "    print(\"\u2022 Automatic fallback on failures\")\n",
    "    print(\"\u2022 Consistent response format\")\n",
    "    print(\"\u2022 Usage tracking and statistics\")\n",
    "    print(\"\u2022 Cost estimation and tracking\")\n",
    "    print(\"\u2022 Easy provider switching\")\n",
    "    print(\"\u2022 Gradual migration support\")\n",
    "    \n",
    "    # Show the usage\n",
    "    demonstrate_usage()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY BENEFITS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    benefits = [\n",
    "        \"\u2705 Provider Independence: Switch providers with one line\",\n",
    "        \"\u2705 Reliability: Automatic fallback prevents downtime\",\n",
    "        \"\u2705 Cost Control: Track and estimate costs across providers\",\n",
    "        \"\u2705 Easy Migration: Gradually shift traffic between providers\",\n",
    "        \"\u2705 Unified Interface: Same code regardless of provider\",\n",
    "        \"\u2705 Statistics: Built-in usage and performance tracking\",\n",
    "        \"\u2705 Extensible: Easy to add new providers\"\n",
    "    ]\n",
    "    \n",
    "    for benefit in benefits:\n",
    "        print(benefit)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXERCISE 4 COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\u2705 You now have a production-ready multi-provider LLM interface!\")\n",
    "    print(\"   This abstraction layer makes provider switching trivial!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7.6 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.1: Secure Key Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_1_7_6_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 1 Solution: Secure Key Storage\n",
    "A complete key management system with multiple sources, validation, and security.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "from dotenv import load_dotenv\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SecureKeyManager:\n",
    "    \"\"\"\n",
    "    Complete key management system that:\n",
    "    - Loads keys from multiple sources\n",
    "    - Validates key format\n",
    "    - Provides fallback options\n",
    "    - Never exposes keys in logs or errors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir: str = \".\"):\n",
    "        \"\"\"Initialize the secure key manager\"\"\"\n",
    "        self.config_dir = Path(config_dir)\n",
    "        self.keys = {}\n",
    "        self.key_sources = {}\n",
    "        self.validation_errors = []\n",
    "        self._load_all_keys()\n",
    "    \n",
    "    def _load_all_keys(self):\n",
    "        \"\"\"Load keys from all available sources in priority order\"\"\"\n",
    "        # Priority order: Environment > .env > config.json\n",
    "        \n",
    "        # 1. Try environment variables first (highest priority)\n",
    "        self._load_from_environment()\n",
    "        \n",
    "        # 2. Try .env file\n",
    "        self._load_from_dotenv()\n",
    "        \n",
    "        # 3. Try config.json (lowest priority)\n",
    "        self._load_from_config()\n",
    "        \n",
    "        # Validate all loaded keys\n",
    "        self._validate_all_keys()\n",
    "    \n",
    "    def _load_from_environment(self):\n",
    "        \"\"\"Load keys directly from environment variables\"\"\"\n",
    "        env_keys = {\n",
    "            \"openai\": \"OPENAI_API_KEY\",\n",
    "            \"anthropic\": \"ANTHROPIC_API_KEY\",\n",
    "            \"google\": \"GOOGLE_API_KEY\",\n",
    "            \"replicate\": \"REPLICATE_API_TOKEN\"\n",
    "        }\n",
    "        \n",
    "        for provider, env_var in env_keys.items():\n",
    "            key = os.environ.get(env_var)\n",
    "            if key and key != \"ADD_YOUR_KEY_HERE\":\n",
    "                self.keys[provider] = key\n",
    "                self.key_sources[provider] = \"environment\"\n",
    "    \n",
    "    def _load_from_dotenv(self):\n",
    "        \"\"\"Load keys from .env file\"\"\"\n",
    "        env_file = self.config_dir / \".env\"\n",
    "        \n",
    "        if env_file.exists():\n",
    "            # Load .env file\n",
    "            load_dotenv(env_file)\n",
    "            \n",
    "            # Check for keys not already loaded\n",
    "            env_keys = {\n",
    "                \"openai\": \"OPENAI_API_KEY\",\n",
    "                \"anthropic\": \"ANTHROPIC_API_KEY\",\n",
    "                \"google\": \"GOOGLE_API_KEY\",\n",
    "                \"replicate\": \"REPLICATE_API_TOKEN\"\n",
    "            }\n",
    "            \n",
    "            for provider, env_var in env_keys.items():\n",
    "                if provider not in self.keys:\n",
    "                    key = os.getenv(env_var)\n",
    "                    if key and key != \"ADD_YOUR_KEY_HERE\":\n",
    "                        self.keys[provider] = key\n",
    "                        self.key_sources[provider] = \".env\"\n",
    "    \n",
    "    def _load_from_config(self):\n",
    "        \"\"\"Load keys from config.json file\"\"\"\n",
    "        config_file = self.config_dir / \"config.json\"\n",
    "        \n",
    "        if config_file.exists():\n",
    "            try:\n",
    "                with open(config_file) as f:\n",
    "                    config = json.load(f)\n",
    "                \n",
    "                # Map config keys to providers\n",
    "                config_map = {\n",
    "                    \"openai\": \"openai_key\",\n",
    "                    \"anthropic\": \"anthropic_key\",\n",
    "                    \"google\": \"google_key\",\n",
    "                    \"replicate\": \"replicate_key\"\n",
    "                }\n",
    "                \n",
    "                for provider, config_key in config_map.items():\n",
    "                    if provider not in self.keys:\n",
    "                        key = config.get(config_key)\n",
    "                        if key and key != \"ADD_YOUR_KEY_HERE\":\n",
    "                            self.keys[provider] = key\n",
    "                            self.key_sources[provider] = \"config.json\"\n",
    "            \n",
    "            except json.JSONDecodeError as e:\n",
    "                self.validation_errors.append(f\"Invalid config.json: {e}\")\n",
    "    \n",
    "    def _validate_all_keys(self):\n",
    "        \"\"\"Validate format of all loaded keys\"\"\"\n",
    "        validation_rules = {\n",
    "            \"openai\": {\n",
    "                \"prefix\": \"sk-\",\n",
    "                \"min_length\": 20,\n",
    "                \"pattern\": r'^sk-[a-zA-Z0-9]{20,}$'\n",
    "            },\n",
    "            \"anthropic\": {\n",
    "                \"prefix\": \"sk-ant-\",\n",
    "                \"min_length\": 20,\n",
    "                \"pattern\": r'^sk-ant-[a-zA-Z0-9-]+$'\n",
    "            },\n",
    "            \"google\": {\n",
    "                \"prefix\": \"AIza\",\n",
    "                \"min_length\": 30,\n",
    "                \"pattern\": r'^AIza[a-zA-Z0-9_-]{35}$'\n",
    "            },\n",
    "            \"replicate\": {\n",
    "                \"prefix\": None,  # No specific prefix\n",
    "                \"min_length\": 40,\n",
    "                \"pattern\": r'^[a-f0-9]{40}$'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for provider, key in self.keys.items():\n",
    "            rules = validation_rules.get(provider, {})\n",
    "            \n",
    "            # Check prefix\n",
    "            if rules.get(\"prefix\") and not key.startswith(rules[\"prefix\"]):\n",
    "                self.validation_errors.append(\n",
    "                    f\"{provider}: Invalid key format (wrong prefix)\"\n",
    "                )\n",
    "            \n",
    "            # Check length\n",
    "            min_length = rules.get(\"min_length\", 20)\n",
    "            if len(key) < min_length:\n",
    "                self.validation_errors.append(\n",
    "                    f\"{provider}: Key too short (min {min_length} chars)\"\n",
    "                )\n",
    "            \n",
    "            # Check for common mistakes\n",
    "            if \" \" in key:\n",
    "                self.validation_errors.append(\n",
    "                    f\"{provider}: Key contains spaces (copy/paste error?)\"\n",
    "                )\n",
    "            \n",
    "            if \"\\n\" in key or \"\\r\" in key:\n",
    "                self.validation_errors.append(\n",
    "                    f\"{provider}: Key contains newlines (copy/paste error?)\"\n",
    "                )\n",
    "    \n",
    "    def get_key(self, provider: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get API key for a provider with fallback options\n",
    "        Never exposes the actual key in logs or errors\n",
    "        \"\"\"\n",
    "        if provider in self.keys:\n",
    "            return self.keys[provider]\n",
    "        \n",
    "        # Try alternative names\n",
    "        alternatives = {\n",
    "            \"gemini\": \"google\",\n",
    "            \"claude\": \"anthropic\",\n",
    "            \"gpt\": \"openai\"\n",
    "        }\n",
    "        \n",
    "        alt_provider = alternatives.get(provider)\n",
    "        if alt_provider and alt_provider in self.keys:\n",
    "            return self.keys[alt_provider]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_masked_key(self, provider: str) -> str:\n",
    "        \"\"\"Get a masked version of the key for display\"\"\"\n",
    "        key = self.get_key(provider)\n",
    "        \n",
    "        if not key:\n",
    "            return \"Not configured\"\n",
    "        \n",
    "        if len(key) < 10:\n",
    "            return \"Invalid\"\n",
    "        \n",
    "        # Show first 7 and last 4 characters only\n",
    "        return f\"{key[:7]}...{key[-4:]}\"\n",
    "    \n",
    "    def get_key_hash(self, provider: str) -> Optional[str]:\n",
    "        \"\"\"Get SHA-256 hash of key for comparison without exposing it\"\"\"\n",
    "        key = self.get_key(provider)\n",
    "        \n",
    "        if not key:\n",
    "            return None\n",
    "        \n",
    "        return hashlib.sha256(key.encode()).hexdigest()\n",
    "    \n",
    "    def is_configured(self, provider: str) -> bool:\n",
    "        \"\"\"Check if a provider is configured\"\"\"\n",
    "        return provider in self.keys\n",
    "    \n",
    "    def get_configured_providers(self) -> List[str]:\n",
    "        \"\"\"Get list of configured providers\"\"\"\n",
    "        return list(self.keys.keys())\n",
    "    \n",
    "    def get_status_report(self) -> Dict:\n",
    "        \"\"\"Get detailed status report without exposing keys\"\"\"\n",
    "        return {\n",
    "            \"configured_providers\": self.get_configured_providers(),\n",
    "            \"total_configured\": len(self.keys),\n",
    "            \"sources\": {\n",
    "                provider: source \n",
    "                for provider, source in self.key_sources.items()\n",
    "            },\n",
    "            \"validation_errors\": self.validation_errors,\n",
    "            \"is_valid\": len(self.validation_errors) == 0\n",
    "        }\n",
    "    \n",
    "    def safe_log_status(self):\n",
    "        \"\"\"Log status information without exposing sensitive data\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\ud83d\udd10 Secure Key Manager Status\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        status = self.get_status_report()\n",
    "        \n",
    "        # Show configured providers\n",
    "        if status[\"configured_providers\"]:\n",
    "            print(f\"\\n\u2705 Configured Providers ({status['total_configured']}):\")\n",
    "            for provider in status[\"configured_providers\"]:\n",
    "                source = status[\"sources\"][provider]\n",
    "                masked = self.get_masked_key(provider)\n",
    "                print(f\"  \u2022 {provider}: {masked} (from {source})\")\n",
    "        else:\n",
    "            print(\"\\n\u274c No providers configured\")\n",
    "        \n",
    "        # Show validation errors\n",
    "        if status[\"validation_errors\"]:\n",
    "            print(f\"\\n\u26a0\ufe0f Validation Errors:\")\n",
    "            for error in status[\"validation_errors\"]:\n",
    "                print(f\"  \u2022 {error}\")\n",
    "        \n",
    "        # Never log actual keys!\n",
    "        print(\"\\n\ud83d\udca1 Keys are loaded and validated but never exposed in logs\")\n",
    "    \n",
    "    def export_safe_config(self, filepath: str = \"config.safe.json\"):\n",
    "        \"\"\"Export configuration info without exposing keys\"\"\"\n",
    "        safe_config = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"providers\": {}\n",
    "        }\n",
    "        \n",
    "        for provider in self.keys:\n",
    "            safe_config[\"providers\"][provider] = {\n",
    "                \"configured\": True,\n",
    "                \"source\": self.key_sources.get(provider),\n",
    "                \"key_hash\": self.get_key_hash(provider),\n",
    "                \"masked_key\": self.get_masked_key(provider)\n",
    "            }\n",
    "        \n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(safe_config, f, indent=2)\n",
    "        \n",
    "        print(f\"\u2705 Safe configuration exported to {filepath}\")\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Secure Key Management System Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create the secure key manager\n",
    "    manager = SecureKeyManager()\n",
    "    \n",
    "    # Display safe status\n",
    "    manager.safe_log_status()\n",
    "    \n",
    "    # Test key retrieval with fallback\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Testing Key Retrieval with Fallback\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_providers = [\"openai\", \"gpt\", \"claude\", \"anthropic\", \"gemini\", \"google\"]\n",
    "    \n",
    "    for provider in test_providers:\n",
    "        if manager.is_configured(provider):\n",
    "            masked = manager.get_masked_key(provider)\n",
    "            print(f\"\u2705 {provider}: {masked}\")\n",
    "        else:\n",
    "            # Try fallback\n",
    "            key = manager.get_key(provider)\n",
    "            if key:\n",
    "                print(f\"\u2705 {provider}: Found via fallback\")\n",
    "            else:\n",
    "                print(f\"\u274c {provider}: Not configured\")\n",
    "    \n",
    "    # Export safe configuration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    manager.export_safe_config()\n",
    "    \n",
    "    print(\"\\n\u2705 Secure Key Manager initialized successfully!\")\n",
    "    print(\"   \u2022 Keys are loaded from multiple sources\")\n",
    "    print(\"   \u2022 All keys are validated\")\n",
    "    print(\"   \u2022 Fallback options are available\")\n",
    "    print(\"   \u2022 Keys are never exposed in logs or errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.2: Multi-Provider Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_2_7_6_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 2 Solution: Multi-Provider Authentication\n",
    "A class that authenticates with multiple providers and automatically fails over.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional, Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ProviderStatus(Enum):\n",
    "    \"\"\"Status of each provider\"\"\"\n",
    "    AVAILABLE = \"available\"\n",
    "    FAILED = \"failed\"\n",
    "    RATE_LIMITED = \"rate_limited\"\n",
    "    NOT_CONFIGURED = \"not_configured\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProviderInfo:\n",
    "    \"\"\"Information about a provider\"\"\"\n",
    "    name: str\n",
    "    status: ProviderStatus\n",
    "    client: Optional[Any] = None\n",
    "    last_error: Optional[str] = None\n",
    "    last_check: Optional[datetime] = None\n",
    "    priority: int = 0\n",
    "    \n",
    "\n",
    "class MultiProviderAuth:\n",
    "    \"\"\"\n",
    "    Multi-provider authentication with automatic failover.\n",
    "    Manages connections to multiple AI providers and switches between them seamlessly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preferred_order: List[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize multi-provider authentication\n",
    "        \n",
    "        Args:\n",
    "            preferred_order: List of providers in order of preference\n",
    "                           Default: [\"openai\", \"anthropic\", \"google\"]\n",
    "        \"\"\"\n",
    "        self.providers = {}\n",
    "        self.preferred_order = preferred_order or [\"openai\", \"anthropic\", \"google\"]\n",
    "        self.current_provider = None\n",
    "        self.fallback_chain = []\n",
    "        \n",
    "        # Initialize all providers\n",
    "        self._initialize_providers()\n",
    "        \n",
    "        # Set up fallback chain\n",
    "        self._setup_fallback_chain()\n",
    "    \n",
    "    def _initialize_providers(self):\n",
    "        \"\"\"Initialize all available providers\"\"\"\n",
    "        \n",
    "        # OpenAI\n",
    "        self._init_openai()\n",
    "        \n",
    "        # Anthropic\n",
    "        self._init_anthropic()\n",
    "        \n",
    "        # Google Gemini\n",
    "        self._init_google()\n",
    "        \n",
    "        print(f\"\u2705 Initialized {len(self.providers)} providers\")\n",
    "    \n",
    "    def _init_openai(self):\n",
    "        \"\"\"Initialize OpenAI provider\"\"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        \n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            self.providers[\"openai\"] = ProviderInfo(\n",
    "                name=\"openai\",\n",
    "                status=ProviderStatus.NOT_CONFIGURED,\n",
    "                priority=self.preferred_order.index(\"openai\") if \"openai\" in self.preferred_order else 99\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI(api_key=api_key)\n",
    "            \n",
    "            # Test the connection\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                max_tokens=1\n",
    "            )\n",
    "            \n",
    "            self.providers[\"openai\"] = ProviderInfo(\n",
    "                name=\"openai\",\n",
    "                status=ProviderStatus.AVAILABLE,\n",
    "                client=client,\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"openai\") if \"openai\" in self.preferred_order else 99\n",
    "            )\n",
    "            print(\"\u2705 OpenAI authenticated successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            self.providers[\"openai\"] = ProviderInfo(\n",
    "                name=\"openai\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=\"Package not installed\",\n",
    "                priority=99\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.providers[\"openai\"] = ProviderInfo(\n",
    "                name=\"openai\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=str(e)[:100],\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"openai\") if \"openai\" in self.preferred_order else 99\n",
    "            )\n",
    "    \n",
    "    def _init_anthropic(self):\n",
    "        \"\"\"Initialize Anthropic provider\"\"\"\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        \n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            self.providers[\"anthropic\"] = ProviderInfo(\n",
    "                name=\"anthropic\",\n",
    "                status=ProviderStatus.NOT_CONFIGURED,\n",
    "                priority=self.preferred_order.index(\"anthropic\") if \"anthropic\" in self.preferred_order else 99\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic(api_key=api_key)\n",
    "            \n",
    "            # Test the connection\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-haiku-20240307\",\n",
    "                max_tokens=1,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}]\n",
    "            )\n",
    "            \n",
    "            self.providers[\"anthropic\"] = ProviderInfo(\n",
    "                name=\"anthropic\",\n",
    "                status=ProviderStatus.AVAILABLE,\n",
    "                client=client,\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"anthropic\") if \"anthropic\" in self.preferred_order else 99\n",
    "            )\n",
    "            print(\"\u2705 Anthropic authenticated successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            self.providers[\"anthropic\"] = ProviderInfo(\n",
    "                name=\"anthropic\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=\"Package not installed\",\n",
    "                priority=99\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.providers[\"anthropic\"] = ProviderInfo(\n",
    "                name=\"anthropic\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=str(e)[:100],\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"anthropic\") if \"anthropic\" in self.preferred_order else 99\n",
    "            )\n",
    "    \n",
    "    def _init_google(self):\n",
    "        \"\"\"Initialize Google Gemini provider\"\"\"\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if not api_key or api_key == \"ADD_YOUR_KEY_HERE\":\n",
    "            self.providers[\"google\"] = ProviderInfo(\n",
    "                name=\"google\",\n",
    "                status=ProviderStatus.NOT_CONFIGURED,\n",
    "                priority=self.preferred_order.index(\"google\") if \"google\" in self.preferred_order else 99\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=api_key)\n",
    "            \n",
    "            # Test the connection\n",
    "            model = genai.GenerativeModel('gemini-pro')\n",
    "            response = model.generate_content(\"test\")\n",
    "            \n",
    "            self.providers[\"google\"] = ProviderInfo(\n",
    "                name=\"google\",\n",
    "                status=ProviderStatus.AVAILABLE,\n",
    "                client=genai,\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"google\") if \"google\" in self.preferred_order else 99\n",
    "            )\n",
    "            print(\"\u2705 Google Gemini authenticated successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            self.providers[\"google\"] = ProviderInfo(\n",
    "                name=\"google\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=\"Package not installed\",\n",
    "                priority=99\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.providers[\"google\"] = ProviderInfo(\n",
    "                name=\"google\",\n",
    "                status=ProviderStatus.FAILED,\n",
    "                last_error=str(e)[:100],\n",
    "                last_check=datetime.now(),\n",
    "                priority=self.preferred_order.index(\"google\") if \"google\" in self.preferred_order else 99\n",
    "            )\n",
    "    \n",
    "    def _setup_fallback_chain(self):\n",
    "        \"\"\"Set up the fallback chain based on availability and preference\"\"\"\n",
    "        # Sort providers by priority and availability\n",
    "        available_providers = [\n",
    "            p for p in self.providers.values()\n",
    "            if p.status == ProviderStatus.AVAILABLE\n",
    "        ]\n",
    "        \n",
    "        # Sort by priority\n",
    "        available_providers.sort(key=lambda p: p.priority)\n",
    "        \n",
    "        self.fallback_chain = [p.name for p in available_providers]\n",
    "        \n",
    "        if self.fallback_chain:\n",
    "            self.current_provider = self.fallback_chain[0]\n",
    "            print(f\"\ud83d\udccd Primary provider: {self.current_provider}\")\n",
    "            if len(self.fallback_chain) > 1:\n",
    "                print(f\"\ud83d\udd04 Fallback chain: {' -> '.join(self.fallback_chain)}\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f No available providers!\")\n",
    "    \n",
    "    def get_client(self, provider: str = None) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get client for a specific provider or current provider\n",
    "        \n",
    "        Args:\n",
    "            provider: Name of provider, or None for current provider\n",
    "        \n",
    "        Returns:\n",
    "            Client object or None if not available\n",
    "        \"\"\"\n",
    "        if provider is None:\n",
    "            provider = self.current_provider\n",
    "        \n",
    "        if provider and provider in self.providers:\n",
    "            provider_info = self.providers[provider]\n",
    "            if provider_info.status == ProviderStatus.AVAILABLE:\n",
    "                return provider_info.client\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def failover_to_next(self) -> bool:\n",
    "        \"\"\"\n",
    "        Switch to the next available provider in the fallback chain\n",
    "        \n",
    "        Returns:\n",
    "            True if failover successful, False if no more providers\n",
    "        \"\"\"\n",
    "        if not self.current_provider or not self.fallback_chain:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            current_index = self.fallback_chain.index(self.current_provider)\n",
    "            if current_index < len(self.fallback_chain) - 1:\n",
    "                self.current_provider = self.fallback_chain[current_index + 1]\n",
    "                print(f\"\ud83d\udd04 Failed over to: {self.current_provider}\")\n",
    "                return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        print(\"\u274c No more providers to fail over to\")\n",
    "        return False\n",
    "    \n",
    "    def mark_provider_failed(self, provider: str, error: str):\n",
    "        \"\"\"Mark a provider as failed and trigger failover\"\"\"\n",
    "        if provider in self.providers:\n",
    "            self.providers[provider].status = ProviderStatus.FAILED\n",
    "            self.providers[provider].last_error = error\n",
    "            self.providers[provider].last_check = datetime.now()\n",
    "            \n",
    "            # Remove from fallback chain\n",
    "            if provider in self.fallback_chain:\n",
    "                self.fallback_chain.remove(provider)\n",
    "            \n",
    "            # Failover if this was the current provider\n",
    "            if provider == self.current_provider:\n",
    "                self.failover_to_next()\n",
    "    \n",
    "    def mark_provider_rate_limited(self, provider: str, retry_after: int = 60):\n",
    "        \"\"\"Mark a provider as rate limited and schedule retry\"\"\"\n",
    "        if provider in self.providers:\n",
    "            self.providers[provider].status = ProviderStatus.RATE_LIMITED\n",
    "            self.providers[provider].last_error = f\"Rate limited. Retry after {retry_after}s\"\n",
    "            self.providers[provider].last_check = datetime.now()\n",
    "            \n",
    "            print(f\"\u23f3 {provider} rate limited. Will retry in {retry_after}s\")\n",
    "            \n",
    "            # Temporarily remove from fallback chain\n",
    "            if provider in self.fallback_chain:\n",
    "                self.fallback_chain.remove(provider)\n",
    "            \n",
    "            # Failover if this was the current provider\n",
    "            if provider == self.current_provider:\n",
    "                self.failover_to_next()\n",
    "    \n",
    "    def make_request(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Make a request with automatic failover\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to send\n",
    "            max_retries: Maximum number of providers to try\n",
    "        \n",
    "        Returns:\n",
    "            Response text or None if all providers failed\n",
    "        \"\"\"\n",
    "        attempts = 0\n",
    "        original_provider = self.current_provider\n",
    "        \n",
    "        while attempts < max_retries and self.current_provider:\n",
    "            provider_name = self.current_provider\n",
    "            provider = self.providers.get(provider_name)\n",
    "            \n",
    "            if not provider or provider.status != ProviderStatus.AVAILABLE:\n",
    "                if not self.failover_to_next():\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"\ud83d\udd04 Attempting with {provider_name}...\")\n",
    "                \n",
    "                # Make provider-specific request\n",
    "                if provider_name == \"openai\":\n",
    "                    response = self._request_openai(provider.client, prompt)\n",
    "                elif provider_name == \"anthropic\":\n",
    "                    response = self._request_anthropic(provider.client, prompt)\n",
    "                elif provider_name == \"google\":\n",
    "                    response = self._request_google(provider.client, prompt)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown provider: {provider_name}\")\n",
    "                \n",
    "                print(f\"\u2705 Success with {provider_name}\")\n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_str = str(e).lower()\n",
    "                \n",
    "                # Check if rate limited\n",
    "                if \"rate\" in error_str and \"limit\" in error_str:\n",
    "                    self.mark_provider_rate_limited(provider_name)\n",
    "                else:\n",
    "                    self.mark_provider_failed(provider_name, str(e)[:100])\n",
    "                \n",
    "                print(f\"\u274c {provider_name} failed: {str(e)[:50]}\")\n",
    "                attempts += 1\n",
    "                \n",
    "                # Try failover\n",
    "                if not self.failover_to_next():\n",
    "                    break\n",
    "        \n",
    "        print(f\"\u274c All providers failed after {attempts} attempts\")\n",
    "        return None\n",
    "    \n",
    "    def _request_openai(self, client, prompt: str) -> str:\n",
    "        \"\"\"Make request to OpenAI\"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _request_anthropic(self, client, prompt: str) -> str:\n",
    "        \"\"\"Make request to Anthropic\"\"\"\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=100,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    \n",
    "    def _request_google(self, client, prompt: str) -> str:\n",
    "        \"\"\"Make request to Google Gemini\"\"\"\n",
    "        model = client.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get status of all providers\"\"\"\n",
    "        return {\n",
    "            \"current_provider\": self.current_provider,\n",
    "            \"fallback_chain\": self.fallback_chain,\n",
    "            \"providers\": {\n",
    "                name: {\n",
    "                    \"status\": p.status.value,\n",
    "                    \"last_error\": p.last_error,\n",
    "                    \"last_check\": p.last_check.isoformat() if p.last_check else None,\n",
    "                    \"priority\": p.priority\n",
    "                }\n",
    "                for name, p in self.providers.items()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def display_status(self):\n",
    "        \"\"\"Display current status of all providers\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\ud83d\udd0c Multi-Provider Authentication Status\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        status = self.get_status()\n",
    "        \n",
    "        print(f\"\\n\ud83d\udccd Current Provider: {status['current_provider'] or 'None'}\")\n",
    "        \n",
    "        if status['fallback_chain']:\n",
    "            print(f\"\ud83d\udd04 Fallback Chain: {' -> '.join(status['fallback_chain'])}\")\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Provider Status:\")\n",
    "        for name, info in status['providers'].items():\n",
    "            status_icon = {\n",
    "                \"available\": \"\u2705\",\n",
    "                \"failed\": \"\u274c\",\n",
    "                \"rate_limited\": \"\u23f3\",\n",
    "                \"not_configured\": \"\u26ab\",\n",
    "                \"unknown\": \"\u2753\"\n",
    "            }.get(info['status'], \"\u2753\")\n",
    "            \n",
    "            print(f\"  {status_icon} {name}: {info['status']}\")\n",
    "            if info['last_error']:\n",
    "                print(f\"      Error: {info['last_error']}\")\n",
    "\n",
    "\n",
    "# Demo and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Multi-Provider Authentication Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create multi-provider auth with custom preference order\n",
    "    auth = MultiProviderAuth(preferred_order=[\"google\", \"openai\", \"anthropic\"])\n",
    "    \n",
    "    # Display initial status\n",
    "    auth.display_status()\n",
    "    \n",
    "    # Test making requests with failover\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Testing Requests with Automatic Failover\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Say 'Hello from AI!'\",\n",
    "        \"What is 2+2?\",\n",
    "        \"Complete: The sky is...\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\n\ud83d\udcdd Prompt: {prompt}\")\n",
    "        response = auth.make_request(prompt)\n",
    "        if response:\n",
    "            print(f\"\ud83d\udcec Response: {response[:100]}\")\n",
    "        else:\n",
    "            print(\"\u274c No response (all providers failed)\")\n",
    "    \n",
    "    # Final status\n",
    "    print(\"\\n\")\n",
    "    auth.display_status()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.3: Rate Limit Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as: exercise_3_7_6_solution.py\n",
    "# From: Zero to AI Agent, Chapter 7, Section 7.6\n",
    "\n",
    "\"\"\"\n",
    "Exercise 3 Solution: Rate Limit Handler\n",
    "Robust rate limit handler with automatic throttling and helpful feedback.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "from typing import Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class RateLimitStatus(Enum):\n",
    "    \"\"\"Status of rate limiting\"\"\"\n",
    "    READY = \"ready\"\n",
    "    THROTTLED = \"throttled\"\n",
    "    WARNING = \"warning\"\n",
    "    BLOCKED = \"blocked\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RequestRecord:\n",
    "    \"\"\"Record of a single request\"\"\"\n",
    "    timestamp: datetime\n",
    "    provider: str\n",
    "    success: bool\n",
    "    tokens: int = 0\n",
    "\n",
    "\n",
    "class RobustRateLimitHandler:\n",
    "    \"\"\"\n",
    "    Advanced rate limit handler that:\n",
    "    - Tracks requests per minute for multiple providers\n",
    "    - Automatically throttles when approaching limits\n",
    "    - Provides helpful feedback about wait times\n",
    "    - Works with any API provider\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default limits for different providers (requests per minute, tokens per minute)\n",
    "    DEFAULT_LIMITS = {\n",
    "        \"openai\": {\"rpm\": 60, \"tpm\": 60000},\n",
    "        \"anthropic\": {\"rpm\": 50, \"tpm\": 100000},\n",
    "        \"google\": {\"rpm\": 60, \"tpm\": 1000000},\n",
    "        \"default\": {\"rpm\": 30, \"tpm\": 50000}\n",
    "    }\n",
    "    \n",
    "    def __init__(self, custom_limits: Dict[str, Dict[str, int]] = None, \n",
    "                 window_seconds: int = 60):\n",
    "        \"\"\"\n",
    "        Initialize the rate limit handler\n",
    "        \n",
    "        Args:\n",
    "            custom_limits: Custom rate limits per provider\n",
    "            window_seconds: Time window for rate limiting (default 60 seconds)\n",
    "        \"\"\"\n",
    "        self.limits = self.DEFAULT_LIMITS.copy()\n",
    "        if custom_limits:\n",
    "            self.limits.update(custom_limits)\n",
    "        \n",
    "        self.window_seconds = window_seconds\n",
    "        \n",
    "        # Track requests per provider\n",
    "        self.requests: Dict[str, deque] = {}\n",
    "        self.tokens: Dict[str, deque] = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_requests: Dict[str, int] = {}\n",
    "        self.total_throttled: Dict[str, int] = {}\n",
    "        self.total_blocked: Dict[str, int] = {}\n",
    "        \n",
    "        # Initialize tracking for each provider\n",
    "        for provider in self.limits:\n",
    "            self._init_provider(provider)\n",
    "    \n",
    "    def _init_provider(self, provider: str):\n",
    "        \"\"\"Initialize tracking structures for a provider\"\"\"\n",
    "        if provider not in self.requests:\n",
    "            self.requests[provider] = deque()\n",
    "            self.tokens[provider] = deque()\n",
    "            self.total_requests[provider] = 0\n",
    "            self.total_throttled[provider] = 0\n",
    "            self.total_blocked[provider] = 0\n",
    "    \n",
    "    def _clean_old_records(self, provider: str):\n",
    "        \"\"\"Remove records older than the time window\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(seconds=self.window_seconds)\n",
    "        \n",
    "        # Clean requests\n",
    "        while self.requests[provider] and self.requests[provider][0].timestamp < cutoff:\n",
    "            self.requests[provider].popleft()\n",
    "        \n",
    "        # Clean tokens\n",
    "        while self.tokens[provider] and self.tokens[provider][0][0] < cutoff:\n",
    "            self.tokens[provider].popleft()\n",
    "    \n",
    "    def _get_provider_key(self, provider: str) -> str:\n",
    "        \"\"\"Get the provider key, falling back to default if not found\"\"\"\n",
    "        if provider in self.limits:\n",
    "            return provider\n",
    "        return \"default\"\n",
    "    \n",
    "    def _calculate_usage(self, provider: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate current usage for a provider\"\"\"\n",
    "        self._clean_old_records(provider)\n",
    "        \n",
    "        limits = self.limits[provider]\n",
    "        current_rpm = len(self.requests[provider])\n",
    "        current_tpm = sum(t[1] for t in self.tokens[provider])\n",
    "        \n",
    "        rpm_percent = (current_rpm / limits[\"rpm\"]) * 100\n",
    "        tpm_percent = (current_tpm / limits[\"tpm\"]) * 100\n",
    "        max_usage = max(rpm_percent, tpm_percent)\n",
    "        \n",
    "        return {\n",
    "            \"current_rpm\": current_rpm,\n",
    "            \"current_tpm\": current_tpm,\n",
    "            \"rpm_percent\": rpm_percent,\n",
    "            \"tpm_percent\": tpm_percent,\n",
    "            \"max_usage\": max_usage,\n",
    "            \"limits\": limits\n",
    "        }\n",
    "    \n",
    "    def get_status(self, provider: str) -> RateLimitStatus:\n",
    "        \"\"\"Get current rate limit status for a provider\"\"\"\n",
    "        provider = self._get_provider_key(provider)\n",
    "        self._init_provider(provider)\n",
    "        \n",
    "        usage = self._calculate_usage(provider)\n",
    "        max_usage = usage[\"max_usage\"]\n",
    "        \n",
    "        if max_usage >= 100:\n",
    "            return RateLimitStatus.BLOCKED\n",
    "        elif max_usage >= 80:\n",
    "            return RateLimitStatus.WARNING\n",
    "        elif max_usage >= 60:\n",
    "            return RateLimitStatus.THROTTLED\n",
    "        else:\n",
    "            return RateLimitStatus.READY\n",
    "    \n",
    "    def check_and_wait(self, provider: str, estimated_tokens: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check rate limits and calculate wait time if necessary\n",
    "        \n",
    "        Args:\n",
    "            provider: API provider name\n",
    "            estimated_tokens: Estimated tokens for the request\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with status and wait time information\n",
    "        \"\"\"\n",
    "        provider = self._get_provider_key(provider)\n",
    "        self._init_provider(provider)\n",
    "        self._clean_old_records(provider)\n",
    "        \n",
    "        limits = self.limits[provider]\n",
    "        current_rpm = len(self.requests[provider])\n",
    "        current_tpm = sum(t[1] for t in self.tokens[provider])\n",
    "        \n",
    "        # Check if we would exceed limits\n",
    "        would_exceed_rpm = current_rpm >= limits[\"rpm\"]\n",
    "        would_exceed_tpm = (current_tpm + estimated_tokens) > limits[\"tpm\"]\n",
    "        \n",
    "        if would_exceed_rpm or would_exceed_tpm:\n",
    "            # Calculate wait time\n",
    "            if would_exceed_rpm and self.requests[provider]:\n",
    "                oldest_request = self.requests[provider][0]\n",
    "                wait_until = oldest_request.timestamp + timedelta(seconds=self.window_seconds)\n",
    "                wait_seconds = (wait_until - datetime.now()).total_seconds()\n",
    "                reason = f\"Request limit ({limits['rpm']} req/{self.window_seconds}s)\"\n",
    "            elif self.tokens[provider]:\n",
    "                oldest_token = self.tokens[provider][0]\n",
    "                wait_until = oldest_token[0] + timedelta(seconds=self.window_seconds)\n",
    "                wait_seconds = (wait_until - datetime.now()).total_seconds()\n",
    "                reason = f\"Token limit ({limits['tpm']} tokens/{self.window_seconds}s)\"\n",
    "            else:\n",
    "                wait_seconds = 0.1\n",
    "                reason = \"Rate limit reached\"\n",
    "            \n",
    "            wait_seconds = max(0, wait_seconds) + 0.1  # Add small buffer\n",
    "            \n",
    "            self.total_throttled[provider] += 1\n",
    "            \n",
    "            return {\n",
    "                \"status\": RateLimitStatus.BLOCKED,\n",
    "                \"wait_seconds\": wait_seconds,\n",
    "                \"reason\": reason,\n",
    "                \"current_usage\": {\"rpm\": current_rpm, \"tpm\": current_tpm},\n",
    "                \"limits\": limits,\n",
    "                \"message\": f\"\u23f3 Rate limit reached: {reason}. Waiting {wait_seconds:.1f}s...\"\n",
    "            }\n",
    "        \n",
    "        # Check usage percentage for warnings\n",
    "        rpm_percent = (current_rpm / limits[\"rpm\"]) * 100\n",
    "        tpm_percent = ((current_tpm + estimated_tokens) / limits[\"tpm\"]) * 100\n",
    "        max_usage = max(rpm_percent, tpm_percent)\n",
    "        \n",
    "        if max_usage >= 80:\n",
    "            status = RateLimitStatus.WARNING\n",
    "            message = f\"\u26a0\ufe0f Approaching rate limit: {max_usage:.0f}% usage\"\n",
    "        elif max_usage >= 60:\n",
    "            status = RateLimitStatus.THROTTLED\n",
    "            message = f\"\ud83d\udd04 Throttling recommended: {max_usage:.0f}% usage\"\n",
    "        else:\n",
    "            status = RateLimitStatus.READY\n",
    "            message = f\"\u2705 Ready: {max_usage:.0f}% usage\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"wait_seconds\": 0,\n",
    "            \"reason\": None,\n",
    "            \"current_usage\": {\"rpm\": current_rpm, \"tpm\": current_tpm},\n",
    "            \"limits\": limits,\n",
    "            \"message\": message\n",
    "        }\n",
    "    \n",
    "    def record_request(self, provider: str, tokens_used: int, success: bool = True):\n",
    "        \"\"\"Record a completed request\"\"\"\n",
    "        provider = self._get_provider_key(provider)\n",
    "        self._init_provider(provider)\n",
    "        \n",
    "        now = datetime.now()\n",
    "        \n",
    "        self.requests[provider].append(RequestRecord(\n",
    "            timestamp=now,\n",
    "            provider=provider,\n",
    "            success=success,\n",
    "            tokens=tokens_used\n",
    "        ))\n",
    "        \n",
    "        self.tokens[provider].append((now, tokens_used))\n",
    "        self.total_requests[provider] += 1\n",
    "    \n",
    "    def wait_if_needed(self, provider: str, estimated_tokens: int = 100) -> float:\n",
    "        \"\"\"\n",
    "        Wait if necessary to avoid rate limits\n",
    "        \n",
    "        Returns:\n",
    "            Number of seconds waited\n",
    "        \"\"\"\n",
    "        check_result = self.check_and_wait(provider, estimated_tokens)\n",
    "        \n",
    "        if check_result[\"wait_seconds\"] > 0:\n",
    "            print(check_result[\"message\"])\n",
    "            time.sleep(check_result[\"wait_seconds\"])\n",
    "            return check_result[\"wait_seconds\"]\n",
    "        \n",
    "        if check_result[\"status\"] in [RateLimitStatus.WARNING, RateLimitStatus.THROTTLED]:\n",
    "            print(check_result[\"message\"])\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_statistics(self, provider: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed statistics for a provider or all providers\"\"\"\n",
    "        if provider:\n",
    "            providers = [self._get_provider_key(provider)]\n",
    "        else:\n",
    "            providers = list(self.limits.keys())\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        for p in providers:\n",
    "            self._init_provider(p)\n",
    "            self._clean_old_records(p)\n",
    "            \n",
    "            current_rpm = len(self.requests[p])\n",
    "            current_tpm = sum(t[1] for t in self.tokens[p])\n",
    "            limits = self.limits[p]\n",
    "            \n",
    "            rpm_percent = (current_rpm / limits[\"rpm\"]) * 100 if limits[\"rpm\"] > 0 else 0\n",
    "            tpm_percent = (current_tpm / limits[\"tpm\"]) * 100 if limits[\"tpm\"] > 0 else 0\n",
    "            \n",
    "            # Calculate status without calling get_status to avoid redundant work\n",
    "            max_usage = max(rpm_percent, tpm_percent)\n",
    "            if max_usage >= 100:\n",
    "                status = \"blocked\"\n",
    "            elif max_usage >= 80:\n",
    "                status = \"warning\"\n",
    "            elif max_usage >= 60:\n",
    "                status = \"throttled\"\n",
    "            else:\n",
    "                status = \"ready\"\n",
    "            \n",
    "            stats[p] = {\n",
    "                \"current_rpm\": current_rpm,\n",
    "                \"current_tpm\": current_tpm,\n",
    "                \"limit_rpm\": limits[\"rpm\"],\n",
    "                \"limit_tpm\": limits[\"tpm\"],\n",
    "                \"rpm_usage_percent\": rpm_percent,\n",
    "                \"tpm_usage_percent\": tpm_percent,\n",
    "                \"total_requests\": self.total_requests[p],\n",
    "                \"times_throttled\": self.total_throttled[p],\n",
    "                \"times_blocked\": self.total_blocked[p],\n",
    "                \"status\": status\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def display_dashboard(self):\n",
    "        \"\"\"Display a comprehensive rate limit dashboard\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"\ud83d\udcca RATE LIMIT DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for provider, data in stats.items():\n",
    "            status_icons = {\n",
    "                \"ready\": \"\u2705\",\n",
    "                \"throttled\": \"\ud83d\udd04\", \n",
    "                \"warning\": \"\u26a0\ufe0f\",\n",
    "                \"blocked\": \"\u274c\"\n",
    "            }\n",
    "            icon = status_icons.get(data[\"status\"], \"\u2753\")\n",
    "            \n",
    "            print(f\"\\n{icon} {provider.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            rpm_bar = self._create_progress_bar(data[\"rpm_usage_percent\"])\n",
    "            print(f\"Requests: {rpm_bar} {data['current_rpm']}/{data['limit_rpm']} ({data['rpm_usage_percent']:.0f}%)\")\n",
    "            \n",
    "            tpm_bar = self._create_progress_bar(data[\"tpm_usage_percent\"])\n",
    "            print(f\"Tokens:   {tpm_bar} {data['current_tpm']}/{data['limit_tpm']} ({data['tpm_usage_percent']:.0f}%)\")\n",
    "            \n",
    "            print(f\"\\nTotal Requests: {data['total_requests']}\")\n",
    "            if data['times_throttled'] > 0:\n",
    "                print(f\"Times Throttled: {data['times_throttled']}\")\n",
    "    \n",
    "    def _create_progress_bar(self, percent: float, width: int = 20) -> str:\n",
    "        \"\"\"Create a visual progress bar\"\"\"\n",
    "        filled = int((percent / 100) * width)\n",
    "        filled = min(filled, width)\n",
    "        bar = \"\u2588\" * filled + \"\u2591\" * (width - filled)\n",
    "        return f\"[{bar}]\"\n",
    "\n",
    "\n",
    "def rate_limited(provider: str, handler: RobustRateLimitHandler):\n",
    "    \"\"\"Decorator to add rate limiting to any function\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            handler.wait_if_needed(provider, estimated_tokens=100)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                handler.record_request(provider, tokens_used=100, success=True)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                handler.record_request(provider, tokens_used=0, success=False)\n",
    "                raise e\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "# Demo and testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Robust Rate Limit Handler Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create handler with LOW limits AND SHORT window for demo\n",
    "    # Using 5-second window so we don't wait forever\n",
    "    handler = RobustRateLimitHandler(\n",
    "        custom_limits={\"test\": {\"rpm\": 5, \"tpm\": 500}},\n",
    "        window_seconds=5  # Short window for demo purposes\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\ud83d\udd0d Simulating API calls...\")\n",
    "    print(\"   (Using 5-second window with 5 requests/window limit)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i in range(12):\n",
    "        print(f\"\\nRequest {i+1}:\")\n",
    "        \n",
    "        # Check and wait if needed\n",
    "        wait_time = handler.wait_if_needed(\"test\", estimated_tokens=100)\n",
    "        if wait_time > 0:\n",
    "            print(f\"  \u2713 Waited {wait_time:.1f}s, now proceeding...\")\n",
    "        \n",
    "        # Simulate making the request\n",
    "        print(f\"  Making API call...\")\n",
    "        time.sleep(0.1)  # Simulate API call time\n",
    "        \n",
    "        # Record the request\n",
    "        handler.record_request(\"test\", tokens_used=100, success=True)\n",
    "        \n",
    "        # Show status\n",
    "        stats = handler.get_statistics(\"test\")[\"test\"]\n",
    "        print(f\"  Status: {stats['status']} - RPM: {stats['current_rpm']}/{stats['limit_rpm']}\")\n",
    "    \n",
    "    # Display final dashboard\n",
    "    print(\"\\n\")\n",
    "    handler.display_dashboard()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\u2705 Rate limiting demonstration complete!\")\n",
    "    print(\"\\nFeatures demonstrated:\")\n",
    "    print(\"  \u2022 Automatic request tracking\")\n",
    "    print(\"  \u2022 Smart throttling when approaching limits\")\n",
    "    print(\"  \u2022 Helpful wait time feedback\")\n",
    "    print(\"  \u2022 Comprehensive statistics dashboard\")\n",
    "    print(\"  \u2022 Works with any API provider\")\n",
    "    print(\"\\n\ud83d\udca1 In production, use window_seconds=60 for real rate limits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.6.4: API Key Audit Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: exercise_4_7_6_solution.py\n",
    "\n",
    "\"\"\"\n",
    "Exercise 4 Solution: API Key Audit Tool\n",
    "Comprehensive tool to scan projects for exposed API keys and generate security reports.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class ComprehensiveAPIAuditor:\n",
    "    \"\"\"\n",
    "    Advanced API key audit tool that:\n",
    "    - Scans projects for exposed API keys\n",
    "    - Checks Git history for accidentally committed keys\n",
    "    - Validates that all keys in use are properly secured\n",
    "    - Generates detailed security reports\n",
    "    \"\"\"\n",
    "    \n",
    "    # Enhanced patterns for different API key types\n",
    "    KEY_PATTERNS = {\n",
    "        \"OpenAI\": {\n",
    "            \"pattern\": r'sk-[a-zA-Z0-9]{48}',\n",
    "            \"description\": \"OpenAI API Key\",\n",
    "            \"severity\": \"CRITICAL\"\n",
    "        },\n",
    "        \"Anthropic\": {\n",
    "            \"pattern\": r'sk-ant-api[0-9]{2}-[a-zA-Z0-9-]+',\n",
    "            \"description\": \"Anthropic API Key\",\n",
    "            \"severity\": \"CRITICAL\"\n",
    "        },\n",
    "        \"Google/Gemini\": {\n",
    "            \"pattern\": r'AIza[a-zA-Z0-9_-]{35}',\n",
    "            \"description\": \"Google AI API Key\",\n",
    "            \"severity\": \"CRITICAL\"\n",
    "        },\n",
    "        \"Replicate\": {\n",
    "            \"pattern\": r'r8_[a-zA-Z0-9]{37}',\n",
    "            \"description\": \"Replicate API Token\",\n",
    "            \"severity\": \"HIGH\"\n",
    "        },\n",
    "        \"GitHub\": {\n",
    "            \"pattern\": r'ghp_[a-zA-Z0-9]{36}',\n",
    "            \"description\": \"GitHub Personal Access Token\",\n",
    "            \"severity\": \"HIGH\"\n",
    "        },\n",
    "        \"AWS\": {\n",
    "            \"pattern\": r'AKIA[A-Z0-9]{16}',\n",
    "            \"description\": \"AWS Access Key\",\n",
    "            \"severity\": \"CRITICAL\"\n",
    "        },\n",
    "        \"Generic API Key\": {\n",
    "            \"pattern\": r'(api[_-]?key|apikey|api_token|access[_-]?token)[\\s]*[:=][\\s]*[\"\\']([a-zA-Z0-9_-]{20,})[\"\\']',\n",
    "            \"description\": \"Generic API Key Pattern\",\n",
    "            \"severity\": \"MEDIUM\"\n",
    "        },\n",
    "        \"Generic Secret\": {\n",
    "            \"pattern\": r'(secret|password|passwd|pwd)[\\s]*[:=][\\s]*[\"\\']([^\"\\']{8,})[\"\\']',\n",
    "            \"description\": \"Generic Secret Pattern\",\n",
    "            \"severity\": \"MEDIUM\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Files and directories to skip\n",
    "    SKIP_PATTERNS = {\n",
    "        # Directories\n",
    "        \".git\", \".env\", \"venv\", \"env\", \"__pycache__\", \"node_modules\",\n",
    "        \".pytest_cache\", \"dist\", \"build\", \".vscode\", \".idea\",\n",
    "        # Files\n",
    "        \"*.pyc\", \"*.pyo\", \"*.pyd\", \"*.so\", \"*.dll\", \"*.dylib\",\n",
    "        \"*.exe\", \"*.bin\", \"*.jpg\", \"*.png\", \"*.gif\", \"*.pdf\"\n",
    "    }\n",
    "    \n",
    "    # Files that should be in .gitignore\n",
    "    SENSITIVE_FILES = [\n",
    "        \".env\", \".env.local\", \".env.production\", \".env.development\",\n",
    "        \"config.json\", \"secrets.json\", \"credentials.json\",\n",
    "        \"*.key\", \"*.pem\", \"*.p12\", \"*.pfx\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, project_dir: str = \".\"):\n",
    "        \"\"\"Initialize the auditor\"\"\"\n",
    "        self.project_dir = Path(project_dir).resolve()\n",
    "        self.findings = []\n",
    "        self.warnings = []\n",
    "        self.recommendations = []\n",
    "        self.scan_summary = {}\n",
    "    \n",
    "    def audit_complete(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a complete security audit\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive audit report\n",
    "        \"\"\"\n",
    "        print(\"\ud83d\udd0d Starting Comprehensive Security Audit...\")\n",
    "        print(f\"\ud83d\udcc1 Project: {self.project_dir}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Reset findings\n",
    "        self.findings = []\n",
    "        self.warnings = []\n",
    "        self.recommendations = []\n",
    "        \n",
    "        # Run all audit checks\n",
    "        self._scan_source_files()\n",
    "        self._check_gitignore()\n",
    "        self._scan_git_history()\n",
    "        self._check_environment_files()\n",
    "        self._validate_active_keys()\n",
    "        self._check_dependencies()\n",
    "        \n",
    "        # Generate report\n",
    "        report = self._generate_report()\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _scan_source_files(self):\n",
    "        \"\"\"Scan all source files for exposed keys\"\"\"\n",
    "        print(\"\\n\ud83d\udcdd Scanning source files...\")\n",
    "        \n",
    "        files_scanned = 0\n",
    "        files_with_issues = 0\n",
    "        \n",
    "        for file_path in self._get_files_to_scan():\n",
    "            violations = self._scan_file(file_path)\n",
    "            \n",
    "            if violations:\n",
    "                files_with_issues += 1\n",
    "                self.findings.extend(violations)\n",
    "            \n",
    "            files_scanned += 1\n",
    "        \n",
    "        self.scan_summary[\"files_scanned\"] = files_scanned\n",
    "        self.scan_summary[\"files_with_issues\"] = files_with_issues\n",
    "        \n",
    "        print(f\"  Scanned {files_scanned} files\")\n",
    "        if files_with_issues > 0:\n",
    "            print(f\"  \u26a0\ufe0f Found issues in {files_with_issues} files\")\n",
    "    \n",
    "    def _get_files_to_scan(self) -> List[Path]:\n",
    "        \"\"\"Get list of files to scan\"\"\"\n",
    "        files_to_scan = []\n",
    "        \n",
    "        for file_path in self.project_dir.rglob(\"*\"):\n",
    "            # Skip directories\n",
    "            if file_path.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Skip based on patterns\n",
    "            should_skip = False\n",
    "            for pattern in self.SKIP_PATTERNS:\n",
    "                if pattern.startswith(\"*\"):\n",
    "                    if file_path.suffix == pattern[1:]:\n",
    "                        should_skip = True\n",
    "                        break\n",
    "                elif pattern in file_path.parts:\n",
    "                    should_skip = True\n",
    "                    break\n",
    "            \n",
    "            if not should_skip:\n",
    "                files_to_scan.append(file_path)\n",
    "        \n",
    "        return files_to_scan\n",
    "    \n",
    "    def _scan_file(self, file_path: Path) -> List[Dict]:\n",
    "        \"\"\"Scan a single file for exposed keys\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "                for line_num, line in enumerate(content.splitlines(), 1):\n",
    "                    # Skip obvious comments\n",
    "                    stripped = line.strip()\n",
    "                    if stripped.startswith(('#', '//', '/*', '*', '\"\"\"', \"'''\")):\n",
    "                        continue\n",
    "                    \n",
    "                    # Check each pattern\n",
    "                    for key_type, info in self.KEY_PATTERNS.items():\n",
    "                        pattern = info[\"pattern\"]\n",
    "                        matches = re.finditer(pattern, line, re.IGNORECASE)\n",
    "                        \n",
    "                        for match in matches:\n",
    "                            # Validate if it's likely a real key\n",
    "                            if self._is_likely_real_key(match.group()):\n",
    "                                violations.append({\n",
    "                                    \"type\": \"exposed_key\",\n",
    "                                    \"file\": str(file_path.relative_to(self.project_dir)),\n",
    "                                    \"line\": line_num,\n",
    "                                    \"key_type\": key_type,\n",
    "                                    \"description\": info[\"description\"],\n",
    "                                    \"severity\": info[\"severity\"],\n",
    "                                    \"preview\": self._mask_line(line)\n",
    "                                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.warnings.append(f\"Could not scan {file_path}: {e}\")\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def _is_likely_real_key(self, text: str) -> bool:\n",
    "        \"\"\"Check if a match is likely a real API key\"\"\"\n",
    "        # Common placeholders to ignore\n",
    "        placeholders = [\n",
    "            \"your-key\", \"your_key\", \"add_your\", \"example\",\n",
    "            \"xxx\", \"...\", \"placeholder\", \"test\", \"demo\",\n",
    "            \"sample\", \"dummy\", \"fake\", \"mock\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for placeholder in placeholders:\n",
    "            if placeholder in text_lower:\n",
    "                return False\n",
    "        \n",
    "        # Check for repeated characters\n",
    "        if len(text) > 10:\n",
    "            unique_chars = len(set(text))\n",
    "            if unique_chars < len(text) / 4:  # Too repetitive\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _mask_line(self, line: str) -> str:\n",
    "        \"\"\"Mask sensitive data in a line\"\"\"\n",
    "        # Replace potential keys with masked versions\n",
    "        for key_type, info in self.KEY_PATTERNS.items():\n",
    "            pattern = info[\"pattern\"]\n",
    "            line = re.sub(\n",
    "                pattern,\n",
    "                lambda m: m.group()[:10] + \"***MASKED***\" if len(m.group()) > 10 else \"***\",\n",
    "                line,\n",
    "                flags=re.IGNORECASE\n",
    "            )\n",
    "        \n",
    "        # Truncate long lines\n",
    "        if len(line) > 80:\n",
    "            line = line[:77] + \"...\"\n",
    "        \n",
    "        return line\n",
    "    \n",
    "    def _check_gitignore(self):\n",
    "        \"\"\"Check if .gitignore properly excludes sensitive files\"\"\"\n",
    "        print(\"\\n\ud83d\udeab Checking .gitignore...\")\n",
    "        \n",
    "        gitignore_path = self.project_dir / \".gitignore\"\n",
    "        \n",
    "        if not gitignore_path.exists():\n",
    "            self.findings.append({\n",
    "                \"type\": \"missing_gitignore\",\n",
    "                \"severity\": \"HIGH\",\n",
    "                \"description\": \"No .gitignore file found\"\n",
    "            })\n",
    "            self.recommendations.append(\n",
    "                \"Create a .gitignore file to exclude sensitive files\"\n",
    "            )\n",
    "            return\n",
    "        \n",
    "        with open(gitignore_path) as f:\n",
    "            gitignore_content = f.read()\n",
    "        \n",
    "        # Check for important exclusions\n",
    "        missing = []\n",
    "        for sensitive_file in self.SENSITIVE_FILES:\n",
    "            if sensitive_file not in gitignore_content:\n",
    "                missing.append(sensitive_file)\n",
    "        \n",
    "        if missing:\n",
    "            self.warnings.append(\n",
    "                f\".gitignore missing important exclusions: {', '.join(missing)}\"\n",
    "            )\n",
    "            self.recommendations.append(\n",
    "                f\"Add these to .gitignore: {', '.join(missing)}\"\n",
    "            )\n",
    "    \n",
    "    def _scan_git_history(self):\n",
    "        \"\"\"Scan git history for accidentally committed keys\"\"\"\n",
    "        print(\"\\n\ud83d\udd52 Scanning git history...\")\n",
    "        \n",
    "        if not (self.project_dir / \".git\").exists():\n",
    "            print(\"  No git repository found\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Check recent commits for key patterns\n",
    "            for key_type, info in self.KEY_PATTERNS.items():\n",
    "                pattern = info[\"pattern\"][:20]  # Use partial pattern for speed\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    [\"git\", \"log\", \"-S\", pattern, \"--oneline\", \"-n\", \"10\"],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    cwd=self.project_dir,\n",
    "                    timeout=5\n",
    "                )\n",
    "                \n",
    "                if result.stdout:\n",
    "                    self.findings.append({\n",
    "                        \"type\": \"git_history\",\n",
    "                        \"severity\": \"HIGH\",\n",
    "                        \"description\": f\"Possible {key_type} in git history\",\n",
    "                        \"commits\": result.stdout.strip().split('\\n')[:3]\n",
    "                    })\n",
    "            \n",
    "            print(\"  Git history scanned\")\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            self.warnings.append(\"Git history scan timed out\")\n",
    "        except Exception as e:\n",
    "            self.warnings.append(f\"Could not scan git history: {e}\")\n",
    "    \n",
    "    def _check_environment_files(self):\n",
    "        \"\"\"Check security of environment files\"\"\"\n",
    "        print(\"\\n\ud83d\udd12 Checking environment files...\")\n",
    "        \n",
    "        env_files = [\".env\", \".env.local\", \".env.production\", \"config.json\"]\n",
    "        \n",
    "        for env_file in env_files:\n",
    "            file_path = self.project_dir / env_file\n",
    "            \n",
    "            if file_path.exists():\n",
    "                # Check if it's in git\n",
    "                try:\n",
    "                    result = subprocess.run(\n",
    "                        [\"git\", \"ls-files\", env_file],\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        cwd=self.project_dir\n",
    "                    )\n",
    "                    \n",
    "                    if result.stdout.strip():\n",
    "                        self.findings.append({\n",
    "                            \"type\": \"tracked_env_file\",\n",
    "                            \"severity\": \"CRITICAL\",\n",
    "                            \"description\": f\"{env_file} is tracked in git!\",\n",
    "                            \"file\": env_file\n",
    "                        })\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Check file permissions (Unix-like systems)\n",
    "                try:\n",
    "                    stats = file_path.stat()\n",
    "                    mode = oct(stats.st_mode)[-3:]\n",
    "                    \n",
    "                    if mode not in [\"600\", \"644\"]:\n",
    "                        self.warnings.append(\n",
    "                            f\"{env_file} has loose permissions: {mode}\"\n",
    "                        )\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    def _validate_active_keys(self):\n",
    "        \"\"\"Validate that active keys are properly secured\"\"\"\n",
    "        print(\"\\n\u2713 Validating active keys...\")\n",
    "        \n",
    "        # Check environment variables\n",
    "        env_vars_to_check = [\n",
    "            \"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\", \n",
    "            \"GOOGLE_API_KEY\", \"AWS_ACCESS_KEY_ID\"\n",
    "        ]\n",
    "        \n",
    "        for var in env_vars_to_check:\n",
    "            value = os.environ.get(var)\n",
    "            if value:\n",
    "                # Check if it looks like a placeholder\n",
    "                if any(p in value.lower() for p in [\"your\", \"add\", \"placeholder\"]):\n",
    "                    self.warnings.append(\n",
    "                        f\"{var} appears to be a placeholder\"\n",
    "                    )\n",
    "    \n",
    "    def _check_dependencies(self):\n",
    "        \"\"\"Check for security issues in dependencies\"\"\"\n",
    "        print(\"\\n\ud83d\udce6 Checking dependencies...\")\n",
    "        \n",
    "        # Check for requirements.txt\n",
    "        req_file = self.project_dir / \"requirements.txt\"\n",
    "        if req_file.exists():\n",
    "            with open(req_file) as f:\n",
    "                deps = f.read()\n",
    "                \n",
    "                # Check for insecure practices\n",
    "                if \"git+\" in deps and \"@\" in deps:\n",
    "                    self.warnings.append(\n",
    "                        \"requirements.txt contains git dependencies with possible tokens\"\n",
    "                    )\n",
    "    \n",
    "    def _generate_report(self) -> Dict:\n",
    "        \"\"\"Generate comprehensive security report\"\"\"\n",
    "        # Calculate severity counts\n",
    "        severity_counts = {\"CRITICAL\": 0, \"HIGH\": 0, \"MEDIUM\": 0, \"LOW\": 0}\n",
    "        \n",
    "        for finding in self.findings:\n",
    "            severity = finding.get(\"severity\", \"MEDIUM\")\n",
    "            severity_counts[severity] += 1\n",
    "        \n",
    "        # Determine overall status\n",
    "        if severity_counts[\"CRITICAL\"] > 0:\n",
    "            status = \"\u274c CRITICAL ISSUES FOUND\"\n",
    "        elif severity_counts[\"HIGH\"] > 0:\n",
    "            status = \"\u26a0\ufe0f HIGH RISK ISSUES FOUND\"\n",
    "        elif len(self.findings) > 0:\n",
    "            status = \"\u26a0\ufe0f ISSUES FOUND\"\n",
    "        elif len(self.warnings) > 0:\n",
    "            status = \"\u26a0\ufe0f WARNINGS\"\n",
    "        else:\n",
    "            status = \"\u2705 SECURE\"\n",
    "        \n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"project\": str(self.project_dir),\n",
    "            \"status\": status,\n",
    "            \"summary\": {\n",
    "                \"total_findings\": len(self.findings),\n",
    "                \"total_warnings\": len(self.warnings),\n",
    "                \"severity_breakdown\": severity_counts,\n",
    "                **self.scan_summary\n",
    "            },\n",
    "            \"findings\": self.findings,\n",
    "            \"warnings\": self.warnings,\n",
    "            \"recommendations\": self.recommendations\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def print_report(self, report: Dict):\n",
    "        \"\"\"Print formatted report to console\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"\ud83d\udd12 SECURITY AUDIT REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(f\"\\nStatus: {report['status']}\")\n",
    "        print(f\"Project: {report['project']}\")\n",
    "        print(f\"Time: {report['timestamp']}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\ud83d\udcca Summary:\")\n",
    "        print(f\"  Files Scanned: {report['summary'].get('files_scanned', 0)}\")\n",
    "        print(f\"  Total Findings: {report['summary']['total_findings']}\")\n",
    "        print(f\"  Total Warnings: {report['summary']['total_warnings']}\")\n",
    "        \n",
    "        # Severity breakdown\n",
    "        print(\"\\n\ud83c\udfaf Severity Breakdown:\")\n",
    "        for severity, count in report['summary']['severity_breakdown'].items():\n",
    "            if count > 0:\n",
    "                print(f\"  {severity}: {count}\")\n",
    "        \n",
    "        # Critical findings\n",
    "        critical = [f for f in report['findings'] if f.get('severity') == 'CRITICAL']\n",
    "        if critical:\n",
    "            print(\"\\n\ud83d\udea8 CRITICAL FINDINGS:\")\n",
    "            for finding in critical[:3]:\n",
    "                print(f\"\\n  \u2022 {finding.get('description', 'Unknown issue')}\")\n",
    "                if 'file' in finding:\n",
    "                    print(f\"    File: {finding['file']}\")\n",
    "                if 'line' in finding:\n",
    "                    print(f\"    Line: {finding['line']}\")\n",
    "                if 'preview' in finding:\n",
    "                    print(f\"    Preview: {finding['preview']}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if report['recommendations']:\n",
    "            print(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "            for rec in report['recommendations']:\n",
    "                print(f\"  \u2022 {rec}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    def save_report(self, report: Dict, format: str = \"json\"):\n",
    "        \"\"\"Save report to file\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if format == \"json\":\n",
    "            filename = f\"security_audit_{timestamp}.json\"\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(report, f, indent=2)\n",
    "        \n",
    "        elif format == \"markdown\":\n",
    "            filename = f\"security_audit_{timestamp}.md\"\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(self._generate_markdown_report(report))\n",
    "        \n",
    "        print(f\"\ud83d\udcc4 Report saved to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def _generate_markdown_report(self, report: Dict) -> str:\n",
    "        \"\"\"Generate markdown formatted report\"\"\"\n",
    "        md = f\"\"\"# Security Audit Report\n",
    "\n",
    "**Date:** {report['timestamp']}  \n",
    "**Project:** {report['project']}  \n",
    "**Status:** {report['status']}\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Files Scanned: {report['summary'].get('files_scanned', 0)}\n",
    "- Total Findings: {report['summary']['total_findings']}\n",
    "- Total Warnings: {report['summary']['total_warnings']}\n",
    "\n",
    "## Severity Breakdown\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for severity, count in report['summary']['severity_breakdown'].items():\n",
    "            if count > 0:\n",
    "                md += f\"- **{severity}:** {count}\\n\"\n",
    "        \n",
    "        if report['findings']:\n",
    "            md += \"\\n## Findings\\n\\n\"\n",
    "            for finding in report['findings']:\n",
    "                md += f\"### {finding.get('description', 'Issue')}\\n\"\n",
    "                md += f\"- **Severity:** {finding.get('severity', 'Unknown')}\\n\"\n",
    "                if 'file' in finding:\n",
    "                    md += f\"- **File:** `{finding['file']}`\\n\"\n",
    "                if 'line' in finding:\n",
    "                    md += f\"- **Line:** {finding['line']}\\n\"\n",
    "                md += \"\\n\"\n",
    "        \n",
    "        if report['recommendations']:\n",
    "            md += \"\\n## Recommendations\\n\\n\"\n",
    "            for rec in report['recommendations']:\n",
    "                md += f\"- {rec}\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\ud83d\udd10 Comprehensive API Key Security Audit Tool\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create auditor\n",
    "    auditor = ComprehensiveAPIAuditor(\".\")\n",
    "    \n",
    "    # Run complete audit\n",
    "    report = auditor.audit_complete()\n",
    "    \n",
    "    # Print report\n",
    "    auditor.print_report(report)\n",
    "    \n",
    "    # Save reports\n",
    "    auditor.save_report(report, \"json\")\n",
    "    auditor.save_report(report, \"markdown\")\n",
    "    \n",
    "    # Exit with appropriate code\n",
    "    if report['status'].startswith(\"\u274c\"):\n",
    "        exit(1)  # Critical issues\n",
    "    elif report['status'].startswith(\"\u26a0\ufe0f\"):\n",
    "        exit(0)  # Warnings but not critical\n",
    "    else:\n",
    "        exit(0)  # All good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Return to **Chapter 8: Next Topic**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}